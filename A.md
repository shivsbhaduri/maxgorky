




£#h3#£A-Integrable£#/h3#£

£#h5#£ References £#/h5#£ £#ul#££#li#£Titchmarsh, E. C. "On Conjugate Functions." Proc. London Math. Soc. 29, 49-80, 1928.£#/li#££#li#£ Titchmarsh, E. C. "On Conjugate Functions." Proc. London Math. Soc. 29, 49-80, 1928. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Measure Theory £#/li#££#/ul#£




£#h3#£Abel's Convergence Theorem£#/h3#£

In mathematics, Abel's theorem for power series relates a limit of a power series to the sum of its coefficients. It is named after Norwegian mathematician Niels Henrik Abel.


£#h5#£Theorem£#/h5#£
Let the Taylor series

be a power series with real coefficients ${\displaystyle a_{k}}$ with radius of convergence ${\displaystyle 1.}$ Suppose that the series converges. Then ${\displaystyle G(x)}$ is continuous from the left at ${\displaystyle x=1,}$ that is,
The same theorem holds for complex power series

provided that ${\displaystyle z\to 1}$ entirely within a single Stolz sector, that is, a region of the open unit disk where for some fixed finite ${\displaystyle M>1}$ . Without this restriction, the limit may fail to exist: for example, the power series converges to ${\displaystyle 0}$ at ${\displaystyle z=1,}$ but is unbounded near any point of the form ${\displaystyle e^{\pi i/3^{n}},}$ so the value at ${\displaystyle z=1}$ is not the limit as ${\displaystyle z}$ tends to 1 in the whole open disk.
Note that ${\displaystyle G(z)}$ is continuous on the real closed interval ${\displaystyle [0,t]}$ for ${\displaystyle t<1,}$ by virtue of the uniform convergence of the series on compact subsets of the disk of convergence. Abel's theorem allows us to say more, namely that ${\displaystyle G(z)}$ is continuous on ${\displaystyle [0,1].}$


£#h5#£Remarks£#/h5#£
As an immediate consequence of this theorem, if ${\displaystyle z}$ is any nonzero complex number for which the series

converges, then it follows that in which the limit is taken from below.
The theorem can also be generalized to account for sums which diverge to infinity. If

then
However, if the series is only known to be divergent, but for reasons other than diverging to infinity, then the claim of the theorem may fail: take, for example, the power series for

At ${\displaystyle z=1}$ the series is equal to ${\displaystyle 1-1+1-1+\cdots ,}$ but ${\displaystyle {\tfrac {1}{1+1}}={\tfrac {1}{2}}.}$

We also remark the theorem holds for radii of convergence other than ${\displaystyle R=1}$ : let

be a power series with radius of convergence ${\displaystyle R,}$ and suppose the series converges at ${\displaystyle x=R.}$ Then ${\displaystyle G(x)}$ is continuous from the left at ${\displaystyle x=R,}$ that is,
£#h5#£Applications£#/h5#£
The utility of Abel's theorem is that it allows us to find the limit of a power series as its argument (that is, ${\displaystyle z}$ ) approaches ${\displaystyle 1}$ from below, even in cases where the radius of convergence, ${\displaystyle R,}$ of the power series is equal to ${\displaystyle 1}$ and we cannot be sure whether the limit should be finite or not. See for example, the binomial series. Abel's theorem allows us to evaluate many series in closed form. For example, when

we obtain by integrating the uniformly convergent geometric power series term by term on ${\displaystyle [-z,0]}$ ; thus the series converges to ${\displaystyle \ln(2)}$ by Abel's theorem. Similarly, converges to ${\displaystyle \arctan(1)={\tfrac {\pi }{4}}.}$
${\displaystyle G_{a}(z)}$ is called the generating function of the sequence ${\displaystyle a.}$ Abel's theorem is frequently useful in dealing with generating functions of real-valued and non-negative sequences, such as probability-generating functions. In particular, it is useful in the theory of Galton–Watson processes.


£#h5#£Outline of proof£#/h5#£
After subtracting a constant from ${\displaystyle a_{0},}$ we may assume that ${\displaystyle \sum _{k=0}^{\infty }a_{k}=0.}$ Let ${\displaystyle s_{n}=\sum _{k=0}^{n}a_{k}\!.}$ Then substituting ${\displaystyle a_{k}=s_{k}-s_{k-1}}$ and performing a simple manipulation of the series (summation by parts) results in

Given ${\displaystyle \varepsilon >0,}$ pick ${\displaystyle n}$ large enough so that ${\displaystyle |s_{k}|<\varepsilon }$ for all ${\displaystyle k\geq n}$ and note that

when ${\displaystyle z}$ lies within the given Stolz angle. Whenever ${\displaystyle z}$ is sufficiently close to ${\displaystyle 1}$ we have so that ${\displaystyle \left|G_{a}(z)\right|<(M+1)\varepsilon }$ when ${\displaystyle z}$ is both sufficiently close to ${\displaystyle 1}$ and within the Stolz angle.
£#h5#£Related concepts£#/h5#£
Converses to a theorem like Abel's are called Tauberian theorems: There is no exact converse, but results conditional on some hypothesis. The field of divergent series, and their summation methods, contains many theorems of abelian type and of tauberian type.


£#h5#£See also£#/h5#£ £#ul#££#li#£Abel's summation formula – Integration by parts version of Abel's method for summation by parts£#/li#£ £#li#£Nachbin resummation£#/li#£ £#li#£Summation by parts – Theorem to simplify sums of products of sequences£#/li#££#/ul#£
£#h5#£Further reading£#/h5#£ £#ul#££#li#£Ahlfors, Lars Valerian (September 1, 1980). Complex Analysis (Third ed.). McGraw Hill Higher Education. pp. 41–42. ISBN 0-07-085008-9. - Ahlfors called it Abel's limit theorem.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Abel summability at PlanetMath. (a more general look at Abelian theorems of this type)£#/li#£ £#li#£A.A. Zakharov (2001) [1994], "Abel summation method", Encyclopedia of Mathematics, EMS Press£#/li#£ £#li#£Weisstein, Eric W. "Abel's Convergence Theorem". MathWorld.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Arfken, G. Mathematical Methods for Physicists, 3rd ed. Orlando, FL: Academic Press, p. 773, 1985.£#/li#££#li#£ Arfken, G. Mathematical Methods for Physicists, 3rd ed. Orlando, FL: Academic Press, p. 773, 1985. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Series > Convergence £#/li#££#/ul#£




£#h3#£Abel's Differential Equation£#/h3#£

In mathematics, Abel's identity (also called Abel's formula or Abel's differential equation identity) is an equation that expresses the Wronskian of two solutions of a homogeneous second-order linear ordinary differential equation in terms of a coefficient of the original differential equation. The relation can be generalised to nth-order linear ordinary differential equations. The identity is named after the Norwegian mathematician Niels Henrik Abel.

Since Abel's identity relates the different linearly independent solutions of the differential equation, it can be used to find one solution from the other. It provides useful identities relating the solutions, and is also useful as a part of other techniques such as the method of variation of parameters. It is especially useful for equations such as Bessel's equation where the solutions do not have a simple analytical form, because in such cases the Wronskian is difficult to compute directly.

A generalisation to first-order systems of homogeneous linear differential equations is given by Liouville's formula.


£#h5#£Statement£#/h5#£
Consider a homogeneous linear second-order ordinary differential equation

${\displaystyle y''+p(x)y'+q(x)\,y=0}$
on an interval I of the real line with real- or complex-valued continuous functions p and q. Abel's identity states that the Wronskian ${\displaystyle W=(y_{1},y_{2})}$ of two real- or complex-valued solutions ${\displaystyle y_{1}}$ and ${\displaystyle y_{2}}$ of this differential equation, that is the function defined by the determinant

${\displaystyle W(y_{1},y_{2})(x)={\begin{vmatrix}y_{1}(x)&y_{2}(x)\\y'_{1}(x)&y'_{2}(x)\end{vmatrix}}=y_{1}(x)\,y'_{2}(x)-y'_{1}(x)\,y_{2}(x),\qquad x\in I,}$
satisfies the relation

${\displaystyle W(y_{1},y_{2})(x)=W(y_{1},y_{2})(x_{0})\cdot \exp {\biggl (}-\int _{x_{0}}^{x}p(x')\,{\textrm {d}}x'{\biggr )},\qquad x\in I,}$
for every point x0 in I.


£#h5#£Remarks£#/h5#£ £#ul#££#li#£In particular, the Wronskian ${\displaystyle W(y_{1},y_{2})}$ is either always the zero function or always different from zero with the same sign at every point ${\displaystyle x}$ in ${\displaystyle I}$ . In the latter case, the two solutions ${\displaystyle y_{1}}$ and ${\displaystyle y_{2}}$ are linearly independent (see the article about the Wronskian for a proof).£#/li#£ £#li#£It is not necessary to assume that the second derivatives of the solutions ${\displaystyle y_{1}}$ and ${\displaystyle y_{2}}$ are continuous.£#/li#£ £#li#£Abel's theorem is particularly useful if ${\displaystyle p(x)=0}$ , because it implies that ${\displaystyle W}$ is constant.£#/li#££#/ul#£
£#h5#£Proof£#/h5#£
Differentiating the Wronskian using the product rule gives (writing ${\displaystyle W}$ for ${\displaystyle W(y_{1},y_{2})}$ and omitting the argument ${\displaystyle x}$ for brevity)

${\displaystyle {\begin{aligned}W'&=y_{1}'y_{2}'+y_{1}y_{2}''-y_{1}''y_{2}-y_{1}'y_{2}'\\&=y_{1}y_{2}''-y_{1}''y_{2}.\end{aligned}}}$
Solving for ${\displaystyle y''}$ in the original differential equation yields

${\displaystyle y''=-(py'+qy).}$
Substituting this result into the derivative of the Wronskian function to replace the second derivatives of ${\displaystyle y_{1}}$ and ${\displaystyle y_{2}}$ gives

${\displaystyle {\begin{aligned}W'&=-y_{1}(py_{2}'+qy_{2})+(py_{1}'+qy_{1})y_{2}\\&=-p(y_{1}y_{2}'-y_{1}'y_{2})\\&=-pW.\end{aligned}}}$
This is a first-order linear differential equation, and it remains to show that Abel's identity gives the unique solution, which attains the value ${\displaystyle W(x_{0})}$ at ${\displaystyle x_{0}}$ . Since the function ${\displaystyle p}$ is continuous on ${\displaystyle I}$ , it is bounded on every closed and bounded subinterval of ${\displaystyle I}$ and therefore integrable, hence

${\displaystyle V(x)=W(x)\exp \left(\int _{x_{0}}^{x}p(\xi )\,{\textrm {d}}\xi \right),\qquad x\in I,}$
is a well-defined function. Differentiating both sides, using the product rule, the chain rule, the derivative of the exponential function and the fundamental theorem of calculus, one obtains

${\displaystyle V'(x)={\bigl (}W'(x)+W(x)p(x){\bigr )}\exp {\biggl (}\int _{x_{0}}^{x}p(\xi )\,{\textrm {d}}\xi {\biggr )}=0,\qquad x\in I,}$
due to the differential equation for ${\displaystyle W}$ . Therefore, ${\displaystyle V}$ has to be constant on ${\displaystyle I}$ , because otherwise we would obtain a contradiction to the mean value theorem (applied separately to the real and imaginary part in the complex-valued case). Since ${\displaystyle V(x_{0})=W(x_{0})}$ , Abel's identity follows by solving the definition of ${\displaystyle V}$ for ${\displaystyle W(x)}$ .


£#h5#£Generalization£#/h5#£
Consider a homogeneous linear ${\displaystyle n}$ th-order ( ${\displaystyle n\geq 1}$ ) ordinary differential equation

${\displaystyle y^{(n)}+p_{n-1}(x)\,y^{(n-1)}+\cdots +p_{1}(x)\,y'+p_{0}(x)\,y=0,}$
on an interval ${\displaystyle I}$ of the real line with a real- or complex-valued continuous function ${\displaystyle p_{n-1}}$ . The generalisation of Abel's identity states that the Wronskian ${\displaystyle W(y_{1},\ldots ,y_{n})}$ of ${\displaystyle n}$ real- or complex-valued solutions ${\displaystyle y_{1},\ldots ,y_{n}}$ of this ${\displaystyle n}$ th-order differential equation, that is the function defined by the determinant

${\displaystyle W(y_{1},\ldots ,y_{n})(x)={\begin{vmatrix}y_{1}(x)&y_{2}(x)&\cdots &y_{n}(x)\\y'_{1}(x)&y'_{2}(x)&\cdots &y'_{n}(x)\\\vdots &\vdots &\ddots &\vdots \\y_{1}^{(n-1)}(x)&y_{2}^{(n-1)}(x)&\cdots &y_{n}^{(n-1)}(x)\end{vmatrix}},\qquad x\in I,}$
satisfies the relation

${\displaystyle W(y_{1},\ldots ,y_{n})(x)=W(y_{1},\ldots ,y_{n})(x_{0})\exp {\biggl (}-\int _{x_{0}}^{x}p_{n-1}(\xi )\,{\textrm {d}}\xi {\biggr )},\qquad x\in I,}$
for every point ${\displaystyle x_{0}}$ in ${\displaystyle I}$ .


£#h5#£Direct proof£#/h5#£
For brevity, we write ${\displaystyle W}$ for ${\displaystyle W(y_{1},\ldots ,y_{n})}$ and omit the argument ${\displaystyle x}$ . It suffices to show that the Wronskian solves the first-order linear differential equation

${\displaystyle W'=-p_{n-1}\,W,}$
because the remaining part of the proof then coincides with the one for the case ${\displaystyle n=2}$ .

In the case ${\displaystyle n=1}$ we have ${\displaystyle W=y_{1}}$ and the differential equation for ${\displaystyle W}$ coincides with the one for ${\displaystyle y_{1}}$ . Therefore, assume ${\displaystyle n\geq 2}$ in the following.

The derivative of the Wronskian ${\displaystyle W}$ is the derivative of the defining determinant. It follows from the Leibniz formula for determinants that this derivative can be calculated by differentiating every row separately, hence

${\displaystyle {\begin{aligned}W'&={\begin{vmatrix}y'_{1}&y'_{2}&\cdots &y'_{n}\\y'_{1}&y'_{2}&\cdots &y'_{n}\\y''_{1}&y''_{2}&\cdots &y''_{n}\\y'''_{1}&y'''_{2}&\cdots &y'''_{n}\\\vdots &\vdots &\ddots &\vdots \\y_{1}^{(n-1)}&y_{2}^{(n-1)}&\cdots &y_{n}^{(n-1)}\end{vmatrix}}+{\begin{vmatrix}y_{1}&y_{2}&\cdots &y_{n}\\y''_{1}&y''_{2}&\cdots &y''_{n}\\y''_{1}&y''_{2}&\cdots &y''_{n}\\y'''_{1}&y'''_{2}&\cdots &y'''_{n}\\\vdots &\vdots &\ddots &\vdots \\y_{1}^{(n-1)}&y_{2}^{(n-1)}&\cdots &y_{n}^{(n-1)}\end{vmatrix}}\\&\qquad +\ \cdots \ +{\begin{vmatrix}y_{1}&y_{2}&\cdots &y_{n}\\y'_{1}&y'_{2}&\cdots &y'_{n}\\\vdots &\vdots &\ddots &\vdots \\y_{1}^{(n-3)}&y_{2}^{(n-3)}&\cdots &y_{n}^{(n-3)}\\y_{1}^{(n-2)}&y_{2}^{(n-2)}&\cdots &y_{n}^{(n-2)}\\y_{1}^{(n)}&y_{2}^{(n)}&\cdots &y_{n}^{(n)}\end{vmatrix}}.\end{aligned}}}$
However, note that every determinant from the expansion contains a pair of identical rows, except the last one. Since determinants with linearly dependent rows are equal to 0, one is only left with the last one:

${\displaystyle W'={\begin{vmatrix}y_{1}&y_{2}&\cdots &y_{n}\\y'_{1}&y'_{2}&\cdots &y'_{n}\\\vdots &\vdots &\ddots &\vdots \\y_{1}^{(n-2)}&y_{2}^{(n-2)}&\cdots &y_{n}^{(n-2)}\\y_{1}^{(n)}&y_{2}^{(n)}&\cdots &y_{n}^{(n)}\end{vmatrix}}.}$
Since every ${\displaystyle y_{i}}$ solves the ordinary differential equation, we have

${\displaystyle y_{i}^{(n)}+p_{n-2}\,y_{i}^{(n-2)}+\cdots +p_{1}\,y'_{i}+p_{0}\,y_{i}=-p_{n-1}\,y_{i}^{(n-1)}}$
for every ${\displaystyle i\in \lbrace 1,\ldots ,n\rbrace }$ . Hence, adding to the last row of the above determinant ${\displaystyle p_{0}}$ times its first row, ${\displaystyle p_{1}}$ times its second row, and so on until ${\displaystyle p_{n-2}}$ times its next to last row, the value of the determinant for the derivative of ${\displaystyle W}$ is unchanged and we get

${\displaystyle W'={\begin{vmatrix}y_{1}&y_{2}&\cdots &y_{n}\\y'_{1}&y'_{2}&\cdots &y'_{n}\\\vdots &\vdots &\ddots &\vdots \\y_{1}^{(n-2)}&y_{2}^{(n-2)}&\cdots &y_{n}^{(n-2)}\\-p_{n-1}\,y_{1}^{(n-1)}&-p_{n-1}\,y_{2}^{(n-1)}&\cdots &-p_{n-1}\,y_{n}^{(n-1)}\end{vmatrix}}=-p_{n-1}W.}$

£#h5#£Proof using Liouville's formula£#/h5#£
The solutions ${\displaystyle y_{1},\ldots ,y_{n}}$ form the square-matrix valued solution

${\displaystyle \Phi (x)={\begin{pmatrix}y_{1}(x)&y_{2}(x)&\cdots &y_{n}(x)\\y'_{1}(x)&y'_{2}(x)&\cdots &y'_{n}(x)\\\vdots &\vdots &\ddots &\vdots \\y_{1}^{(n-2)}(x)&y_{2}^{(n-2)}(x)&\cdots &y_{n}^{(n-2)}(x)\\y_{1}^{(n-1)}(x)&y_{2}^{(n-1)}(x)&\cdots &y_{n}^{(n-1)}(x)\end{pmatrix}},\qquad x\in I,}$
of the ${\displaystyle n}$ -dimensional first-order system of homogeneous linear differential equations

${\displaystyle {\begin{pmatrix}y'\\y''\\\vdots \\y^{(n-1)}\\y^{(n)}\end{pmatrix}}={\begin{pmatrix}0&1&0&\cdots &0\\0&0&1&\cdots &0\\\vdots &\vdots &\vdots &\ddots &\vdots \\0&0&0&\cdots &1\\-p_{0}(x)&-p_{1}(x)&-p_{2}(x)&\cdots &-p_{n-1}(x)\end{pmatrix}}{\begin{pmatrix}y\\y'\\\vdots \\y^{(n-2)}\\y^{(n-1)}\end{pmatrix}}.}$
The trace of this matrix is ${\displaystyle -p_{n-1}(x)}$ , hence Abel's identity follows directly from Liouville's formula.


£#h5#£References£#/h5#£ £#ul#££#li#£Abel, N. H., "Précis d'une théorie des fonctions elliptiques" J. Reine Angew. Math., 4 (1829) pp. 309–348.£#/li#£ £#li#£Boyce, W. E. and DiPrima, R. C. (1986). Elementary Differential Equations and Boundary Value Problems, 4th ed. New York: Wiley.£#/li#£ £#li#£Teschl, Gerald (2012). Ordinary Differential Equations and Dynamical Systems. Providence: American Mathematical Society. ISBN 978-0-8218-8328-0.£#/li#£ £#li#£Weisstein, Eric W. "Abel's Differential Equation Identity". MathWorld.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Murphy, G. M. Ordinary Differential Equations and Their Solution. Princeton, NJ: Van Nostrand, 1960.£#/li#££#li#£Zwillinger, D. Handbook of Differential Equations, 3rd ed. Boston, MA: Academic Press, p. 120, 1997.£#/li#££#li#£ Murphy, G. M. Ordinary Differential Equations and Their Solution. Princeton, NJ: Van Nostrand, 1960. £#/li#££#li#£ Zwillinger, D. Handbook of Differential Equations, 3rd ed. Boston, MA: Academic Press, p. 120, 1997. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Differential Equations > Ordinary Differential Equations £#/li#££#/ul#£




£#h3#£Abel's Differential Equation Identity£#/h3#£

In mathematics, Abel's identity (also called Abel's formula or Abel's differential equation identity) is an equation that expresses the Wronskian of two solutions of a homogeneous second-order linear ordinary differential equation in terms of a coefficient of the original differential equation. The relation can be generalised to nth-order linear ordinary differential equations. The identity is named after the Norwegian mathematician Niels Henrik Abel.

Since Abel's identity relates the different linearly independent solutions of the differential equation, it can be used to find one solution from the other. It provides useful identities relating the solutions, and is also useful as a part of other techniques such as the method of variation of parameters. It is especially useful for equations such as Bessel's equation where the solutions do not have a simple analytical form, because in such cases the Wronskian is difficult to compute directly.

A generalisation to first-order systems of homogeneous linear differential equations is given by Liouville's formula.


£#h5#£Statement£#/h5#£
Consider a homogeneous linear second-order ordinary differential equation

${\displaystyle y''+p(x)y'+q(x)\,y=0}$
on an interval I of the real line with real- or complex-valued continuous functions p and q. Abel's identity states that the Wronskian ${\displaystyle W=(y_{1},y_{2})}$ of two real- or complex-valued solutions ${\displaystyle y_{1}}$ and ${\displaystyle y_{2}}$ of this differential equation, that is the function defined by the determinant

${\displaystyle W(y_{1},y_{2})(x)={\begin{vmatrix}y_{1}(x)&y_{2}(x)\\y'_{1}(x)&y'_{2}(x)\end{vmatrix}}=y_{1}(x)\,y'_{2}(x)-y'_{1}(x)\,y_{2}(x),\qquad x\in I,}$
satisfies the relation

${\displaystyle W(y_{1},y_{2})(x)=W(y_{1},y_{2})(x_{0})\cdot \exp {\biggl (}-\int _{x_{0}}^{x}p(x')\,{\textrm {d}}x'{\biggr )},\qquad x\in I,}$
for every point x0 in I.


£#h5#£Remarks£#/h5#£ £#ul#££#li#£In particular, the Wronskian ${\displaystyle W(y_{1},y_{2})}$ is either always the zero function or always different from zero with the same sign at every point ${\displaystyle x}$ in ${\displaystyle I}$ . In the latter case, the two solutions ${\displaystyle y_{1}}$ and ${\displaystyle y_{2}}$ are linearly independent (see the article about the Wronskian for a proof).£#/li#£ £#li#£It is not necessary to assume that the second derivatives of the solutions ${\displaystyle y_{1}}$ and ${\displaystyle y_{2}}$ are continuous.£#/li#£ £#li#£Abel's theorem is particularly useful if ${\displaystyle p(x)=0}$ , because it implies that ${\displaystyle W}$ is constant.£#/li#££#/ul#£
£#h5#£Proof£#/h5#£
Differentiating the Wronskian using the product rule gives (writing ${\displaystyle W}$ for ${\displaystyle W(y_{1},y_{2})}$ and omitting the argument ${\displaystyle x}$ for brevity)

${\displaystyle {\begin{aligned}W'&=y_{1}'y_{2}'+y_{1}y_{2}''-y_{1}''y_{2}-y_{1}'y_{2}'\\&=y_{1}y_{2}''-y_{1}''y_{2}.\end{aligned}}}$
Solving for ${\displaystyle y''}$ in the original differential equation yields

${\displaystyle y''=-(py'+qy).}$
Substituting this result into the derivative of the Wronskian function to replace the second derivatives of ${\displaystyle y_{1}}$ and ${\displaystyle y_{2}}$ gives

${\displaystyle {\begin{aligned}W'&=-y_{1}(py_{2}'+qy_{2})+(py_{1}'+qy_{1})y_{2}\\&=-p(y_{1}y_{2}'-y_{1}'y_{2})\\&=-pW.\end{aligned}}}$
This is a first-order linear differential equation, and it remains to show that Abel's identity gives the unique solution, which attains the value ${\displaystyle W(x_{0})}$ at ${\displaystyle x_{0}}$ . Since the function ${\displaystyle p}$ is continuous on ${\displaystyle I}$ , it is bounded on every closed and bounded subinterval of ${\displaystyle I}$ and therefore integrable, hence

${\displaystyle V(x)=W(x)\exp \left(\int _{x_{0}}^{x}p(\xi )\,{\textrm {d}}\xi \right),\qquad x\in I,}$
is a well-defined function. Differentiating both sides, using the product rule, the chain rule, the derivative of the exponential function and the fundamental theorem of calculus, one obtains

${\displaystyle V'(x)={\bigl (}W'(x)+W(x)p(x){\bigr )}\exp {\biggl (}\int _{x_{0}}^{x}p(\xi )\,{\textrm {d}}\xi {\biggr )}=0,\qquad x\in I,}$
due to the differential equation for ${\displaystyle W}$ . Therefore, ${\displaystyle V}$ has to be constant on ${\displaystyle I}$ , because otherwise we would obtain a contradiction to the mean value theorem (applied separately to the real and imaginary part in the complex-valued case). Since ${\displaystyle V(x_{0})=W(x_{0})}$ , Abel's identity follows by solving the definition of ${\displaystyle V}$ for ${\displaystyle W(x)}$ .


£#h5#£Generalization£#/h5#£
Consider a homogeneous linear ${\displaystyle n}$ th-order ( ${\displaystyle n\geq 1}$ ) ordinary differential equation

${\displaystyle y^{(n)}+p_{n-1}(x)\,y^{(n-1)}+\cdots +p_{1}(x)\,y'+p_{0}(x)\,y=0,}$
on an interval ${\displaystyle I}$ of the real line with a real- or complex-valued continuous function ${\displaystyle p_{n-1}}$ . The generalisation of Abel's identity states that the Wronskian ${\displaystyle W(y_{1},\ldots ,y_{n})}$ of ${\displaystyle n}$ real- or complex-valued solutions ${\displaystyle y_{1},\ldots ,y_{n}}$ of this ${\displaystyle n}$ th-order differential equation, that is the function defined by the determinant

${\displaystyle W(y_{1},\ldots ,y_{n})(x)={\begin{vmatrix}y_{1}(x)&y_{2}(x)&\cdots &y_{n}(x)\\y'_{1}(x)&y'_{2}(x)&\cdots &y'_{n}(x)\\\vdots &\vdots &\ddots &\vdots \\y_{1}^{(n-1)}(x)&y_{2}^{(n-1)}(x)&\cdots &y_{n}^{(n-1)}(x)\end{vmatrix}},\qquad x\in I,}$
satisfies the relation

${\displaystyle W(y_{1},\ldots ,y_{n})(x)=W(y_{1},\ldots ,y_{n})(x_{0})\exp {\biggl (}-\int _{x_{0}}^{x}p_{n-1}(\xi )\,{\textrm {d}}\xi {\biggr )},\qquad x\in I,}$
for every point ${\displaystyle x_{0}}$ in ${\displaystyle I}$ .


£#h5#£Direct proof£#/h5#£
For brevity, we write ${\displaystyle W}$ for ${\displaystyle W(y_{1},\ldots ,y_{n})}$ and omit the argument ${\displaystyle x}$ . It suffices to show that the Wronskian solves the first-order linear differential equation

${\displaystyle W'=-p_{n-1}\,W,}$
because the remaining part of the proof then coincides with the one for the case ${\displaystyle n=2}$ .

In the case ${\displaystyle n=1}$ we have ${\displaystyle W=y_{1}}$ and the differential equation for ${\displaystyle W}$ coincides with the one for ${\displaystyle y_{1}}$ . Therefore, assume ${\displaystyle n\geq 2}$ in the following.

The derivative of the Wronskian ${\displaystyle W}$ is the derivative of the defining determinant. It follows from the Leibniz formula for determinants that this derivative can be calculated by differentiating every row separately, hence

${\displaystyle {\begin{aligned}W'&={\begin{vmatrix}y'_{1}&y'_{2}&\cdots &y'_{n}\\y'_{1}&y'_{2}&\cdots &y'_{n}\\y''_{1}&y''_{2}&\cdots &y''_{n}\\y'''_{1}&y'''_{2}&\cdots &y'''_{n}\\\vdots &\vdots &\ddots &\vdots \\y_{1}^{(n-1)}&y_{2}^{(n-1)}&\cdots &y_{n}^{(n-1)}\end{vmatrix}}+{\begin{vmatrix}y_{1}&y_{2}&\cdots &y_{n}\\y''_{1}&y''_{2}&\cdots &y''_{n}\\y''_{1}&y''_{2}&\cdots &y''_{n}\\y'''_{1}&y'''_{2}&\cdots &y'''_{n}\\\vdots &\vdots &\ddots &\vdots \\y_{1}^{(n-1)}&y_{2}^{(n-1)}&\cdots &y_{n}^{(n-1)}\end{vmatrix}}\\&\qquad +\ \cdots \ +{\begin{vmatrix}y_{1}&y_{2}&\cdots &y_{n}\\y'_{1}&y'_{2}&\cdots &y'_{n}\\\vdots &\vdots &\ddots &\vdots \\y_{1}^{(n-3)}&y_{2}^{(n-3)}&\cdots &y_{n}^{(n-3)}\\y_{1}^{(n-2)}&y_{2}^{(n-2)}&\cdots &y_{n}^{(n-2)}\\y_{1}^{(n)}&y_{2}^{(n)}&\cdots &y_{n}^{(n)}\end{vmatrix}}.\end{aligned}}}$
However, note that every determinant from the expansion contains a pair of identical rows, except the last one. Since determinants with linearly dependent rows are equal to 0, one is only left with the last one:

${\displaystyle W'={\begin{vmatrix}y_{1}&y_{2}&\cdots &y_{n}\\y'_{1}&y'_{2}&\cdots &y'_{n}\\\vdots &\vdots &\ddots &\vdots \\y_{1}^{(n-2)}&y_{2}^{(n-2)}&\cdots &y_{n}^{(n-2)}\\y_{1}^{(n)}&y_{2}^{(n)}&\cdots &y_{n}^{(n)}\end{vmatrix}}.}$
Since every ${\displaystyle y_{i}}$ solves the ordinary differential equation, we have

${\displaystyle y_{i}^{(n)}+p_{n-2}\,y_{i}^{(n-2)}+\cdots +p_{1}\,y'_{i}+p_{0}\,y_{i}=-p_{n-1}\,y_{i}^{(n-1)}}$
for every ${\displaystyle i\in \lbrace 1,\ldots ,n\rbrace }$ . Hence, adding to the last row of the above determinant ${\displaystyle p_{0}}$ times its first row, ${\displaystyle p_{1}}$ times its second row, and so on until ${\displaystyle p_{n-2}}$ times its next to last row, the value of the determinant for the derivative of ${\displaystyle W}$ is unchanged and we get

${\displaystyle W'={\begin{vmatrix}y_{1}&y_{2}&\cdots &y_{n}\\y'_{1}&y'_{2}&\cdots &y'_{n}\\\vdots &\vdots &\ddots &\vdots \\y_{1}^{(n-2)}&y_{2}^{(n-2)}&\cdots &y_{n}^{(n-2)}\\-p_{n-1}\,y_{1}^{(n-1)}&-p_{n-1}\,y_{2}^{(n-1)}&\cdots &-p_{n-1}\,y_{n}^{(n-1)}\end{vmatrix}}=-p_{n-1}W.}$

£#h5#£Proof using Liouville's formula£#/h5#£
The solutions ${\displaystyle y_{1},\ldots ,y_{n}}$ form the square-matrix valued solution

${\displaystyle \Phi (x)={\begin{pmatrix}y_{1}(x)&y_{2}(x)&\cdots &y_{n}(x)\\y'_{1}(x)&y'_{2}(x)&\cdots &y'_{n}(x)\\\vdots &\vdots &\ddots &\vdots \\y_{1}^{(n-2)}(x)&y_{2}^{(n-2)}(x)&\cdots &y_{n}^{(n-2)}(x)\\y_{1}^{(n-1)}(x)&y_{2}^{(n-1)}(x)&\cdots &y_{n}^{(n-1)}(x)\end{pmatrix}},\qquad x\in I,}$
of the ${\displaystyle n}$ -dimensional first-order system of homogeneous linear differential equations

${\displaystyle {\begin{pmatrix}y'\\y''\\\vdots \\y^{(n-1)}\\y^{(n)}\end{pmatrix}}={\begin{pmatrix}0&1&0&\cdots &0\\0&0&1&\cdots &0\\\vdots &\vdots &\vdots &\ddots &\vdots \\0&0&0&\cdots &1\\-p_{0}(x)&-p_{1}(x)&-p_{2}(x)&\cdots &-p_{n-1}(x)\end{pmatrix}}{\begin{pmatrix}y\\y'\\\vdots \\y^{(n-2)}\\y^{(n-1)}\end{pmatrix}}.}$
The trace of this matrix is ${\displaystyle -p_{n-1}(x)}$ , hence Abel's identity follows directly from Liouville's formula.


£#h5#£References£#/h5#£ £#ul#££#li#£Abel, N. H., "Précis d'une théorie des fonctions elliptiques" J. Reine Angew. Math., 4 (1829) pp. 309–348.£#/li#£ £#li#£Boyce, W. E. and DiPrima, R. C. (1986). Elementary Differential Equations and Boundary Value Problems, 4th ed. New York: Wiley.£#/li#£ £#li#£Teschl, Gerald (2012). Ordinary Differential Equations and Dynamical Systems. Providence: American Mathematical Society. ISBN 978-0-8218-8328-0.£#/li#£ £#li#£Weisstein, Eric W. "Abel's Differential Equation Identity". MathWorld.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Boyce, W. E. and DiPrima, R. C. Elementary Differential Equations and Boundary Value Problems, 4th ed. New York: Wiley, pp. 118, 262, 277, and 355, 1986.£#/li#££#li#£ Boyce, W. E. and DiPrima, R. C. Elementary Differential Equations and Boundary Value Problems, 4th ed. New York: Wiley, pp. 118, 262, 277, and 355, 1986. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Differential Equations > Ordinary Differential Equations £#/li#££#/ul#£




£#h3#£Abel's Duplication Formula£#/h3#£

£#h5#£ References £#/h5#£ £#ul#££#li#£Gordon, B. and McIntosh, R. J. "Algebraic Dilogarithm Identities." Ramanujan J. 1, 431-448, 1997.£#/li#££#li#£ Gordon, B. and McIntosh, R. J. "Algebraic Dilogarithm Identities." Ramanujan J. 1, 431-448, 1997. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Polylogarithms £#/li#££#li#£ Calculus and Analysis > Functional Analysis £#/li#££#/ul#£




£#h3#£Abel's Functional Equation£#/h3#£

In mathematics, a functional equation is, in the broadest meaning, an equation in which one or several functions appear as unknowns. So, differential equations and integral equations are functional equations. However, a more restricted meaning is often used, where a functional equation is an equation that relates several values of the same function. For example, the logarithm functions are essentially characterized by the logarithmic functional equation ${\displaystyle \log(xy)=\log(x)+\log(y).}$

If the domain of the unknown function is supposed to be the natural numbers, the function is generally viewed as a sequence, and, in this case, a functional equation (in the narrower meaning) is called a recurrence relation. Thus the term functional equation is used mainly for real functions and complex functions. Moreover a smoothness condition is often assumed for the solutions, since without such a condition, most functional equations have very irregular solutions. For example, the gamma function is a function that satisfies the functional equation ${\displaystyle f(x+1)=xf(x)}$ and the initial value ${\displaystyle f(1)=1.}$ There are many functions that satisfy these conditions, but the gamma function is the unique one that is meromorphic in the whole complex plane, and logarithmically convex for x real and positive (Bohr–Mollerup theorem).


£#h5#£Examples£#/h5#£ £#ul#££#li#£Recurrence relations can be seen as functional equations in functions over the integers or natural numbers, in which the differences between terms' indexes can be seen as an application of the shift operator. For example, the recurrence relation defining the Fibonacci numbers, ${\displaystyle F_{n}=F_{n-1}+F_{n-2}}$ , where ${\displaystyle F_{0}=0}$ and ${\displaystyle F_{1}=1}$ £#/li#££#/ul#££#ul#££#li#£ ${\displaystyle f(x+P)=f(x)}$ , which characterizes the periodic functions£#/li#££#/ul#££#ul#££#li#£ ${\displaystyle f(x)=f(-x)}$ , which characterizes the even functions, and likewise ${\displaystyle f(x)=-f(-x)}$ , which characterizes the odd functions£#/li#££#/ul#££#ul#££#li#£ ${\displaystyle f(f(x))=g(x)}$ , which characterizes the functional square roots of the function g£#/li#££#/ul#££#ul#££#li#£ ${\displaystyle f(x+y)=f(x)+f(y)\,\!}$ (Cauchy's functional equation), satisfied by linear maps. The equation may, contingent on the axiom of choice, also have other pathological nonlinear solutions, whose existence can be proven with a Hamel basis for the real numbers£#/li#£ £#li#£ ${\displaystyle f(x+y)=f(x)f(y),\,\!}$ satisfied by all exponential functions. Like Cauchy's additive functional equation, this too may have pathological, discontinuous solutions£#/li#£ £#li#£ ${\displaystyle f(xy)=f(x)+f(y)\,\!}$ , satisfied by all logarithmic functions and, over coprime integer arguments, additive functions£#/li#£ £#li#£ ${\displaystyle f(xy)=f(x)f(y)\,\!}$ , satisfied by all power functions and, over coprime integer arguments, multiplicative functions£#/li#£ £#li#£ ${\displaystyle f(x+y)+f(x-y)=2[f(x)+f(y)]\,\!}$ (quadratic equation or parallelogram law)£#/li#£ £#li#£ ${\displaystyle f((x+y)/2)=(f(x)+f(y))/2\,\!}$ (Jensen's functional equation)£#/li#£ £#li#£ ${\displaystyle g(x+y)+g(x-y)=2[g(x)g(y)]\,\!}$ (d'Alembert's functional equation)£#/li#£ £#li#£ ${\displaystyle f(h(x))=h(x+1)\,\!}$ (Abel equation)£#/li#£ £#li#£ ${\displaystyle f(h(x))=cf(x)\,\!}$ (Schröder's equation).£#/li#£ £#li#£ ${\displaystyle f(h(x))=(f(x))^{c}\,\!}$ (Böttcher's equation).£#/li#£ £#li#£ ${\displaystyle f(h(x))=h'(x)f(x)\,\!}$ (Julia's equation).£#/li#£ £#li#£ ${\displaystyle f(xy)=\sum g_{l}(x)h_{l}(y)\,\!}$ (Levi-Civita),£#/li#£ £#li#£ ${\displaystyle f(x+y)=f(x)g(y)+f(y)g(x)\,\!}$ (sine addition formula and hyperbolic sine addition formula),£#/li#£ £#li#£ ${\displaystyle g(x+y)=g(x)g(y)-f(y)f(x)\,\!}$ (cosine addition formula),£#/li#£ £#li#£ ${\displaystyle g(x+y)=g(x)g(y)+f(y)f(x)\,\!}$ (hyperbolic cosine addition formula).£#/li#£ £#li#£The commutative and associative laws are functional equations. In its familiar form, the associative law is expressed by writing the binary operation in infix notation, but if we write f(a, b) instead of a ○ b then the associative law looks more like a conventional functional equation, £#/li#££#/ul#££#ul#££#li#£The functional equation is satisfied by the Riemann zeta function. The capital Γ denotes the gamma function.£#/li#££#/ul#££#ul#££#li#£The gamma function is the unique solution of the following system of three equations:£#ul#££#li#£ ${\displaystyle f(x)={f(x+1) \over x}}$ £#/li#£ £#li#£ ${\displaystyle f(y)f\left(y+{\frac {1}{2}}\right)={\frac {\sqrt {\pi }}{2^{2y-1}}}f(2y)}$ £#/li#£ £#li#£ ${\displaystyle f(z)f(1-z)={\pi \over \sin(\pi z)}}$           (Euler's reflection formula)£#/li#££#/ul#££#/li#£ £#li#£The functional equation where a, b, c, d are integers satisfying ${\displaystyle ad-bc=1}$ , i.e. ${\displaystyle {\begin{vmatrix}a&b\\c&d\end{vmatrix}}}$ = 1, defines f to be a modular form of order k.£#/li#££#/ul#£
One feature that all of the examples listed above share in common is that, in each case, two or more known functions (sometimes multiplication by a constant, sometimes addition of two variables, sometimes the identity function) are inside the argument of the unknown functions to be solved for.

When it comes to asking for all solutions, it may be the case that conditions from mathematical analysis should be applied; for example, in the case of the Cauchy equation mentioned above, the solutions that are continuous functions are the 'reasonable' ones, while other solutions that are not likely to have practical application can be constructed (by using a Hamel basis for the real numbers as vector space over the rational numbers). The Bohr–Mollerup theorem is another well-known example.


£#h5#£Involutions£#/h5#£
The involutions are characterized by the functional equation ${\displaystyle f(f(x))=x}$ . These appear in Babbage's functional equation (1820),

${\displaystyle f(f(x))=1-(1-x)=x\,.}$
Other involutions, and solutions of the equation, include

£#ul#££#li#£ ${\displaystyle f(x)=a-x\,,}$ £#/li#£ £#li#£ ${\displaystyle f(x)={\frac {a}{x}}\,,}$ and£#/li#£ £#li#£ ${\displaystyle f(x)={\frac {b-x}{1+cx}}~,}$ £#/li#££#/ul#£
which includes the previous three as special cases or limits.


£#h5#£Solution£#/h5#£
One method of solving elementary functional equations is substitution.

Some solutions to functional equations have exploited surjectivity, injectivity, oddness, and evenness.

Some functional equations have been solved with the use of ansatzes, mathematical induction.

Some classes of functional equations can be solved by computer-assisted techniques.

In dynamic programming a variety of successive approximation methods are used to solve Bellman's functional equation, including methods based on fixed point iterations.


£#h5#£See also£#/h5#£ £#ul#££#li#£Functional equation (L-function)£#/li#£ £#li#£Bellman equation£#/li#£ £#li#£Dynamic programming£#/li#£ £#li#£Implicit function£#/li#£ £#li#£Functional differential equation£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£János Aczél, Lectures on Functional Equations and Their Applications, Academic Press, 1966, reprinted by Dover Publications, ISBN 0486445232.£#/li#£ £#li#£János Aczél & J. Dhombres, Functional Equations in Several Variables, Cambridge University Press, 1989.£#/li#£ £#li#£C. Efthimiou, Introduction to Functional Equations, AMS, 2011, ISBN 978-0-8218-5314-6 ; online.£#/li#£ £#li#£Pl. Kannappan, Functional Equations and Inequalities with Applications, Springer, 2009.£#/li#£ £#li#£Marek Kuczma, Introduction to the Theory of Functional Equations and Inequalities, second edition, Birkhäuser, 2009.£#/li#£ £#li#£Henrik Stetkær, Functional Equations on Groups, first edition, World Scientific Publishing, 2013.£#/li#£ £#li#£Christopher G. Small (3 April 2007). Functional Equations and How to Solve Them. Springer Science & Business Media. ISBN 978-0-387-48901-8.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Functional Equations: Exact Solutions at EqWorld: The World of Mathematical Equations.£#/li#£ £#li#£Functional Equations: Index at EqWorld: The World of Mathematical Equations.£#/li#£ £#li#£IMO Compendium text (archived) on functional equations in problem solving.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Abel, N. H. Oeuvres Completes, Vol. 2 (Ed. L. Sylow and S. Lie). New York: Johnson Reprint Corp., pp. 189-192, 1988.£#/li#££#li#£Bytsko, A. G. "Two-Term Dilogarithm Identities Related to Conformal Field Theory." 9 Nov 1999. http://arxiv.org/abs/math-ph/9911012.£#/li#££#li#£Gordon, B. and McIntosh, R. J. "Algebraic Dilogarithm Identities." Ramanujan J. 1, 431-448, 1997.£#/li#££#li#£Hardy, G. H. Ramanujan: Twelve Lectures on Subjects Suggested by His Life and Work, 3rd ed. New York: Chelsea, pp. 14 and 21, 1999.£#/li#££#li#£Rogers, L. J. "On Function Sum Theorems Connected with the Series sum_1^(infty)x^n/n^2." Proc. London Math. Soc. 4, 169-189, 1907.£#/li#££#li#£ Abel, N. H. Oeuvres Completes, Vol. 2 (Ed. L. Sylow and S. Lie). New York: Johnson Reprint Corp., pp. 189-192, 1988. £#/li#££#li#£ Bytsko, A. G. "Two-Term Dilogarithm Identities Related to Conformal Field Theory." 9 Nov 1999. http://arxiv.org/abs/math-ph/9911012. £#/li#££#li#£ Gordon, B. and McIntosh, R. J. "Algebraic Dilogarithm Identities." Ramanujan J. 1, 431-448, 1997. £#/li#££#li#£ Hardy, G. H. Ramanujan: Twelve Lectures on Subjects Suggested by His Life and Work, 3rd ed. New York: Chelsea, pp. 14 and 21, 1999. £#/li#££#li#£ Rogers, L. J. "On Function Sum Theorems Connected with the Series ." Proc. London Math. Soc. 4, 169-189, 1907. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Polylogarithms £#/li#££#li#£ Calculus and Analysis > Functional Analysis £#/li#££#/ul#£




£#h3#£Abel's Inequality£#/h3#£

In mathematics, Abel's inequality, named after Niels Henrik Abel, supplies a simple bound on the absolute value of the inner product of two vectors in an important special case.


£#h5#£Mathematical description£#/h5#£
Let {a1, a2,...} be a sequence of real numbers that is either nonincreasing or nondecreasing, and let {b1, b2,...} be a sequence of real or complex numbers. If {an} is nondecreasing, it holds that

${\displaystyle \left|\sum _{k=1}^{n}a_{k}b_{k}\right|\leq \operatorname {max} _{k=1,\dots ,n}|B_{k}|(|a_{n}|+a_{n}-a_{1}),}$
and if {an} is nonincreasing, it holds that

${\displaystyle \left|\sum _{k=1}^{n}a_{k}b_{k}\right|\leq \operatorname {max} _{k=1,\dots ,n}|B_{k}|(|a_{n}|-a_{n}+a_{1}),}$
where

${\displaystyle B_{k}=b_{1}+\cdots +b_{k}.}$
In particular, if the sequence {an} is nonincreasing and nonnegative, it follows that

${\displaystyle \left|\sum _{k=1}^{n}a_{k}b_{k}\right|\leq \operatorname {max} _{k=1,\dots ,n}|B_{k}|a_{1},}$

£#h5#£Relation to Abel's transformation£#/h5#£
Abel's inequality follows easily from Abel's transformation, which is the discrete version of integration by parts: If {a1, a2, ...} and {b1, b2, ...} are sequences of real or complex numbers, it holds that

${\displaystyle \sum _{k=1}^{n}a_{k}b_{k}=a_{n}B_{n}-\sum _{k=1}^{n-1}B_{k}(a_{k+1}-a_{k}).}$

£#h5#£References£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Abel's inequality". MathWorld.£#/li#£ £#li#£Abel's inequality in Encyclopedia of Mathematics.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Inequalities £#/li#££#/ul#£




£#h3#£Abel's Integral£#/h3#£

A tautochrone or isochrone curve (from Greek prefixes tauto- meaning same or iso- equal, and chrono time) is the curve for which the time taken by an object sliding without friction in uniform gravity to its lowest point is independent of its starting point on the curve. The curve is a cycloid, and the time is equal to π times the square root of the radius (of the circle which generates the cycloid) over the acceleration of gravity. The tautochrone curve is related to the brachistochrone curve, which is also a cycloid.


£#h5#£The tautochrone problem£#/h5#£
The tautochrone problem, the attempt to identify this curve, was solved by Christiaan Huygens in 1659. He proved geometrically in his Horologium Oscillatorium, originally published in 1673, that the curve is a cycloid.

On a cycloid whose axis is erected on the perpendicular and whose vertex is located at the bottom, the times of descent, in which a body arrives at the lowest point at the vertex after having departed from any point on the cycloid, are equal to each other ...

The cycloid is given by a point on a circle of radius ${\displaystyle r}$ tracing a curve as the circle rolls along the ${\displaystyle x}$ axis, as:

${\displaystyle {\begin{aligned}x&=r(\theta -\sin \theta )\\y&=r(1-\cos \theta ),\end{aligned}}}$
Huygens also proved that the time of descent is equal to the time a body takes to fall vertically the same distance as diameter of the circle that generates the cycloid, multiplied by ${\displaystyle \pi /2}$ . In modern terms, this means that the time of descent is ${\displaystyle \pi {\sqrt {r/g}}}$ , where ${\displaystyle r}$ is the radius of the circle which generates the cycloid, and ${\displaystyle g}$ is the gravity of Earth, or more accurately, the earth's gravitational acceleration.

This solution was later used to solve the problem of the brachistochrone curve. Johann Bernoulli solved the problem in a paper (Acta Eruditorum, 1697).

The tautochrone problem was studied by Huygens more closely when it was realized that a pendulum, which follows a circular path, was not isochronous and thus his pendulum clock would keep different time depending on how far the pendulum swung. After determining the correct path, Christiaan Huygens attempted to create pendulum clocks that used a string to suspend the bob and curb cheeks near the top of the string to change the path to the tautochrone curve. These attempts proved unhelpful for a number of reasons. First, the bending of the string causes friction, changing the timing. Second, there were much more significant sources of timing errors that overwhelmed any theoretical improvements that traveling on the tautochrone curve helps. Finally, the "circular error" of a pendulum decreases as length of the swing decreases, so better clock escapements could greatly reduce this source of inaccuracy.

Later, the mathematicians Joseph Louis Lagrange and Leonhard Euler provided an analytical solution to the problem.


£#h5#£Lagrangian solution£#/h5#£
If the particle's position is parametrized by the arclength s(t) from the lowest point, the kinetic energy is proportional to ${\displaystyle {\dot {s}}^{2}.}$ The potential energy is proportional to the height y(s). One way the curve can be an isochrone is if the Lagrangian is that of a simple harmonic oscillator: the height of the curve must be proportional to the arclength squared.

where the constant of proportionality has been set to 1 by changing units of length.

The differential form of this relation is

which eliminates s, and leaves a differential equation for dx and dy. To find the solution, integrate for x in terms of y:

where ${\displaystyle u={\sqrt {y}}}$ . This integral is the area under a circle, which can be naturally cut into a triangle and a circular wedge:

To see that this is a strangely parametrized cycloid, change variables to disentangle the transcendental and algebraic parts by defining the angle ${\displaystyle \theta =\arcsin 2u}$ . This yields

which is the standard parametrization, except for the scale of x, y and θ.


£#h5#£"Virtual gravity" solution£#/h5#£
The simplest solution to the tautochrone problem is to note a direct relation between the angle of an incline and the gravity felt by a particle on the incline. A particle on a 90° vertical incline undergoes full gravitational acceleration ${\displaystyle g}$ , while a particle on a horizontal plane undergoes zero gravitational acceleration. At intermediate angles, the acceleration due to "virtual gravity" by the particle is ${\displaystyle g\sin \theta }$ . Note that ${\displaystyle \theta }$ is measured between the tangent to the curve and the horizontal, with angles above the horizontal being treated as positive angles. Thus, ${\displaystyle \theta }$ varies from ${\displaystyle -\pi /2}$ to ${\displaystyle \pi /2}$ .

The position of a mass measured along a tautochrone curve, ${\displaystyle s(t)}$ , must obey the following differential equation:

which, along with the initial conditions ${\displaystyle s(0)=s_{0}}$ and ${\displaystyle s'(0)=0}$ , has solution:

It can be easily verified both that this solution solves the differential equation and that a particle will reach ${\displaystyle s=0}$ at time ${\displaystyle \pi /2\omega }$ from any starting position ${\displaystyle s_{0}}$ . The problem is now to construct a curve that will cause the mass to obey the above motion. Newton's second law shows that the force of gravity and the acceleration of the mass are related by:

The explicit appearance of the distance, ${\displaystyle s}$ , is troublesome, but we can differentiate to obtain a more manageable form:

or

This equation relates the change in the curve's angle to the change in the distance along the curve. We now use trigonometry to relate the angle ${\displaystyle \theta }$ to the differential lengths ${\displaystyle dx}$ , ${\displaystyle dy}$ and ${\displaystyle ds}$ :

Replacing ${\displaystyle ds}$ with ${\displaystyle dx}$ in the above equation lets us solve for ${\displaystyle x}$ in terms of ${\displaystyle \theta }$ :

Likewise, we can also express ${\displaystyle ds}$ in terms of ${\displaystyle dy}$ and solve for ${\displaystyle y}$ in terms of ${\displaystyle \theta }$ :

Substituting ${\displaystyle \phi =2\theta \,}$ and ${\displaystyle r={\frac {g}{4\omega ^{2}}}\,}$ , we see that these parametric equations for ${\displaystyle x}$ and ${\displaystyle y}$ are those of a point on a circle of radius ${\displaystyle r}$ rolling along a horizontal line (a cycloid), with the circle center at the coordinates ${\displaystyle (C_{x}+r\phi ,C_{y})}$ :

Note that ${\displaystyle \phi }$ ranges from ${\displaystyle -\pi \leq \phi \leq \pi }$ . It is typical to set ${\displaystyle C_{x}=0}$ and ${\displaystyle C_{y}=r}$ so that the lowest point on the curve coincides with the origin. Therefore:

Solving for ${\displaystyle \omega }$ and remembering that ${\displaystyle T={\frac {2\pi }{\omega }}}$ is the time required for descent, we find the descent time in terms of the radius ${\displaystyle r}$ :

(Based loosely on Proctor, pp. 135–139)


£#h5#£Abel's solution£#/h5#£
Niels Henrik Abel attacked a generalized version of the tautochrone problem (Abel's mechanical problem), namely, given a function ${\displaystyle T(y)}$ that specifies the total time of descent for a given starting height, find an equation of the curve that yields this result. The tautochrone problem is a special case of Abel's mechanical problem when ${\displaystyle T(y)}$ is a constant.

Abel's solution begins with the principle of conservation of energy – since the particle is frictionless, and thus loses no energy to heat, its kinetic energy at any point is exactly equal to the difference in gravitational potential energy from its starting point. The kinetic energy is ${\displaystyle {\frac {1}{2}}mv^{2}}$ , and since the particle is constrained to move along a curve, its velocity is simply ${\displaystyle {\frac {d\ell }{dt}}}$ , where ${\displaystyle \ell }$ is the distance measured along the curve. Likewise, the gravitational potential energy gained in falling from an initial height ${\displaystyle y_{0}\,}$ to a height ${\displaystyle y\,}$ is ${\displaystyle mg(y_{0}-y)\,}$ , thus:

In the last equation, we have anticipated writing the distance remaining along the curve as a function of height ( ${\displaystyle \ell (y))}$ , recognized that the distance remaining must decrease as time increases (thus the minus sign), and used the chain rule in the form ${\displaystyle d\ell ={\frac {d\ell }{dy}}dy}$ .

Now we integrate from ${\displaystyle y=y_{0}}$ to ${\displaystyle y=0}$ to get the total time required for the particle to fall:

This is called Abel's integral equation and allows us to compute the total time required for a particle to fall along a given curve (for which ${\displaystyle {\frac {d\ell }{dy}}}$ would be easy to calculate). But Abel's mechanical problem requires the converse – given ${\displaystyle T(y_{0})\,}$ , we wish to find ${\displaystyle f(y)={\frac {d\ell }{dy}}}$ , from which an equation for the curve would follow in a straightforward manner. To proceed, we note that the integral on the right is the convolution of ${\displaystyle {\frac {d\ell }{dy}}}$ with ${\displaystyle {\frac {1}{\sqrt {y}}}}$ and thus take the Laplace transform of both sides with respect to variable ${\displaystyle y}$ :

where ${\displaystyle F(s)={\mathcal {L}}\left[{\frac {d\ell }{dy}}\right]}$ Since ${\displaystyle {\mathcal {L}}\left[{\frac {1}{\sqrt {y}}}\right]={\sqrt {\frac {\pi }{s}}}}$ , we now have an expression for the Laplace transform of ${\displaystyle {\frac {d\ell }{dy}}}$ in terms of ${\displaystyle T(y_{0})\,}$ 's Laplace transform:

This is as far as we can go without specifying ${\displaystyle T(y_{0})\,}$ . Once ${\displaystyle T(y_{0})\,}$ is known, we can compute its Laplace transform, calculate the Laplace transform of ${\displaystyle {\frac {d\ell }{dy}}}$ and then take the inverse transform (or try to) to find ${\displaystyle {\frac {d\ell }{dy}}}$ .

For the tautochrone problem, ${\displaystyle T(y_{0})=T_{0}\,}$ is constant. Since the Laplace transform of 1 is ${\displaystyle {\frac {1}{s}}}$ , i.e., ${\displaystyle {\mathcal {L}}[T(y_{0})]={\frac {T_{0}}{s}}}$ , we find the shape function ${\displaystyle f(y)={\frac {d\ell }{dy}}}$ :

Making use again of the Laplace transform above, we invert the transform and conclude:

It can be shown that the cycloid obeys this equation. It needs one step further to do the integral with respect to ${\displaystyle y}$ to obtain the expression of the path shape.

(Simmons, Section 54).


£#h5#£See also£#/h5#£ £#ul#££#li#£Beltrami identity£#/li#£ £#li#£Brachistochrone curve£#/li#£ £#li#£Calculus of variations£#/li#£ £#li#£Catenary£#/li#£ £#li#£Cycloid£#/li#£ £#li#£Uniformly accelerated motion£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£Bibliography£#/h5#£ £#ul#££#li#£Simmons, George (1972). Differential Equations with Applications and Historical Notes. McGraw–Hill. ISBN 0-07-057540-1.£#/li#£ £#li#£Proctor, Richard Anthony (1878). A Treatise on the Cycloid and All Forms of Cycloidal Curves, and on the Use of Such Curves in Dealing with the Motions of Planets, Comets, etc., and of Matter Projected from the Sun.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Mathworld£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Abel, N. H. "Oplösning af et Par Opgaver ved Hjelp af bestemt Integraler." Mag. for naturvidenskaberne 1, 55-68, 1823. Reprinted as "Solution de quelques problémes à l'aide d'intégrales définies." Paper II in Abel, N. H. Œ (Ed. L. Sylow and S. Lie). Christiania [Oslo], Norway, pp. 11-27, 1881. Reprinted by J. Gabay, 1992.£#/li#££#li#£Abel, N. H. "Oplösning af nogle Opgaver ved Hjelp af bestemt Integraler." Mag. for naturvidenskaberne 1, 205-215, 1823. Reprinted as "Solution de quelques problémes à l'aide d'intégrales définies." Paper II in Abel, N. H. Œ (Ed. L. Sylow and S. Lie). Christiania [Oslo], Norway, 1881. Reprinted by J. Gabay, 1992.£#/li#££#li#£Sloane, N. J. A. Sequence A102047 in "The On-Line Encyclopedia of Integer Sequences."£#/li#££#li#£ Abel, N. H. "Oplösning af et Par Opgaver ved Hjelp af bestemt Integraler." Mag. for naturvidenskaberne 1, 55-68, 1823. Reprinted as "Solution de quelques problémes à l'aide d'intégrales définies." Paper II in Abel, N. H. Œ (Ed. L. Sylow and S. Lie). Christiania [Oslo], Norway, pp. 11-27, 1881. Reprinted by J. Gabay, 1992. £#/li#££#li#£ Abel, N. H. "Oplösning af nogle Opgaver ved Hjelp af bestemt Integraler." Mag. for naturvidenskaberne 1, 205-215, 1823. Reprinted as "Solution de quelques problémes à l'aide d'intégrales définies." Paper II in Abel, N. H. Œ (Ed. L. Sylow and S. Lie). Christiania [Oslo], Norway, 1881. Reprinted by J. Gabay, 1992. £#/li#££#li#£ Sloane, N. J. A. Sequence A102047 in "The On-Line Encyclopedia of Integer Sequences." £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Calculus > Integrals > Definite Integrals £#/li#££#/ul#£




£#h3#£Abel's Irreducibility Theorem£#/h3#£

In mathematics, Abel's irreducibility theorem, a field theory result described in 1829 by Niels Henrik Abel, asserts that if ƒ(x) is a polynomial over a field F that shares a root with a polynomial g(x) that is irreducible over F, then every root of g(x) is a root of ƒ(x). Equivalently, if ƒ(x) shares at least one root with g(x) then ƒ is divisible evenly by g(x), meaning that ƒ(x) can be factored as g(x)h(x) with h(x) also having coefficients in F.

Corollaries of the theorem include:

£#ul#££#li#£If ƒ(x) is irreducible, there is no lower-degree polynomial (other than the zero polynomial) that shares any root with it. For example, x2 − 2 is irreducible over the rational numbers and has ${\displaystyle {\sqrt {2}}}$ as a root; hence there is no linear or constant polynomial over the rationals having ${\displaystyle {\sqrt {2}}}$ as a root. Furthermore, there is no same-degree polynomial that shares any roots with ƒ(x), other than constant multiples of ƒ(x).£#/li#£ £#li#£If ƒ(x) ≠ g(x) are two different irreducible monic polynomials, then they share no roots.£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Larry Freeman. Fermat's Last Theorem blog: Abel's Lemmas on Irreducibility. September 4, 2008.£#/li#£ £#li#£Weisstein, Eric W. "Abel's Irreducibility Theorem". MathWorld.£#/li#££#/ul#£




£#h5#£ References £#/h5#£ £#ul#££#li#£Abel, N. H. "Mémoire sur une classe particulière d'équations résolubles algébriquement." J. reine angew. Math. 4, 131-156, 1829. Reprinted as Ch. 25 in Abel, N. H. Oeuvres complètes, tome 1. J. Gabay, pp. 478-507, 1992.£#/li#££#li#£Dörrie, H. 100 Great Problems of Elementary Mathematics: Their History and Solutions. New York: Dover, p. 120, 1965.£#/li#££#li#£ Abel, N. H. "Mémoire sur une classe particulière d'équations résolubles algébriquement." J. reine angew. Math. 4, 131-156, 1829. Reprinted as Ch. 25 in Abel, N. H. Oeuvres complètes, tome 1. J. Gabay, pp. 478-507, 1992. £#/li#££#li#£ Dörrie, H. 100 Great Problems of Elementary Mathematics: Their History and Solutions. New York: Dover, p. 120, 1965. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Algebra > Polynomials £#/li#££#/ul#£




£#h3#£Abel's Lemma£#/h3#£

In mathematics, summation by parts transforms the summation of products of sequences into other summations, often simplifying the computation or (especially) estimation of certain types of sums. It is also called Abel's lemma or Abel transformation, named after Niels Henrik Abel who introduced it in 1826.


£#h5#£Statement£#/h5#£
Suppose ${\displaystyle \{f_{k}\}}$ and ${\displaystyle \{g_{k}\}}$ are two sequences. Then,

${\displaystyle \sum _{k=m}^{n}f_{k}(g_{k+1}-g_{k})=\left(f_{n}g_{n+1}-f_{m}g_{m}\right)-\sum _{k=m+1}^{n}g_{k}(f_{k}-f_{k-1}).}$
Using the forward difference operator ${\displaystyle \Delta }$ , it can be stated more succinctly as

${\displaystyle \sum _{k=m}^{n}f_{k}\Delta g_{k}=\left(f_{n}g_{n+1}-f_{m}g_{m}\right)-\sum _{k=m}^{n-1}g_{k+1}\Delta f_{k},}$
Summation by parts is an analogue to integration by parts:

${\displaystyle \int f\,dg=fg-\int g\,df,}$
or to Abel's summation formula:

${\displaystyle \sum _{k=m+1}^{n}f(k)(g_{k}-g_{k-1})=\left(f(n)g_{n}-f(m)g_{m}\right)-\int _{m}^{n}g_{\lfloor t\rfloor }f'(t)dt.}$
An alternative statement is

${\displaystyle f_{n}g_{n}-f_{m}g_{m}=\sum _{k=m}^{n-1}f_{k}\Delta g_{k}+\sum _{k=m}^{n-1}g_{k}\Delta f_{k}+\sum _{k=m}^{n-1}\Delta f_{k}\Delta g_{k}}$
which is analogous to the integration by parts formula for semimartingales.

Although applications almost always deal with convergence of sequences, the statement is purely algebraic and will work in any field. It will also work when one sequence is in a vector space, and the other is in the relevant field of scalars.


£#h5#£Newton series£#/h5#£
The formula is sometimes given in one of these - slightly different - forms

${\displaystyle {\begin{aligned}\sum _{k=0}^{n}f_{k}g_{k}&=f_{0}\sum _{k=0}^{n}g_{k}+\sum _{j=0}^{n-1}(f_{j+1}-f_{j})\sum _{k=j+1}^{n}g_{k}\\&=f_{n}\sum _{k=0}^{n}g_{k}-\sum _{j=0}^{n-1}\left(f_{j+1}-f_{j}\right)\sum _{k=0}^{j}g_{k},\end{aligned}}}$
which represent a special case ( ${\displaystyle M=1}$ ) of the more general rule

${\displaystyle {\begin{aligned}\sum _{k=0}^{n}f_{k}g_{k}&=\sum _{i=0}^{M-1}f_{0}^{(i)}G_{i}^{(i+1)}+\sum _{j=0}^{n-M}f_{j}^{(M)}G_{j+M}^{(M)}=\\&=\sum _{i=0}^{M-1}\left(-1\right)^{i}f_{n-i}^{(i)}{\tilde {G}}_{n-i}^{(i+1)}+\left(-1\right)^{M}\sum _{j=0}^{n-M}f_{j}^{(M)}{\tilde {G}}_{j}^{(M)};\end{aligned}}}$
both result from iterated application of the initial formula. The auxiliary quantities are Newton series:

${\displaystyle f_{j}^{(M)}:=\sum _{k=0}^{M}\left(-1\right)^{M-k}{M \choose k}f_{j+k}}$
and

${\displaystyle G_{j}^{(M)}:=\sum _{k=j}^{n}{k-j+M-1 \choose M-1}g_{k},}$
${\displaystyle {\tilde {G}}_{j}^{(M)}:=\sum _{k=0}^{j}{j-k+M-1 \choose M-1}g_{k}.}$
A particular ( ${\displaystyle M=n+1}$ ) result is the identity

${\displaystyle \sum _{k=0}^{n}f_{k}g_{k}=\sum _{i=0}^{n}f_{0}^{(i)}G_{i}^{(i+1)}=\sum _{i=0}^{n}(-1)^{i}f_{n-i}^{(i)}{\tilde {G}}_{n-i}^{(i+1)}.}$
Here, ${\textstyle {n \choose k}}$ is the binomial coefficient.


£#h5#£Method£#/h5#£
For two given sequences ${\displaystyle (a_{n})}$ and ${\displaystyle (b_{n})}$ , with ${\displaystyle n\in \mathbb {N} }$ , one wants to study the sum of the following series:

If we define ${\textstyle B_{n}=\sum _{k=0}^{n}b_{k},}$ then for every ${\displaystyle n>0,}$ ${\displaystyle b_{n}=B_{n}-B_{n-1}}$ and

Finally ${\textstyle S_{N}=a_{N}B_{N}-\sum _{n=0}^{N-1}B_{n}(a_{n+1}-a_{n}).}$

This process, called an Abel transformation, can be used to prove several criteria of convergence for ${\displaystyle S_{N}}$ .


£#h5#£Similarity with an integration by parts£#/h5#£
The formula for an integration by parts is ${\textstyle \int _{a}^{b}f(x)g'(x)\,dx=\left[f(x)g(x)\right]_{a}^{b}-\int _{a}^{b}f'(x)g(x)\,dx}$ .

Beside the boundary conditions, we notice that the first integral contains two multiplied functions, one which is integrated in the final integral ( ${\displaystyle g'}$ becomes ${\displaystyle g}$ ) and one which is differentiated ( ${\displaystyle f}$ becomes ${\displaystyle f'}$ ).

The process of the Abel transformation is similar, since one of the two initial sequences is summed ( ${\displaystyle b_{n}}$ becomes ${\displaystyle B_{n}}$ ) and the other one is differenced ( ${\displaystyle a_{n}}$ becomes ${\displaystyle a_{n+1}-a_{n}}$ ).


£#h5#£Applications£#/h5#£ £#ul#££#li#£It is used to prove Kronecker's lemma, which in turn, is used to prove a version of the strong law of large numbers under variance constraints.£#/li#£ £#li#£It may be used to prove Nicomachus's theorem that the sum of the first ${\displaystyle n}$ cubes equals the square of the sum of the first ${\displaystyle n}$ positive integers.£#/li#£ £#li#£Summation by parts is frequently used to prove Abel's theorem and Dirichlet's test.£#/li#£ £#li#£One can also use this technique to prove Abel's test: If ${\textstyle \sum _{n}b_{n}}$ is a convergent series, and ${\displaystyle a_{n}}$ a bounded monotone sequence, then ${\textstyle S_{N}=\sum _{n=0}^{N}a_{n}b_{n}}$ converges.£#/li#££#/ul#£
Proof of Abel's test. Summation by parts gives

where a is the limit of ${\displaystyle a_{n}}$ . As ${\textstyle \sum _{n}b_{n}}$ is convergent, ${\displaystyle B_{N}}$ is bounded independently of ${\displaystyle N}$ , say by ${\displaystyle B}$ . As ${\displaystyle a_{n}-a}$ go to zero, so go the first two terms. The third term goes to zero by the Cauchy criterion for ${\textstyle \sum _{n}b_{n}}$ . The remaining sum is bounded by by the monotonicity of ${\displaystyle a_{n}}$ , and also goes to zero as ${\displaystyle N\to \infty }$ .
Using the same proof as above, one can show that if

£#li#£the partial sums ${\displaystyle B_{N}}$ form a bounded sequence independently of ${\displaystyle N}$ ;£#/li#£ £#li#£ ${\displaystyle \sum _{n=0}^{\infty }|a_{n+1}-a_{n}|<\infty }$ (so that the sum ${\displaystyle \sum _{n=N}^{M-1}|a_{n+1}-a_{n}|}$ goes to zero as ${\displaystyle N}$ goes to infinity)£#/li#£ £#li#£ ${\displaystyle \lim a_{n}=0}$ £#/li#£
then ${\textstyle S_{N}=\sum _{n=0}^{N}a_{n}b_{n}}$ converges.

In both cases, the sum of the series satisfies:


£#h5#£Summation-by-parts operators for high order finite difference methods£#/h5#£
A summation-by-parts (SBP) finite difference operator conventionally consists of a centered difference interior scheme and specific boundary stencils that mimics behaviors of the corresponding integration-by-parts formulation. The boundary conditions are usually imposed by the Simultaneous-Approximation-Term (SAT) technique. The combination of SBP-SAT is a powerful framework for boundary treatment. The method is preferred for well-proven stability for long-time simulation, and high order of accuracy.


£#h5#£See also£#/h5#£ £#ul#££#li#£Convergent series£#/li#£ £#li#£Divergent series£#/li#£ £#li#£Integration by parts£#/li#£ £#li#£Cesàro summation£#/li#£ £#li#£Abel's theorem£#/li#£ £#li#£Abel sum formula£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£Bibliography£#/h5#£ £#ul#££#li#£Abel, Neils Henrik (1826). "Untersuchungen über die Reihe ${\displaystyle 1+{\frac {m}{x}}+{\frac {m\cdot (m-1)}{2\cdot 1}}x^{2}+{\frac {m\cdot (m-1)\cdot (m-2)}{3\cdot 2\cdot 1}}x^{3}+\ldots }$ u.s.w.". J. Reine Angew. Math. 1: 311–339.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Dörrie, H. 100 Great Problems of Elementary Mathematics: Their History and Solutions. New York: Dover, p. 118, 1965.£#/li#££#li#£Jeffreys, H. and Jeffreys, B. S. "Abel's Lemma." §1.1153 in Methods of Mathematical Physics, 3rd ed. Cambridge, England: Cambridge University Press, pp. 41-42, 1988.£#/li#££#li#£ Dörrie, H. 100 Great Problems of Elementary Mathematics: Their History and Solutions. New York: Dover, p. 118, 1965. £#/li#££#li#£ Jeffreys, H. and Jeffreys, B. S. "Abel's Lemma." §1.1153 in Methods of Mathematical Physics, 3rd ed. Cambridge, England: Cambridge University Press, pp. 41-42, 1988. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Algebra > Polynomials £#/li#££#/ul#£




£#h3#£Abel's Test£#/h3#£

In mathematics, Abel's test (also known as Abel's criterion) is a method of testing for the convergence of an infinite series. The test is named after mathematician Niels Henrik Abel. There are two slightly different versions of Abel's test – one is used with series of real numbers, and the other is used with power series in complex analysis. Abel's uniform convergence test is a criterion for the uniform convergence of a series of functions dependent on parameters.


£#h5#£Abel's test in real analysis£#/h5#£
Suppose the following statements are true:

£#li#£ ${\displaystyle \sum a_{n}}$ is a convergent series,£#/li#£ £#li#£{bn} is a monotone sequence, and£#/li#£ £#li#£{bn} is bounded.£#/li#£
Then ${\displaystyle \sum a_{n}b_{n}}$ is also convergent.

It is important to understand that this test is mainly pertinent and useful in the context of non absolutely convergent series ${\displaystyle \sum a_{n}}$ . For absolutely convergent series, this theorem, albeit true, is almost self evident.

This theorem can be proved directly using summation by parts.


£#h5#£Abel's test in complex analysis£#/h5#£
A closely related convergence test, also known as Abel's test, can often be used to establish the convergence of a power series on the boundary of its circle of convergence. Specifically, Abel's test states that if a sequence of positive real numbers ${\displaystyle (a_{n})}$ is decreasing monotonically (or at least that for all n greater than some natural number m, we have ${\displaystyle a_{n}\geq a_{n+1}}$ ) with

${\displaystyle \lim _{n\rightarrow \infty }a_{n}=0}$
then the power series

${\displaystyle f(z)=\sum _{n=0}^{\infty }a_{n}z^{n}}$
converges everywhere on the closed unit circle, except when z = 1. Abel's test cannot be applied when z = 1, so convergence at that single point must be investigated separately. Notice that Abel's test implies in particular that the radius of convergence is at least 1. It can also be applied to a power series with radius of convergence R ≠ 1 by a simple change of variables ζ = z/R. Notice that Abel's test is a generalization of the Leibniz Criterion by taking z = −1.

Proof of Abel's test: Suppose that z is a point on the unit circle, z ≠ 1. For each ${\displaystyle n\geq 1}$ , we define

${\displaystyle f_{n}(z):=\sum _{k=0}^{n}a_{k}z^{k}.}$
By multiplying this function by (1 − z), we obtain

${\displaystyle {\begin{aligned}(1-z)f_{n}(z)&=\sum _{k=0}^{n}a_{k}(1-z)z^{k}=\sum _{k=0}^{n}a_{k}z^{k}-\sum _{k=0}^{n}a_{k}z^{k+1}=a_{0}+\sum _{k=1}^{n}a_{k}z^{k}-\sum _{k=1}^{n+1}a_{k-1}z^{k}\\&=a_{0}-a_{n}z^{n+1}+\sum _{k=1}^{n}(a_{k}-a_{k-1})z^{k}.\end{aligned}}}$
The first summand is constant, the second converges uniformly to zero (since by assumption the sequence ${\displaystyle (a_{n})}$ converges to zero). It only remains to show that the series converges. We will show this by showing that it even converges absolutely: ${\displaystyle \sum _{k=1}^{\infty }\left|(a_{k}-a_{k-1})z^{k}\right|=\sum _{k=1}^{\infty }|a_{k}-a_{k-1}|\cdot |z|^{k}\leq \sum _{k=1}^{\infty }(a_{k-1}-a_{k})}$ where the last sum is a converging telescoping sum. The absolute value vanished because the sequence ${\displaystyle (a_{n})}$ is decreasing by assumption.

Hence, the sequence ${\displaystyle (1-z)f_{n}(z)}$ converges (even uniformly) on the closed unit disc. If ${\displaystyle z\not =1}$ , we may divide by (1 − z) and obtain the result.

Another way to obtain the result is to apply the Dirichlet's test. Indeed, for ${\displaystyle z\neq 1,\ |z|=1}$ holds ${\displaystyle \left|\sum _{k=0}^{n}z^{k}\right|=\left|{\frac {z^{n+1}-1}{z-1}}\right|\leq {\frac {2}{|z-1|}}}$ , hence the assumptions of the Dirichlet's test are fulfilled.


£#h5#£Abel's uniform convergence test£#/h5#£
Abel's uniform convergence test is a criterion for the uniform convergence of a series of functions or an improper integration of functions dependent on parameters. It is related to Abel's test for the convergence of an ordinary series of real numbers, and the proof relies on the same technique of summation by parts.

The test is as follows. Let {gn} be a uniformly bounded sequence of real-valued continuous functions on a set E such that gn+1(x) ≤ gn(x) for all x ∈ E and positive integers n, and let {fn} be a sequence of real-valued functions such that the series Σfn(x) converges uniformly on E. Then Σfn(x)gn(x) converges uniformly on E.


£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Gino Moretti, Functions of a Complex Variable, Prentice-Hall, Inc., 1964£#/li#£ £#li#£Apostol, Tom M. (1974), Mathematical analysis (2nd ed.), Addison-Wesley, ISBN 978-0-201-00288-1£#/li#£ £#li#£Weisstein, Eric W. "Abel's uniform convergence test". MathWorld.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Proof (for real series) at PlanetMath.org£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Series > Convergence £#/li#££#/ul#£




£#h3#£Abel's Theorem£#/h3#£

In mathematics, Abel's theorem for power series relates a limit of a power series to the sum of its coefficients. It is named after Norwegian mathematician Niels Henrik Abel.


£#h5#£Theorem£#/h5#£
Let the Taylor series

be a power series with real coefficients ${\displaystyle a_{k}}$ with radius of convergence ${\displaystyle 1.}$ Suppose that the series converges. Then ${\displaystyle G(x)}$ is continuous from the left at ${\displaystyle x=1,}$ that is,
The same theorem holds for complex power series

provided that ${\displaystyle z\to 1}$ entirely within a single Stolz sector, that is, a region of the open unit disk where for some fixed finite ${\displaystyle M>1}$ . Without this restriction, the limit may fail to exist: for example, the power series converges to ${\displaystyle 0}$ at ${\displaystyle z=1,}$ but is unbounded near any point of the form ${\displaystyle e^{\pi i/3^{n}},}$ so the value at ${\displaystyle z=1}$ is not the limit as ${\displaystyle z}$ tends to 1 in the whole open disk.
Note that ${\displaystyle G(z)}$ is continuous on the real closed interval ${\displaystyle [0,t]}$ for ${\displaystyle t<1,}$ by virtue of the uniform convergence of the series on compact subsets of the disk of convergence. Abel's theorem allows us to say more, namely that ${\displaystyle G(z)}$ is continuous on ${\displaystyle [0,1].}$


£#h5#£Remarks£#/h5#£
As an immediate consequence of this theorem, if ${\displaystyle z}$ is any nonzero complex number for which the series

converges, then it follows that in which the limit is taken from below.
The theorem can also be generalized to account for sums which diverge to infinity. If

then
However, if the series is only known to be divergent, but for reasons other than diverging to infinity, then the claim of the theorem may fail: take, for example, the power series for

At ${\displaystyle z=1}$ the series is equal to ${\displaystyle 1-1+1-1+\cdots ,}$ but ${\displaystyle {\tfrac {1}{1+1}}={\tfrac {1}{2}}.}$

We also remark the theorem holds for radii of convergence other than ${\displaystyle R=1}$ : let

be a power series with radius of convergence ${\displaystyle R,}$ and suppose the series converges at ${\displaystyle x=R.}$ Then ${\displaystyle G(x)}$ is continuous from the left at ${\displaystyle x=R,}$ that is,
£#h5#£Applications£#/h5#£
The utility of Abel's theorem is that it allows us to find the limit of a power series as its argument (that is, ${\displaystyle z}$ ) approaches ${\displaystyle 1}$ from below, even in cases where the radius of convergence, ${\displaystyle R,}$ of the power series is equal to ${\displaystyle 1}$ and we cannot be sure whether the limit should be finite or not. See for example, the binomial series. Abel's theorem allows us to evaluate many series in closed form. For example, when

we obtain by integrating the uniformly convergent geometric power series term by term on ${\displaystyle [-z,0]}$ ; thus the series converges to ${\displaystyle \ln(2)}$ by Abel's theorem. Similarly, converges to ${\displaystyle \arctan(1)={\tfrac {\pi }{4}}.}$
${\displaystyle G_{a}(z)}$ is called the generating function of the sequence ${\displaystyle a.}$ Abel's theorem is frequently useful in dealing with generating functions of real-valued and non-negative sequences, such as probability-generating functions. In particular, it is useful in the theory of Galton–Watson processes.


£#h5#£Outline of proof£#/h5#£
After subtracting a constant from ${\displaystyle a_{0},}$ we may assume that ${\displaystyle \sum _{k=0}^{\infty }a_{k}=0.}$ Let ${\displaystyle s_{n}=\sum _{k=0}^{n}a_{k}\!.}$ Then substituting ${\displaystyle a_{k}=s_{k}-s_{k-1}}$ and performing a simple manipulation of the series (summation by parts) results in

Given ${\displaystyle \varepsilon >0,}$ pick ${\displaystyle n}$ large enough so that ${\displaystyle |s_{k}|<\varepsilon }$ for all ${\displaystyle k\geq n}$ and note that

when ${\displaystyle z}$ lies within the given Stolz angle. Whenever ${\displaystyle z}$ is sufficiently close to ${\displaystyle 1}$ we have so that ${\displaystyle \left|G_{a}(z)\right|<(M+1)\varepsilon }$ when ${\displaystyle z}$ is both sufficiently close to ${\displaystyle 1}$ and within the Stolz angle.
£#h5#£Related concepts£#/h5#£
Converses to a theorem like Abel's are called Tauberian theorems: There is no exact converse, but results conditional on some hypothesis. The field of divergent series, and their summation methods, contains many theorems of abelian type and of tauberian type.


£#h5#£See also£#/h5#£ £#ul#££#li#£Abel's summation formula – Integration by parts version of Abel's method for summation by parts£#/li#£ £#li#£Nachbin resummation£#/li#£ £#li#£Summation by parts – Theorem to simplify sums of products of sequences£#/li#££#/ul#£
£#h5#£Further reading£#/h5#£ £#ul#££#li#£Ahlfors, Lars Valerian (September 1, 1980). Complex Analysis (Third ed.). McGraw Hill Higher Education. pp. 41–42. ISBN 0-07-085008-9. - Ahlfors called it Abel's limit theorem.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Abel summability at PlanetMath. (a more general look at Abelian theorems of this type)£#/li#£ £#li#£A.A. Zakharov (2001) [1994], "Abel summation method", Encyclopedia of Mathematics, EMS Press£#/li#£ £#li#£Weisstein, Eric W. "Abel's Convergence Theorem". MathWorld.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Discrete Mathematics > Combinatorics > Binomial Coefficients £#/li#££#li#£ Calculus and Analysis > Series > Convergence £#/li#££#li#£ Algebra > Algebraic Geometry > Abstract Algebraic Curves £#/li#££#li#£ Algebra > Algebraic Equations £#/li#££#li#£ Algebra > Polynomials £#/li#££#li#£ Calculus and Analysis > Series > Asymptotic Series £#/li#££#li#£ Calculus and Analysis > Special Functions > q-Series £#/li#££#/ul#£




£#h3#£Abel's Uniform Convergence Test£#/h3#£

In mathematics, Abel's test (also known as Abel's criterion) is a method of testing for the convergence of an infinite series. The test is named after mathematician Niels Henrik Abel. There are two slightly different versions of Abel's test – one is used with series of real numbers, and the other is used with power series in complex analysis. Abel's uniform convergence test is a criterion for the uniform convergence of a series of functions dependent on parameters.


£#h5#£Abel's test in real analysis£#/h5#£
Suppose the following statements are true:

£#li#£ ${\displaystyle \sum a_{n}}$ is a convergent series,£#/li#£ £#li#£{bn} is a monotone sequence, and£#/li#£ £#li#£{bn} is bounded.£#/li#£
Then ${\displaystyle \sum a_{n}b_{n}}$ is also convergent.

It is important to understand that this test is mainly pertinent and useful in the context of non absolutely convergent series ${\displaystyle \sum a_{n}}$ . For absolutely convergent series, this theorem, albeit true, is almost self evident.

This theorem can be proved directly using summation by parts.


£#h5#£Abel's test in complex analysis£#/h5#£
A closely related convergence test, also known as Abel's test, can often be used to establish the convergence of a power series on the boundary of its circle of convergence. Specifically, Abel's test states that if a sequence of positive real numbers ${\displaystyle (a_{n})}$ is decreasing monotonically (or at least that for all n greater than some natural number m, we have ${\displaystyle a_{n}\geq a_{n+1}}$ ) with

${\displaystyle \lim _{n\rightarrow \infty }a_{n}=0}$
then the power series

${\displaystyle f(z)=\sum _{n=0}^{\infty }a_{n}z^{n}}$
converges everywhere on the closed unit circle, except when z = 1. Abel's test cannot be applied when z = 1, so convergence at that single point must be investigated separately. Notice that Abel's test implies in particular that the radius of convergence is at least 1. It can also be applied to a power series with radius of convergence R ≠ 1 by a simple change of variables ζ = z/R. Notice that Abel's test is a generalization of the Leibniz Criterion by taking z = −1.

Proof of Abel's test: Suppose that z is a point on the unit circle, z ≠ 1. For each ${\displaystyle n\geq 1}$ , we define

${\displaystyle f_{n}(z):=\sum _{k=0}^{n}a_{k}z^{k}.}$
By multiplying this function by (1 − z), we obtain

${\displaystyle {\begin{aligned}(1-z)f_{n}(z)&=\sum _{k=0}^{n}a_{k}(1-z)z^{k}=\sum _{k=0}^{n}a_{k}z^{k}-\sum _{k=0}^{n}a_{k}z^{k+1}=a_{0}+\sum _{k=1}^{n}a_{k}z^{k}-\sum _{k=1}^{n+1}a_{k-1}z^{k}\\&=a_{0}-a_{n}z^{n+1}+\sum _{k=1}^{n}(a_{k}-a_{k-1})z^{k}.\end{aligned}}}$
The first summand is constant, the second converges uniformly to zero (since by assumption the sequence ${\displaystyle (a_{n})}$ converges to zero). It only remains to show that the series converges. We will show this by showing that it even converges absolutely: ${\displaystyle \sum _{k=1}^{\infty }\left|(a_{k}-a_{k-1})z^{k}\right|=\sum _{k=1}^{\infty }|a_{k}-a_{k-1}|\cdot |z|^{k}\leq \sum _{k=1}^{\infty }(a_{k-1}-a_{k})}$ where the last sum is a converging telescoping sum. The absolute value vanished because the sequence ${\displaystyle (a_{n})}$ is decreasing by assumption.

Hence, the sequence ${\displaystyle (1-z)f_{n}(z)}$ converges (even uniformly) on the closed unit disc. If ${\displaystyle z\not =1}$ , we may divide by (1 − z) and obtain the result.

Another way to obtain the result is to apply the Dirichlet's test. Indeed, for ${\displaystyle z\neq 1,\ |z|=1}$ holds ${\displaystyle \left|\sum _{k=0}^{n}z^{k}\right|=\left|{\frac {z^{n+1}-1}{z-1}}\right|\leq {\frac {2}{|z-1|}}}$ , hence the assumptions of the Dirichlet's test are fulfilled.


£#h5#£Abel's uniform convergence test£#/h5#£
Abel's uniform convergence test is a criterion for the uniform convergence of a series of functions or an improper integration of functions dependent on parameters. It is related to Abel's test for the convergence of an ordinary series of real numbers, and the proof relies on the same technique of summation by parts.

The test is as follows. Let {gn} be a uniformly bounded sequence of real-valued continuous functions on a set E such that gn+1(x) ≤ gn(x) for all x ∈ E and positive integers n, and let {fn} be a sequence of real-valued functions such that the series Σfn(x) converges uniformly on E. Then Σfn(x)gn(x) converges uniformly on E.


£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Gino Moretti, Functions of a Complex Variable, Prentice-Hall, Inc., 1964£#/li#£ £#li#£Apostol, Tom M. (1974), Mathematical analysis (2nd ed.), Addison-Wesley, ISBN 978-0-201-00288-1£#/li#£ £#li#£Weisstein, Eric W. "Abel's uniform convergence test". MathWorld.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Proof (for real series) at PlanetMath.org£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Bromwich, T. J. I'A. and MacRobert, T. M. An Introduction to the Theory of Infinite Series, 3rd ed. New York: Chelsea, p. 59, 1991.£#/li#££#li#£Jeffreys, H. and Jeffreys, B. S. "Abel's Lemma" and "Abel's Test." §1.1153-1.1154 in Methods of Mathematical Physics, 3rd ed. Cambridge, England: Cambridge University Press, pp. 41-42, 1988.£#/li#££#li#£Whittaker, E. T. and Watson, G. N. A Course in Modern Analysis, 4th ed. Cambridge, England: Cambridge University Press, p. 17, 1990.£#/li#££#li#£ Bromwich, T. J. I'A. and MacRobert, T. M. An Introduction to the Theory of Infinite Series, 3rd ed. New York: Chelsea, p. 59, 1991. £#/li#££#li#£ Jeffreys, H. and Jeffreys, B. S. "Abel's Lemma" and "Abel's Test." §1.1153-1.1154 in Methods of Mathematical Physics, 3rd ed. Cambridge, England: Cambridge University Press, pp. 41-42, 1988. £#/li#££#li#£ Whittaker, E. T. and Watson, G. N. A Course in Modern Analysis, 4th ed. Cambridge, England: Cambridge University Press, p. 17, 1990. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Series > Convergence £#/li#££#/ul#£




£#h3#£Abel-Plana Formula£#/h3#£

In mathematics, the Abel–Plana formula is a summation formula discovered independently by Niels Henrik Abel (1823) and Giovanni Antonio Amedeo Plana (1820). It states that

${\displaystyle \sum _{n=0}^{\infty }f(n)=\int _{0}^{\infty }f(x)\,dx+{\frac {1}{2}}f(0)+i\int _{0}^{\infty }{\frac {f(it)-f(-it)}{e^{2\pi t}-1}}\,dt.}$
It holds for functions f that are holomorphic in the region Re(z) ≥ 0, and satisfy a suitable growth condition in this region; for example it is enough to assume that |f| is bounded by C/|z|1+ε in this region for some constants C, ε > 0, though the formula also holds under much weaker bounds. (Olver 1997, p.290).

An example is provided by the Hurwitz zeta function,

${\displaystyle \zeta (s,\alpha )=\sum _{n=0}^{\infty }{\frac {1}{(n+\alpha )^{s}}}={\frac {\alpha ^{1-s}}{s-1}}+{\frac {1}{2\alpha ^{s}}}+2\int _{0}^{\infty }{\frac {\sin \left(s\arctan {\frac {t}{\alpha }}\right)}{(\alpha ^{2}+t^{2})^{\frac {s}{2}}}}{\frac {dt}{e^{2\pi t}-1}},}$
which holds for all ${\displaystyle s\in \mathbb {C} }$ , s ≠ 1.

Abel also gave the following variation for alternating sums:

${\displaystyle \sum _{n=0}^{\infty }(-1)^{n}f(n)={\frac {1}{2}}f(0)+i\int _{0}^{\infty }{\frac {f(it)-f(-it)}{2\sinh(\pi t)}}\,dt.}$

£#h5#£Proof£#/h5#£
Let ${\displaystyle f}$ be holomorphic on ${\displaystyle \Re (z)\geq 0}$ , such that ${\displaystyle f(0)=0}$ , ${\displaystyle f(z)=O(|z|^{k})}$ and for ${\displaystyle {\text{arg}}(z)\in (-\beta ,\beta )}$ , ${\displaystyle f(z)=O(|z|^{-1-\delta })}$ . Taking ${\displaystyle a=e^{i\beta /2}}$ with the residue theorem

Then

Using the Cauchy integral theorem for the last one.

thus obtaining
This identity stays true by analytic continuation everywhere the integral converges, letting ${\displaystyle a\to i}$ we obtain Abel-Plana's formula

The case f(0) ≠ 0 is obtained similarly, replacing ${\textstyle \int _{a^{-1}\infty }^{a\infty }{\frac {f(z)}{e^{-2i\pi z}-1}}dz}$ by two integrals following the same curves with a small indentation on the left and right of 0.


£#h5#£See also£#/h5#£ £#ul#££#li#£Euler–Maclaurin summation formula£#/li#£ £#li#£Euler–Boole summation£#/li#££#/ul#£
£#h5#£References£#/h5#£ £#ul#££#li#£Abel, N.H. (1823), Solution de quelques problèmes à l'aide d'intégrales définies£#/li#£ £#li#£Butzer, P. L.; Ferreira, P. J. S. G.; Schmeisser, G.; Stens, R. L. (2011), "The summation formulae of Euler–Maclaurin, Abel–Plana, Poisson, and their interconnections with the approximate sampling formula of signal analysis", Results in Mathematics, 59 (3): 359–400, doi:10.1007/s00025-010-0083-8, ISSN 1422-6383, MR 2793463, S2CID 54634413£#/li#£ £#li#£Olver, Frank William John (1997) [1974], Asymptotics and special functions, AKP Classics, Wellesley, MA: A K Peters Ltd., ISBN 978-1-56881-069-0, MR 1429619£#/li#£ £#li#£Plana, G.A.A. (1820), "Sur une nouvelle expression analytique des nombres Bernoulliens, propre à exprimer en termes finis la formule générale pour la sommation des suites", Mem. Accad. Sci. Torino, 25: 403–418£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Anderson, David. "Abel-Plana Formula". MathWorld.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Mostepanenko, V. M. and Trunov, N. N. §2.2 in The Casimir Effect and Its Applications. Oxford, England: Clarendon Press, 1997.£#/li#££#li#£Saharian, A. A. "The Generalized Abel-Plana Formula. Applications to Bessel Functions and Casimir Effect." http://www.ictp.trieste.it/~pub_off/preprints-sources/2000/IC2000014P.pdf.£#/li#££#li#£ Mostepanenko, V. M. and Trunov, N. N. §2.2 in The Casimir Effect and Its Applications. Oxford, England: Clarendon Press, 1997. £#/li#££#li#£ Saharian, A. A. "The Generalized Abel-Plana Formula. Applications to Bessel Functions and Casimir Effect." http://www.ictp.trieste.it/~pub_off/preprints-sources/2000/IC2000014P.pdf. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Complex Analysis > Contours £#/li#££#/ul#£




£#h3#£Abelian Differential£#/h3#£

In mathematics, differential of the first kind is a traditional term used in the theories of Riemann surfaces (more generally, complex manifolds) and algebraic curves (more generally, algebraic varieties), for everywhere-regular differential 1-forms. Given a complex manifold M, a differential of the first kind ω is therefore the same thing as a 1-form that is everywhere holomorphic; on an algebraic variety V that is non-singular it would be a global section of the coherent sheaf Ω1 of Kähler differentials. In either case the definition has its origins in the theory of abelian integrals.

The dimension of the space of differentials of the first kind, by means of this identification, is the Hodge number

h1,0.
The differentials of the first kind, when integrated along paths, give rise to integrals that generalise the elliptic integrals to all curves over the complex numbers. They include for example the hyperelliptic integrals of type

${\displaystyle \int {\frac {x^{k}\,dx}{\sqrt {Q(x)}}}}$
where Q is a square-free polynomial of any given degree > 4. The allowable power k has to be determined by analysis of the possible pole at the point at infinity on the corresponding hyperelliptic curve. When this is done, one finds that the condition is

k ≤ g − 1,
or in other words, k at most 1 for degree of Q 5 or 6, at most 2 for degree 7 or 8, and so on (as g = [(1+ deg Q)/2]).

Quite generally, as this example illustrates, for a compact Riemann surface or algebraic curve, the Hodge number is the genus g. For the case of algebraic surfaces, this is the quantity known classically as the irregularity q. It is also, in general, the dimension of the Albanese variety, which takes the place of the Jacobian variety.


£#h5#£Differentials of the second and third kind£#/h5#£
The traditional terminology also included differentials of the second kind and of the third kind. The idea behind this has been supported by modern theories of algebraic differential forms, both from the side of more Hodge theory, and through the use of morphisms to commutative algebraic groups.

The Weierstrass zeta function was called an integral of the second kind in elliptic function theory; it is a logarithmic derivative of a theta function, and therefore has simple poles, with integer residues. The decomposition of a (meromorphic) elliptic function into pieces of 'three kinds' parallels the representation as (i) a constant, plus (ii) a linear combination of translates of the Weierstrass zeta function, plus (iii) a function with arbitrary poles but no residues at them.

The same type of decomposition exists in general, mutatis mutandis, though the terminology is not completely consistent. In the algebraic group (generalized Jacobian) theory the three kinds are abelian varieties, algebraic tori, and affine spaces, and the decomposition is in terms of a composition series.

On the other hand, a meromorphic abelian differential of the second kind has traditionally been one with residues at all poles being zero. One of the third kind is one where all poles are simple. There is a higher-dimensional analogue available, using the Poincaré residue.


£#h5#£See also£#/h5#£ £#ul#££#li#£Logarithmic form£#/li#££#/ul#£
£#h5#£References£#/h5#£ £#ul#££#li#£"Abelian differential", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > General Analysis £#/li#££#/ul#£




£#h3#£Abelian Function£#/h3#£

In mathematics, particularly in algebraic geometry, complex analysis and algebraic number theory, an abelian variety is a projective algebraic variety that is also an algebraic group, i.e., has a group law that can be defined by regular functions. Abelian varieties are at the same time among the most studied objects in algebraic geometry and indispensable tools for much research on other topics in algebraic geometry and number theory.

An abelian variety can be defined by equations having coefficients in any field; the variety is then said to be defined over that field. Historically the first abelian varieties to be studied were those defined over the field of complex numbers. Such abelian varieties turn out to be exactly those complex tori that can be embedded into a complex projective space.

Abelian varieties defined over algebraic number fields are a special case, which is important also from the viewpoint of number theory. Localization techniques lead naturally from abelian varieties defined over number fields to ones defined over finite fields and various local fields. Since a number field is the fraction field of a Dedekind domain, for any nonzero prime of your Dedekind domain, there is a map from the Dedekind domain to the quotient of the Dedekind domain by the prime, which is a finite field for all finite primes. This induces a map from the fraction field to any such finite field. Given a curve with equation defined over the number field, we can apply this map to the coefficients to get a curve defined over some finite field, where the choices of finite field correspond to the finite primes of the number field.

Abelian varieties appear naturally as Jacobian varieties (the connected components of zero in Picard varieties) and Albanese varieties of other algebraic varieties. The group law of an abelian variety is necessarily commutative and the variety is non-singular. An elliptic curve is an abelian variety of dimension 1. Abelian varieties have Kodaira dimension 0.


£#h5#£History and motivation£#/h5#£
In the early nineteenth century, the theory of elliptic functions succeeded in giving a basis for the theory of elliptic integrals, and this left open an obvious avenue of research. The standard forms for elliptic integrals involved the square roots of cubic and quartic polynomials. When those were replaced by polynomials of higher degree, say quintics, what would happen?

In the work of Niels Abel and Carl Jacobi, the answer was formulated: this would involve functions of two complex variables, having four independent periods (i.e. period vectors). This gave the first glimpse of an abelian variety of dimension 2 (an abelian surface): what would now be called the Jacobian of a hyperelliptic curve of genus 2.

After Abel and Jacobi, some of the most important contributors to the theory of abelian functions were Riemann, Weierstrass, Frobenius, Poincaré and Picard. The subject was very popular at the time, already having a large literature.

By the end of the 19th century, mathematicians had begun to use geometric methods in the study of abelian functions. Eventually, in the 1920s, Lefschetz laid the basis for the study of abelian functions in terms of complex tori. He also appears to be the first to use the name "abelian variety". It was André Weil in the 1940s who gave the subject its modern foundations in the language of algebraic geometry.

Today, abelian varieties form an important tool in number theory, in dynamical systems (more specifically in the study of Hamiltonian systems), and in algebraic geometry (especially Picard varieties and Albanese varieties).


£#h5#£Analytic theory£#/h5#£
£#h5#£Definition£#/h5#£
A complex torus of dimension g is a torus of real dimension 2g that carries the structure of a complex manifold. It can always be obtained as the quotient of a g-dimensional complex vector space by a lattice of rank 2g. A complex abelian variety of dimension g is a complex torus of dimension g that is also a projective algebraic variety over the field of complex numbers. Since they are complex tori, abelian varieties carry the structure of a group. A morphism of abelian varieties is a morphism of the underlying algebraic varieties that preserves the identity element for the group structure. An isogeny is a finite-to-one morphism.

When a complex torus carries the structure of an algebraic variety, this structure is necessarily unique. In the case g = 1, the notion of abelian variety is the same as that of elliptic curve, and every complex torus gives rise to such a curve; for g > 1 it has been known since Riemann that the algebraic variety condition imposes extra constraints on a complex torus.


£#h5#£Riemann conditions£#/h5#£
The following criterion by Riemann decides whether or not a given complex torus is an abelian variety, i.e. whether or not it can be embedded into a projective space. Let X be a g-dimensional torus given as X = V/L where V is a complex vector space of dimension g and L is a lattice in V. Then X is an abelian variety if and only if there exists a positive definite hermitian form on V whose imaginary part takes integral values on L×L. Such a form on X is usually called a (non-degenerate) Riemann form. Choosing a basis for V and L, one can make this condition more explicit. There are several equivalent formulations of this; all of them are known as the Riemann conditions.


£#h5#£The Jacobian of an algebraic curve£#/h5#£
Every algebraic curve C of genus g ≥ 1 is associated with an abelian variety J of dimension g, by means of an analytic map of C into J. As a torus, J carries a commutative group structure, and the image of C generates J as a group. More accurately, J is covered by Cg: any point in J comes from a g-tuple of points in C. The study of differential forms on C, which give rise to the abelian integrals with which the theory started, can be derived from the simpler, translation-invariant theory of differentials on J. The abelian variety J is called the Jacobian variety of C, for any non-singular curve C over the complex numbers. From the point of view of birational geometry, its function field is the fixed field of the symmetric group on g letters acting on the function field of Cg.


£#h5#£Abelian functions£#/h5#£
An abelian function is a meromorphic function on an abelian variety, which may be regarded therefore as a periodic function of n complex variables, having 2n independent periods; equivalently, it is a function in the function field of an abelian variety. For example, in the nineteenth century there was much interest in hyperelliptic integrals that may be expressed in terms of elliptic integrals. This comes down to asking that J is a product of elliptic curves, up to an isogeny.


£#h5#£Important Theorems£#/h5#£
One important structure theorem of abelian varieties is Matsusaka's theorem. It states that over an algebraically closed field every abelian variety ${\displaystyle A}$ is the quotient of the Jacobian of some curve; that is, there is some surjection of abelian varieties ${\displaystyle J\to A}$ where ${\displaystyle J}$ is a Jacobian. This theorem remains true if the ground field is infinite.


£#h5#£Algebraic definition£#/h5#£
Two equivalent definitions of abelian variety over a general field k are commonly in use:

£#ul#££#li#£a connected and complete algebraic group over k£#/li#£ £#li#£a connected and projective algebraic group over k.£#/li#££#/ul#£
When the base is the field of complex numbers, these notions coincide with the previous definition. Over all bases, elliptic curves are abelian varieties of dimension 1.

In the early 1940s, Weil used the first definition (over an arbitrary base field) but could not at first prove that it implied the second. Only in 1948 did he prove that complete algebraic groups can be embedded into projective space. Meanwhile, in order to make the proof of the Riemann hypothesis for curves over finite fields that he had announced in 1940 work, he had to introduce the notion of an abstract variety and to rewrite the foundations of algebraic geometry to work with varieties without projective embeddings (see also the history section in the Algebraic Geometry article).


£#h5#£Structure of the group of points£#/h5#£
By the definitions, an abelian variety is a group variety. Its group of points can be proven to be commutative.

For C, and hence by the Lefschetz principle for every algebraically closed field of characteristic zero, the torsion group of an abelian variety of dimension g is isomorphic to (Q/Z)2g. Hence, its n-torsion part is isomorphic to (Z/nZ)2g, i.e. the product of 2g copies of the cyclic group of order n.

When the base field is an algebraically closed field of characteristic p, the n-torsion is still isomorphic to (Z/nZ)2g when n and p are coprime. When n and p are not coprime, the same result can be recovered provided one interprets it as saying that the n-torsion defines a finite flat group scheme of rank 2g. If instead of looking at the full scheme structure on the n-torsion, one considers only the geometric points, one obtains a new invariant for varieties in characteristic p (the so-called p-rank when n = p).

The group of k-rational points for a global field k is finitely generated by the Mordell-Weil theorem. Hence, by the structure theorem for finitely generated abelian groups, it is isomorphic to a product of a free abelian group Zr and a finite commutative group for some non-negative integer r called the rank of the abelian variety. Similar results hold for some other classes of fields k.


£#h5#£Products£#/h5#£
The product of an abelian variety A of dimension m, and an abelian variety B of dimension n, over the same field, is an abelian variety of dimension m + n. An abelian variety is simple if it is not isogenous to a product of abelian varieties of lower dimension. Any abelian variety is isogenous to a product of simple abelian varieties.


£#h5#£Polarisation and dual abelian variety£#/h5#£
£#h5#£Dual abelian variety£#/h5#£
To an abelian variety A over a field k, one associates a dual abelian variety Av (over the same field), which is the solution to the following moduli problem. A family of degree 0 line bundles parametrised by a k-variety T is defined to be a line bundle L on A×T such that

£#li#£for all t in T, the restriction of L to A×{t} is a degree 0 line bundle,£#/li#£ £#li#£the restriction of L to {0}×T is a trivial line bundle (here 0 is the identity of A).£#/li#£
Then there is a variety Av and a family of degree 0 line bundles P, the Poincaré bundle, parametrised by Av such that a family L on T is associated a unique morphism f: T → Av so that L is isomorphic to the pullback of P along the morphism 1A×f: A×T → A×Av. Applying this to the case when T is a point, we see that the points of Av correspond to line bundles of degree 0 on A, so there is a natural group operation on Av given by tensor product of line bundles, which makes it into an abelian variety.

This association is a duality in the sense that there is a natural isomorphism between the double dual Avv and A (defined via the Poincaré bundle) and that it is contravariant functorial, i.e. it associates to all morphisms f: A → B dual morphisms fv: Bv → Av in a compatible way. The n-torsion of an abelian variety and the n-torsion of its dual are dual to each other when n is coprime to the characteristic of the base. In general - for all n - the n-torsion group schemes of dual abelian varieties are Cartier duals of each other. This generalises the Weil pairing for elliptic curves.


£#h5#£Polarisations£#/h5#£
A polarisation of an abelian variety is an isogeny from an abelian variety to its dual that is symmetric with respect to double-duality for abelian varieties and for which the pullback of the Poincaré bundle along the associated graph morphism is ample (so it is analogous to a positive-definite quadratic form). Polarised abelian varieties have finite automorphism groups. A principal polarisation is a polarisation that is an isomorphism. Jacobians of curves are naturally equipped with a principal polarisation as soon as one picks an arbitrary rational base point on the curve, and the curve can be reconstructed from its polarised Jacobian when the genus is > 1. Not all principally polarised abelian varieties are Jacobians of curves; see the Schottky problem. A polarisation induces a Rosati involution on the endomorphism ring ${\displaystyle \mathrm {End} (A)\otimes \mathbb {Q} }$ of A.


£#h5#£Polarisations over the complex numbers£#/h5#£
Over the complex numbers, a polarised abelian variety can also be defined as an abelian variety A together with a choice of a Riemann form H. Two Riemann forms H1 and H2 are called equivalent if there are positive integers n and m such that nH1=mH2. A choice of an equivalence class of Riemann forms on A is called a polarisation of A. A morphism of polarised abelian varieties is a morphism A → B of abelian varieties such that the pullback of the Riemann form on B to A is equivalent to the given form on A.


£#h5#£Abelian scheme£#/h5#£
One can also define abelian varieties scheme-theoretically and relative to a base. This allows for a uniform treatment of phenomena such as reduction mod p of abelian varieties (see Arithmetic of abelian varieties), and parameter-families of abelian varieties. An abelian scheme over a base scheme S of relative dimension g is a proper, smooth group scheme over S whose geometric fibers are connected and of dimension g. The fibers of an abelian scheme are abelian varieties, so one could think of an abelian scheme over S as being a family of abelian varieties parametrised by S.

For an abelian scheme A / S, the group of n-torsion points forms a finite flat group scheme. The union of the pn-torsion points, for all n, forms a p-divisible group. Deformations of abelian schemes are, according to the Serre–Tate theorem, governed by the deformation properties of the associated p-divisible groups.


£#h5#£Example£#/h5#£
Let ${\displaystyle A,B\in \mathbb {Z} }$ be such that ${\displaystyle x^{3}+Ax+B}$ has no repeated complex roots. Then the discriminant ${\displaystyle \Delta =-16(4A^{3}+27B^{2})}$ is nonzero. Let ${\displaystyle R=\mathbb {Z} [1/\Delta ]}$ , so ${\displaystyle \operatorname {Spec} R}$ is an open subscheme of ${\displaystyle \operatorname {Spec} \mathbb {Z} }$ . Then ${\displaystyle \operatorname {Proj} R[x,y,z]/(y^{2}z-x^{3}-Axz^{2}-Bz^{3})}$ is an abelian scheme over ${\displaystyle \operatorname {Spec} R}$ . It can be extended to a Néron model over ${\displaystyle \operatorname {Spec} \mathbb {Z} }$ , which is a smooth group scheme over ${\displaystyle \operatorname {Spec} \mathbb {Z} }$ , but the Néron model is not proper and hence is not an abelian scheme over ${\displaystyle \operatorname {Spec} \mathbb {Z} }$ .


£#h5#£Non-existence£#/h5#£
V. A. Abrashkin and Jean-Marc Fontaine independently proved that there are no nonzero abelian varieties over Q with good reduction at all primes. Equivalently, there are no nonzero abelian schemes over Spec Z. The proof involves showing that the coordinates of pn-torsion points generate number fields with very little ramification and hence of small discriminant, while, on the other hand, there are lower bounds on discriminants of number fields.


£#h5#£Semiabelian variety£#/h5#£
A semiabelian variety is a commutative group variety which is an extension of an abelian variety by a torus.


£#h5#£See also£#/h5#£ £#ul#££#li#£Complex torus£#/li#£ £#li#£Motives£#/li#£ £#li#£Timeline of abelian varieties£#/li#£ £#li#£Moduli of abelian varieties£#/li#£ £#li#£Equations defining abelian varieties£#/li#£ £#li#£Horrocks–Mumford bundle£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£Sources£#/h5#£ £#ul#££#li#£Birkenhake, Christina; Lange, H. (1992), Complex Abelian Varieties, Berlin, New York: Springer-Verlag, ISBN 978-0-387-54747-3. A comprehensive treatment of the complex theory, with an overview of the history of the subject.£#/li#£ £#li#£Dolgachev, I.V. (2001) [1994], "Abelian scheme", Encyclopedia of Mathematics, EMS Press£#/li#£ £#li#£Faltings, Gerd; Chai, Ching-Li (1990), Degeneration of Abelian Varieties, Springer Verlag, ISBN 3-540-52015-5£#/li#£ £#li#£Milne, James, Abelian Varieties, retrieved 6 October 2016. Online course notes.£#/li#£ £#li#£Mumford, David (2008) [1970], Abelian varieties, Tata Institute of Fundamental Research Studies in Mathematics, vol. 5, Providence, R.I.: American Mathematical Society, ISBN 978-81-85931-86-9, MR 0282985, OCLC 138290£#/li#£ £#li#£Venkov, B.B.; Parshin, A.N. (2001) [1994], "Abelian_variety", Encyclopedia of Mathematics, EMS Press£#/li#£ £#li#£Bruin, N; Flynn, E.V., N-COVERS OF HYPERELLIPTIC CURVES (PDF), Oxford: Mathematical Institute, University of Oxford. Description of the Jacobian of the Covering Curves£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Baker, H. F. An Introduction to the Theory of Multiply Periodic Functions. London: Cambridge University Press, 1907.£#/li#££#li#£Baker, H. F. Abelian Functions: Abel's Theorem and the Allied Theory, Including the Theory of the Theta Functions. New York: Cambridge University Press, 1995.£#/li#££#li#£Deconinck, B.; Heil, M.; Bobenko, A.; van Hoeij, M.; and Schmies, M. "Computing Riemann Theta Functions." Math. Comput. 73, 1417-1442, 2004.£#/li#££#li#£Igusa, J.-I. Theta Functions. New York: Springer-Verlag, 1972.£#/li#££#li#£Weisstein, E. W. "Books about Abelian Functions." http://www.ericweisstein.com/encyclopedias/books/AbelianFunctions.html.£#/li#££#li#£ Baker, H. F. An Introduction to the Theory of Multiply Periodic Functions. London: Cambridge University Press, 1907. £#/li#££#li#£ Baker, H. F. Abelian Functions: Abel's Theorem and the Allied Theory, Including the Theory of the Theta Functions. New York: Cambridge University Press, 1995. £#/li#££#li#£ Deconinck, B.; Heil, M.; Bobenko, A.; van Hoeij, M.; and Schmies, M. "Computing Riemann Theta Functions." Math. Comput. 73, 1417-1442, 2004. £#/li#££#li#£ Igusa, J.-I. Theta Functions. New York: Springer-Verlag, 1972. £#/li#££#li#£ Weisstein, E. W. "Books about Abelian Functions." http://www.ericweisstein.com/encyclopedias/books/AbelianFunctions.html. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Elliptic Functions £#/li#££#/ul#£




£#h3#£Abelian Integral£#/h3#£

In mathematics, an abelian integral, named after the Norwegian mathematician Niels Henrik Abel, is an integral in the complex plane of the form

${\displaystyle \int _{z_{0}}^{z}R(x,w)\,dx,}$
where ${\displaystyle R(x,w)}$ is an arbitrary rational function of the two variables ${\displaystyle x}$ and ${\displaystyle w}$ , which are related by the equation

${\displaystyle F(x,w)=0,}$
where ${\displaystyle F(x,w)}$ is an irreducible polynomial in ${\displaystyle w}$ ,

${\displaystyle F(x,w)\equiv \varphi _{n}(x)w^{n}+\cdots +\varphi _{1}(x)w+\varphi _{0}\left(x\right),}$
whose coefficients ${\displaystyle \varphi _{j}(x)}$ , ${\displaystyle j=0,1,\ldots ,n}$ are rational functions of ${\displaystyle x}$ . The value of an abelian integral depends not only on the integration limits, but also on the path along which the integral is taken; it is thus a multivalued function of ${\displaystyle z}$ .

Abelian integrals are natural generalizations of elliptic integrals, which arise when

${\displaystyle F(x,w)=w^{2}-P(x),\,}$
where ${\displaystyle P\left(x\right)}$ is a polynomial of degree 3 or 4. Another special case of an abelian integral is a hyperelliptic integral, where ${\displaystyle P(x)}$ , in the formula above, is a polynomial of degree greater than 4.


£#h5#£History£#/h5#£
The theory of abelian integrals originated with a paper by Abel published in 1841. This paper was written during his stay in Paris in 1826 and presented to Augustin-Louis Cauchy in October of the same year. This theory, later fully developed by others, was one of the crowning achievements of nineteenth century mathematics and has had a major impact on the development of modern mathematics. In more abstract and geometric language, it is contained in the concept of abelian variety, or more precisely in the way an algebraic curve can be mapped into abelian varieties. Abelian integrals were later connected to the prominent mathematician David Hilbert's 16th Problem, and they continue to be considered one of the foremost challenges in contemporary mathematics.


£#h5#£Modern view£#/h5#£
In the theory of Riemann surfaces, an abelian integral is a function related to the indefinite integral of a differential of the first kind. Suppose we are given a Riemann surface ${\displaystyle S}$ and on it a differential 1-form ${\displaystyle \omega }$ that is everywhere holomorphic on ${\displaystyle S}$ , and fix a point ${\displaystyle P_{0}}$ on ${\displaystyle S}$ , from which to integrate. We can regard

${\displaystyle \int _{P_{0}}^{P}\omega }$
as a multi-valued function ${\displaystyle f\left(P\right)}$ , or (better) an honest function of the chosen path ${\displaystyle C}$ drawn on ${\displaystyle S}$ from ${\displaystyle P_{0}}$ to ${\displaystyle P}$ . Since ${\displaystyle S}$ will in general be multiply connected, one should specify ${\displaystyle C}$ , but the value will in fact only depend on the homology class of ${\displaystyle C}$ .

In the case of ${\displaystyle S}$ a compact Riemann surface of genus 1, i.e. an elliptic curve, such functions are the elliptic integrals. Logically speaking, therefore, an abelian integral should be a function such as ${\displaystyle f}$ .

Such functions were first introduced to study hyperelliptic integrals, i.e., for the case where ${\displaystyle S}$ is a hyperelliptic curve. This is a natural step in the theory of integration to the case of integrals involving algebraic functions ${\displaystyle {\sqrt {A}}}$ , where ${\displaystyle A}$ is a polynomial of degree ${\displaystyle >4}$ . The first major insights of the theory were given by Abel; it was later formulated in terms of the Jacobian variety ${\displaystyle J\left(S\right)}$ . Choice of ${\displaystyle P_{0}}$ gives rise to a standard holomorphic function

${\displaystyle S\to J(S)}$
of complex manifolds. It has the defining property that the holomorphic 1-forms on ${\displaystyle S\to J(S)}$ , of which there are g independent ones if g is the genus of S, pull back to a basis for the differentials of the first kind on S.


£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Abel, Niels H. (1841). "Mémoire sur une propriété générale d'une classe très étendue de fonctions transcendantes". Mémoires présentés par divers savants à l’Académie Royale des Sciences de l’Institut de France (in French). Paris. pp. 176–264.£#/li#£ £#li#£Appell, Paul; Goursat, Édouard (1895). Théorie des fonctions algébriques et de leurs intégrales (in French). Paris: Gauthier-Villars.£#/li#£ £#li#£Bliss, Gilbert A. (1933). Algebraic Functions. Providence: American Mathematical Society.£#/li#£ £#li#£Forsyth, Andrew R. (1893). Theory of Functions of a Complex Variable. Providence: Cambridge University Press.£#/li#£ £#li#£Griffiths, Phillip; Harris, Joseph (1978). Principles of Algebraic Geometry. New York: John Wiley & Sons.£#/li#£ £#li#£Neumann, Carl (1884). Vorlesungen über Riemann's Theorie der Abel'schen Integrale (2nd ed.). Leipzig: B. G. Teubner.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Siegel, C. L. Topics in Complex Function Theory, Vol. 2: Automorphic Functions and Abelian Integrals. New York: Wiley, 1988.£#/li#££#li#£ Siegel, C. L. Topics in Complex Function Theory, Vol. 2: Automorphic Functions and Abelian Integrals. New York: Wiley, 1988. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Elliptic Integrals £#/li#££#/ul#£




£#h3#£Abelian Theorem£#/h3#£

In mathematics, Abelian and Tauberian theorems are theorems giving conditions for two methods of summing divergent series to give the same result, named after Niels Henrik Abel and Alfred Tauber. The original examples are Abel's theorem showing that if a series converges to some limit then its Abel sum is the same limit, and Tauber's theorem showing that if the Abel sum of a series exists and the coefficients are sufficiently small (o(1/n)) then the series converges to the Abel sum. More general Abelian and Tauberian theorems give similar results for more general summation methods.

There is not yet a clear distinction between Abelian and Tauberian theorems, and no generally accepted definition of what these terms mean. Often, a theorem is called "Abelian" if it shows that some summation method gives the usual sum for convergent series, and is called "Tauberian" if it gives conditions for a series summable by some method that allows it to be summable in the usual sense.

In the theory of integral transforms, Abelian theorems give the asymptotic behaviour of the transform based on properties of the original function. Conversely, Tauberian theorems give the asymptotic behaviour of the original function based on properties of the transform but usually require some restrictions on the original function.


£#h5#£Abelian theorems£#/h5#£
For any summation method L, its Abelian theorem is the result that if c = (cn) is a convergent sequence, with limit C, then L(c) = C.

An example is given by the Cesàro method, in which L is defined as the limit of the arithmetic means of the first N terms of c, as N tends to infinity. One can prove that if c does converge to C, then so does the sequence (dN) where

${\displaystyle d_{N}={\frac {c_{1}+c_{2}+\cdots +c_{N}}{N}}.}$
To see that, subtract C everywhere to reduce to the case C = 0. Then divide the sequence into an initial segment, and a tail of small terms: given any ε > 0 we can take N large enough to make the initial segment of terms up to cN average to at most ε/2, while each term in the tail is bounded by ε/2 so that the average is also necessarily bounded.

The name derives from Abel's theorem on power series. In that case L is the radial limit (thought of within the complex unit disk), where we let r tend to the limit 1 from below along the real axis in the power series with term

anzn
and set z = r · eiθ. That theorem has its main interest in the case that the power series has radius of convergence exactly 1: if the radius of convergence is greater than one, the convergence of the power series is uniform for r in [0,1] so that the sum is automatically continuous and it follows directly that the limit as r tends up to 1 is simply the sum of the an. When the radius is 1 the power series will have some singularity on |z| = 1; the assertion is that, nonetheless, if the sum of the an exists, it is equal to the limit over r. This therefore fits exactly into the abstract picture.


£#h5#£Tauberian theorems£#/h5#£
Partial converses to Abelian theorems are called Tauberian theorems. The original result of Alfred Tauber (1897) stated that if we assume also

an = o(1/n)
(see Little o notation) and the radial limit exists, then the series obtained by setting z = 1 is actually convergent. This was strengthened by John Edensor Littlewood: we need only assume O(1/n). A sweeping generalization is the Hardy–Littlewood Tauberian theorem.

In the abstract setting, therefore, an Abelian theorem states that the domain of L contains the convergent sequences, and its values there are equal to those of the Lim functional. A Tauberian theorem states, under some growth condition, that the domain of L is exactly the convergent sequences and no more.

If one thinks of L as some generalised type of weighted average, taken to the limit, a Tauberian theorem allows one to discard the weighting, under the correct hypotheses. There are many applications of this kind of result in number theory, in particular in handling Dirichlet series.

The development of the field of Tauberian theorems received a fresh turn with Norbert Wiener's very general results, namely Wiener's Tauberian theorem and its large collection of corollaries. The central theorem can now be proved by Banach algebra methods, and contains much, though not all, of the previous theory.


£#h5#£See also£#/h5#£ £#ul#££#li#£Wiener's Tauberian theorem£#/li#£ £#li#£Hardy–Littlewood Tauberian theorem£#/li#£ £#li#£Haar's Tauberian theorem£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Tauberian theorems", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#£ £#li#£Korevaar, Jacob (2004). Tauberian theory. A century of developments. Grundlehren der Mathematischen Wissenschaften. Vol. 329. Springer-Verlag. pp. xvi+483. doi:10.1007/978-3-662-10225-1. ISBN 978-3-540-21058-0. MR 2073637. Zbl 1056.40002.£#/li#£ £#li#£Montgomery, Hugh L.; Vaughan, Robert C. (2007). Multiplicative number theory I. Classical theory. Cambridge Studies in Advanced Mathematics. Vol. 97. Cambridge: Cambridge University Press. pp. 147–167. ISBN 978-0-521-84903-6. MR 2378655. Zbl 1142.11001.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Hardy, G. H. Ramanujan: Twelve Lectures on Subjects Suggested by His Life and Work, 3rd ed. New York: Chelsea, p. 46, 1999.£#/li#££#li#£ Hardy, G. H. Ramanujan: Twelve Lectures on Subjects Suggested by His Life and Work, 3rd ed. New York: Chelsea, p. 46, 1999. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Series > Asymptotic Series £#/li#££#/ul#£




£#h3#£Abel Transform£#/h3#£

In mathematics, the Abel transform, named for Niels Henrik Abel, is an integral transform often used in the analysis of spherically symmetric or axially symmetric functions. The Abel transform of a function f(r) is given by

${\displaystyle F(y)=2\int _{y}^{\infty }{\frac {f(r)r}{\sqrt {r^{2}-y^{2}}}}\,dr.}$
Assuming that f(r) drops to zero more quickly than 1/r, the inverse Abel transform is given by

${\displaystyle f(r)=-{\frac {1}{\pi }}\int _{r}^{\infty }{\frac {dF}{dy}}\,{\frac {dy}{\sqrt {y^{2}-r^{2}}}}.}$
In image analysis, the forward Abel transform is used to project an optically thin, axially symmetric emission function onto a plane, and the inverse Abel transform is used to calculate the emission function given a projection (i.e. a scan or a photograph) of that emission function.

In absorption spectroscopy of cylindrical flames or plumes, the forward Abel transform is the integrated absorbance along a ray with closest distance y from the center of the flame, while the inverse Abel transform gives the local absorption coefficient at a distance r from the center. Abel transform is limited to applications with axially symmetric geometries. For more general asymmetrical cases, more general-oriented reconstruction algorithms such as algebraic reconstruction technique (ART), maximum likelihood expectation maximization (MLEM), filtered back-projection (FBP) algorithms should be employed.

In recent years, the inverse Abel transform (and its variants) has become the cornerstone of data analysis in photofragment-ion imaging and photoelectron imaging. Among recent most notable extensions of inverse Abel transform are the "onion peeling" and "basis set expansion" (BASEX) methods of photoelectron and photoion image analysis.


£#h5#£Geometrical interpretation£#/h5#£
In two dimensions, the Abel transform F(y) can be interpreted as the projection of a circularly symmetric function f(r) along a set of parallel lines of sight at a distance y from the origin. Referring to the figure on the right, the observer (I) will see

${\displaystyle F(y)=\int _{-\infty }^{\infty }f\left({\sqrt {x^{2}+y^{2}}}\right)\,dx,}$
where f(r) is the circularly symmetric function represented by the gray color in the figure. It is assumed that the observer is actually at x = ∞, so that the limits of integration are ±∞, and all lines of sight are parallel to the x axis. Realizing that the radius r is related to x and y as r2 = x2 + y2, it follows that

${\displaystyle dx={\frac {r\,dr}{\sqrt {r^{2}-y^{2}}}}}$
for x > 0. Since f(r) is an even function in x, we may write

${\displaystyle F(y)=2\int _{0}^{\infty }f\left({\sqrt {x^{2}+y^{2}}}\right)\,dx=2\int _{|y|}^{\infty }f(r)\,{\frac {r\,dr}{\sqrt {r^{2}-y^{2}}}},}$
which yields the Abel transform of f(r).

The Abel transform may be extended to higher dimensions. Of particular interest is the extension to three dimensions. If we have an axially symmetric function f(ρ, z), where ρ2 = x2 + y2 is the cylindrical radius, then we may want to know the projection of that function onto a plane parallel to the z axis. Without loss of generality, we can take that plane to be the yz plane, so that

${\displaystyle F(y,z)=\int _{-\infty }^{\infty }f(\rho ,z)\,dx=2\int _{y}^{\infty }{\frac {f(\rho ,z)\rho \,d\rho }{\sqrt {\rho ^{2}-y^{2}}}},}$
which is just the Abel transform of f(ρ, z) in ρ and y.

A particular type of axial symmetry is spherical symmetry. In this case, we have a function f(r), where r2 = x2 + y2 + z2. The projection onto, say, the yz plane will then be circularly symmetric and expressible as F(s), where s2 = y2 + z2. Carrying out the integration, we have

${\displaystyle F(s)=\int _{-\infty }^{\infty }f(r)\,dx=2\int _{s}^{\infty }{\frac {f(r)r\,dr}{\sqrt {r^{2}-s^{2}}}},}$
which is again, the Abel transform of f(r) in r and s.


£#h5#£Verification of the inverse Abel transform£#/h5#£
Assuming ${\displaystyle f}$ is continuously differentiable and ${\displaystyle f}$ , ${\displaystyle f'}$ drop to zero faster than ${\displaystyle 1/r}$ , we can set ${\displaystyle u=f(r)}$ and ${\displaystyle v={\sqrt {r^{2}-y^{2}}}}$ . Integration by parts then yields

${\displaystyle F(y)=-2\int _{y}^{\infty }f'(r){\sqrt {r^{2}-y^{2}}}\,dr.}$
Differentiating formally,

${\displaystyle F'(y)=2y\int _{y}^{\infty }{\frac {f'(r)}{\sqrt {r^{2}-y^{2}}}}\,dr.}$
Now substitute this into the inverse Abel transform formula:

${\displaystyle -{\frac {1}{\pi }}\int _{r}^{\infty }{\frac {F'(y)}{\sqrt {y^{2}-r^{2}}}}\,dy=\int _{r}^{\infty }\int _{y}^{\infty }{\frac {-2y}{\pi {\sqrt {(y^{2}-r^{2})(s^{2}-y^{2})}}}}f'(s)\,dsdy.}$
By Fubini's theorem, the last integral equals

${\displaystyle \int _{r}^{\infty }\int _{r}^{s}{\frac {-2y}{\pi {\sqrt {(y^{2}-r^{2})(s^{2}-y^{2})}}}}\,dyf'(s)\,ds=\int _{r}^{\infty }(-1)f'(s)\,ds=f(r).}$

£#h5#£Generalization of the Abel transform to discontinuous F(y)£#/h5#£
Consider the case where ${\displaystyle F(y)}$ is discontinuous at ${\displaystyle y=y_{\Delta }}$ , where it abruptly changes its value by a finite amount ${\displaystyle \Delta F}$ . That is, ${\displaystyle y_{\Delta }}$ and ${\displaystyle \Delta F}$ are defined by ${\displaystyle \Delta F\equiv \lim _{\epsilon \rightarrow 0}[F(y_{\Delta }-\epsilon )-F(y_{\Delta }+\epsilon )]}$ . Such a situation is encountered in tethered polymers (Polymer brush) exhibiting a vertical phase separation, where ${\displaystyle F(y)}$ stands for the polymer density profile and ${\displaystyle f(r)}$ is related to the spatial distribution of terminal, non-tethered monomers of the polymers.

The Abel transform of a function f(r) is under these circumstances again given by:

${\displaystyle F(y)=2\int _{y}^{\infty }{\frac {f(r)r\,dr}{\sqrt {r^{2}-y^{2}}}}.}$
Assuming f(r) drops to zero more quickly than 1/r, the inverse Abel transform is however given by

${\displaystyle f(r)=\left[{\frac {1}{2}}\delta (r-y_{\Delta }){\sqrt {1-(y_{\Delta }/r)^{2}}}-{\frac {1}{\pi }}{\frac {H(y_{\Delta }-r)}{\sqrt {y_{\Delta }^{2}-r^{2}}}}\right]\Delta F-{\frac {1}{\pi }}\int _{r}^{\infty }{\frac {dF}{dy}}{\frac {dy}{\sqrt {y^{2}-r^{2}}}}.}$
where ${\displaystyle \delta }$ is the Dirac delta function and ${\displaystyle H(x)}$ the Heaviside step function. The extended version of the Abel transform for discontinuous F is proven upon applying the Abel transform to shifted, continuous ${\displaystyle F(y)}$ , and it reduces to the classical Abel transform when ${\displaystyle \Delta F=0}$ . If ${\displaystyle F(y)}$ has more than a single discontinuity, one has to introduce shifts for any of them to come up with a generalized version of the inverse Abel transform which contains n additional terms, each of them corresponding to one of the n discontinuities.


£#h5#£Relationship to other integral transforms£#/h5#£
£#h5#£Relationship to the Fourier and Hankel transforms£#/h5#£
The Abel transform is one member of the FHA cycle of integral operators. For example, in two dimensions, if we define A as the Abel transform operator, F as the Fourier transform operator and H as the zeroth-order Hankel transform operator, then the special case of the projection-slice theorem for circularly symmetric functions states that

${\displaystyle FA=H.}$
In other words, applying the Abel transform to a 1-dimensional function and then applying the Fourier transform to that result is the same as applying the Hankel transform to that function. This concept can be extended to higher dimensions.


£#h5#£Relationship to the Radon transform£#/h5#£
Abel transform can be viewed as the Radon transform of an isotropic 2D function f(r). As f(r) is isotropic, its Radon transform is the same at different angles of the viewing axis. Thus, the Abel transform is a function of the distance along the viewing axis only.


£#h5#£See also£#/h5#£ £#ul#££#li#£GPS radio occultation£#/li#££#/ul#£
£#h5#£References£#/h5#£ £#ul#££#li#£Bracewell, R. (1965). The Fourier Transform and its Applications. New York: McGraw-Hill. ISBN 0-07-007016-4.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Abel, N. H. Oeuvres Completes (Ed. L. Sylow and S. Lie). New York: Johnson Reprint Corp., pp. 11 and 97, 1988.£#/li#££#li#£Arfken, G. and Weber, H. J. Mathematical Methods for Physicists, 6th ed. Orlando, FL: Academic Press, p. 1014, 2005.£#/li#££#li#£Binney, J. and Tremaine, S. Galactic Dynamics. Princeton, NJ: Princeton University Press, p. 651, 1987.£#/li#££#li#£Bracewell, R. The Fourier Transform and Its Applications, 3rd ed. New York: McGraw-Hill, pp. 262-266, 1999.£#/li#££#li#£Hilfer, R. (Ed.). Applications of Fractional Calculus in Physics. Singapore: World Scientific, pp. 3-4, 2000.£#/li#££#li#£Liouville, J. "Memoire sur quelques quéstions de géométrie et de mécanique, et sur un nouveau genre pour réspondre ces quéstions." J. École Polytech. 13, 1-69, 1832.£#/li#££#li#£Lützen, J. Joseph Liouville, 1809-1882. Master of Pure and Applied Mathematics. New York: Springer-Verlag, p. 314, 1990.£#/li#££#li#£Whittaker, E. T. and Robinson, G. The Calculus of Observations: A Treatise on Numerical Mathematics, 4th ed. New York: Dover, pp. 376-377, 1967.£#/li#££#li#£ Abel, N. H. Oeuvres Completes (Ed. L. Sylow and S. Lie). New York: Johnson Reprint Corp., pp. 11 and 97, 1988. £#/li#££#li#£ Arfken, G. and Weber, H. J. Mathematical Methods for Physicists, 6th ed. Orlando, FL: Academic Press, p. 1014, 2005. £#/li#££#li#£ Binney, J. and Tremaine, S. Galactic Dynamics. Princeton, NJ: Princeton University Press, p. 651, 1987. £#/li#££#li#£ Bracewell, R. The Fourier Transform and Its Applications, 3rd ed. New York: McGraw-Hill, pp. 262-266, 1999. £#/li#££#li#£ Hilfer, R. (Ed.). Applications of Fractional Calculus in Physics. Singapore: World Scientific, pp. 3-4, 2000. £#/li#££#li#£ Liouville, J. "Memoire sur quelques quéstions de géométrie et de mécanique, et sur un nouveau genre pour réspondre ces quéstions." J. École Polytech. 13, 1-69, 1832. £#/li#££#li#£ Lützen, J. Joseph Liouville, 1809-1882. Master of Pure and Applied Mathematics. New York: Springer-Verlag, p. 314, 1990. £#/li#££#li#£ Whittaker, E. T. and Robinson, G. The Calculus of Observations: A Treatise on Numerical Mathematics, 4th ed. New York: Dover, pp. 376-377, 1967. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Integral Transforms > General Integral Transforms £#/li#££#/ul#£




£#h3#£Abi-Khuzam Inequality£#/h3#£

£#h5#£ References £#/h5#£ £#ul#££#li#£Abi-Khuzam, F. "Proof of Yff's Conjecture on the Brocard Angle of a Triangle." Elem. Math. 29, 141-142, 1974.£#/li#££#li#£Abi-Khuzam, F. F. and Boghossian, A. B. "Some Recent Geometric Inequalities." Amer. Math. Monthly 96, 576-589, 1989.£#/li#££#li#£Flanders, H. "Review of 'Problems and Theorems in Analysis,' by Pólya and Szegö." Bull. Amer. Math. Soc. 86, 53-62, 1978.£#/li#££#li#£Klamkin, M. S. "On Yff's Inequality for the Brocard Angle of a Triangle." Elem. Math. 32, 188, 1977.£#/li#££#li#£Kuipers, L. "Extension of Abi-Khuzam's Inequality to More than Four Angles." Nieuw Tijdschr. Wisk. 69, 166-169, 1981-1982.£#/li#££#li#£Sloane, N. J. A. Sequence A127205 in "The On-Line Encyclopedia of Integer Sequences."£#/li#££#li#£ Abi-Khuzam, F. "Proof of Yff's Conjecture on the Brocard Angle of a Triangle." Elem. Math. 29, 141-142, 1974. £#/li#££#li#£ Abi-Khuzam, F. F. and Boghossian, A. B. "Some Recent Geometric Inequalities." Amer. Math. Monthly 96, 576-589, 1989. £#/li#££#li#£ Flanders, H. "Review of 'Problems and Theorems in Analysis,' by Pólya and Szegö." Bull. Amer. Math. Soc. 86, 53-62, 1978. £#/li#££#li#£ Klamkin, M. S. "On Yff's Inequality for the Brocard Angle of a Triangle." Elem. Math. 32, 188, 1977. £#/li#££#li#£ Kuipers, L. "Extension of Abi-Khuzam's Inequality to More than Four Angles." Nieuw Tijdschr. Wisk. 69, 166-169, 1981-1982. £#/li#££#li#£ Sloane, N. J. A. Sequence A127205 in "The On-Line Encyclopedia of Integer Sequences." £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Inequalities £#/li#££#/ul#£




£#h3#£Ablowitz-Ramani-Segur Conjecture£#/h3#£

£#h5#£ References £#/h5#£ £#ul#££#li#£Tabor, M. Chaos and Integrability in Nonlinear Dynamics: An Introduction. New York: Wiley, p. 351, 1989.£#/li#££#li#£ Tabor, M. Chaos and Integrability in Nonlinear Dynamics: An Introduction. New York: Wiley, p. 351, 1989. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Differential Equations > Partial Differential Equations £#/li#££#li#£ Foundations of Mathematics > Mathematical Problems > Unsolved Problems £#/li#££#/ul#£




£#h3#£Absolute Convergence£#/h3#£

In mathematics, an infinite series of numbers is said to converge absolutely (or to be absolutely convergent) if the sum of the absolute values of the summands is finite. More precisely, a real or complex series ${\displaystyle \textstyle \sum _{n=0}^{\infty }a_{n}}$ is said to converge absolutely if ${\displaystyle \textstyle \sum _{n=0}^{\infty }\left|a_{n}\right|=L}$ for some real number ${\displaystyle \textstyle L.}$ Similarly, an improper integral of a function, ${\displaystyle \textstyle \int _{0}^{\infty }f(x)\,dx,}$ is said to converge absolutely if the integral of the absolute value of the integrand is finite—that is, if ${\displaystyle \textstyle \int _{0}^{\infty }|f(x)|dx=L.}$

Absolute convergence is important for the study of infinite series because its definition is strong enough to have properties of finite sums that not all convergent series possess - a convergent series that is not absolutely convergent is called conditionally convergent, while absolutely convergent series behave "nicely". For instance, rearrangements do not change the value of the sum. This is not true for conditionally convergent series: The alternating harmonic series ${\textstyle 1-{\frac {1}{2}}+{\frac {1}{3}}-{\frac {1}{4}}+{\frac {1}{5}}-{\frac {1}{6}}+\cdots }$ converges to ${\displaystyle \ln 2,}$ while its rearrangement ${\textstyle 1+{\frac {1}{3}}-{\frac {1}{2}}+{\frac {1}{5}}+{\frac {1}{7}}-{\frac {1}{4}}+\cdots }$ (in which the repeating pattern of signs is two positive terms followed by one negative term) converges to ${\textstyle {\frac {3}{2}}\ln 2.}$


£#h5#£Background£#/h5#£
In finite sums, the order in which terms are added does not matter. 1 + 2 + 3 is the same as 3 + 2 + 1. However, this is not true when adding infinitely many numbers, and wrongly assuming that it is true can lead to apparent paradoxes. One classic example is the alternating sum

${\displaystyle S=1-1+1-1+1-1...}$

whose terms alternate between +1 and -1. What is the value of S? One way to evaluate S is to group the first and second term, the third and fourth, and so on:

${\displaystyle S_{1}=(1-1)+(1-1)+(1-1)....=0+0+0...=0}$

But another way to evaluate S is to leave the first term alone and group the second and third term, then the fourth and fifth term, and so on:

${\displaystyle S_{2}=1+(-1+1)+(-1+1)+(-1+1)....=1+0+0+0...=1}$

This leads to an apparent paradox: does ${\displaystyle S=0}$ or ${\displaystyle S=1}$ ?

The answer is that because S is not absolutely convergent, rearranging its terms changes the value of the sum. This means ${\displaystyle S_{1}}$ and ${\displaystyle S_{2}}$ are not equal. In fact, the series ${\displaystyle 1-1+1-1+...}$ does not converge, so S does not have a value to find in the first place. A series that is absolutely convergent does not have this problem: rearranging its terms does not change the value of the sum.


£#h5#£Explanation£#/h5#£
This is an example of a mathematical sleight of hand. If the terms of S are rearranged in such a way that every term remains in its original position, one finds that S is either the infinite series

${\displaystyle S=1-1+1-1+...+1-1+1-1}$

or with equal possibility, that

${\displaystyle S=1-1+1-1+...+1-1+1}$

Evaluating S as before, by grouping every -1 with the +1 preceding it or by grouping every +1 except the first with the -1 preceding it, gives in the first case:

${\displaystyle S_{1}=(1-1)+....+(1-1)=0+....+0=0}$
${\displaystyle S_{2}=1+(-1+1)+....+(-1+1)-1=1+0+....+0-1=1-1=0}$

and in the second case:

${\displaystyle S_{1}=(1-1)+....+(1-1)+1=0+....+0+1=1}$
${\displaystyle S_{2}=1+(-1+1)+....+(-1+1)=1+0+...+0=1}$

This reveals the trick: the definition of S was interpreted as defining its last term as negative when evaluating ${\displaystyle S_{1}=0}$ but positive when evaluating ${\displaystyle S_{2}=1}$ when in fact the definition of S didn't define (and the rearrangement was independent of) either option.


£#h5#£Definition for real and complex numbers£#/h5#£
A sum of real numbers or complex numbers ${\textstyle \sum _{n=0}^{\infty }a_{n}}$ is absolutely convergent if the sum of the absolute values of the terms ${\textstyle \sum _{n=0}^{\infty }|a_{n}|}$ converges.


£#h5#£Sums of more general elements£#/h5#£
The same definition can be used for series ${\textstyle \sum _{n=0}^{\infty }a_{n}}$ whose terms ${\displaystyle a_{n}}$ are not numbers but rather elements of an arbitrary abelian topological group. In that case, instead of using the absolute value, the definition requires the group to have a norm, which is a positive real-valued function ${\textstyle \|\cdot \|:G\to \mathbb {R} _{+}}$ on an abelian group ${\displaystyle G}$ (written additively, with identity element 0) such that:

£#li#£The norm of the identity element of ${\displaystyle G}$ is zero: ${\displaystyle \|0\|=0.}$ £#/li#£ £#li#£For every ${\displaystyle x\in G,}$ ${\displaystyle \|x\|=0}$ implies ${\displaystyle x=0.}$ £#/li#£ £#li#£For every ${\displaystyle x\in G,}$ ${\displaystyle \|-x\|=\|x\|.}$ £#/li#£ £#li#£For every ${\displaystyle x,y\in G,}$ ${\displaystyle \|x+y\|\leq \|x\|+\|y\|.}$ £#/li#£
In this case, the function ${\displaystyle d(x,y)=\|x-y\|}$ induces the structure of a metric space (a type of topology) on ${\displaystyle G.}$

Then, a ${\displaystyle G}$ -valued series is absolutely convergent if ${\textstyle \sum _{n=0}^{\infty }\|a_{n}\|<\infty .}$

In particular, these statements apply using the norm ${\displaystyle |x|}$ (absolute value) in the space of real numbers or complex numbers.


£#h5#£In topological vector spaces£#/h5#£
If ${\displaystyle X}$ is a topological vector space (TVS) and ${\textstyle \left(x_{\alpha }\right)_{\alpha \in A}}$ is a (possibly uncountable) family in ${\displaystyle X}$ then this family is absolutely summable if

£#li#£ ${\textstyle \left(x_{\alpha }\right)_{\alpha \in A}}$ is summable in ${\displaystyle X}$ (that is, if the limit ${\textstyle \lim _{H\in {\mathcal {F}}(A)}x_{H}}$ of the net ${\displaystyle \left(x_{H}\right)_{H\in {\mathcal {F}}(A)}}$ converges in ${\displaystyle X,}$ where ${\displaystyle {\mathcal {F}}(A)}$ is the directed set of all finite subsets of ${\displaystyle A}$ directed by inclusion ${\displaystyle \subseteq }$ and ${\textstyle x_{H}:=\sum _{i\in H}x_{i}}$ ), and£#/li#£ £#li#£for every continuous seminorm ${\displaystyle p}$ on ${\displaystyle X,}$ the family ${\textstyle \left(p\left(x_{\alpha }\right)\right)_{\alpha \in A}}$ is summable in ${\displaystyle \mathbb {R} .}$ £#/li#£
If ${\displaystyle X}$ is a normable space and if ${\textstyle \left(x_{\alpha }\right)_{\alpha \in A}}$ is an absolutely summable family in ${\displaystyle X,}$ then necessarily all but a countable collection of ${\displaystyle x_{\alpha }}$ 's are 0.

Absolutely summable families play an important role in the theory of nuclear spaces.


£#h5#£Relation to convergence£#/h5#£
If ${\displaystyle G}$ is complete with respect to the metric ${\displaystyle d,}$ then every absolutely convergent series is convergent. The proof is the same as for complex-valued series: use the completeness to derive the Cauchy criterion for convergence—a series is convergent if and only if its tails can be made arbitrarily small in norm—and apply the triangle inequality.

In particular, for series with values in any Banach space, absolute convergence implies convergence. The converse is also true: if absolute convergence implies convergence in a normed space, then the space is a Banach space.

If a series is convergent but not absolutely convergent, it is called conditionally convergent. An example of a conditionally convergent series is the alternating harmonic series. Many standard tests for divergence and convergence, most notably including the ratio test and the root test, demonstrate absolute convergence. This is because a power series is absolutely convergent on the interior of its disk of convergence.


£#h5#£Proof that any absolutely convergent series of complex numbers is convergent£#/h5#£
Suppose that ${\textstyle \sum \left|a_{k}\right|,a_{k}\in \mathbb {C} }$ is convergent. Then equivalently, ${\textstyle \sum \left[\operatorname {Re} \left(a_{k}\right)^{2}+\operatorname {Im} \left(a_{k}\right)^{2}\right]^{1/2}}$ is convergent, which implies that ${\textstyle \sum \left|\operatorname {Re} \left(a_{k}\right)\right|}$ and ${\textstyle \sum \left|\operatorname {Im} \left(a_{k}\right)\right|}$ converge by termwise comparison of non-negative terms. It suffices to show that the convergence of these series implies the convergence of ${\textstyle \sum \operatorname {Re} \left(a_{k}\right)}$ and ${\textstyle \sum \operatorname {Im} \left(a_{k}\right),}$ for then, the convergence of ${\textstyle \sum a_{k}=\sum \operatorname {Re} \left(a_{k}\right)+i\sum \operatorname {Im} \left(a_{k}\right)}$ would follow, by the definition of the convergence of complex-valued series.

The preceding discussion shows that we need only prove that convergence of ${\textstyle \sum \left|a_{k}\right|,a_{k}\in \mathbb {R} }$ implies the convergence of ${\textstyle \sum a_{k}.}$

Let ${\textstyle \sum \left|a_{k}\right|,a_{k}\in \mathbb {R} }$ be convergent. Since ${\displaystyle 0\leq a_{k}+\left|a_{k}\right|\leq 2\left|a_{k}\right|,}$ we have

Since ${\textstyle \sum 2\left|a_{k}\right|}$ is convergent, ${\textstyle s_{n}=\sum _{k=1}^{n}\left(a_{k}+\left|a_{k}\right|\right)}$ is a bounded monotonic sequence of partial sums, and ${\textstyle \sum \left(a_{k}+\left|a_{k}\right|\right)}$ must also converge. Noting that ${\textstyle \sum a_{k}=\sum \left(a_{k}+\left|a_{k}\right|\right)-\sum \left|a_{k}\right|}$ is the difference of convergent series, we conclude that it too is a convergent series, as desired.
£#h5#£Alternative proof using the Cauchy criterion and triangle inequality£#/h5#£
By applying the Cauchy criterion for the convergence of a complex series, we can also prove this fact as a simple implication of the triangle inequality. By the Cauchy criterion, ${\textstyle \sum |a_{i}|}$ converges if and only if for any ${\displaystyle \varepsilon >0,}$ there exists ${\displaystyle N}$ such that ${\textstyle \left|\sum _{i=m}^{n}\left|a_{i}\right|\right|=\sum _{i=m}^{n}|a_{i}|<\varepsilon }$ for any ${\displaystyle n>m\geq N.}$ But the triangle inequality implies that ${\textstyle {\big |}\sum _{i=m}^{n}a_{i}{\big |}\leq \sum _{i=m}^{n}|a_{i}|,}$ so that ${\textstyle \left|\sum _{i=m}^{n}a_{i}\right|<\varepsilon }$ for any ${\displaystyle n>m\geq N,}$ which is exactly the Cauchy criterion for ${\textstyle \sum a_{i}.}$


£#h5#£Proof that any absolutely convergent series in a Banach space is convergent£#/h5#£
The above result can be easily generalized to every Banach space ${\displaystyle (X,\|\,\cdot \,\|).}$ Let ${\textstyle \sum x_{n}}$ be an absolutely convergent series in ${\displaystyle X.}$ As ${\textstyle \sum _{k=1}^{n}\|x_{k}\|}$ is a Cauchy sequence of real numbers, for any ${\displaystyle \varepsilon >0}$ and large enough natural numbers ${\displaystyle m>n}$ it holds:

By the triangle inequality for the norm ǁ⋅ǁ, one immediately gets:

which means that ${\textstyle \sum _{k=1}^{n}x_{k}}$ is a Cauchy sequence in ${\displaystyle X,}$ hence the series is convergent in ${\displaystyle X.}$
£#h5#£Rearrangements and unconditional convergence£#/h5#£
£#h5#£Real and complex numbers£#/h5#£
When a series of real or complex numbers is absolutely convergent, any rearrangement or reordering of that series' terms will still converge to the same value. This fact is one reason absolutely convergent series are useful: showing a series is absolutely convergent allows terms to be paired or rearranged in convenient ways without changing the sum's value.

The Riemann rearrangement theorem shows that the converse is also true: every real or complex-valued series whose terms cannot be reordered to give a different value is absolutely convergent.


£#h5#£Series with coefficients in more general space£#/h5#£
The term unconditional convergence is used to refer to a series where any rearrangement of its terms still converges to the same value. For any series with values in a normed abelian group ${\displaystyle G}$ , as long as ${\displaystyle G}$ is complete, every series which converges absolutely also converges unconditionally.

Stated more formally:

For series with more general coefficients, the converse is more complicated. As stated in the previous section, for real-valued and complex-valued series, unconditional convergence always implies absolute convergence. However, in the more general case of a series with values in any normed abelian group ${\displaystyle G}$ , the converse does not always hold: there can exist series which are not absolutely convergent, yet unconditionally convergent.

For example, in the Banach space ℓ∞, one series which is unconditionally convergent but not absolutely convergent is:

where ${\displaystyle \{e_{n}\}_{n=1}^{\infty }}$ is an orthonormal basis. A theorem of A. Dvoretzky and C. A. Rogers asserts that every infinite-dimensional Banach space has an unconditionally convergent series that is not absolutely convergent.


£#h5#£Proof of the theorem£#/h5#£
For any ${\displaystyle \varepsilon >0,}$ we can choose some ${\displaystyle \kappa _{\varepsilon },\lambda _{\varepsilon }\in \mathbb {N} ,}$ such that:

Let

where ${\displaystyle \sigma ^{-1}\left(\left\{1,\ldots ,N_{\varepsilon }\right\}\right)=\left\{\sigma ^{-1}(1),\ldots ,\sigma ^{-1}\left(N_{\varepsilon }\right)\right\}}$ so that ${\displaystyle M_{\sigma ,\varepsilon }}$ is the smallest natural number such that the list ${\displaystyle a_{\sigma (0)},\ldots ,a_{\sigma \left(M_{\sigma ,\varepsilon }\right)}}$ includes all of the terms ${\displaystyle a_{0},\ldots ,a_{N_{\varepsilon }}}$ (and possibly others).
Finally for any integer ${\displaystyle N>M_{\sigma ,\varepsilon }}$ let

so that and thus
This shows that

that is:
Q.E.D.


£#h5#£Products of series£#/h5#£
The Cauchy product of two series converges to the product of the sums if at least one of the series converges absolutely. That is, suppose that

The Cauchy product is defined as the sum of terms ${\displaystyle c_{n}}$ where:

If either the ${\displaystyle a_{n}}$ or ${\displaystyle b_{n}}$ sum converges absolutely then


£#h5#£Absolute convergence over sets£#/h5#£
A generalization of the absolute convergence of a series, is the absolute convergence of a sum of a function over a set. We can first consider a countable set ${\displaystyle X}$ and a function ${\displaystyle f:X\to \mathbb {R} .}$ We will give a definition below of the sum of ${\displaystyle f}$ over ${\displaystyle X,}$ written as ${\textstyle \sum _{x\in X}f(x).}$

First note that because no particular enumeration (or "indexing") of ${\displaystyle X}$ has yet been specified, the series ${\textstyle \sum _{x\in X}f(x)}$ cannot be understood by the more basic definition of a series. In fact, for certain examples of ${\displaystyle X}$ and ${\displaystyle f,}$ the sum of ${\displaystyle f}$ over ${\displaystyle X}$ may not be defined at all, since some indexing may produce a conditionally convergent series.

Therefore we define ${\textstyle \sum _{x\in X}f(x)}$ only in the case where there exists some bijection ${\displaystyle g:\mathbb {Z} ^{+}\to X}$ such that ${\textstyle \sum _{n=1}^{\infty }f(g(n))}$ is absolutely convergent. Note that here, "absolutely convergent" uses the more basic definition, applied to an indexed series. In this case, the value of the sum of ${\displaystyle f}$ over ${\displaystyle X}$ is defined by

Note that because the series is absolutely convergent, then every rearrangement is identical to a different choice of bijection ${\displaystyle g.}$ Since all of these sums have the same value, then the sum of ${\displaystyle f}$ over ${\displaystyle X}$ is well-defined.

Even more generally we may define the sum of ${\displaystyle f}$ over ${\displaystyle X}$ when ${\displaystyle X}$ is uncountable. But first we define what it means for the sum to be convergent.

Let ${\displaystyle X}$ be any set, countable or uncountable, and ${\displaystyle f:X\to \mathbb {R} }$ a function. We say that the sum of ${\displaystyle f}$ over ${\displaystyle X}$ converges absolutely if

There is a theorem which states that, if the sum of ${\displaystyle f}$ over ${\displaystyle X}$ is absolutely convergent, then ${\displaystyle f}$ takes non-zero values on a set that is at most countable. Therefore, the following is a consistent definition of the sum of ${\displaystyle f}$ over ${\displaystyle X}$ when the sum is absolutely convergent.

Note that the final series uses the definition of a series over a countable set.

Some authors define an iterated sum ${\textstyle \sum _{m=1}^{\infty }\sum _{n=1}^{\infty }a_{m,n}}$ to be absolutely convergent if the iterated series ${\textstyle \sum _{m=1}^{\infty }\sum _{n=1}^{\infty }|a_{m,n}|<\infty .}$ This is in fact equivalent to the absolute convergence of ${\textstyle \sum _{(m,n)\in \mathbb {N} \times \mathbb {N} }a_{m,n}.}$ That is to say, if the sum of ${\displaystyle f}$ over ${\displaystyle X,}$ ${\textstyle \sum _{(m,n)\in \mathbb {N} \times \mathbb {N} }a_{m,n},}$ converges absolutely, as defined above, then the iterated sum ${\textstyle \sum _{m=1}^{\infty }\sum _{n=1}^{\infty }a_{m,n}}$ converges absolutely, and vice versa.


£#h5#£Absolute convergence of integrals£#/h5#£
The integral ${\textstyle \int _{A}f(x)\,dx}$ of a real or complex-valued function is said to converge absolutely if ${\textstyle \int _{A}\left|f(x)\right|\,dx<\infty .}$ One also says that ${\displaystyle f}$ is absolutely integrable. The issue of absolute integrability is intricate and depends on whether the Riemann, Lebesgue, or Kurzweil-Henstock (gauge) integral is considered; for the Riemann integral, it also depends on whether we only consider integrability in its proper sense ( ${\displaystyle f}$ and ${\displaystyle A}$ both bounded), or permit the more general case of improper integrals.

As a standard property of the Riemann integral, when ${\displaystyle A=[a,b]}$ is a bounded interval, every continuous function is bounded and (Riemann) integrable, and since ${\displaystyle f}$ continuous implies ${\displaystyle |f|}$ continuous, every continuous function is absolutely integrable. In fact, since ${\displaystyle g\circ f}$ is Riemann integrable on ${\displaystyle [a,b]}$ if ${\displaystyle f}$ is (properly) integrable and ${\displaystyle g}$ is continuous, it follows that ${\displaystyle |f|=|\cdot |\circ f}$ is properly Riemann integrable if ${\displaystyle f}$ is. However, this implication does not hold in the case of improper integrals. For instance, the function ${\textstyle f:[1,\infty )\to \mathbb {R} :x\mapsto {\frac {\sin x}{x}}}$ is improperly Riemann integrable on its unbounded domain, but it is not absolutely integrable:

Indeed, more generally, given any series ${\textstyle \sum _{n=0}^{\infty }a_{n}}$ one can consider the associated step function ${\displaystyle f_{a}:[0,\infty )\to \mathbb {R} }$ defined by ${\displaystyle f_{a}([n,n+1))=a_{n}.}$ Then ${\textstyle \int _{0}^{\infty }f_{a}\,dx}$ converges absolutely, converges conditionally or diverges according to the corresponding behavior of ${\textstyle \sum _{n=0}^{\infty }a_{n}.}$
The situation is different for the Lebesgue integral, which does not handle bounded and unbounded domains of integration separately (see below). The fact that the integral of ${\displaystyle |f|}$ is unbounded in the examples above implies that ${\displaystyle f}$ is also not integrable in the Lebesgue sense. In fact, in the Lebesgue theory of integration, given that ${\displaystyle f}$ is measurable, ${\displaystyle f}$ is (Lebesgue) integrable if and only if ${\displaystyle |f|}$ is (Lebesgue) integrable. However, the hypothesis that ${\displaystyle f}$ is measurable is crucial; it is not generally true that absolutely integrable functions on ${\displaystyle [a,b]}$ are integrable (simply because they may fail to be measurable): let ${\displaystyle S\subset [a,b]}$ be a nonmeasurable subset and consider ${\displaystyle f=\chi _{S}-1/2,}$ where ${\displaystyle \chi _{S}}$ is the characteristic function of ${\displaystyle S.}$ Then ${\displaystyle f}$ is not Lebesgue measurable and thus not integrable, but ${\displaystyle |f|\equiv 1/2}$ is a constant function and clearly integrable.

On the other hand, a function ${\displaystyle f}$ may be Kurzweil-Henstock integrable (gauge integrable) while ${\displaystyle |f|}$ is not. This includes the case of improperly Riemann integrable functions.

In a general sense, on any measure space ${\displaystyle A,}$ the Lebesgue integral of a real-valued function is defined in terms of its positive and negative parts, so the facts:

£#li#£ ${\displaystyle f}$ integrable implies ${\displaystyle |f|}$ integrable£#/li#£ £#li#£ ${\displaystyle f}$ measurable, ${\displaystyle |f|}$ integrable implies ${\displaystyle f}$ integrable£#/li#£
are essentially built into the definition of the Lebesgue integral. In particular, applying the theory to the counting measure on a set ${\displaystyle S,}$ one recovers the notion of unordered summation of series developed by Moore–Smith using (what are now called) nets. When ${\displaystyle S=\mathbb {N} }$ is the set of natural numbers, Lebesgue integrability, unordered summability and absolute convergence all coincide.

Finally, all of the above holds for integrals with values in a Banach space. The definition of a Banach-valued Riemann integral is an evident modification of the usual one. For the Lebesgue integral one needs to circumvent the decomposition into positive and negative parts with Daniell's more functional analytic approach, obtaining the Bochner integral.


£#h5#£See also£#/h5#£ £#ul#££#li#£Cauchy principal value – Method for assigning values to certain improper integrals which would otherwise be undefined£#/li#£ £#li#£Conditional convergence£#/li#£ £#li#£Convergence of Fourier series£#/li#£ £#li#£Fubini's theorem – Conditions for switching order of integration in calculus£#/li#£ £#li#£Modes of convergence (annotated index) – An annotated index of various modes of convergence£#/li#£ £#li#£Radius of convergence – Domain of convergence of power series£#/li#£ £#li#£Riemann series theorem – Unconditional series converge absolutely£#/li#£ £#li#£Unconditional convergence£#/li#£ £#li#£1/2 − 1/4 + 1/8 − 1/16 + · · ·£#/li#£ £#li#£1/2 + 1/4 + 1/8 + 1/16 + · · ·£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£
£#h5#£Works cited£#/h5#£ £#ul#££#li#£Schaefer, Helmut H.; Wolff, Manfred P. (1999). Topological Vector Spaces. GTM. Vol. 8 (Second ed.). New York, NY: Springer New York Imprint Springer. ISBN 978-1-4612-7155-0. OCLC 840278135.£#/li#££#/ul#£
£#h5#£General references£#/h5#£ £#ul#££#li#£Narici, Lawrence; Beckenstein, Edward (2011). Topological Vector Spaces. Pure and applied mathematics (Second ed.). Boca Raton, FL: CRC Press. ISBN 978-1584888666. OCLC 144216834.£#/li#£ £#li#£Walter Rudin, Principles of Mathematical Analysis (McGraw-Hill: New York, 1964).£#/li#£ £#li#£Pietsch, Albrecht (1979). Nuclear Locally Convex Spaces. Ergebnisse der Mathematik und ihrer Grenzgebiete. Vol. 66 (Second ed.). Berlin, New York: Springer-Verlag. ISBN 978-0-387-05644-9. OCLC 539541.£#/li#£ £#li#£Robertson, A. P. (1973). Topological vector spaces. Cambridge England: University Press. ISBN 0-521-29882-2. OCLC 589250.£#/li#£ £#li#£Ryan, Raymond A. (2002). Introduction to Tensor Products of Banach Spaces. Springer Monographs in Mathematics. London New York: Springer. ISBN 978-1-85233-437-6. OCLC 48092184.£#/li#£ £#li#£Trèves, François (2006) [1967]. Topological Vector Spaces, Distributions and Kernels. Mineola, N.Y.: Dover Publications. ISBN 978-0-486-45352-1. OCLC 853623322.£#/li#£ £#li#£Wong, Yau-Chuen (1979). Schwartz Spaces, Nuclear Spaces, and Tensor Products. Lecture Notes in Mathematics. Vol. 726. Berlin New York: Springer-Verlag. ISBN 978-3-540-09513-2. OCLC 5126158.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Bromwich, T. J. I'A. and MacRobert, T. M. "Absolute Convergence." Ch. 4 in An Introduction to the Theory of Infinite Series, 3rd ed. New York: Chelsea, pp. 69-77, 1991.£#/li#££#li#£Jeffreys, H. and Jeffreys, B. S. "Absolute Convergence." §1.051 in Methods of Mathematical Physics, 3rd ed. Cambridge, England: Cambridge University Press, p. 16, 1988.£#/li#££#li#£ Bromwich, T. J. I'A. and MacRobert, T. M. "Absolute Convergence." Ch. 4 in An Introduction to the Theory of Infinite Series, 3rd ed. New York: Chelsea, pp. 69-77, 1991. £#/li#££#li#£ Jeffreys, H. and Jeffreys, B. S. "Absolute Convergence." §1.051 in Methods of Mathematical Physics, 3rd ed. Cambridge, England: Cambridge University Press, p. 16, 1988. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Series > Convergence £#/li#££#/ul#£




£#h3#£Absolutely Continuous£#/h3#£

In calculus, absolute continuity is a smoothness property of functions that is stronger than continuity and uniform continuity. The notion of absolute continuity allows one to obtain generalizations of the relationship between the two central operations of calculus—differentiation and integration. This relationship is commonly characterized (by the fundamental theorem of calculus) in the framework of Riemann integration, but with absolute continuity it may be formulated in terms of Lebesgue integration. For real-valued functions on the real line, two interrelated notions appear: absolute continuity of functions and absolute continuity of measures. These two notions are generalized in different directions. The usual derivative of a function is related to the Radon–Nikodym derivative, or density, of a measure.

We have the following chains of inclusions for functions over a compact subset of the real line:

absolutely continuous ⊆ uniformly continuous ${\displaystyle =}$ continuous
and, for a compact interval,

continuously differentiable ⊆ Lipschitz continuous ⊆ absolutely continuous ⊆ bounded variation ⊆ differentiable almost everywhere

£#h5#£Absolute continuity of functions£#/h5#£
A continuous function fails to be absolutely continuous if it fails to be uniformly continuous, which can happen if the domain of the function is not compact – examples are tan(x) over [0, π/2), x2 over the entire real line, and sin(1/x) over (0, 1]. But a continuous function f can fail to be absolutely continuous even on a compact interval. It may not be "differentiable almost everywhere" (like the Weierstrass function, which is not differentiable anywhere). Or it may be differentiable almost everywhere and its derivative f ′ may be Lebesgue integrable, but the integral of f ′ differs from the increment of f (how much f changes over an interval). This happens for example with the Cantor function.


£#h5#£Definition£#/h5#£
Let ${\displaystyle I}$ be an interval in the real line ${\displaystyle \mathbb {R} }$ . A function ${\displaystyle f\colon I\to \mathbb {R} }$ is absolutely continuous on ${\displaystyle I}$ if for every positive number ${\displaystyle \varepsilon }$ , there is a positive number ${\displaystyle \delta }$ such that whenever a finite sequence of pairwise disjoint sub-intervals ${\displaystyle (x_{k},y_{k})}$ of ${\displaystyle I}$ with ${\displaystyle x_{k}<y_{k}\in I}$ satisfies

${\displaystyle \sum _{k}(y_{k}-x_{k})<\delta }$
then

${\displaystyle \sum _{k}|f(y_{k})-f(x_{k})|<\varepsilon .}$
The collection of all absolutely continuous functions on ${\displaystyle I}$ is denoted ${\displaystyle \operatorname {AC} (I)}$ .


£#h5#£Equivalent definitions£#/h5#£
The following conditions on a real-valued function f on a compact interval [a,b] are equivalent:

£#li#£f is absolutely continuous;£#/li#£ £#li#£f has a derivative f ′ almost everywhere, the derivative is Lebesgue integrable, and for all x on [a,b];£#/li#£ £#li#£there exists a Lebesgue integrable function g on [a,b] such that for all x in [a,b].£#/li#£
If these equivalent conditions are satisfied then necessarily g = f ′ almost everywhere.

Equivalence between (1) and (3) is known as the fundamental theorem of Lebesgue integral calculus, due to Lebesgue.

For an equivalent definition in terms of measures see the section Relation between the two notions of absolute continuity.


£#h5#£Properties£#/h5#£ £#ul#££#li#£The sum and difference of two absolutely continuous functions are also absolutely continuous. If the two functions are defined on a bounded closed interval, then their product is also absolutely continuous.£#/li#£ £#li#£If an absolutely continuous function is defined on a bounded closed interval and is nowhere zero then its reciprocal is absolutely continuous.£#/li#£ £#li#£Every absolutely continuous function (over a compact interval) is uniformly continuous and, therefore, continuous. Every Lipschitz-continuous function is absolutely continuous.£#/li#£ £#li#£If f: [a,b] → R is absolutely continuous, then it is of bounded variation on [a,b].£#/li#£ £#li#£If f: [a,b] → R is absolutely continuous, then it can be written as the difference of two monotonic nondecreasing absolutely continuous functions on [a,b].£#/li#£ £#li#£If f: [a,b] → R is absolutely continuous, then it has the Luzin N property (that is, for any ${\displaystyle N\subseteq [a,b]}$ such that ${\displaystyle \lambda (N)=0}$ , it holds that ${\displaystyle \lambda (f(N))=0}$ , where ${\displaystyle \lambda }$ stands for the Lebesgue measure on R).£#/li#£ £#li#£f: I → R is absolutely continuous if and only if it is continuous, is of bounded variation and has the Luzin N property.£#/li#££#/ul#£
£#h5#£Examples£#/h5#£
The following functions are uniformly continuous but not absolutely continuous:

£#ul#££#li#£the Cantor function on [0, 1] (it is of bounded variation but not absolutely continuous);£#/li#£ £#li#£the function on a finite interval containing the origin.£#/li#££#/ul#£
The following functions are absolutely continuous but not α-Hölder continuous:

£#ul#££#li#£the function f(x) = xβ on [0, c], for any 0 < β < α < 1£#/li#££#/ul#£
The following functions are absolutely continuous and α-Hölder continuous but not Lipschitz continuous:

£#ul#££#li#£the function f(x) = √x on [0, c], for α ≤ 1/2.£#/li#££#/ul#£
£#h5#£Generalizations£#/h5#£
Let (X, d) be a metric space and let I be an interval in the real line R. A function f: I → X is absolutely continuous on I if for every positive number ${\displaystyle \epsilon }$ , there is a positive number ${\displaystyle \delta }$ such that whenever a finite sequence of pairwise disjoint sub-intervals [xk, yk] of I satisfies

${\displaystyle \sum _{k}\left|y_{k}-x_{k}\right|<\delta }$
then

${\displaystyle \sum _{k}d\left(f(y_{k}),f(x_{k})\right)<\epsilon .}$
The collection of all absolutely continuous functions from I into X is denoted AC(I; X).

A further generalization is the space ACp(I; X) of curves f: I → X such that

${\displaystyle d\left(f(s),f(t)\right)\leq \int _{s}^{t}m(\tau )\,d\tau {\text{ for all }}[s,t]\subseteq I}$
for some m in the Lp space Lp(I).


£#h5#£Properties of these generalizations£#/h5#£ £#ul#££#li#£Every absolutely continuous function (over a compact interval) is uniformly continuous and, therefore, continuous. Every Lipschitz-continuous function is absolutely continuous.£#/li#£ £#li#£If f: [a,b] → X is absolutely continuous, then it is of bounded variation on [a,b].£#/li#£ £#li#£For f ∈ ACp(I; X), the metric derivative of f exists for λ-almost all times in I, and the metric derivative is the smallest m ∈ Lp(I; R) such that£#/li#££#/ul#£
£#h5#£Absolute continuity of measures£#/h5#£
£#h5#£Definition£#/h5#£
A measure ${\displaystyle \mu }$ on Borel subsets of the real line is absolutely continuous with respect to the Lebesgue measure ${\displaystyle \lambda }$ if for every ${\displaystyle \lambda }$ -measurable set ${\displaystyle A,}$ ${\displaystyle \lambda (A)=0}$ implies ${\displaystyle \mu (A)=0.}$ This is written as ${\displaystyle \mu \ll \lambda .}$ We say ${\displaystyle \mu }$ is dominated by ${\displaystyle \lambda .}$

In most applications, if a measure on the real line is simply said to be absolutely continuous — without specifying with respect to which other measure it is absolutely continuous — then absolute continuity with respect to the Lebesgue measure is meant.

The same principle holds for measures on Borel subsets of ${\displaystyle \mathbb {R} ^{n},n\geq 2.}$


£#h5#£Equivalent definitions£#/h5#£
The following conditions on a finite measure ${\displaystyle \mu }$ on Borel subsets of the real line are equivalent:

£#li#£ ${\displaystyle \mu }$ is absolutely continuous;£#/li#£ £#li#£for every positive number ${\displaystyle \varepsilon }$ there is a positive number ${\displaystyle \delta >0}$ such that ${\displaystyle \mu (A)<\varepsilon }$ for all Borel sets ${\displaystyle A}$ of Lebesgue measure less than ${\displaystyle \delta ;}$ £#/li#£ £#li#£there exists a Lebesgue integrable function ${\displaystyle g}$ on the real line such that for all Borel subsets ${\displaystyle A}$ of the real line.£#/li#£
For an equivalent definition in terms of functions see the section Relation between the two notions of absolute continuity.

Any other function satisfying (3) is equal to ${\displaystyle g}$ almost everywhere. Such a function is called Radon–Nikodym derivative, or density, of the absolutely continuous measure ${\displaystyle \mu .}$

Equivalence between (1), (2) and (3) holds also in ${\displaystyle \mathbb {R} ^{n}}$ for all ${\displaystyle n=1,2,3,\ldots .}$

Thus, the absolutely continuous measures on ${\displaystyle \mathbb {R} ^{n}}$ are precisely those that have densities; as a special case, the absolutely continuous probability measures are precisely the ones that have probability density functions.


£#h5#£Generalizations£#/h5#£
If ${\displaystyle \mu }$ and ${\displaystyle \nu }$ are two measures on the same measurable space ${\displaystyle (X,{\mathcal {A}}),}$ ${\displaystyle \mu }$ is said to be absolutely continuous with respect to ${\displaystyle \nu }$ if ${\displaystyle \mu (A)=0}$ for every set ${\displaystyle A}$ for which ${\displaystyle \nu (A)=0.}$ This is written as " ${\displaystyle \mu \ll \nu }$ ". That is:

When ${\displaystyle \mu \ll \nu ,}$ then ${\displaystyle \nu }$ is said to be dominating ${\displaystyle \mu .}$

Absolute continuity of measures is reflexive and transitive, but is not antisymmetric, so it is a preorder rather than a partial order. Instead, if ${\displaystyle \mu \ll \nu }$ and ${\displaystyle \nu \ll \mu ,}$ the measures ${\displaystyle \mu }$ and ${\displaystyle \nu }$ are said to be equivalent. Thus absolute continuity induces a partial ordering of such equivalence classes.

If ${\displaystyle \mu }$ is a signed or complex measure, it is said that ${\displaystyle \mu }$ is absolutely continuous with respect to ${\displaystyle \nu }$ if its variation ${\displaystyle |\mu |}$ satisfies ${\displaystyle |\mu |\ll \nu ;}$ equivalently, if every set ${\displaystyle A}$ for which ${\displaystyle \nu (A)=0}$ is ${\displaystyle \mu }$ -null.

The Radon–Nikodym theorem states that if ${\displaystyle \mu }$ is absolutely continuous with respect to ${\displaystyle \nu ,}$ and both measures are σ-finite, then ${\displaystyle \mu }$ has a density, or "Radon-Nikodym derivative", with respect to ${\displaystyle \nu ,}$ which means that there exists a ${\displaystyle \nu }$ -measurable function ${\displaystyle f}$ taking values in ${\displaystyle [0,+\infty ),}$ denoted by ${\displaystyle f=d\mu /d\nu ,}$ such that for any ${\displaystyle \nu }$ -measurable set ${\displaystyle A}$ we have


£#h5#£Singular measures£#/h5#£
Via Lebesgue's decomposition theorem, every σ-finite measure can be decomposed into the sum of an absolutely continuous measure and a singular measure with respect to another σ-finite measure. See singular measure for examples of measures that are not absolutely continuous.


£#h5#£Relation between the two notions of absolute continuity£#/h5#£
A finite measure μ on Borel subsets of the real line is absolutely continuous with respect to Lebesgue measure if and only if the point function

${\displaystyle F(x)=\mu ((-\infty ,x])}$
is an absolutely continuous real function. More generally, a function is locally (meaning on every bounded interval) absolutely continuous if and only if its distributional derivative is a measure that is absolutely continuous with respect to the Lebesgue measure.

If absolute continuity holds then the Radon–Nikodym derivative of μ is equal almost everywhere to the derivative of F.

More generally, the measure μ is assumed to be locally finite (rather than finite) and F(x) is defined as μ((0,x]) for x > 0, 0 for x = 0, and −μ((x,0]) for x < 0. In this case μ is the Lebesgue–Stieltjes measure generated by F. The relation between the two notions of absolute continuity still holds.


£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Ambrosio, Luigi; Gigli, Nicola; Savaré, Giuseppe (2005), Gradient Flows in Metric Spaces and in the Space of Probability Measures, ETH Zürich, Birkhäuser Verlag, Basel, ISBN 3-7643-2428-7£#/li#£ £#li#£Athreya, Krishna B.; Lahiri, Soumendra N. (2006), Measure theory and probability theory, Springer, ISBN 0-387-32903-X£#/li#£ £#li#£Leoni, Giovanni (2009), A First Course in Sobolev Spaces, Graduate Studies in Mathematics, American Mathematical Society, pp. xvi+607 ISBN 978-0-8218-4768-8, MR2527916, Zbl 1180.46001, MAA£#/li#£ £#li#£Nielsen, Ole A. (1997), An introduction to integration and measure theory, Wiley-Interscience, ISBN 0-471-59518-7£#/li#£ £#li#£Royden, H.L. (1988), Real Analysis (third ed.), Collier Macmillan, ISBN 0-02-404151-3£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Absolute continuity at Encyclopedia of Mathematics£#/li#£ £#li#£Topics in Real and Functional Analysis by Gerald Teschl£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Measure Theory £#/li#££#/ul#£




£#h3#£Absolutely Monotonic Function£#/h3#£

In mathematics, a monotonic function (or monotone function) is a function between ordered sets that preserves or reverses the given order. This concept first arose in calculus, and was later generalized to the more abstract setting of order theory.


£#h5#£In calculus and analysis£#/h5#£
In calculus, a function ${\displaystyle f}$ defined on a subset of the real numbers with real values is called monotonic if and only if it is either entirely non-increasing, or entirely non-decreasing. That is, as per Fig. 1, a function that increases monotonically does not exclusively have to increase, it simply must not decrease.

A function is called monotonically increasing (also increasing or non-decreasing), if for all ${\displaystyle x}$ and ${\displaystyle y}$ such that ${\displaystyle x\leq y}$ one has ${\displaystyle f\!\left(x\right)\leq f\!\left(y\right)}$ , so ${\displaystyle f}$ preserves the order (see Figure 1). Likewise, a function is called monotonically decreasing (also decreasing or non-increasing) if, whenever ${\displaystyle x\leq y}$ , then ${\displaystyle f\!\left(x\right)\geq f\!\left(y\right)}$ , so it reverses the order (see Figure 2).

If the order ${\displaystyle \leq }$ in the definition of monotonicity is replaced by the strict order ${\displaystyle <}$ , then one obtains a stronger requirement. A function with this property is called strictly increasing (also increasing). Again, by inverting the order symbol, one finds a corresponding concept called strictly decreasing (also decreasing). A function may be called strictly monotone if it is either strictly increasing or strictly decreasing. Functions that are strictly monotone are one-to-one (because for ${\displaystyle x}$ not equal to ${\displaystyle y}$ , either ${\displaystyle x<y}$ or ${\displaystyle x>y}$ and so, by monotonicity, either ${\displaystyle f\!\left(x\right)<f\!\left(y\right)}$ or ${\displaystyle f\!\left(x\right)>f\!\left(y\right)}$ , thus ${\displaystyle f\!\left(x\right)\neq f\!\left(y\right)}$ .)

If it is not clear that "increasing" and "decreasing" are taken to include the possibility of repeating the same value at successive arguments, one may use the terms weakly monotone, weakly increasing and weakly decreasing to stress this possibility.

The terms "non-decreasing" and "non-increasing" should not be confused with the (much weaker) negative qualifications "not decreasing" and "not increasing". For example, the function of figure 3 first falls, then rises, then falls again. It is therefore not decreasing and not increasing, but it is neither non-decreasing nor non-increasing.

A function ${\displaystyle f\!\left(x\right)}$ is said to be absolutely monotonic over an interval ${\displaystyle \left(a,b\right)}$ if the derivatives of all orders of ${\displaystyle f}$ are nonnegative or all nonpositive at all points on the interval.


£#h5#£Inverse of function£#/h5#£
A function that is monotonic, but not strictly monotonic, and thus constant on an interval, doesn't have an inverse. This is because in order for a function to have an inverse, there needs to be a one-to-one mapping from the range to the domain of the function. Since a monotonic function has some values that are constant in its domain, this means that there would be more than one value in the range that maps to this constant value.

However, a function y = g(x) that is strictly monotonic, has an inverse function such that x = h(y) because there is guaranteed to always be a one-to-one mapping from range to domain of the function. Also, a function can be said to be strictly monotonic on a range of values, and thus have an inverse on that range of value. For example, if y = g(x) is strictly monotonic on the range [a, b], then it has an inverse x = h(y) on the range [g(a), g(b)], but we cannot say the entire range of the function has an inverse.

Note, some textbooks mistakenly state that an inverse exists for a monotonic function, when they really mean that an inverse exists for a strictly monotonic function.


£#h5#£Monotonic transformation£#/h5#£
The term monotonic transformation (or monotone transformation) can also possibly cause some confusion because it refers to a transformation by a strictly increasing function. This is the case in economics with respect to the ordinal properties of a utility function being preserved across a monotonic transform (see also monotone preferences). In this context, what we are calling a "monotonic transformation" is, more accurately, called a "positive monotonic transformation", in order to distinguish it from a “negative monotonic transformation,” which reverses the order of the numbers.


£#h5#£Some basic applications and results£#/h5#£
The following properties are true for a monotonic function ${\displaystyle f\colon \mathbb {R} \to \mathbb {R} }$ :

£#ul#££#li#£ ${\displaystyle f}$ has limits from the right and from the left at every point of its domain;£#/li#£ £#li#£ ${\displaystyle f}$ has a limit at positive or negative infinity ( ${\displaystyle \pm \infty }$ ) of either a real number, ${\displaystyle \infty }$ , or ${\displaystyle -\infty }$ .£#/li#£ £#li#£ ${\displaystyle f}$ can only have jump discontinuities;£#/li#£ £#li#£ ${\displaystyle f}$ can only have countably many discontinuities in its domain. The discontinuities, however, do not necessarily consist of isolated points and may even be dense in an interval (a, b). For example, for any summable sequence $(a_{i})$ of positive numbers and any enumeration ${\displaystyle (q_{i})}$ of the rational numbers, the monotonically increasing function is continuous exactly at every irrational number (cf. picture). It is the cumulative distribution function of the discrete measure on the rational numbers, where ${\displaystyle a_{i}}$ is the weight of ${\displaystyle q_{i}}$ .£#/li#££#/ul#£
These properties are the reason why monotonic functions are useful in technical work in analysis. Some more facts about these functions are:

£#ul#££#li#£if ${\displaystyle f}$ is a monotonic function defined on an interval ${\displaystyle I}$ , then ${\displaystyle f}$ is differentiable almost everywhere on ${\displaystyle I}$ ; i.e. the set of numbers ${\displaystyle x}$ in ${\displaystyle I}$ such that ${\displaystyle f}$ is not differentiable in ${\displaystyle x}$ has Lebesgue measure zero. In addition, this result cannot be improved to countable: see Cantor function.£#/li#£ £#li#£if this set is countable, then ${\displaystyle f}$ is absolutely continuous£#/li#£ £#li#£if ${\displaystyle f}$ is a monotonic function defined on an interval ${\displaystyle \left[a,b\right]}$ , then ${\displaystyle f}$ is Riemann integrable.£#/li#££#/ul#£
An important application of monotonic functions is in probability theory. If ${\displaystyle X}$ is a random variable, its cumulative distribution function ${\displaystyle F_{X}\!\left(x\right)={\text{Prob}}\!\left(X\leq x\right)}$ is a monotonically increasing function.

A function is unimodal if it is monotonically increasing up to some point (the mode) and then monotonically decreasing.

When ${\displaystyle f}$ is a strictly monotonic function, then ${\displaystyle f}$ is injective on its domain, and if ${\displaystyle T}$ is the range of ${\displaystyle f}$ , then there is an inverse function on ${\displaystyle T}$ for ${\displaystyle f}$ . In contrast, each constant function is monotonic, but not injective, and hence cannot have an inverse.


£#h5#£In topology£#/h5#£

A map ${\displaystyle f:X\to Y}$ is said to be monotone if each of its fibers is connected; that is, for each element ${\displaystyle y\in Y,}$ the (possibly empty) set ${\displaystyle f^{-1}(y)}$ is a connected subspace of ${\displaystyle X.}$


£#h5#£In functional analysis£#/h5#£
In functional analysis on a topological vector space ${\displaystyle X}$ , a (possibly non-linear) operator ${\displaystyle T:X\rightarrow X^{*}}$ is said to be a monotone operator if

${\displaystyle (Tu-Tv,u-v)\geq 0\quad \forall u,v\in X.}$
Kachurovskii's theorem shows that convex functions on Banach spaces have monotonic operators as their derivatives.

A subset ${\displaystyle G}$ of ${\displaystyle X\times X^{*}}$ is said to be a monotone set if for every pair ${\displaystyle [u_{1},w_{1}]}$ and ${\displaystyle [u_{2},w_{2}]}$ in ${\displaystyle G}$ ,

${\displaystyle (w_{1}-w_{2},u_{1}-u_{2})\geq 0.}$
${\displaystyle G}$ is said to be maximal monotone if it is maximal among all monotone sets in the sense of set inclusion. The graph of a monotone operator ${\displaystyle G(T)}$ is a monotone set. A monotone operator is said to be maximal monotone if its graph is a maximal monotone set.


£#h5#£In order theory£#/h5#£
Order theory deals with arbitrary partially ordered sets and preordered sets as a generalization of real numbers. The above definition of monotonicity is relevant in these cases as well. However, the terms "increasing" and "decreasing" are avoided, since their conventional pictorial representation does not apply to orders that are not total. Furthermore, the strict relations < and > are of little use in many non-total orders and hence no additional terminology is introduced for them.

Letting ≤ denote the partial order relation of any partially ordered set, a monotone function, also called isotone, or order-preserving, satisfies the property

x ≤ y implies f(x) ≤ f(y),
for all x and y in its domain. The composite of two monotone mappings is also monotone.

The dual notion is often called antitone, anti-monotone, or order-reversing. Hence, an antitone function f satisfies the property

x ≤ y implies f(y) ≤ f(x),
for all x and y in its domain.

A constant function is both monotone and antitone; conversely, if f is both monotone and antitone, and if the domain of f is a lattice, then f must be constant.

Monotone functions are central in order theory. They appear in most articles on the subject and examples from special applications are found in these places. Some notable special monotone functions are order embeddings (functions for which x ≤ y if and only if f(x) ≤ f(y)) and order isomorphisms (surjective order embeddings).


£#h5#£In the context of search algorithms£#/h5#£
In the context of search algorithms monotonicity (also called consistency) is a condition applied to heuristic functions. A heuristic h(n) is monotonic if, for every node n and every successor n' of n generated by any action a, the estimated cost of reaching the goal from n is no greater than the step cost of getting to n' plus the estimated cost of reaching the goal from n' ,

${\displaystyle h(n)\leq c\left(n,a,n'\right)+h\left(n'\right).}$
This is a form of triangle inequality, with n, n', and the goal Gn closest to n. Because every monotonic heuristic is also admissible, monotonicity is a stricter requirement than admissibility. Some heuristic algorithms such as A* can be proven optimal provided that the heuristic they use is monotonic.


£#h5#£In Boolean functions£#/h5#£
In Boolean algebra, a monotonic function is one such that for all ai and bi in {0,1}, if a1 ≤ b1, a2 ≤ b2, ..., an ≤ bn (i.e. the Cartesian product {0, 1}n is ordered coordinatewise), then f(a1, ..., an) ≤ f(b1, ..., bn). In other words, a Boolean function is monotonic if, for every combination of inputs, switching one of the inputs from false to true can only cause the output to switch from false to true and not from true to false. Graphically, this means that an n-ary Boolean function is monotonic when its representation as an n-cube labelled with truth values has no upward edge from true to false. (This labelled Hasse diagram is the dual of the function's labelled Venn diagram, which is the more common representation for n ≤ 3.)

The monotonic Boolean functions are precisely those that can be defined by an expression combining the inputs (which may appear more than once) using only the operators and and or (in particular not is forbidden). For instance "at least two of a, b, c hold" is a monotonic function of a, b, c, since it can be written for instance as ((a and b) or (a and c) or (b and c)).

The number of such functions on n variables is known as the Dedekind number of n.


£#h5#£See also£#/h5#£ £#ul#££#li#£Monotone cubic interpolation£#/li#£ £#li#£Pseudo-monotone operator£#/li#£ £#li#£Spearman's rank correlation coefficient - measure of monotonicity in a set of data£#/li#£ £#li#£Total monotonicity£#/li#£ £#li#£Cyclical monotonicity£#/li#£ £#li#£Operator monotone function£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£Bibliography£#/h5#£ £#ul#££#li#£Bartle, Robert G. (1976). The elements of real analysis (second ed.).£#/li#£ £#li#£Grätzer, George (1971). Lattice theory: first concepts and distributive lattices. ISBN 0-7167-0442-0.£#/li#£ £#li#£Pemberton, Malcolm; Rau, Nicholas (2001). Mathematics for economists: an introductory textbook. Manchester University Press. ISBN 0-7190-3341-1.£#/li#£ £#li#£Renardy, Michael & Rogers, Robert C. (2004). An introduction to partial differential equations. Texts in Applied Mathematics 13 (Second ed.). New York: Springer-Verlag. p. 356. ISBN 0-387-00444-0.£#/li#£ £#li#£Riesz, Frigyes & Béla Szőkefalvi-Nagy (1990). Functional Analysis. Courier Dover Publications. ISBN 978-0-486-66289-3.£#/li#£ £#li#£Russell, Stuart J.; Norvig, Peter (2010). Artificial Intelligence: A Modern Approach (3rd ed.). Upper Saddle River, New Jersey: Prentice Hall. ISBN 978-0-13-604259-4.£#/li#£ £#li#£Simon, Carl P.; Blume, Lawrence (April 1994). Mathematics for Economists (first ed.). ISBN 978-0-393-95733-4. (Definition 9.31)£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Monotone function", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#£ £#li#£Convergence of a Monotonic Sequence by Anik Debnath and Thomas Roxlo (The Harker School), Wolfram Demonstrations Project.£#/li#£ £#li#£Weisstein, Eric W. "Monotonic Function". MathWorld.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Widder, D. V. Ch. 4 in The Laplace Transform. Princeton, NJ: Princeton University Press, 1941.£#/li#££#li#£ Widder, D. V. Ch. 4 in The Laplace Transform. Princeton, NJ: Princeton University Press, 1941. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Functions £#/li#££#/ul#£




£#h3#£Absolute Square£#/h3#£

In mathematics, the absolute value or modulus of a real number ${\displaystyle x}$ , denoted ${\displaystyle |x|}$ , is the non-negative value of ${\displaystyle x}$ without regard to its sign. Namely, ${\displaystyle |x|=x}$ if x is a positive number, and ${\displaystyle |x|=-x}$ if ${\displaystyle x}$ is negative (in which case negating ${\displaystyle x}$ makes ${\displaystyle -x}$ positive), and ${\displaystyle |0|=0}$ . For example, the absolute value of 3 is 3, and the absolute value of −3 is also 3. The absolute value of a number may be thought of as its distance from zero.

Generalisations of the absolute value for real numbers occur in a wide variety of mathematical settings. For example, an absolute value is also defined for the complex numbers, the quaternions, ordered rings, fields and vector spaces. The absolute value is closely related to the notions of magnitude, distance, and norm in various mathematical and physical contexts.


£#h5#£Terminology and notation£#/h5#£
In 1806, Jean-Robert Argand introduced the term module, meaning unit of measure in French, specifically for the complex absolute value, and it was borrowed into English in 1866 as the Latin equivalent modulus. The term absolute value has been used in this sense from at least 1806 in French and 1857 in English. The notation |x|, with a vertical bar on each side, was introduced by Karl Weierstrass in 1841. Other names for absolute value include numerical value and magnitude. In programming languages and computational software packages, the absolute value of x is generally represented by abs(x), or a similar expression.

The vertical bar notation also appears in a number of other mathematical contexts: for example, when applied to a set, it denotes its cardinality; when applied to a matrix, it denotes its determinant. Vertical bars denote the absolute value only for algebraic objects for which the notion of an absolute value is defined, notably an element of a normed division algebra, for example a real number, a complex number, or a quaternion. A closely related but distinct notation is the use of vertical bars for either the Euclidean norm or sup norm of a vector in ${\displaystyle \mathbb {R} ^{n}}$ , although double vertical bars with subscripts ( ${\displaystyle \|\cdot \|_{2}}$ and ${\displaystyle \|\cdot \|_{\infty }}$ , respectively) are a more common and less ambiguous notation.


£#h5#£Definition and properties£#/h5#£
£#h5#£Real numbers£#/h5#£
For any real number ${\displaystyle x}$ , the absolute value or modulus of ${\displaystyle x}$ is denoted by ${\displaystyle |x|}$ , with a vertical bar on each side of the quantity, and is defined as

The absolute value of ${\displaystyle x}$ is thus always either a positive number or zero, but never negative. When ${\displaystyle x}$ itself is negative ( ${\displaystyle x<0}$ ), then its absolute value is necessarily positive ( ${\displaystyle |x|=-x>0}$ ).

From an analytic geometry point of view, the absolute value of a real number is that number's distance from zero along the real number line, and more generally the absolute value of the difference of two real numbers is the distance between them. The notion of an abstract distance function in mathematics can be seen to be a generalisation of the absolute value of the difference (see "Distance" below).

Since the square root symbol represents the unique positive square root, when applied to a positive number, it follows that

This is equivalent to the definition above, and may be used as an alternative definition of the absolute value of real numbers.
The absolute value has the following four fundamental properties (a, b are real numbers), that are used for generalization of this notion to other domains:

Non-negativity, positive definiteness, and multiplicativity are readily apparent from the definition. To see that subadditivity holds, first note that ${\displaystyle |a+b|=s(a+b)}$ where ${\displaystyle s=\pm 1}$ , with its sign chosen to make the result positive. Now, since ${\displaystyle -1\cdot x\leq |x|}$ and ${\displaystyle +1\cdot x\leq |x|}$ , it follows that, whichever of ${\displaystyle \pm 1}$ is the value of ${\displaystyle s}$ , one has ${\displaystyle s\cdot x\leq |x|}$ for all real ${\displaystyle x}$ . Consequently, ${\displaystyle |a+b|=s\cdot (a+b)=s\cdot a+s\cdot b\leq |a|+|b|}$ , as desired.

Some additional useful properties are given below. These are either immediate consequences of the definition or implied by the four fundamental properties above.

Two other useful properties concerning inequalities are:

These relations may be used to solve inequalities involving absolute values. For example:

The absolute value, as "distance from zero", is used to define the absolute difference between arbitrary real numbers, the standard metric on the real numbers.


£#h5#£Complex numbers£#/h5#£

Since the complex numbers are not ordered, the definition given at the top for the real absolute value cannot be directly applied to complex numbers. However, the geometric interpretation of the absolute value of a real number as its distance from 0 can be generalised. The absolute value of a complex number is defined by the Euclidean distance of its corresponding point in the complex plane from the origin. This can be computed using the Pythagorean theorem: for any complex number

where ${\displaystyle x}$ and ${\displaystyle y}$ are real numbers, the absolute value or modulus of ${\displaystyle z}$ is denoted ${\displaystyle |z|}$ and is defined by the Pythagorean addition of ${\displaystyle x}$ and ${\displaystyle y}$ , where ${\displaystyle \operatorname {Re} (z)=x}$ and ${\displaystyle \operatorname {Im} (z)=y}$ denote the real and imaginary parts of ${\displaystyle z}$ , respectively. When the imaginary part ${\displaystyle y}$ is zero, this coincides with the definition of the absolute value of the real number ${\displaystyle x}$ .
When a complex number ${\displaystyle z}$ is expressed in its polar form as ${\displaystyle z=re^{i\theta },}$ its absolute value is ${\displaystyle |z|=r.}$

Since the product of any complex number ${\displaystyle z}$ and its complex conjugate ${\displaystyle {\bar {z}}=x-iy}$ , with the same absolute value, is always the non-negative real number ${\displaystyle \left(x^{2}+y^{2}\right)}$ , the absolute value of a complex number ${\displaystyle z}$ is the square root of ${\displaystyle z\cdot {\overline {z}},}$ which is therefore called the absolute square or squared modulus of ${\displaystyle z}$ :

This generalizes the alternative definition for reals: ${\textstyle |x|={\sqrt {x\cdot x}}}$ .
The complex absolute value shares the four fundamental properties given above for the real absolute value. The identity ${\displaystyle |z|^{2}=|z^{2}|}$ is a special case of multiplicativity that is often useful by itself.


£#h5#£Absolute value function£#/h5#£
The real absolute value function is continuous everywhere. It is differentiable everywhere except for x = 0. It is monotonically decreasing on the interval (−∞, 0] and monotonically increasing on the interval [0, +∞). Since a real number and its opposite have the same absolute value, it is an even function, and is hence not invertible. The real absolute value function is a piecewise linear, convex function.

For both real and complex numbers the absolute value function is idempotent (meaning that the absolute value of any absolute value is itself).


£#h5#£Relationship to the sign function£#/h5#£
The absolute value function of a real number returns its value irrespective of its sign, whereas the sign (or signum) function returns a number's sign irrespective of its value. The following equations show the relationship between these two functions:

${\displaystyle |x|=x\operatorname {sgn}(x),}$
or

${\displaystyle |x|\operatorname {sgn}(x)=x,}$
and for x ≠ 0,

${\displaystyle \operatorname {sgn}(x)={\frac {|x|}{x}}={\frac {x}{|x|}}.}$

£#h5#£Derivative£#/h5#£
The real absolute value function has a derivative for every x ≠ 0, but is not differentiable at x = 0. Its derivative for x ≠ 0 is given by the step function:

${\displaystyle {\frac {d\left|x\right|}{dx}}={\frac {x}{|x|}}={\begin{cases}-1&x<0\\1&x>0.\end{cases}}}$
The real absolute value function is an example of a continuous function that achieves a global minimum where the derivative does not exist.

The subdifferential of |x| at x = 0 is the interval [−1, 1].

The complex absolute value function is continuous everywhere but complex differentiable nowhere because it violates the Cauchy–Riemann equations.

The second derivative of |x| with respect to x is zero everywhere except zero, where it does not exist. As a generalised function, the second derivative may be taken as two times the Dirac delta function.


£#h5#£Antiderivative£#/h5#£
The antiderivative (indefinite integral) of the real absolute value function is

${\displaystyle \int \left|x\right|dx={\frac {x\left|x\right|}{2}}+C,}$
where C is an arbitrary constant of integration. This is not a complex antiderivative because complex antiderivatives can only exist for complex-differentiable (holomorphic) functions, which the complex absolute value function is not.


£#h5#£Distance£#/h5#£
The absolute value is closely related to the idea of distance. As noted above, the absolute value of a real or complex number is the distance from that number to the origin, along the real number line, for real numbers, or in the complex plane, for complex numbers, and more generally, the absolute value of the difference of two real or complex numbers is the distance between them.

The standard Euclidean distance between two points

${\displaystyle a=(a_{1},a_{2},\dots ,a_{n})}$
and

${\displaystyle b=(b_{1},b_{2},\dots ,b_{n})}$
in Euclidean n-space is defined as:

${\displaystyle {\sqrt {\textstyle \sum _{i=1}^{n}(a_{i}-b_{i})^{2}}}.}$
This can be seen as a generalisation, since for ${\displaystyle a_{1}}$ and ${\displaystyle b_{1}}$ real, i.e. in a 1-space, according to the alternative definition of the absolute value,

${\displaystyle |a_{1}-b_{1}|={\sqrt {(a_{1}-b_{1})^{2}}}={\sqrt {\textstyle \sum _{i=1}^{1}(a_{i}-b_{i})^{2}}},}$
and for ${\displaystyle a=a_{1}+ia_{2}}$ and ${\displaystyle b=b_{1}+ib_{2}}$ complex numbers, i.e. in a 2-space,

The above shows that the "absolute value"-distance, for real and complex numbers, agrees with the standard Euclidean distance, which they inherit as a result of considering them as one and two-dimensional Euclidean spaces, respectively.

The properties of the absolute value of the difference of two real or complex numbers: non-negativity, identity of indiscernibles, symmetry and the triangle inequality given above, can be seen to motivate the more general notion of a distance function as follows:

A real valued function d on a set X × X is called a metric (or a distance function) on X, if it satisfies the following four axioms:


£#h5#£Generalizations£#/h5#£
£#h5#£Ordered rings£#/h5#£
The definition of absolute value given for real numbers above can be extended to any ordered ring. That is, if a is an element of an ordered ring R, then the absolute value of a, denoted by |a|, is defined to be:

${\displaystyle |a|=\left\{{\begin{array}{rl}a,&{\text{if }}a\geq 0\\-a,&{\text{if }}a<0.\end{array}}\right.}$
where −a is the additive inverse of a, 0 is the additive identity, and < and ≥ have the usual meaning with respect to the ordering in the ring.


£#h5#£Fields£#/h5#£
The four fundamental properties of the absolute value for real numbers can be used to generalise the notion of absolute value to an arbitrary field, as follows.

A real-valued function v on a field F is called an absolute value (also a modulus, magnitude, value, or valuation) if it satisfies the following four axioms:

Where 0 denotes the additive identity of F. It follows from positive-definiteness and multiplicativity that v(1) = 1, where 1 denotes the multiplicative identity of F. The real and complex absolute values defined above are examples of absolute values for an arbitrary field.

If v is an absolute value on F, then the function d on F × F, defined by d(a, b) = v(a − b), is a metric and the following are equivalent:

£#ul#££#li#£d satisfies the ultrametric inequality ${\displaystyle d(x,y)\leq \max(d(x,z),d(y,z))}$ for all x, y, z in F.£#/li#£ £#li#£ ${\textstyle \left\{v\left(\sum _{k=1}^{n}\mathbf {1} \right):n\in \mathbb {N} \right\}}$ is bounded in R.£#/li#£ £#li#£ ${\displaystyle v\left({\textstyle \sum _{k=1}^{n}}\mathbf {1} \right)\leq 1\ }$ for every ${\displaystyle n\in \mathbb {N} }$ .£#/li#£ £#li#£ ${\displaystyle v(a)\leq 1\Rightarrow v(1+a)\leq 1\ }$ for all ${\displaystyle a\in F}$ .£#/li#£ £#li#£ ${\displaystyle v(a+b)\leq \max\{v(a),v(b)\}\ }$ for all ${\displaystyle a,b\in F}$ .£#/li#££#/ul#£
An absolute value which satisfies any (hence all) of the above conditions is said to be non-Archimedean, otherwise it is said to be Archimedean.


£#h5#£Vector spaces£#/h5#£
Again the fundamental properties of the absolute value for real numbers can be used, with a slight modification, to generalise the notion to an arbitrary vector space.

A real-valued function on a vector space V over a field F, represented as || · ||, is called an absolute value, but more usually a norm, if it satisfies the following axioms:

For all a in F, and v, u in V,

The norm of a vector is also called its length or magnitude.

In the case of Euclidean space ${\displaystyle \mathbb {R} ^{n}}$ , the function defined by

${\displaystyle \|(x_{1},x_{2},\dots ,x_{n})\|={\sqrt {\textstyle \sum _{i=1}^{n}x_{i}^{2}}}}$
is a norm called the Euclidean norm. When the real numbers ${\displaystyle \mathbb {R} }$ are considered as the one-dimensional vector space ${\displaystyle \mathbb {R} ^{1}}$ , the absolute value is a norm, and is the p-norm (see Lp space) for any p. In fact the absolute value is the "only" norm on ${\displaystyle \mathbb {R} ^{1}}$ , in the sense that, for every norm || · || on ${\displaystyle \mathbb {R} ^{1}}$ , ||x|| = ||1|| ⋅ |x|.

The complex absolute value is a special case of the norm in an inner product space, which is identical to the Euclidean norm when the complex plane is identified as the Euclidean plane  ${\displaystyle \mathbb {R} ^{2}}$ .


£#h5#£Composition algebras£#/h5#£
Every composition algebra A has an involution x → x* called its conjugation. The product in A of an element x and its conjugate x* is written N(x) = x x* and called the norm of x.

The real numbers ${\displaystyle \mathbb {R} }$ , complex numbers ${\displaystyle \mathbb {C} }$ , and quaternions ${\displaystyle \mathbb {H} }$ are all composition algebras with norms given by definite quadratic forms. The absolute value in these division algebras is given by the square root of the composition algebra norm.

In general the norm of a composition algebra may be a quadratic form that is not definite and has null vectors. However, as in the case of division algebras, when an element x has a non-zero norm, then x has a multiplicative inverse given by x*/N(x).


£#h5#£See also£#/h5#£ £#ul#££#li#£Least absolute values£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Bartle; Sherbert; Introduction to real analysis (4th ed.), John Wiley & Sons, 2011 ISBN 978-0-471-43331-6.£#/li#£ £#li#£Nahin, Paul J.; An Imaginary Tale; Princeton University Press; (hardcover, 1998). ISBN 0-691-02795-1.£#/li#£ £#li#£Mac Lane, Saunders, Garrett Birkhoff, Algebra, American Mathematical Soc., 1999. ISBN 978-0-8218-1646-2.£#/li#£ £#li#£Mendelson, Elliott, Schaum's Outline of Beginning Calculus, McGraw-Hill Professional, 2008. ISBN 978-0-07-148754-2.£#/li#£ £#li#£O'Connor, J.J. and Robertson, E.F.; "Jean Robert Argand".£#/li#£ £#li#£Schechter, Eric; Handbook of Analysis and Its Foundations, pp. 259–263, "Absolute Values", Academic Press (1997) ISBN 0-12-622760-8.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Absolute value". Encyclopedia of Mathematics. EMS Press. 2001 [1994].£#/li#£ £#li#£absolute value at PlanetMath.£#/li#£ £#li#£Weisstein, Eric W. "Absolute Value". MathWorld.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Complex Analysis > Complex Numbers £#/li#££#li#£ Interactive Entries > webMathematica Examples £#/li#££#/ul#£




£#h3#£Absolute Value£#/h3#£

In mathematics, the absolute value or modulus of a real number ${\displaystyle x}$ , denoted ${\displaystyle |x|}$ , is the non-negative value of ${\displaystyle x}$ without regard to its sign. Namely, ${\displaystyle |x|=x}$ if x is a positive number, and ${\displaystyle |x|=-x}$ if ${\displaystyle x}$ is negative (in which case negating ${\displaystyle x}$ makes ${\displaystyle -x}$ positive), and ${\displaystyle |0|=0}$ . For example, the absolute value of 3 is 3, and the absolute value of −3 is also 3. The absolute value of a number may be thought of as its distance from zero.

Generalisations of the absolute value for real numbers occur in a wide variety of mathematical settings. For example, an absolute value is also defined for the complex numbers, the quaternions, ordered rings, fields and vector spaces. The absolute value is closely related to the notions of magnitude, distance, and norm in various mathematical and physical contexts.


£#h5#£Terminology and notation£#/h5#£
In 1806, Jean-Robert Argand introduced the term module, meaning unit of measure in French, specifically for the complex absolute value, and it was borrowed into English in 1866 as the Latin equivalent modulus. The term absolute value has been used in this sense from at least 1806 in French and 1857 in English. The notation |x|, with a vertical bar on each side, was introduced by Karl Weierstrass in 1841. Other names for absolute value include numerical value and magnitude. In programming languages and computational software packages, the absolute value of x is generally represented by abs(x), or a similar expression.

The vertical bar notation also appears in a number of other mathematical contexts: for example, when applied to a set, it denotes its cardinality; when applied to a matrix, it denotes its determinant. Vertical bars denote the absolute value only for algebraic objects for which the notion of an absolute value is defined, notably an element of a normed division algebra, for example a real number, a complex number, or a quaternion. A closely related but distinct notation is the use of vertical bars for either the Euclidean norm or sup norm of a vector in ${\displaystyle \mathbb {R} ^{n}}$ , although double vertical bars with subscripts ( ${\displaystyle \|\cdot \|_{2}}$ and ${\displaystyle \|\cdot \|_{\infty }}$ , respectively) are a more common and less ambiguous notation.


£#h5#£Definition and properties£#/h5#£
£#h5#£Real numbers£#/h5#£
For any real number ${\displaystyle x}$ , the absolute value or modulus of ${\displaystyle x}$ is denoted by ${\displaystyle |x|}$ , with a vertical bar on each side of the quantity, and is defined as

The absolute value of ${\displaystyle x}$ is thus always either a positive number or zero, but never negative. When ${\displaystyle x}$ itself is negative ( ${\displaystyle x<0}$ ), then its absolute value is necessarily positive ( ${\displaystyle |x|=-x>0}$ ).

From an analytic geometry point of view, the absolute value of a real number is that number's distance from zero along the real number line, and more generally the absolute value of the difference of two real numbers is the distance between them. The notion of an abstract distance function in mathematics can be seen to be a generalisation of the absolute value of the difference (see "Distance" below).

Since the square root symbol represents the unique positive square root, when applied to a positive number, it follows that

This is equivalent to the definition above, and may be used as an alternative definition of the absolute value of real numbers.
The absolute value has the following four fundamental properties (a, b are real numbers), that are used for generalization of this notion to other domains:

Non-negativity, positive definiteness, and multiplicativity are readily apparent from the definition. To see that subadditivity holds, first note that ${\displaystyle |a+b|=s(a+b)}$ where ${\displaystyle s=\pm 1}$ , with its sign chosen to make the result positive. Now, since ${\displaystyle -1\cdot x\leq |x|}$ and ${\displaystyle +1\cdot x\leq |x|}$ , it follows that, whichever of ${\displaystyle \pm 1}$ is the value of ${\displaystyle s}$ , one has ${\displaystyle s\cdot x\leq |x|}$ for all real ${\displaystyle x}$ . Consequently, ${\displaystyle |a+b|=s\cdot (a+b)=s\cdot a+s\cdot b\leq |a|+|b|}$ , as desired.

Some additional useful properties are given below. These are either immediate consequences of the definition or implied by the four fundamental properties above.

Two other useful properties concerning inequalities are:

These relations may be used to solve inequalities involving absolute values. For example:

The absolute value, as "distance from zero", is used to define the absolute difference between arbitrary real numbers, the standard metric on the real numbers.


£#h5#£Complex numbers£#/h5#£

Since the complex numbers are not ordered, the definition given at the top for the real absolute value cannot be directly applied to complex numbers. However, the geometric interpretation of the absolute value of a real number as its distance from 0 can be generalised. The absolute value of a complex number is defined by the Euclidean distance of its corresponding point in the complex plane from the origin. This can be computed using the Pythagorean theorem: for any complex number

where ${\displaystyle x}$ and ${\displaystyle y}$ are real numbers, the absolute value or modulus of ${\displaystyle z}$ is denoted ${\displaystyle |z|}$ and is defined by the Pythagorean addition of ${\displaystyle x}$ and ${\displaystyle y}$ , where ${\displaystyle \operatorname {Re} (z)=x}$ and ${\displaystyle \operatorname {Im} (z)=y}$ denote the real and imaginary parts of ${\displaystyle z}$ , respectively. When the imaginary part ${\displaystyle y}$ is zero, this coincides with the definition of the absolute value of the real number ${\displaystyle x}$ .
When a complex number ${\displaystyle z}$ is expressed in its polar form as ${\displaystyle z=re^{i\theta },}$ its absolute value is ${\displaystyle |z|=r.}$

Since the product of any complex number ${\displaystyle z}$ and its complex conjugate ${\displaystyle {\bar {z}}=x-iy}$ , with the same absolute value, is always the non-negative real number ${\displaystyle \left(x^{2}+y^{2}\right)}$ , the absolute value of a complex number ${\displaystyle z}$ is the square root of ${\displaystyle z\cdot {\overline {z}},}$ which is therefore called the absolute square or squared modulus of ${\displaystyle z}$ :

This generalizes the alternative definition for reals: ${\textstyle |x|={\sqrt {x\cdot x}}}$ .
The complex absolute value shares the four fundamental properties given above for the real absolute value. The identity ${\displaystyle |z|^{2}=|z^{2}|}$ is a special case of multiplicativity that is often useful by itself.


£#h5#£Absolute value function£#/h5#£
The real absolute value function is continuous everywhere. It is differentiable everywhere except for x = 0. It is monotonically decreasing on the interval (−∞, 0] and monotonically increasing on the interval [0, +∞). Since a real number and its opposite have the same absolute value, it is an even function, and is hence not invertible. The real absolute value function is a piecewise linear, convex function.

For both real and complex numbers the absolute value function is idempotent (meaning that the absolute value of any absolute value is itself).


£#h5#£Relationship to the sign function£#/h5#£
The absolute value function of a real number returns its value irrespective of its sign, whereas the sign (or signum) function returns a number's sign irrespective of its value. The following equations show the relationship between these two functions:

${\displaystyle |x|=x\operatorname {sgn}(x),}$
or

${\displaystyle |x|\operatorname {sgn}(x)=x,}$
and for x ≠ 0,

${\displaystyle \operatorname {sgn}(x)={\frac {|x|}{x}}={\frac {x}{|x|}}.}$

£#h5#£Derivative£#/h5#£
The real absolute value function has a derivative for every x ≠ 0, but is not differentiable at x = 0. Its derivative for x ≠ 0 is given by the step function:

${\displaystyle {\frac {d\left|x\right|}{dx}}={\frac {x}{|x|}}={\begin{cases}-1&x<0\\1&x>0.\end{cases}}}$
The real absolute value function is an example of a continuous function that achieves a global minimum where the derivative does not exist.

The subdifferential of |x| at x = 0 is the interval [−1, 1].

The complex absolute value function is continuous everywhere but complex differentiable nowhere because it violates the Cauchy–Riemann equations.

The second derivative of |x| with respect to x is zero everywhere except zero, where it does not exist. As a generalised function, the second derivative may be taken as two times the Dirac delta function.


£#h5#£Antiderivative£#/h5#£
The antiderivative (indefinite integral) of the real absolute value function is

${\displaystyle \int \left|x\right|dx={\frac {x\left|x\right|}{2}}+C,}$
where C is an arbitrary constant of integration. This is not a complex antiderivative because complex antiderivatives can only exist for complex-differentiable (holomorphic) functions, which the complex absolute value function is not.


£#h5#£Distance£#/h5#£
The absolute value is closely related to the idea of distance. As noted above, the absolute value of a real or complex number is the distance from that number to the origin, along the real number line, for real numbers, or in the complex plane, for complex numbers, and more generally, the absolute value of the difference of two real or complex numbers is the distance between them.

The standard Euclidean distance between two points

${\displaystyle a=(a_{1},a_{2},\dots ,a_{n})}$
and

${\displaystyle b=(b_{1},b_{2},\dots ,b_{n})}$
in Euclidean n-space is defined as:

${\displaystyle {\sqrt {\textstyle \sum _{i=1}^{n}(a_{i}-b_{i})^{2}}}.}$
This can be seen as a generalisation, since for ${\displaystyle a_{1}}$ and ${\displaystyle b_{1}}$ real, i.e. in a 1-space, according to the alternative definition of the absolute value,

${\displaystyle |a_{1}-b_{1}|={\sqrt {(a_{1}-b_{1})^{2}}}={\sqrt {\textstyle \sum _{i=1}^{1}(a_{i}-b_{i})^{2}}},}$
and for ${\displaystyle a=a_{1}+ia_{2}}$ and ${\displaystyle b=b_{1}+ib_{2}}$ complex numbers, i.e. in a 2-space,

The above shows that the "absolute value"-distance, for real and complex numbers, agrees with the standard Euclidean distance, which they inherit as a result of considering them as one and two-dimensional Euclidean spaces, respectively.

The properties of the absolute value of the difference of two real or complex numbers: non-negativity, identity of indiscernibles, symmetry and the triangle inequality given above, can be seen to motivate the more general notion of a distance function as follows:

A real valued function d on a set X × X is called a metric (or a distance function) on X, if it satisfies the following four axioms:


£#h5#£Generalizations£#/h5#£
£#h5#£Ordered rings£#/h5#£
The definition of absolute value given for real numbers above can be extended to any ordered ring. That is, if a is an element of an ordered ring R, then the absolute value of a, denoted by |a|, is defined to be:

${\displaystyle |a|=\left\{{\begin{array}{rl}a,&{\text{if }}a\geq 0\\-a,&{\text{if }}a<0.\end{array}}\right.}$
where −a is the additive inverse of a, 0 is the additive identity, and < and ≥ have the usual meaning with respect to the ordering in the ring.


£#h5#£Fields£#/h5#£
The four fundamental properties of the absolute value for real numbers can be used to generalise the notion of absolute value to an arbitrary field, as follows.

A real-valued function v on a field F is called an absolute value (also a modulus, magnitude, value, or valuation) if it satisfies the following four axioms:

Where 0 denotes the additive identity of F. It follows from positive-definiteness and multiplicativity that v(1) = 1, where 1 denotes the multiplicative identity of F. The real and complex absolute values defined above are examples of absolute values for an arbitrary field.

If v is an absolute value on F, then the function d on F × F, defined by d(a, b) = v(a − b), is a metric and the following are equivalent:

£#ul#££#li#£d satisfies the ultrametric inequality ${\displaystyle d(x,y)\leq \max(d(x,z),d(y,z))}$ for all x, y, z in F.£#/li#£ £#li#£ ${\textstyle \left\{v\left(\sum _{k=1}^{n}\mathbf {1} \right):n\in \mathbb {N} \right\}}$ is bounded in R.£#/li#£ £#li#£ ${\displaystyle v\left({\textstyle \sum _{k=1}^{n}}\mathbf {1} \right)\leq 1\ }$ for every ${\displaystyle n\in \mathbb {N} }$ .£#/li#£ £#li#£ ${\displaystyle v(a)\leq 1\Rightarrow v(1+a)\leq 1\ }$ for all ${\displaystyle a\in F}$ .£#/li#£ £#li#£ ${\displaystyle v(a+b)\leq \max\{v(a),v(b)\}\ }$ for all ${\displaystyle a,b\in F}$ .£#/li#££#/ul#£
An absolute value which satisfies any (hence all) of the above conditions is said to be non-Archimedean, otherwise it is said to be Archimedean.


£#h5#£Vector spaces£#/h5#£
Again the fundamental properties of the absolute value for real numbers can be used, with a slight modification, to generalise the notion to an arbitrary vector space.

A real-valued function on a vector space V over a field F, represented as || · ||, is called an absolute value, but more usually a norm, if it satisfies the following axioms:

For all a in F, and v, u in V,

The norm of a vector is also called its length or magnitude.

In the case of Euclidean space ${\displaystyle \mathbb {R} ^{n}}$ , the function defined by

${\displaystyle \|(x_{1},x_{2},\dots ,x_{n})\|={\sqrt {\textstyle \sum _{i=1}^{n}x_{i}^{2}}}}$
is a norm called the Euclidean norm. When the real numbers ${\displaystyle \mathbb {R} }$ are considered as the one-dimensional vector space ${\displaystyle \mathbb {R} ^{1}}$ , the absolute value is a norm, and is the p-norm (see Lp space) for any p. In fact the absolute value is the "only" norm on ${\displaystyle \mathbb {R} ^{1}}$ , in the sense that, for every norm || · || on ${\displaystyle \mathbb {R} ^{1}}$ , ||x|| = ||1|| ⋅ |x|.

The complex absolute value is a special case of the norm in an inner product space, which is identical to the Euclidean norm when the complex plane is identified as the Euclidean plane  ${\displaystyle \mathbb {R} ^{2}}$ .


£#h5#£Composition algebras£#/h5#£
Every composition algebra A has an involution x → x* called its conjugation. The product in A of an element x and its conjugate x* is written N(x) = x x* and called the norm of x.

The real numbers ${\displaystyle \mathbb {R} }$ , complex numbers ${\displaystyle \mathbb {C} }$ , and quaternions ${\displaystyle \mathbb {H} }$ are all composition algebras with norms given by definite quadratic forms. The absolute value in these division algebras is given by the square root of the composition algebra norm.

In general the norm of a composition algebra may be a quadratic form that is not definite and has null vectors. However, as in the case of division algebras, when an element x has a non-zero norm, then x has a multiplicative inverse given by x*/N(x).


£#h5#£See also£#/h5#£ £#ul#££#li#£Least absolute values£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Bartle; Sherbert; Introduction to real analysis (4th ed.), John Wiley & Sons, 2011 ISBN 978-0-471-43331-6.£#/li#£ £#li#£Nahin, Paul J.; An Imaginary Tale; Princeton University Press; (hardcover, 1998). ISBN 0-691-02795-1.£#/li#£ £#li#£Mac Lane, Saunders, Garrett Birkhoff, Algebra, American Mathematical Soc., 1999. ISBN 978-0-8218-1646-2.£#/li#£ £#li#£Mendelson, Elliott, Schaum's Outline of Beginning Calculus, McGraw-Hill Professional, 2008. ISBN 978-0-07-148754-2.£#/li#£ £#li#£O'Connor, J.J. and Robertson, E.F.; "Jean Robert Argand".£#/li#£ £#li#£Schechter, Eric; Handbook of Analysis and Its Foundations, pp. 259–263, "Absolute Values", Academic Press (1997) ISBN 0-12-622760-8.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Absolute value". Encyclopedia of Mathematics. EMS Press. 2001 [1994].£#/li#£ £#li#£absolute value at PlanetMath.£#/li#£ £#li#£Weisstein, Eric W. "Absolute Value". MathWorld.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Milton, K. A. "The Casimir Effect: Physical Manifestations of Zero-Point Energy." 4 Jan 1999. http://arxiv.org/abs/hep-th/9901011.£#/li#££#li#£Milton, K. A. and Ng, J. "Observability of the Bulk Casimir Effect: Can the Dynamical Casimir Effect be Relevant to Sonoluminescence?" Phys. Rev. E 57, 5504-5510, 1998.£#/li#££#li#£Sloane, N. J. A. Sequences A000217/M2535, A116419, and A116420 in "The On-Line Encyclopedia of Integer Sequences."£#/li#££#li#£ Milton, K. A. "The Casimir Effect: Physical Manifestations of Zero-Point Energy." 4 Jan 1999. http://arxiv.org/abs/hep-th/9901011. £#/li#££#li#£ Milton, K. A. and Ng, J. "Observability of the Bulk Casimir Effect: Can the Dynamical Casimir Effect be Relevant to Sonoluminescence?" Phys. Rev. E 57, 5504-5510, 1998. £#/li#££#li#£ Sloane, N. J. A. Sequences A000217/M2535, A116419, and A116420 in "The On-Line Encyclopedia of Integer Sequences." £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Functions £#/li#££#li#£ Calculus and Analysis > Complex Analysis > Complex Numbers £#/li#££#li#£ Calculus and Analysis > Calculus > Integrals > Definite Integrals £#/li#££#li#£ History and Terminology > Disciplinary Terminology > Religious Terminology £#/li#££#li#£ Interactive Entries > webMathematica Examples £#/li#££#/ul#£




£#h3#£Absorbing Set£#/h3#£

In functional analysis and related areas of mathematics an absorbing set in a vector space is a set ${\displaystyle S}$ which can be "inflated" or "scaled up" to eventually always include any given point of the vector space. Alternative terms are radial or absorbent set. Every neighborhood of the origin in every topological vector space is an absorbing subset.


£#h5#£Definition£#/h5#£
Suppose that ${\displaystyle X}$ is a vector space over the field ${\displaystyle \mathbb {K} }$ of real numbers ${\displaystyle \mathbb {R} }$ or complex numbers ${\displaystyle \mathbb {C} .}$


£#h5#£Notation£#/h5#£
Products of scalars and vectors

For any ${\displaystyle -\infty \leq r\leq R\leq \infty ,}$ vector ${\displaystyle x,}$ and subset ${\displaystyle A\subseteq X,}$ let

denote the open ball (respectively, the closed ball) of radius ${\displaystyle r}$ in ${\displaystyle \mathbb {K} }$ centered at ${\displaystyle 0,}$ and let
Similarly, if ${\displaystyle K\subseteq \mathbb {K} }$ and ${\displaystyle k}$ is a scalar then let ${\displaystyle KA=\{ka:k\in K,a\in A\},}$ ${\displaystyle Kx=\{kx:k\in K\},}$ ${\displaystyle kA=\{ka:a\in A\},}$ and ${\displaystyle \mathbb {K} x=\{kx:k\in \mathbb {K} \}=\operatorname {span} \{x\}.}$


£#h5#£One set absorbing another£#/h5#£
If ${\displaystyle S}$ and ${\displaystyle A}$ are subsets of ${\displaystyle X,}$ then ${\displaystyle A}$ is said to absorb ${\displaystyle S}$ if it satisfies any of the following equivalent conditions:

£#li#£Definition: There exists a real ${\displaystyle r>0}$ such that ${\displaystyle S\subseteq cA}$ for every scalar ${\displaystyle c}$ satisfying ${\displaystyle |c|\geq r.}$ £#ul#££#li#£If the scalar field is ${\displaystyle \mathbb {R} }$ then intuitively, " ${\displaystyle A}$ absorbs ${\displaystyle S}$ " means that if ${\displaystyle A}$ is perpetually "scaled up" or "inflated" (referring to ${\displaystyle tA}$ as ${\displaystyle t\to \infty }$ ) then eventually, all ${\displaystyle tA}$ will contain ${\displaystyle S}$ (for all positive ${\displaystyle t>0}$ sufficiently large); and similarly, ${\displaystyle tA}$ must also eventually contain ${\displaystyle S}$ for all negative ${\displaystyle t<0}$ sufficiently large in magnitude.£#/li#£ £#li#£This definition depends on the canonical norm on the underlying scalar field, which ties this definition to the usual Euclidean topology on the scalar field. Consequently, the definition of an absorbing set (given below) is also tied to this topology.£#/li#££#/ul#££#/li#£ £#li#£There exists a real ${\displaystyle r>0}$ such that ${\displaystyle cS\subseteq A}$ for every scalar ${\displaystyle c\neq 0}$ satisfying ${\displaystyle |c|\leq r.}$ £#ul#££#li#£If it is known that ${\displaystyle 0\in A}$ then the restriction ${\displaystyle c\neq 0}$ may be removed, giving the characterization: There exists a real ${\displaystyle r>0}$ such that ${\displaystyle cS\subseteq A}$ for every scalar ${\displaystyle c}$ satisfying ${\displaystyle |c|\leq r.}$ £#/li#££#/ul#££#/li#£ £#li#£There exists a real ${\displaystyle r>0}$ such that ${\displaystyle \left(B_{r}\setminus \{0\}\right)S\subseteq A.}$ £#ul#££#li#£The closed ball ${\displaystyle B_{\leq r}}$ (with the origin removed) can be used in place of the open ball ${\displaystyle B_{r},}$ giving the next characterization.£#/li#££#/ul#££#/li#£ £#li#£There exists a real ${\displaystyle r>0}$ such that ${\displaystyle \left(B_{\leq r}\setminus \{0\}\right)S\subseteq A.}$ £#/li#£
If ${\displaystyle A}$ is a balanced set then to this list can be appended:

There exists a scalar ${\displaystyle c\neq 0}$ such that ${\displaystyle S\subseteq cA.}$ £#/li#£ £#li#£There exists a scalar ${\displaystyle c\neq 0}$ such that ${\displaystyle cS\subseteq A.}$ £#/li#£
A set is said to absorb a point ${\displaystyle x}$ if it absorbs the singleton set ${\displaystyle \{x\}.}$ A set ${\displaystyle A}$ absorbs the origin if and only if it contains the origin; that is, if and only if ${\displaystyle 0\in A.}$

Examples

Every set absorbs the empty set but the empty set does not absorbs any non-empty set. The singleton set ${\displaystyle \{\mathbf {0} \}}$ containing the origin is the one and only singleton subset that absorbs itself.

Suppose that ${\displaystyle X}$ is equal to either ${\displaystyle \mathbb {R} ^{2}}$ or ${\displaystyle \mathbb {C} .}$ If ${\displaystyle A:=S^{1}\cup \{\mathbf {0} \}}$ is the unit circle (centered at the origin ${\displaystyle \mathbf {0} }$ ) together with the origin, then ${\displaystyle \{\mathbf {0} \}}$ is the one and only non-empty set that ${\displaystyle A}$ absorbs. Moreover, there does not exist any non-empty subset of ${\displaystyle X}$ that is absorbed by the unit circle ${\displaystyle S^{1}.}$ In contrast, every neighborhood of the origin absorbs every bounded subset of ${\displaystyle X}$ (and so in particular, absorbs every singleton subset/point).


£#h5#£Absorbing set£#/h5#£
A subset ${\displaystyle A}$ of a vector space ${\displaystyle X}$ over a field ${\displaystyle \mathbb {K} }$ is called an absorbing (or absorbent) subset of ${\displaystyle X}$ and is said to be absorbing in ${\displaystyle X}$ if it satisfies any of the following equivalent conditions (here ordered so that each condition is an easy consequence of the previous one, starting with the definition):

£#li#£Definition: For every ${\displaystyle x\in X,A}$ absorbs ${\displaystyle \{x\}.}$ Said differently, ${\displaystyle A}$ absorbs every point of ${\displaystyle X.}$ £#ul#££#li#£So in particular, ${\displaystyle A}$ can not be absorbing if ${\displaystyle 0\not \in A.}$ £#/li#££#/ul#££#/li#£ £#li#£For every ${\displaystyle x\in X,}$ there exists a real ${\displaystyle r>0}$ such that ${\displaystyle x\in cA}$ for any scalar ${\displaystyle c\in \mathbb {K} }$ satisfying ${\displaystyle |c|\geq r.}$ £#/li#£ £#li#£For every ${\displaystyle x\in X,}$ there exists a real ${\displaystyle r>0}$ such that ${\displaystyle cx\in A}$ for any scalar ${\displaystyle c\in \mathbb {K} }$ satisfying ${\displaystyle |c|\leq r.}$ £#/li#£ £#li#£For every ${\displaystyle x\in X,}$ there exists a real ${\displaystyle r>0}$ such that ${\displaystyle B_{r}x\subseteq A.}$ £#ul#££#li#£Here ${\displaystyle B_{r}=\{c\in \mathbb {K} :|c|<r\}}$ is the open ball of radius ${\displaystyle r}$ in the scalar field centered at the origin and ${\displaystyle B_{r}x=\left\{cx:c\in B_{r}\right\}=\{cx:c\in \mathbb {K} {\text{ and }}|c|<r\}.}$ £#/li#£ £#li#£The closed ball can be used in place of the open ball.£#/li#££#/ul#££#/li#£ £#li#£For every ${\displaystyle x\in X,}$ there exists a real ${\displaystyle r>0}$ such that ${\displaystyle B_{r}x\subseteq A\cap \mathbb {K} x,}$ where ${\displaystyle \mathbb {K} x=\operatorname {span} \{x\}.}$ £#ul#££#li#£Proof: This follows from the previous condition since ${\displaystyle B_{r}x\subseteq \mathbb {K} x,}$ so that ${\displaystyle B_{r}x\subseteq A}$ if and only if ${\displaystyle B_{r}x\subseteq A\cap \mathbb {K} x.}$ £#/li#£ £#li#£Connection to topology: If ${\displaystyle \mathbb {K} x}$ is given its usual Hausdorff Euclidean topology then the set ${\displaystyle B_{r}x}$ is a neighborhood of the origin in ${\displaystyle \mathbb {K} x;}$ thus, there exists a real ${\displaystyle r>0}$ such that ${\displaystyle B_{r}x\subseteq A\cap \mathbb {K} x}$ if and only if ${\displaystyle A\cap \mathbb {K} x}$ is a neighborhood of the origin in ${\displaystyle \mathbb {K} x.}$ £#/li#£ £#li#£Every 1-dimensional vector subspace of ${\displaystyle X}$ is of the form ${\displaystyle \mathbb {K} x=\operatorname {span} \{x\}}$ for some non-zero ${\displaystyle x\in X}$ and if this 1-dimensional space ${\displaystyle \mathbb {K} x}$ is endowed with the unique Hausdorff vector topology, then the map ${\displaystyle \mathbb {K} \to \mathbb {K} x}$ defined by ${\displaystyle c\mapsto cx}$ is necessarily a TVS-isomorphism (where as usual, ${\displaystyle \mathbb {K} }$ has the normed Euclidean topology).£#/li#££#/ul#££#/li#£ £#li#£ ${\displaystyle A}$ contains the origin and for every 1-dimensional vector subspace ${\displaystyle Y}$ of ${\displaystyle X,}$ ${\displaystyle A\cap Y}$ is a neighborhood of the origin in ${\displaystyle Y}$ when ${\displaystyle Y}$ is given its unique Hausdorff vector topology. £#ul#££#li#£The Hausdorff vector topology on a 1-dimensional vector space is necessarily TVS-isomorphic to ${\displaystyle \mathbb {K} }$ with its usual normed Euclidean topology.£#/li#£ £#li#£Intuition: This condition shows that it is only natural that any neighborhood of 0 in any topological vector space (TVS) ${\displaystyle X}$ be absorbing: if ${\displaystyle U}$ is a neighborhood of the origin in ${\displaystyle X}$ then it would be pathological if there existed any 1-dimensional vector subspace ${\displaystyle Y}$ in which ${\displaystyle U\cap Y}$ was not a neighborhood of the origin in at least some TVS topology on ${\displaystyle Y.}$ The only TVS topologies on ${\displaystyle Y}$ are the Hausdorff Euclidean topology and the trivial topology, which is a subset of the Euclidean topology. Consequently, it is natural to expect for ${\displaystyle U\cap Y}$ to be a neighborhood of ${\displaystyle 0}$ in the Euclidean topology for all 1-dimensional vector subspaces ${\displaystyle Y,}$ which is exactly the condition that ${\displaystyle U}$ be absorbing in ${\displaystyle X.}$ The fact that all neighborhoods of the origin in all TVSs are necessarily absorbing means that this pathological behavior does not occur. The reason why the Euclidean topology is distinguished is ultimately due to the defining requirement on TVS topologies that scalar multiplication ${\displaystyle \mathbb {K} \times X\to X}$ be continuous when the scalar field ${\displaystyle \mathbb {K} }$ is given the Euclidean topology.£#/li#£ £#li#£This condition is equivalent to: For every ${\displaystyle x\in X,}$ ${\displaystyle A\cap \operatorname {span} \{x\}}$ is a neighborhood of ${\displaystyle 0}$ in ${\displaystyle \operatorname {span} \{x\}=\mathbb {K} x}$ when ${\displaystyle \operatorname {span} \{x\}}$ is given its unique Hausdorff TVS topology.£#/li#££#/ul#££#/li#£ £#li#£ ${\displaystyle A}$ contains the origin and for every 1-dimensional vector subspace ${\displaystyle Y}$ of ${\displaystyle X,}$ ${\displaystyle A\cap Y}$ is absorbing in the ${\displaystyle Y.}$ £#ul#££#li#£Here "absorbing" means absorbing according to any defining condition other than this one.£#/li#£ £#li#£This shows that the property of being absorbing in ${\displaystyle X}$ depends only on how ${\displaystyle A}$ behaves with respect to 1 (or 0) dimensional vector subspaces of ${\displaystyle X.}$ In contrast, if a finite-dimensional vector subspace ${\displaystyle Z}$ of ${\displaystyle X}$ has dimension ${\displaystyle n>1}$ then ${\displaystyle A\cap Z}$ being absorbing in ${\displaystyle Z}$ is no longer sufficient to guarantee that ${\displaystyle A\cap Z}$ is a neighborhood of the origin in ${\displaystyle Z}$ when ${\displaystyle Z}$ is endowed with its unique Hausdorff TVS topology (although it will still be a necessary condition). For this to happen, it suffices for ${\displaystyle A\cap Z}$ to be a barrel in this Hausdorff TVS ${\displaystyle Z}$ (because every finite-dimensional Euclidean space is a barrelled space).£#/li#££#/ul#££#/li#£
If ${\displaystyle \mathbb {K} =\mathbb {R} }$ then to this list can be appended:

The algebraic interior of ${\displaystyle A}$ contains the origin (that is, ${\displaystyle 0\in {}^{i}A}$ ).£#/li#£
If ${\displaystyle A}$ is balanced then to this list can be appended:

For every ${\displaystyle x\in X,}$ there exists a scalar ${\displaystyle c\neq 0}$ such that ${\displaystyle x\in cA.}$ £#/li#£
If ${\displaystyle A}$ is convex or balanced then to this list can be appended:

For every ${\displaystyle x\in X,}$ there exists a positive real ${\displaystyle r>0}$ such that ${\displaystyle rx\in A.}$ £#ul#££#li#£The proof that a balanced set ${\displaystyle A}$ satisfying this condition is necessarily absorbing in ${\displaystyle X}$ is almost immediate from the definition of a "balanced set".£#/li#£ £#li#£The proof that a convex set ${\displaystyle A}$ satisfying this condition is necessarily absorbing in ${\displaystyle X}$ is less trivial (but not difficult). A detailed proof is given in this footnote and a summary is given below. £#ul#££#li#£Summary of proof: By assumption, for any non-zero ${\displaystyle 0\neq y\in X,}$ it is possible to pick positive real ${\displaystyle r>0}$ and ${\displaystyle R>0}$ such that ${\displaystyle Ry\in A}$ and ${\displaystyle r(-y)\in A}$ so that the convex set ${\displaystyle A\cap \mathbb {R} y}$ contains the open sub-interval ${\displaystyle (-r,R)y:=\{ty:-r<t<R,t\in \mathbb {R} \},}$ which contains the origin ( ${\displaystyle A\cap \mathbb {R} y}$ is called an interval since we identify ${\displaystyle \mathbb {R} y}$ with ${\displaystyle \mathbb {R} }$ and every non-empty convex subset of ${\displaystyle \mathbb {R} }$ is an interval). Give ${\displaystyle \mathbb {K} y}$ its unique Hausdorff vector topology so it remains to show that ${\displaystyle A\cap \mathbb {K} y}$ is a neighborhood of the origin in ${\displaystyle \mathbb {K} y.}$ If ${\displaystyle \mathbb {K} =\mathbb {R} }$ then we are done, so assume that ${\displaystyle \mathbb {K} =\mathbb {C} .}$ The set ${\displaystyle S\,:=\,(A\cap \mathbb {R} y)\,\cup \,(A\cap \mathbb {R} (iy))\,\subseteq \,A\cap (\mathbb {C} y)}$ is a union of two intervals, each of which contains an open sub-interval that contains the origin; moreover, the intersection of these two intervals is precisely the origin. So the convex hull of ${\displaystyle S,}$ which is contained in the convex set ${\displaystyle A\cap \mathbb {C} y,}$ clearly contains an open ball around the origin. ${\displaystyle \blacksquare }$ £#/li#££#/ul#££#/li#££#/ul#££#/li#£ £#li#£For every ${\displaystyle x\in X,}$ there exists a positive real ${\displaystyle r>0}$ such that ${\displaystyle x\in rA.}$ £#ul#££#li#£This condition is equivalent to: every ${\displaystyle x\in X}$ belongs to the set ${\displaystyle \bigcup _{0<r<\infty }rA=\{ra:0<r<\infty ,a\in A\}=(0,\infty )A.}$ This happens if and only if ${\displaystyle X=(0,\infty )A,}$ which gives the next characterization.£#/li#££#/ul#££#/li#£ £#li#£ ${\displaystyle (0,\infty )A=X.}$ £#ul#££#li#£It can be shown that for any subset ${\displaystyle T}$ of ${\displaystyle X,}$ ${\displaystyle (0,\infty )T=X}$ if and only if ${\displaystyle T\cap (0,\infty )x\neq \varnothing {\text{ for every }}x\in X.}$ £#/li#££#/ul#££#/li#£ £#li#£For every ${\displaystyle x\in X,A\cap (0,\infty )x\neq \varnothing ,}$ where ${\displaystyle (0,\infty )x:=\{rx:0<r<\infty \}}$ £#/li#£
If ${\displaystyle 0\in A}$ (which is necessary for ${\displaystyle A}$ to be absorbing) then it suffices to check any of the above conditions for all non-zero ${\displaystyle x\in X,}$ rather than all ${\displaystyle x\in X.}$


£#h5#£Examples and sufficient conditions£#/h5#£
£#h5#£For one set to absorb another£#/h5#£
Let ${\displaystyle F:X\to Y}$ be a linear map between vector spaces and let ${\displaystyle B\subseteq X}$ and ${\displaystyle C\subseteq Y}$ be balanced sets. Then ${\displaystyle C}$ absorbs ${\displaystyle F(B)}$ if and only if ${\displaystyle F^{-1}(C)}$ absorbs ${\displaystyle B.}$

If a set ${\displaystyle A}$ absorbs another set ${\displaystyle B}$ then any superset of ${\displaystyle A}$ also absorbs ${\displaystyle B.}$ A set ${\displaystyle A}$ absorbs the origin if and only if the origin is an element of ${\displaystyle A.}$


£#h5#£For a set to be absorbing£#/h5#£
In a semi normed vector space the unit ball is absorbing. More generally, if ${\displaystyle X}$ is a topological vector space (TVS) then any neighborhood of the origin in ${\displaystyle X}$ is absorbing in ${\displaystyle X.}$ This fact is one of the primary motivations for even defining the property "absorbing in ${\displaystyle X.}$ "

If ${\displaystyle D\neq \varnothing }$ is a disk in ${\displaystyle X}$ then ${\displaystyle \operatorname {span} D=\bigcup _{n=1}^{\infty }nD}$ so that in particular, ${\displaystyle D}$ is an absorbing subset of ${\displaystyle \operatorname {span} D.}$ Thus if ${\displaystyle D}$ is a disk in ${\displaystyle X,}$ then ${\displaystyle X}$ is absorbing in ${\displaystyle X}$ if and only if ${\displaystyle \operatorname {span} D=X.}$

Any superset of an absorbing set is absorbing. Thus the union of any family of (one or more) absorbing sets is absorbing. The intersection of a finite family of (one or more) absorbing sets is absorbing.

The image of an absorbing set under a surjective linear operator is again absorbing. The inverse image of an absorbing subset (of the codomain) under a linear operator is again absorbing (in the domain).


£#h5#£Properties£#/h5#£
Every absorbing set contains the origin.

If ${\displaystyle D}$ is an absorbing disk in a vector space ${\displaystyle X}$ then there exists an absorbing disk ${\displaystyle E}$ in ${\displaystyle X}$ such that ${\displaystyle E+E\subseteq D.}$


£#h5#£See also£#/h5#£ £#ul#££#li#£Algebraic interior – Generalization of topological interior£#/li#£ £#li#£Absolutely convex set£#/li#£ £#li#£Balanced set – Construct in functional analysis£#/li#£ £#li#£Bornivorous set – A set that can absorb any bounded subset£#/li#£ £#li#£Bounded set (topological vector space) – Generalization of boundedness£#/li#£ £#li#£Convex set – In geometry, set that intersects every line into a single line segment£#/li#£ £#li#£Locally convex topological vector space – A vector space with a topology defined by convex open sets£#/li#£ £#li#£Radial set£#/li#£ £#li#£Star domain – Property of point sets in Euclidean spaces£#/li#£ £#li#£Symmetric set£#/li#£ £#li#£Topological vector space – Vector space with a notion of nearness£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£Citations£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Berberian, Sterling K. (1974). Lectures in Functional Analysis and Operator Theory. Graduate Texts in Mathematics. Vol. 15. New York: Springer. ISBN 978-0-387-90081-0. OCLC 878109401.£#/li#£ £#li#£Bourbaki, Nicolas (1987) [1981]. Topological Vector Spaces: Chapters 1–5. Éléments de mathématique. Translated by Eggleston, H.G.; Madan, S. Berlin New York: Springer-Verlag. ISBN 3-540-13627-4. OCLC 17499190.£#/li#£ £#li#£Nicolas, Bourbaki (2003). Topological vector spaces Chapter 1-5 (English Translation). New York: Springer-Verlag. p. I.7. ISBN 3-540-42338-9.£#/li#£ £#li#£Conway, John (1990). A course in functional analysis. Graduate Texts in Mathematics. Vol. 96 (2nd ed.). New York: Springer-Verlag. ISBN 978-0-387-97245-9. OCLC 21195908.£#/li#£ £#li#£Diestel, Joe (2008). The Metric Theory of Tensor Products: Grothendieck's Résumé Revisited. Vol. 16. Providence, R.I.: American Mathematical Society. ISBN 9781470424831. OCLC 185095773.£#/li#£ £#li#£Dineen, Seán (1981). Complex Analysis in Locally Convex Spaces. North-Holland Mathematics Studies. Vol. 57. Amsterdam New York New York: North-Holland Pub. Co., Elsevier Science Pub. Co. ISBN 978-0-08-087168-4. OCLC 16549589.£#/li#£ £#li#£Dunford, Nelson; Schwartz, Jacob T. (1988). Linear Operators. Pure and applied mathematics. Vol. 1. New York: Wiley-Interscience. ISBN 978-0-471-60848-6. OCLC 18412261.£#/li#£ £#li#£Edwards, Robert E. (1995). Functional Analysis: Theory and Applications. New York: Dover Publications. ISBN 978-0-486-68143-6. OCLC 30593138.£#/li#£ £#li#£Grothendieck, Alexander (1973). Topological Vector Spaces. Translated by Chaljub, Orlando. New York: Gordon and Breach Science Publishers. ISBN 978-0-677-30020-7. OCLC 886098.£#/li#£ £#li#£Hogbe-Nlend, Henri (1977). Bornologies and Functional Analysis: Introductory Course on the Theory of Duality Topology-Bornology and its use in Functional Analysis. North-Holland Mathematics Studies. Vol. 26. Amsterdam New York New York: North Holland. ISBN 978-0-08-087137-0. OCLC 316549583.£#/li#£ £#li#£Hogbe-Nlend, Henri; Moscatelli, V. B. (1981). Nuclear and Conuclear Spaces: Introductory Course on Nuclear and Conuclear Spaces in the Light of the Duality "topology-bornology". North-Holland Mathematics Studies. Vol. 52. Amsterdam New York New York: North Holland. ISBN 978-0-08-087163-9. OCLC 316564345.£#/li#£ £#li#£Husain, Taqdir; Khaleelulla, S. M. (1978). Barrelledness in Topological and Ordered Vector Spaces. Lecture Notes in Mathematics. Vol. 692. Berlin, New York, Heidelberg: Springer-Verlag. ISBN 978-3-540-09096-0. OCLC 4493665.£#/li#£ £#li#£Jarchow, Hans (1981). Locally convex spaces. Stuttgart: B.G. Teubner. ISBN 978-3-519-02224-4. OCLC 8210342.£#/li#£ £#li#£Keller, Hans (1974). Differential Calculus in Locally Convex Spaces. Lecture Notes in Mathematics. Vol. 417. Berlin New York: Springer-Verlag. ISBN 978-3-540-06962-1. OCLC 1103033.£#/li#£ £#li#£Khaleelulla, S. M. (1982). Counterexamples in Topological Vector Spaces. Lecture Notes in Mathematics. Vol. 936. Berlin, Heidelberg, New York: Springer-Verlag. ISBN 978-3-540-11565-6. OCLC 8588370.£#/li#£ £#li#£Jarchow, Hans (1981). Locally convex spaces. Stuttgart: B.G. Teubner. ISBN 978-3-519-02224-4. OCLC 8210342.£#/li#£ £#li#£Köthe, Gottfried (1983) [1969]. Topological Vector Spaces I. Grundlehren der mathematischen Wissenschaften. Vol. 159. Translated by Garling, D.J.H. New York: Springer Science & Business Media. ISBN 978-3-642-64988-2. MR 0248498. OCLC 840293704.£#/li#£ £#li#£Köthe, Gottfried (1979). Topological Vector Spaces II. Grundlehren der mathematischen Wissenschaften. Vol. 237. New York: Springer Science & Business Media. ISBN 978-0-387-90400-9. OCLC 180577972.£#/li#£ £#li#£Narici, Lawrence; Beckenstein, Edward (2011). Topological Vector Spaces. Pure and applied mathematics (Second ed.). Boca Raton, FL: CRC Press. ISBN 978-1584888666. OCLC 144216834.£#/li#£ £#li#£Pietsch, Albrecht (1979). Nuclear Locally Convex Spaces. Ergebnisse der Mathematik und ihrer Grenzgebiete. Vol. 66 (Second ed.). Berlin, New York: Springer-Verlag. ISBN 978-0-387-05644-9. OCLC 539541.£#/li#£ £#li#£Robertson, Alex P.; Robertson, Wendy J. (1980). Topological Vector Spaces. Cambridge Tracts in Mathematics. Vol. 53. Cambridge England: Cambridge University Press. ISBN 978-0-521-29882-7. OCLC 589250.£#/li#£ £#li#£Robertson, A.P.; W.J. Robertson (1964). Topological vector spaces. Cambridge Tracts in Mathematics. Vol. 53. Cambridge University Press. p. 4.£#/li#£ £#li#£Rudin, Walter (1991). Functional Analysis. International Series in Pure and Applied Mathematics. Vol. 8 (Second ed.). New York, NY: McGraw-Hill Science/Engineering/Math. ISBN 978-0-07-054236-5. OCLC 21163277.£#/li#£ £#li#£Thompson, Anthony C. (1996). Minkowski Geometry. Encyclopedia of Mathematics and Its Applications. Cambridge University Press. ISBN 0-521-40472-X.£#/li#£ £#li#£Schaefer, Helmut H. (1971). Topological vector spaces. GTM. Vol. 3. New York: Springer-Verlag. p. 11. ISBN 0-387-98726-6.£#/li#£ £#li#£Schaefer, Helmut H.; Wolff, Manfred P. (1999). Topological Vector Spaces. GTM. Vol. 8 (Second ed.). New York, NY: Springer New York Imprint Springer. ISBN 978-1-4612-7155-0. OCLC 840278135.£#/li#£ £#li#£Schechter, Eric (1996). Handbook of Analysis and Its Foundations. San Diego, CA: Academic Press. ISBN 978-0-12-622760-4. OCLC 175294365.£#/li#£ £#li#£Schaefer, H. H. (1999). Topological Vector Spaces. New York, NY: Springer New York Imprint Springer. ISBN 978-1-4612-7155-0. OCLC 840278135.£#/li#£ £#li#£Swartz, Charles (1992). An introduction to Functional Analysis. New York: M. Dekker. ISBN 978-0-8247-8643-4. OCLC 24909067.£#/li#£ £#li#£Trèves, François (2006) [1967]. Topological Vector Spaces, Distributions and Kernels. Mineola, N.Y.: Dover Publications. ISBN 978-0-486-45352-1. OCLC 853623322.£#/li#£ £#li#£Wilansky, Albert (2013). Modern Methods in Topological Vector Spaces. Mineola, New York: Dover Publications, Inc. ISBN 978-0-486-49353-4. OCLC 849801114.£#/li#£ £#li#£Wong, Yau-Chuen (1979). Schwartz Spaces, Nuclear Spaces, and Tensor Products. Lecture Notes in Mathematics. Vol. 726. Berlin New York: Springer-Verlag. ISBN 978-3-540-09513-2. OCLC 5126158.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Wong, Y. Introductory Theory of Topological Vector Spaces. New York: Dekker, 1992.£#/li#££#li#£ Wong, Y. Introductory Theory of Topological Vector Spaces. New York: Dekker, 1992. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Functional Analysis £#/li#££#li#£ Foundations of Mathematics > Set Theory > Sets £#/li#££#/ul#£




£#h3#£Abstract Manifold£#/h3#£

In mathematics, a manifold is a topological space that locally resembles Euclidean space near each point. More precisely, an n-dimensional manifold, or n-manifold for short, is a topological space with the property that each point has a neighborhood that is homeomorphic to an open subset of n-dimensional Euclidean space.

One-dimensional manifolds include lines and circles, but not lemniscates. Two-dimensional manifolds are also called surfaces. Examples include the plane, the sphere, and the torus, and also the Klein bottle and real projective plane.

The concept of a manifold is central to many parts of geometry and modern mathematical physics because it allows complicated structures to be described in terms of well-understood topological properties of simpler spaces. Manifolds naturally arise as solution sets of systems of equations and as graphs of functions. The concept has applications in computer-graphics given the need to associate pictures with coordinates (e.g. CT scans).

Manifolds can be equipped with additional structure. One important class of manifolds are differentiable manifolds; their differentiable structure allows calculus to be done. A Riemannian metric on a manifold allows distances and angles to be measured. Symplectic manifolds serve as the phase spaces in the Hamiltonian formalism of classical mechanics, while four-dimensional Lorentzian manifolds model spacetime in general relativity.

The study of manifolds requires working knowledge of calculus and topology.


£#h5#£Motivating examples£#/h5#£
£#h5#£Circle£#/h5#£
After a line, a circle is the simplest example of a topological manifold. Topology ignores bending, so a small piece of a circle is treated the same as a small piece of a line. Considering, for instance, the top part of the unit circle, x2 + y2 = 1, where the y-coordinate is positive (indicated by the yellow arc in Figure 1). Any point of this arc can be uniquely described by its x-coordinate. So, projection onto the first coordinate is a continuous and invertible mapping from the upper arc to the open interval (−1, 1):

Such functions along with the open regions they map are called charts. Similarly, there are charts for the bottom (red), left (blue), and right (green) parts of the circle:

Together, these parts cover the whole circle, and the four charts form an atlas for the circle.

The top and right charts, ${\displaystyle \chi _{\mathrm {top} }}$ and ${\displaystyle \chi _{\mathrm {right} }}$ respectively, overlap in their domain: their intersection lies in the quarter of the circle where both ${\displaystyle x}$ and ${\displaystyle y}$ -coordinates are positive. Both map this part into the interval ${\displaystyle (0,1)}$ , though differently. Thus a function ${\displaystyle T:(0,1)\rightarrow (0,1)=\chi _{\mathrm {right} }\circ \chi _{\mathrm {top} }^{-1}}$ can be constructed, which takes values from the co-domain of ${\displaystyle \chi _{\mathrm {top} }}$ back to the circle using the inverse, followed by ${\displaystyle \chi _{\mathrm {right} }}$ back to the interval. For any number a in ${\displaystyle (0,1)}$ , then:

Such a function is called a transition map.

The top, bottom, left, and right charts do not form the only possible atlas. Charts need not be geometric projections, and the number of charts is a matter of choice. Consider the charts

and
Here s is the slope of the line through the point at coordinates (x, y) and the fixed pivot point (−1, 0); similarly, t is the opposite of the slope of the line through the points at coordinates (x, y) and (+1, 0). The inverse mapping from s to (x, y) is given by

It can be confirmed that x2 + y2 = 1 for all values of s and t. These two charts provide a second atlas for the circle, with the transition map

(that is, one has this relation between s and t for every point where s and t are both nonzero).
Each chart omits a single point, either (−1, 0) for s or (+1, 0) for t, so neither chart alone is sufficient to cover the whole circle. It can be proved that it is not possible to cover the full circle with a single chart. For example, although it is possible to construct a circle from a single line interval by overlapping and "gluing" the ends, this does not produce a chart; a portion of the circle will be mapped to both ends at once, losing invertibility.


£#h5#£Sphere£#/h5#£
The sphere is an example of a surface. The unit sphere of implicit equation

x2 + y2 + z2 – 1 = 0
may be covered by an atlas of six charts: the plane z = 0 divides the sphere into two half spheres (z > 0 and z < 0), which may both be mapped on the disc x2 + y2 < 1 by the projection on the xy plane of coordinates. This provides two charts; the four other charts are provided by a similar construction with the two other coordinate planes.

As for the circle, one may define one chart that covers the whole sphere excluding one point. Thus two charts are sufficient, but the sphere cannot be covered by a single chart.

This example is historically significant, as it has motivated the terminology; it became apparent that the whole surface of the Earth cannot have a plane representation consisting of a single map (also called "chart", see nautical chart), and therefore one needs atlases for covering the whole Earth surface.


£#h5#£Other curves£#/h5#£
Manifolds need not be connected (all in "one piece"); an example is a pair of separate circles.

Manifolds need not be closed; thus a line segment without its end points is a manifold. They are never countable, unless the dimension of the manifold is 0. Putting these freedoms together, other examples of manifolds are a parabola, a hyperbola, and the locus of points on a cubic curve y2 = x3 − x (a closed loop piece and an open, infinite piece).

However, excluded are examples like two touching circles that share a point to form a figure-8; at the shared point, a satisfactory chart cannot be created. Even with the bending allowed by topology, the vicinity of the shared point looks like a "+", not a line. A "+" is not homeomorphic to a line segment, since deleting the center point from the "+" gives a space with four components (i.e. pieces), whereas deleting a point from a line segment gives a space with at most two pieces; topological operations always preserve the number of pieces.


£#h5#£Mathematical definition£#/h5#£
Informally, a manifold is a space that is "modeled on" Euclidean space.

There are many different kinds of manifolds. In geometry and topology, all manifolds are topological manifolds, possibly with additional structure. A manifold can be constructed by giving a collection of coordinate charts, that is, a covering by open sets with homeomorphisms to a Euclidean space, and patching functions: homeomorphisms from one region of Euclidean space to another region if they correspond to the same part of the manifold in two different coordinate charts. A manifold can be given additional structure if the patching functions satisfy axioms beyond continuity. For instance, differentiable manifolds have homeomorphisms on overlapping neighborhoods diffeomorphic with each other, so that the manifold has a well-defined set of functions which are differentiable in each neighborhood, thus differentiable on the manifold as a whole.

Formally, a (topological) manifold is a second countable Hausdorff space that is locally homeomorphic to Euclidean space.

Second countable and Hausdorff are point-set conditions; second countable excludes spaces which are in some sense 'too large' such as the long line, while Hausdorff excludes spaces such as "the line with two origins" (these generalizations of manifolds are discussed in non-Hausdorff manifolds).

Locally homeomorphic to Euclidean space means that every point has a neighborhood homeomorphic to an open Euclidean n-ball,

More precisely, locally homeomorphic here means that each point m in the manifold M has an open neighborhood homeomorphic to an open neighborhood in Euclidean space. However, given such a homeomorphism, the pre-image of an ${\displaystyle \epsilon }$ -ball gives a homeomorphism between the unit ball and a smaller neighborhood of m, so this is no loss of generality. For topological or differentiable manifolds, one can also ask that every point have a neighborhood homeomorphic to all of Euclidean space (as this is diffeomorphic to the unit ball), but this cannot be done for complex manifolds, as the complex unit ball is not holomorphic to complex space.
Generally manifolds are taken to have a fixed dimension (the space must be locally homeomorphic to a fixed n-ball), and such a space is called an n-manifold; however, some authors admit manifolds where different points can have different dimensions. If a manifold has a fixed dimension, it is called a pure manifold. For example, the (surface of a) sphere has a constant dimension of 2 and is therefore a pure manifold whereas the disjoint union of a sphere and a line in three-dimensional space is not a pure manifold. Since dimension is a local invariant (i.e. the map sending each point to the dimension of its neighbourhood over which a chart is defined, is locally constant), each connected component has a fixed dimension.

Scheme-theoretically, a manifold is a locally ringed space, whose structure sheaf is locally isomorphic to the sheaf of continuous (or differentiable, or complex-analytic, etc.) functions on Euclidean space. This definition is mostly used when discussing analytic manifolds in algebraic geometry.


£#h5#£Charts, atlases, and transition maps£#/h5#£
The spherical Earth is navigated using flat maps or charts, collected in an atlas. Similarly, a differentiable manifold can be described using mathematical maps, called coordinate charts, collected in a mathematical atlas. It is not generally possible to describe a manifold with just one chart, because the global structure of the manifold is different from the simple structure of the charts. For example, no single flat map can represent the entire Earth without separation of adjacent features across the map's boundaries or duplication of coverage. When a manifold is constructed from multiple overlapping charts, the regions where they overlap carry information essential to understanding the global structure.


£#h5#£Charts£#/h5#£
A coordinate map, a coordinate chart, or simply a chart, of a manifold is an invertible map between a subset of the manifold and a simple space such that both the map and its inverse preserve the desired structure. For a topological manifold, the simple space is a subset of some Euclidean space ${\displaystyle \mathbb {R} ^{n}}$ and interest focuses on the topological structure. This structure is preserved by homeomorphisms, invertible maps that are continuous in both directions.

In the case of a differentiable manifold, a set of charts called an atlas allows us to do calculus on manifolds. Polar coordinates, for example, form a chart for the plane ${\displaystyle \mathbb {R} ^{2}}$ minus the positive x-axis and the origin. Another example of a chart is the map χtop mentioned above, a chart for the circle.


£#h5#£Atlases£#/h5#£
The description of most manifolds requires more than one chart. A specific collection of charts which covers a manifold is called an atlas. An atlas is not unique as all manifolds can be covered in multiple ways using different combinations of charts. Two atlases are said to be equivalent if their union is also an atlas.

The atlas containing all possible charts consistent with a given atlas is called the maximal atlas (i.e. an equivalence class containing that given atlas). Unlike an ordinary atlas, the maximal atlas of a given manifold is unique. Though useful for definitions, it is an abstract object and not used directly (e.g. in calculations).


£#h5#£Transition maps£#/h5#£
Charts in an atlas may overlap and a single point of a manifold may be represented in several charts. If two charts overlap, parts of them represent the same region of the manifold, just as a map of Europe and a map of Russia may both contain Moscow. Given two overlapping charts, a transition function can be defined which goes from an open ball in ${\displaystyle \mathbb {R} ^{n}}$ to the manifold and then back to another (or perhaps the same) open ball in ${\displaystyle \mathbb {R} ^{n}}$ . The resultant map, like the map T in the circle example above, is called a change of coordinates, a coordinate transformation, a transition function, or a transition map.


£#h5#£Additional structure£#/h5#£
An atlas can also be used to define additional structure on the manifold. The structure is first defined on each chart separately. If all transition maps are compatible with this structure, the structure transfers to the manifold.

This is the standard way differentiable manifolds are defined. If the transition functions of an atlas for a topological manifold preserve the natural differential structure of ${\displaystyle \mathbb {R} ^{n}}$ (that is, if they are diffeomorphisms), the differential structure transfers to the manifold and turns it into a differentiable manifold. Complex manifolds are introduced in an analogous way by requiring that the transition functions of an atlas are holomorphic functions. For symplectic manifolds, the transition functions must be symplectomorphisms.

The structure on the manifold depends on the atlas, but sometimes different atlases can be said to give rise to the same structure. Such atlases are called compatible.

These notions are made precise in general through the use of pseudogroups.


£#h5#£Manifold with boundary£#/h5#£
A manifold with boundary is a manifold with an edge. For example, a sheet of paper is a 2-manifold with a 1-dimensional boundary. The boundary of an n-manifold with boundary is an (n−1)-manifold. A disk (circle plus interior) is a 2-manifold with boundary. Its boundary is a circle, a 1-manifold. A square with interior is also a 2-manifold with boundary. A ball (sphere plus interior) is a 3-manifold with boundary. Its boundary is a sphere, a 2-manifold. (Do not confuse with Boundary (topology)).

In technical language, a manifold with boundary is a space containing both interior points and boundary points. Every interior point has a neighborhood homeomorphic to the open n-ball {(x1, x2, ..., xn) | Σxi2 < 1}. Every boundary point has a neighborhood homeomorphic to the "half" n-ball {(x1, x2, ..., xn) | Σxi2 < 1 and x1 ≥ 0} . The homeomorphism must send each boundary point to a point with x1 = 0.


£#h5#£Boundary and interior£#/h5#£
Let M be a manifold with boundary. The interior of M, denoted Int M, is the set of points in M which have neighborhoods homeomorphic to an open subset of ${\displaystyle \mathbb {R} ^{n}}$ . The boundary of M, denoted ∂M, is the complement of Int M in M. The boundary points can be characterized as those points which land on the boundary hyperplane (xn = 0) of ${\displaystyle \mathbb {R} _{+}^{n}}$ under some coordinate chart.

If M is a manifold with boundary of dimension n, then Int M is a manifold (without boundary) of dimension n and ∂M is a manifold (without boundary) of dimension n − 1.


£#h5#£Construction£#/h5#£
A single manifold can be constructed in different ways, each stressing a different aspect of the manifold, thereby leading to a slightly different viewpoint.


£#h5#£Charts£#/h5#£
Perhaps the simplest way to construct a manifold is the one used in the example above of the circle. First, a subset of ${\displaystyle \mathbb {R} ^{2}}$ is identified, and then an atlas covering this subset is constructed. The concept of manifold grew historically from constructions like this. Here is another example, applying this method to the construction of a sphere:


£#h5#£Sphere with charts£#/h5#£
A sphere can be treated in almost the same way as the circle. In mathematics a sphere is just the surface (not the solid interior), which can be defined as a subset of ${\displaystyle \mathbb {R} ^{3}}$ :

The sphere is two-dimensional, so each chart will map part of the sphere to an open subset of ${\displaystyle \mathbb {R} ^{2}}$ . Consider the northern hemisphere, which is the part with positive z coordinate (coloured red in the picture on the right). The function χ defined by

maps the northern hemisphere to the open unit disc by projecting it on the (x, y) plane. A similar chart exists for the southern hemisphere. Together with two charts projecting on the (x, z) plane and two charts projecting on the (y, z) plane, an atlas of six charts is obtained which covers the entire sphere.

This can be easily generalized to higher-dimensional spheres.


£#h5#£Patchwork£#/h5#£
A manifold can be constructed by gluing together pieces in a consistent manner, making them into overlapping charts. This construction is possible for any manifold and hence it is often used as a characterisation, especially for differentiable and Riemannian manifolds. It focuses on an atlas, as the patches naturally provide charts, and since there is no exterior space involved it leads to an intrinsic view of the manifold.

The manifold is constructed by specifying an atlas, which is itself defined by transition maps. A point of the manifold is therefore an equivalence class of points which are mapped to each other by transition maps. Charts map equivalence classes to points of a single patch. There are usually strong demands on the consistency of the transition maps. For topological manifolds they are required to be homeomorphisms; if they are also diffeomorphisms, the resulting manifold is a differentiable manifold.

This can be illustrated with the transition map t = 1⁄s from the second half of the circle example. Start with two copies of the line. Use the coordinate s for the first copy, and t for the second copy. Now, glue both copies together by identifying the point t on the second copy with the point s = 1⁄t on the first copy (the points t = 0 and s = 0 are not identified with any point on the first and second copy, respectively). This gives a circle.


£#h5#£Intrinsic and extrinsic view£#/h5#£
The first construction and this construction are very similar, but represent rather different points of view. In the first construction, the manifold is seen as embedded in some Euclidean space. This is the extrinsic view. When a manifold is viewed in this way, it is easy to use intuition from Euclidean spaces to define additional structure. For example, in a Euclidean space, it is always clear whether a vector at some point is tangential or normal to some surface through that point.

The patchwork construction does not use any embedding, but simply views the manifold as a topological space by itself. This abstract point of view is called the intrinsic view. It can make it harder to imagine what a tangent vector might be, and there is no intrinsic notion of a normal bundle, but instead there is an intrinsic stable normal bundle.


£#h5#£n-Sphere as a patchwork£#/h5#£
The n-sphere Sn is a generalisation of the idea of a circle (1-sphere) and sphere (2-sphere) to higher dimensions. An n-sphere Sn can be constructed by gluing together two copies of ${\displaystyle \mathbb {R} ^{n}}$ . The transition map between them is inversion in a sphere, defined as

This function is its own inverse and thus can be used in both directions. As the transition map is a smooth function, this atlas defines a smooth manifold. In the case n = 1, the example simplifies to the circle example given earlier.


£#h5#£Identifying points of a manifold£#/h5#£
It is possible to define different points of a manifold to be same. This can be visualized as gluing these points together in a single point, forming a quotient space. There is, however, no reason to expect such quotient spaces to be manifolds. Among the possible quotient spaces that are not necessarily manifolds, orbifolds and CW complexes are considered to be relatively well-behaved. An example of a quotient space of a manifold that is also a manifold is the real projective space, identified as a quotient space of the corresponding sphere.

One method of identifying points (gluing them together) is through a right (or left) action of a group, which acts on the manifold. Two points are identified if one is moved onto the other by some group element. If M is the manifold and G is the group, the resulting quotient space is denoted by M / G (or G \ M).

Manifolds which can be constructed by identifying points include tori and real projective spaces (starting with a plane and a sphere, respectively).


£#h5#£Gluing along boundaries£#/h5#£
Two manifolds with boundaries can be glued together along a boundary. If this is done the right way, the result is also a manifold. Similarly, two boundaries of a single manifold can be glued together.

Formally, the gluing is defined by a bijection between the two boundaries. Two points are identified when they are mapped onto each other. For a topological manifold, this bijection should be a homeomorphism, otherwise the result will not be a topological manifold. Similarly, for a differentiable manifold, it has to be a diffeomorphism. For other manifolds, other structures should be preserved.

A finite cylinder may be constructed as a manifold by starting with a strip [0,1] × [0,1] and gluing a pair of opposite edges on the boundary by a suitable diffeomorphism. A projective plane may be obtained by gluing a sphere with a hole in it to a Möbius strip along their respective circular boundaries.


£#h5#£ Cartesian products£#/h5#£
The Cartesian product of manifolds is also a manifold.

The dimension of the product manifold is the sum of the dimensions of its factors. Its topology is the product topology, and a Cartesian product of charts is a chart for the product manifold. Thus, an atlas for the product manifold can be constructed using atlases for its factors. If these atlases define a differential structure on the factors, the corresponding atlas defines a differential structure on the product manifold. The same is true for any other structure defined on the factors. If one of the factors has a boundary, the product manifold also has a boundary. Cartesian products may be used to construct tori and finite cylinders, for example, as S1 × S1 and S1 × [0,1], respectively.


£#h5#£History£#/h5#£
The study of manifolds combines many important areas of mathematics: it generalizes concepts such as curves and surfaces as well as ideas from linear algebra and topology.


£#h5#£Early development£#/h5#£
Before the modern concept of a manifold there were several important results.

Non-Euclidean geometry considers spaces where Euclid's parallel postulate fails. Saccheri first studied such geometries in 1733, but sought only to disprove them. Gauss, Bolyai and Lobachevsky independently discovered them 100 years later. Their research uncovered two types of spaces whose geometric structures differ from that of classical Euclidean space; these gave rise to hyperbolic geometry and elliptic geometry. In the modern theory of manifolds, these notions correspond to Riemannian manifolds with constant negative and positive curvature, respectively.

Carl Friedrich Gauss may have been the first to consider abstract spaces as mathematical objects in their own right. His theorema egregium gives a method for computing the curvature of a surface without considering the ambient space in which the surface lies. Such a surface would, in modern terminology, be called a manifold; and in modern terms, the theorem proved that the curvature of the surface is an intrinsic property. Manifold theory has come to focus exclusively on these intrinsic properties (or invariants), while largely ignoring the extrinsic properties of the ambient space.

Another, more topological example of an intrinsic property of a manifold is its Euler characteristic. Leonhard Euler showed that for a convex polytope in the three-dimensional Euclidean space with V vertices (or corners), E edges, and F faces,

The same formula will hold if we project the vertices and edges of the polytope onto a sphere, creating a topological map with V vertices, E edges, and F faces, and in fact, will remain true for any spherical map, even if it does not arise from any convex polytope. Thus 2 is a topological invariant of the sphere, called its Euler characteristic. On the other hand, a torus can be sliced open by its 'parallel' and 'meridian' circles, creating a map with V = 1 vertex, E = 2 edges, and F = 1 face. Thus the Euler characteristic of the torus is 1 − 2 + 1 = 0. The Euler characteristic of other surfaces is a useful topological invariant, which can be extended to higher dimensions using Betti numbers. In the mid nineteenth century, the Gauss–Bonnet theorem linked the Euler characteristic to the Gaussian curvature.
£#h5#£Synthesis£#/h5#£
Investigations of Niels Henrik Abel and Carl Gustav Jacobi on inversion of elliptic integrals in the first half of 19th century led them to consider special types of complex manifolds, now known as Jacobians. Bernhard Riemann further contributed to their theory, clarifying the geometric meaning of the process of analytic continuation of functions of complex variables.

Another important source of manifolds in 19th century mathematics was analytical mechanics, as developed by Siméon Poisson, Jacobi, and William Rowan Hamilton. The possible states of a mechanical system are thought to be points of an abstract space, phase space in Lagrangian and Hamiltonian formalisms of classical mechanics. This space is, in fact, a high-dimensional manifold, whose dimension corresponds to the degrees of freedom of the system and where the points are specified by their generalized coordinates. For an unconstrained movement of free particles the manifold is equivalent to the Euclidean space, but various conservation laws constrain it to more complicated formations, e.g. Liouville tori. The theory of a rotating solid body, developed in the 18th century by Leonhard Euler and Joseph-Louis Lagrange, gives another example where the manifold is nontrivial. Geometrical and topological aspects of classical mechanics were emphasized by Henri Poincaré, one of the founders of topology.

Riemann was the first one to do extensive work generalizing the idea of a surface to higher dimensions. The name manifold comes from Riemann's original German term, Mannigfaltigkeit, which William Kingdon Clifford translated as "manifoldness". In his Göttingen inaugural lecture, Riemann described the set of all possible values of a variable with certain constraints as a Mannigfaltigkeit, because the variable can have many values. He distinguishes between stetige Mannigfaltigkeit and diskrete Mannigfaltigkeit (continuous manifoldness and discontinuous manifoldness), depending on whether the value changes continuously or not. As continuous examples, Riemann refers to not only colors and the locations of objects in space, but also the possible shapes of a spatial figure. Using induction, Riemann constructs an n-fach ausgedehnte Mannigfaltigkeit (n times extended manifoldness or n-dimensional manifoldness) as a continuous stack of (n−1) dimensional manifoldnesses. Riemann's intuitive notion of a Mannigfaltigkeit evolved into what is today formalized as a manifold. Riemannian manifolds and Riemann surfaces are named after Riemann.


£#h5#£Poincaré's definition£#/h5#£
In his very influential paper, Analysis Situs, Henri Poincaré gave a definition of a differentiable manifold (variété) which served as a precursor to the modern concept of a manifold.

In the first section of Analysis Situs, Poincaré defines a manifold as the level set of a continuously differentiable function between Euclidean spaces that satisfies the nondegeneracy hypothesis of the implicit function theorem. In the third section, he begins by remarking that the graph of a continuously differentiable function is a manifold in the latter sense. He then proposes a new, more general, definition of manifold based on a 'chain of manifolds' (une chaîne des variétés).

Poincaré's notion of a chain of manifolds is a precursor to the modern notion of atlas. In particular, he considers two manifolds defined respectively as graphs of functions ${\displaystyle \theta (y)}$ and ${\displaystyle \theta '\left(y'\right)}$ . If these manifolds overlap (a une partie commune), then he requires that the coordinates ${\displaystyle y}$ depend continuously differentiably on the coordinates ${\displaystyle y'}$ and vice versa ('...les ${\displaystyle y}$ sont fonctions analytiques des ${\displaystyle y'}$ et inversement'). In this way he introduces a precursor to the notion of a chart and of a transition map.

For example, the unit circle in the plane can be thought of as the graph of the function ${\textstyle y={\sqrt {1-x^{2}}}}$ or else the function ${\textstyle y=-{\sqrt {1-x^{2}}}}$ in a neighborhood of every point except the points (1, 0) and (−1, 0); and in a neighborhood of those points, it can be thought of as the graph of, respectively, ${\textstyle x={\sqrt {1-y^{2}}}}$ and ${\textstyle x=-{\sqrt {1-y^{2}}}}$ . The circle can be represented by a graph in the neighborhood of every point because the left hand side of its defining equation ${\displaystyle x^{2}+y^{2}-1=0}$ has nonzero gradient at every point of the circle. By the implicit function theorem, every submanifold of Euclidean space is locally the graph of a function.

Hermann Weyl gave an intrinsic definition for differentiable manifolds in his lecture course on Riemann surfaces in 1911–1912, opening the road to the general concept of a topological space that followed shortly. During the 1930s Hassler Whitney and others clarified the foundational aspects of the subject, and thus intuitions dating back to the latter half of the 19th century became precise, and developed through differential geometry and Lie group theory. Notably, the Whitney embedding theorem showed that the intrinsic definition in terms of charts was equivalent to Poincaré's definition in terms of subsets of Euclidean space.


£#h5#£Topology of manifolds: highlights£#/h5#£
Two-dimensional manifolds, also known as a 2D surfaces embedded in our common 3D space, were considered by Riemann under the guise of Riemann surfaces, and rigorously classified in the beginning of the 20th century by Poul Heegaard and Max Dehn. Poincaré pioneered the study of three-dimensional manifolds and raised a fundamental question about them, today known as the Poincaré conjecture. After nearly a century, Grigori Perelman proved the Poincaré conjecture (see the Solution of the Poincaré conjecture). William Thurston's geometrization program, formulated in the 1970s, provided a far-reaching extension of the Poincaré conjecture to the general three-dimensional manifolds. Four-dimensional manifolds were brought to the forefront of mathematical research in the 1980s by Michael Freedman and in a different setting, by Simon Donaldson, who was motivated by the then recent progress in theoretical physics (Yang–Mills theory), where they serve as a substitute for ordinary 'flat' spacetime. Andrey Markov Jr. showed in 1960 that no algorithm exists for classifying four-dimensional manifolds. Important work on higher-dimensional manifolds, including analogues of the Poincaré conjecture, had been done earlier by René Thom, John Milnor, Stephen Smale and Sergei Novikov. A very pervasive and flexible technique underlying much work on the topology of manifolds is Morse theory.


£#h5#£Additional structure£#/h5#£
£#h5#£Topological manifolds£#/h5#£
The simplest kind of manifold to define is the topological manifold, which looks locally like some "ordinary" Euclidean space ${\displaystyle \mathbb {R} ^{n}}$ . By definition, all manifolds are topological manifolds, so the phrase "topological manifold" is usually used to emphasize that a manifold lacks additional structure, or that only its topological properties are being considered. Formally, a topological manifold is a topological space locally homeomorphic to a Euclidean space. This means that every point has a neighbourhood for which there exists a homeomorphism (a bijective continuous function whose inverse is also continuous) mapping that neighbourhood to ${\displaystyle \mathbb {R} ^{n}}$ . These homeomorphisms are the charts of the manifold.

A topological manifold looks locally like a Euclidean space in a rather weak manner: while for each individual chart it is possible to distinguish differentiable functions or measure distances and angles, merely by virtue of being a topological manifold a space does not have any particular and consistent choice of such concepts. In order to discuss such properties for a manifold, one needs to specify further structure and consider differentiable manifolds and Riemannian manifolds discussed below. In particular, the same underlying topological manifold can have several mutually incompatible classes of differentiable functions and an infinite number of ways to specify distances and angles.

Usually additional technical assumptions on the topological space are made to exclude pathological cases. It is customary to require that the space be Hausdorff and second countable.

The dimension of the manifold at a certain point is the dimension of the Euclidean space that the charts at that point map to (number n in the definition). All points in a connected manifold have the same dimension. Some authors require that all charts of a topological manifold map to Euclidean spaces of same dimension. In that case every topological manifold has a topological invariant, its dimension.


£#h5#£Differentiable manifolds£#/h5#£
For most applications, a special kind of topological manifold, namely, a differentiable manifold, is used. If the local charts on a manifold are compatible in a certain sense, one can define directions, tangent spaces, and differentiable functions on that manifold. In particular it is possible to use calculus on a differentiable manifold. Each point of an n-dimensional differentiable manifold has a tangent space. This is an n-dimensional Euclidean space consisting of the tangent vectors of the curves through the point.

Two important classes of differentiable manifolds are smooth and analytic manifolds. For smooth manifolds the transition maps are smooth, that is, infinitely differentiable. Analytic manifolds are smooth manifolds with the additional condition that the transition maps are analytic (they can be expressed as power series). The sphere can be given analytic structure, as can most familiar curves and surfaces.

A rectifiable set generalizes the idea of a piecewise smooth or rectifiable curve to higher dimensions; however, rectifiable sets are not in general manifolds.


£#h5#£Riemannian manifolds£#/h5#£
To measure distances and angles on manifolds, the manifold must be Riemannian. A Riemannian manifold is a differentiable manifold in which each tangent space is equipped with an inner product ⟨⋅ , ⋅⟩ in a manner which varies smoothly from point to point. Given two tangent vectors u and v, the inner product ⟨u , v⟩ gives a real number. The dot (or scalar) product is a typical example of an inner product. This allows one to define various notions such as length, angles, areas (or volumes), curvature and divergence of vector fields.

All differentiable manifolds (of constant dimension) can be given the structure of a Riemannian manifold. The Euclidean space itself carries a natural structure of Riemannian manifold (the tangent spaces are naturally identified with the Euclidean space itself and carry the standard scalar product of the space). Many familiar curves and surfaces, including for example all n-spheres, are specified as subspaces of a Euclidean space and inherit a metric from their embedding in it.


£#h5#£Finsler manifolds£#/h5#£
A Finsler manifold allows the definition of distance but does not require the concept of angle; it is an analytic manifold in which each tangent space is equipped with a norm, ||·||, in a manner which varies smoothly from point to point. This norm can be extended to a metric, defining the length of a curve; but it cannot in general be used to define an inner product.

Any Riemannian manifold is a Finsler manifold.


£#h5#£Lie groups£#/h5#£
Lie groups, named after Sophus Lie, are differentiable manifolds that carry also the structure of a group which is such that the group operations are defined by smooth maps.

A Euclidean vector space with the group operation of vector addition is an example of a non-compact Lie group. A simple example of a compact Lie group is the circle: the group operation is simply rotation. This group, known as U(1), can be also characterised as the group of complex numbers of modulus 1 with multiplication as the group operation.

Other examples of Lie groups include special groups of matrices, which are all subgroups of the general linear group, the group of n by n matrices with non-zero determinant. If the matrix entries are real numbers, this will be an n2-dimensional disconnected manifold. The orthogonal groups, the symmetry groups of the sphere and hyperspheres, are n(n−1)/2 dimensional manifolds, where n−1 is the dimension of the sphere. Further examples can be found in the table of Lie groups.


£#h5#£Other types of manifolds£#/h5#£ £#ul#££#li#£A complex manifold is a manifold whose charts take values in ${\displaystyle \mathbb {C} ^{n}}$ and whose transition functions are holomorphic on the overlaps. These manifolds are the basic objects of study in complex geometry. A one-complex-dimensional manifold is called a Riemann surface. An n-dimensional complex manifold has dimension 2n as a real differentiable manifold.£#/li#£ £#li#£A CR manifold is a manifold modeled on boundaries of domains in ${\displaystyle \mathbb {C} ^{n}}$ .£#/li#£ £#li#£'Infinite dimensional manifolds': to allow for infinite dimensions, one may consider Banach manifolds which are locally homeomorphic to Banach spaces. Similarly, Fréchet manifolds are locally homeomorphic to Fréchet spaces.£#/li#£ £#li#£A symplectic manifold is a kind of manifold which is used to represent the phase spaces in classical mechanics. They are endowed with a 2-form that defines the Poisson bracket. A closely related type of manifold is a contact manifold.£#/li#£ £#li#£A combinatorial manifold is a kind of manifold which is discretization of a manifold. It usually means a piecewise linear manifold made by simplicial complexes.£#/li#£ £#li#£A digital manifold is a special kind of combinatorial manifold which is defined in digital space. See digital topology£#/li#££#/ul#£
£#h5#£Classification and invariants£#/h5#£
Different notions of manifolds have different notions of classification and invariant; in this section we focus on smooth closed manifolds.

The classification of smooth closed manifolds is well understood in principle, except in dimension 4: in low dimensions (2 and 3) it is geometric, via the uniformization theorem and the solution of the Poincaré conjecture, and in high dimension (5 and above) it is algebraic, via surgery theory. This is a classification in principle: the general question of whether two smooth manifolds are diffeomorphic is not computable in general. Further, specific computations remain difficult, and there are many open questions.

Orientable surfaces can be visualized, and their diffeomorphism classes enumerated, by genus. Given two orientable surfaces, one can determine if they are diffeomorphic by computing their respective genera and comparing: they are diffeomorphic if and only if the genera are equal, so the genus forms a complete set of invariants.

This is much harder in higher dimensions: higher-dimensional manifolds cannot be directly visualized (though visual intuition is useful in understanding them), nor can their diffeomorphism classes be enumerated, nor can one in general determine if two different descriptions of a higher-dimensional manifold refer to the same object.

However, one can determine if two manifolds are different if there is some intrinsic characteristic that differentiates them. Such criteria are commonly referred to as invariants, because, while they may be defined in terms of some presentation (such as the genus in terms of a triangulation), they are the same relative to all possible descriptions of a particular manifold: they are invariant under different descriptions.

Naively, one could hope to develop an arsenal of invariant criteria that would definitively classify all manifolds up to isomorphism. Unfortunately, it is known that for manifolds of dimension 4 and higher, no program exists that can decide whether two manifolds are diffeomorphic.

Smooth manifolds have a rich set of invariants, coming from point-set topology, classic algebraic topology, and geometric topology. The most familiar invariants, which are visible for surfaces, are orientability (a normal invariant, also detected by homology) and genus (a homological invariant).

Smooth closed manifolds have no local invariants (other than dimension), though geometric manifolds have local invariants, notably the curvature of a Riemannian manifold and the torsion of a manifold equipped with an affine connection. This distinction between local invariants and no local invariants is a common way to distinguish between geometry and topology. All invariants of a smooth closed manifold are thus global.

Algebraic topology is a source of a number of important global invariant properties. Some key criteria include the simply connected property and orientability (see below). Indeed, several branches of mathematics, such as homology and homotopy theory, and the theory of characteristic classes were founded in order to study invariant properties of manifolds.


£#h5#£Surfaces£#/h5#£
£#h5#£Orientability£#/h5#£
In dimensions two and higher, a simple but important invariant criterion is the question of whether a manifold admits a meaningful orientation. Consider a topological manifold with charts mapping to ${\displaystyle \mathbb {R} ^{n}}$ . Given an ordered basis for ${\displaystyle \mathbb {R} ^{n}}$ , a chart causes its piece of the manifold to itself acquire a sense of ordering, which in 3-dimensions can be viewed as either right-handed or left-handed. Overlapping charts are not required to agree in their sense of ordering, which gives manifolds an important freedom. For some manifolds, like the sphere, charts can be chosen so that overlapping regions agree on their "handedness"; these are orientable manifolds. For others, this is impossible. The latter possibility is easy to overlook, because any closed surface embedded (without self-intersection) in three-dimensional space is orientable.

Some illustrative examples of non-orientable manifolds include: (1) the Möbius strip, which is a manifold with boundary, (2) the Klein bottle, which must intersect itself in its 3-space representation, and (3) the real projective plane, which arises naturally in geometry.


£#h5#£Möbius strip£#/h5#£
Begin with an infinite circular cylinder standing vertically, a manifold without boundary. Slice across it high and low to produce two circular boundaries, and the cylindrical strip between them. This is an orientable manifold with boundary, upon which "surgery" will be performed. Slice the strip open, so that it could unroll to become a rectangle, but keep a grasp on the cut ends. Twist one end 180°, making the inner surface face out, and glue the ends back together seamlessly. This results in a strip with a permanent half-twist: the Möbius strip. Its boundary is no longer a pair of circles, but (topologically) a single circle; and what was once its "inside" has merged with its "outside", so that it now has only a single side. Similarly to the Klein Bottle below, this two dimensional surface would need to intersect itself in two dimensions, but can easily be constructed in three or more dimensions.


£#h5#£Klein bottle£#/h5#£
Take two Möbius strips; each has a single loop as a boundary. Straighten out those loops into circles, and let the strips distort into cross-caps. Gluing the circles together will produce a new, closed manifold without boundary, the Klein bottle. Closing the surface does nothing to improve the lack of orientability, it merely removes the boundary. Thus, the Klein bottle is a closed surface with no distinction between inside and outside. In three-dimensional space, a Klein bottle's surface must pass through itself. Building a Klein bottle which is not self-intersecting requires four or more dimensions of space.


£#h5#£Real projective plane£#/h5#£
Begin with a sphere centered on the origin. Every line through the origin pierces the sphere in two opposite points called antipodes. Although there is no way to do so physically, it is possible (by considering a quotient space) to mathematically merge each antipode pair into a single point. The closed surface so produced is the real projective plane, yet another non-orientable surface. It has a number of equivalent descriptions and constructions, but this route explains its name: all the points on any given line through the origin project to the same "point" on this "plane".


£#h5#£Genus and the Euler characteristic£#/h5#£
For two dimensional manifolds a key invariant property is the genus, or "number of handles" present in a surface. A torus is a sphere with one handle, a double torus is a sphere with two handles, and so on. Indeed, it is possible to fully characterize compact, two-dimensional manifolds on the basis of genus and orientability. In higher-dimensional manifolds genus is replaced by the notion of Euler characteristic, and more generally Betti numbers and homology and cohomology.


£#h5#£Maps of manifolds£#/h5#£
Just as there are various types of manifolds, there are various types of maps of manifolds. In addition to continuous functions and smooth functions generally, there are maps with special properties. In geometric topology a basic type are embeddings, of which knot theory is a central example, and generalizations such as immersions, submersions, covering spaces, and ramified covering spaces. Basic results include the Whitney embedding theorem and Whitney immersion theorem.

In Riemannian geometry, one may ask for maps to preserve the Riemannian metric, leading to notions of isometric embeddings, isometric immersions, and Riemannian submersions; a basic result is the Nash embedding theorem.


£#h5#£Scalar-valued functions£#/h5#£
A basic example of maps between manifolds are scalar-valued functions on a manifold,

or
sometimes called regular functions or functionals, by analogy with algebraic geometry or linear algebra. These are of interest both in their own right, and to study the underlying manifold.

In geometric topology, most commonly studied are Morse functions, which yield handlebody decompositions, while in mathematical analysis, one often studies solution to partial differential equations, an important example of which is harmonic analysis, where one studies harmonic functions: the kernel of the Laplace operator. This leads to such functions as the spherical harmonics, and to heat kernel methods of studying manifolds, such as hearing the shape of a drum and some proofs of the Atiyah–Singer index theorem.


£#h5#£Generalizations of manifolds£#/h5#£
Infinite dimensional manifolds
The definition of a manifold can be generalized by dropping the requirement of finite dimensionality. Thus an infinite dimensional manifold is a topological space locally homeomorphic to a topological vector space over the reals. This omits the point-set axioms, allowing higher cardinalities and non-Hausdorff manifolds; and it omits finite dimension, allowing structures such as Hilbert manifolds to be modeled on Hilbert spaces, Banach manifolds to be modeled on Banach spaces, and Fréchet manifolds to be modeled on Fréchet spaces. Usually one relaxes one or the other condition: manifolds with the point-set axioms are studied in general topology, while infinite-dimensional manifolds are studied in functional analysis.
Orbifolds
An orbifold is a generalization of manifold allowing for certain kinds of "singularities" in the topology. Roughly speaking, it is a space which locally looks like the quotients of some simple space (e.g. Euclidean space) by the actions of various finite groups. The singularities correspond to fixed points of the group actions, and the actions must be compatible in a certain sense.
Algebraic varieties and schemes
Non-singular algebraic varieties over the real or complex numbers are manifolds. One generalizes this first by allowing singularities, secondly by allowing different fields, and thirdly by emulating the patching construction of manifolds: just as a manifold is glued together from open subsets of Euclidean space, an algebraic variety is glued together from affine algebraic varieties, which are zero sets of polynomials over algebraically closed fields. Schemes are likewise glued together from affine schemes, which are a generalization of algebraic varieties. Both are related to manifolds, but are constructed algebraically using sheaves instead of atlases.
Because of singular points, a variety is in general not a manifold, though linguistically the French variété, German Mannigfaltigkeit and English manifold are largely synonymous. In French an algebraic variety is called une variété algébrique (an algebraic variety), while a smooth manifold is called une variété différentielle (a differential variety).
Stratified space
A "stratified space" is a space that can be divided into pieces ("strata"), with each stratum a manifold, with the strata fitting together in prescribed ways (formally, a filtration by closed subsets). There are various technical definitions, notably a Whitney stratified space (see Whitney conditions) for smooth manifolds and a topologically stratified space for topological manifolds. Basic examples include manifold with boundary (top dimensional manifold and codimension 1 boundary) and manifolds with corners (top dimensional manifold, codimension 1 boundary, codimension 2 corners). Whitney stratified spaces are a broad class of spaces, including algebraic varieties, analytic varieties, semialgebraic sets, and subanalytic sets.
CW-complexes
A CW complex is a topological space formed by gluing disks of different dimensionality together. In general the resulting space is singular, hence not a manifold. However, they are of central interest in algebraic topology, especially in homotopy theory.
Homology manifolds
A homology manifold is a space that behaves like a manifold from the point of view of homology theory. These are not all manifolds, but (in high dimension) can be analyzed by surgery theory similarly to manifolds, and failure to be a manifold is a local obstruction, as in surgery theory.
Differential spaces
Let ${\displaystyle M}$ be a nonempty set. Suppose that some family of real functions on ${\displaystyle M}$ was chosen. Denote it by ${\displaystyle C\subseteq \mathbb {R} ^{M}}$ . It is an algebra with respect to the pointwise addition and multiplication. Let ${\displaystyle M}$ be equipped with the topology induced by ${\displaystyle C}$ . Suppose also that the following conditions hold. First: for every ${\displaystyle H\in C^{\infty }\left(\mathbb {R} ^{n}\right)}$ , where ${\displaystyle n\in \mathbb {N} }$ , and arbitrary ${\displaystyle f_{1},\dots ,f_{n}\in C}$ , the composition ${\displaystyle H\circ \left(f_{1},\dots ,f_{n}\right)\in C}$ . Second: every function, which in every point of ${\displaystyle M}$ locally coincides with some function from ${\displaystyle C}$ , also belongs to ${\displaystyle C}$ . A pair ${\displaystyle (M,C)}$ for which the above conditions hold, is called a Sikorski differential space.

£#h5#£See also£#/h5#£ £#ul#££#li#£Geodesic – Straight path on a curved surface or a Riemannian manifold£#/li#£ £#li#£Directional statistics: statistics on manifolds£#/li#£ £#li#£List of manifolds£#/li#£ £#li#£Timeline of manifolds – Mathematics timeline£#/li#£ £#li#£Mathematics of general relativity – Mathematical structures and techniques used in the theory of general relativity£#/li#££#/ul#£
£#h5#£By dimension£#/h5#£ £#ul#££#li#£3-manifold – Mathematical space£#/li#£ £#li#£4-manifold – Mathematical space£#/li#£ £#li#£5-manifold – Manifold of dimension five£#/li#£ £#li#£Manifolds of mappings£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Freedman, Michael H., and Quinn, Frank (1990) Topology of 4-Manifolds. Princeton University Press. ISBN 0-691-08577-3.£#/li#£ £#li#£Guillemin, Victor and Pollack, Alan (1974) Differential Topology. Prentice-Hall. ISBN 0-13-212605-2. Advanced undergraduate / first-year graduate text inspired by Milnor.£#/li#£ £#li#£Hempel, John (1976) 3-Manifolds. Princeton University Press. ISBN 0-8218-3695-1.£#/li#£ £#li#£Hirsch, Morris, (1997) Differential Topology. Springer Verlag. ISBN 0-387-90148-5. The most complete account, with historical insights and excellent, but difficult, problems. The standard reference for those wishing to have a deep understanding of the subject.£#/li#£ £#li#£Kirby, Robion C. and Siebenmann, Laurence C. (1977) Foundational Essays on Topological Manifolds. Smoothings, and Triangulations. Princeton University Press. ISBN 0-691-08190-5. A detailed study of the category of topological manifolds.£#/li#£ £#li#£Lee, John M. (2000) Introduction to Topological Manifolds. Springer-Verlag. ISBN 0-387-98759-2. Detailed and comprehensive first-year graduate text.£#/li#£ £#li#£Lee, John M. (2003) Introduction to Smooth Manifolds. Springer-Verlag. ISBN 0-387-95495-3. Detailed and comprehensive first-year graduate text; sequel to Introduction to Topological Manifolds.£#/li#£ £#li#£Massey, William S. (1977) Algebraic Topology: An Introduction. Springer-Verlag. ISBN 0-387-90271-6.£#/li#£ £#li#£Milnor, John (1997) Topology from the Differentiable Viewpoint. Princeton University Press. ISBN 0-691-04833-9. Classic brief introduction to differential topology.£#/li#£ £#li#£Munkres, James R. (1991) Analysis on Manifolds. Addison-Wesley (reprinted by Westview Press) ISBN 0-201-51035-9. Undergraduate text treating manifolds in ${\displaystyle \mathbb {R} ^{n}}$ .£#/li#£ £#li#£Munkres, James R. (2000) Topology. Prentice Hall. ISBN 0-13-181629-2.£#/li#£ £#li#£Neuwirth, L. P., ed. (1975) Knots, Groups, and 3-Manifolds. Papers Dedicated to the Memory of R. H. Fox. Princeton University Press. ISBN 978-0-691-08170-0.£#/li#£ £#li#£Riemann, Bernhard, Gesammelte mathematische Werke und wissenschaftlicher Nachlass, Sändig Reprint. ISBN 3-253-03059-8. £#ul#££#li#£Grundlagen für eine allgemeine Theorie der Functionen einer veränderlichen complexen Grösse. The 1851 doctoral thesis in which "manifold" (Mannigfaltigkeit) first appears.£#/li#£ £#li#£Ueber die Hypothesen, welche der Geometrie zu Grunde liegen. The 1854 Göttingen inaugural lecture (Habilitationsschrift).£#/li#££#/ul#££#/li#£ £#li#£Spivak, Michael (1965) Calculus on Manifolds: A Modern Approach to Classical Theorems of Advanced Calculus. W.A. Benjamin Inc. (reprinted by Addison-Wesley and Westview Press). ISBN 0-8053-9021-9. Famously terse advanced undergraduate / first-year graduate text.£#/li#£ £#li#£Spivak, Michael (1999) A Comprehensive Introduction to Differential Geometry (3rd edition) Publish or Perish Inc. Encyclopedic five-volume series presenting a systematic treatment of the theory of manifolds, Riemannian geometry, classical differential geometry, and numerous other topics at the first- and second-year graduate levels.£#/li#£ £#li#£Tu, Loring W. (2011). An Introduction to Manifolds (2nd ed.). New York: Springer. ISBN 978-1-4419-7399-3.. Concise first-year graduate text.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Manifold", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#£ £#li#£Dimensions-math.org (A film explaining and visualizing manifolds up to fourth dimension.)£#/li#£ £#li#£The manifold atlas project of the Max Planck Institute for Mathematics in Bonn£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Topology > Manifolds £#/li#££#/ul#£




£#h3#£Accelerated Convergence£#/h3#£

In mathematics, series acceleration is one of a collection of sequence transformations for improving the rate of convergence of a series. Techniques for series acceleration are often applied in numerical analysis, where they are used to improve the speed of numerical integration. Series acceleration techniques may also be used, for example, to obtain a variety of identities on special functions. Thus, the Euler transform applied to the hypergeometric series gives some of the classic, well-known hypergeometric series identities.


£#h5#£Definition£#/h5#£
Given a sequence

${\displaystyle S=\{s_{n}\}_{n\in \mathbb {N} }}$
having a limit

${\displaystyle \lim _{n\to \infty }s_{n}=\ell ,}$
an accelerated series is a second sequence

${\displaystyle S'=\{s'_{n}\}_{n\in \mathbb {N} }}$
which converges faster to ${\displaystyle \ell }$ than the original sequence, in the sense that

${\displaystyle \lim _{n\to \infty }{\frac {s'_{n}-\ell }{s_{n}-\ell }}=0.}$
If the original sequence is divergent, the sequence transformation acts as an extrapolation method to the antilimit ${\displaystyle \ell }$ .

The mappings from the original to the transformed series may be linear (as defined in the article sequence transformations), or non-linear. In general, the non-linear sequence transformations tend to be more powerful.


£#h5#£Overview£#/h5#£
Two classical techniques for series acceleration are Euler's transformation of series and Kummer's transformation of series. A variety of much more rapidly convergent and special-case tools have been developed in the 20th century, including Richardson extrapolation, introduced by Lewis Fry Richardson in the early 20th century but also known and used by Katahiro Takebe in 1722; the Aitken delta-squared process, introduced by Alexander Aitken in 1926 but also known and used by Takakazu Seki in the 18th century; the epsilon method given by Peter Wynn in 1956; the Levin u-transform; and the Wilf-Zeilberger-Ekhad method or WZ method.

For alternating series, several powerful techniques, offering convergence rates from ${\displaystyle 5.828^{-n}}$ all the way to ${\displaystyle 17.93^{-n}}$ for a summation of ${\displaystyle n}$ terms, are described by Cohen et al.


£#h5#£Euler's transform£#/h5#£
A basic example of a linear sequence transformation, offering improved convergence, is Euler's transform. It is intended to be applied to an alternating series; it is given by

${\displaystyle \sum _{n=0}^{\infty }(-1)^{n}a_{n}=\sum _{n=0}^{\infty }(-1)^{n}{\frac {(\Delta ^{n}a)_{0}}{2^{n+1}}}}$
where ${\displaystyle \Delta }$ is the forward difference operator, for which one has the formula

${\displaystyle (\Delta ^{n}a)_{0}=\sum _{k=0}^{n}(-1)^{k}{n \choose k}a_{n-k}.}$
If the original series, on the left hand side, is only slowly converging, the forward differences will tend to become small quite rapidly; the additional power of two further improves the rate at which the right hand side converges.

A particularly efficient numerical implementation of the Euler transform is the van Wijngaarden transformation.


£#h5#£Conformal mappings£#/h5#£
A series

${\displaystyle S=\sum _{n=0}^{\infty }a_{n}}$
can be written as f(1), where the function f is defined as

${\displaystyle f(z)=\sum _{n=0}^{\infty }a_{n}z^{n}.}$
The function f(z) can have singularities in the complex plane (branch point singularities, poles or essential singularities), which limit the radius of convergence of the series. If the point z = 1 is close to or on the boundary of the disk of convergence, the series for S will converge very slowly. One can then improve the convergence of the series by means of a conformal mapping that moves the singularities such that the point that is mapped to z = 1 ends up deeper in the new disk of convergence.

The conformal transform ${\displaystyle z=\Phi (w)}$ needs to be chosen such that ${\displaystyle \Phi (0)=0}$ , and one usually chooses a function that has a finite derivative at w = 0. One can assume that ${\displaystyle \Phi (1)=1}$ without loss of generality, as one can always rescale w to redefine ${\displaystyle \Phi }$ . We then consider the function

${\displaystyle g(w)=f(\Phi (w)).}$
Since ${\displaystyle \Phi (1)=1}$ , we have f(1) = g(1). We can obtain the series expansion of g(w) by putting ${\displaystyle z=\Phi (w)}$ in the series expansion of f(z) because ${\displaystyle \Phi (0)=0}$ ; the first n terms of the series expansion for f(z) will yield the first n terms of the series expansion for g(w) if ${\displaystyle \Phi '(0)\neq 0}$ . Putting w = 1 in that series expansion will thus yield a series such that if it converges, it will converge to the same value as the original series.


£#h5#£Non-linear sequence transformations£#/h5#£
Examples of such nonlinear sequence transformations are Padé approximants, the Shanks transformation, and Levin-type sequence transformations.

Especially nonlinear sequence transformations often provide powerful numerical methods for the summation of divergent series or asymptotic series that arise for instance in perturbation theory, and may be used as highly effective extrapolation methods.


£#h5#£Aitken method£#/h5#£
A simple nonlinear sequence transformation is the Aitken extrapolation or delta-squared method,

${\displaystyle \mathbb {A} :S\to S'=\mathbb {A} (S)={(s'_{n})}_{n\in \mathbb {N} }}$
defined by

${\displaystyle s'_{n}=s_{n+2}-{\frac {(s_{n+2}-s_{n+1})^{2}}{s_{n+2}-2s_{n+1}+s_{n}}}.}$
This transformation is commonly used to improve the rate of convergence of a slowly converging sequence; heuristically, it eliminates the largest part of the absolute error.


£#h5#£See also£#/h5#£ £#ul#££#li#£Shanks transformation£#/li#£ £#li#£Minimum polynomial extrapolation£#/li#£ £#li#£Van Wijngaarden transformation£#/li#££#/ul#£
£#h5#£References£#/h5#£ £#ul#££#li#£C. Brezinski and M. Redivo Zaglia, Extrapolation Methods. Theory and Practice, North-Holland, 1991.£#/li#£ £#li#£G. A. Baker Jr. and P. Graves-Morris, Padé Approximants, Cambridge U.P., 1996.£#/li#£ £#li#£Weisstein, Eric W. "Convergence Improvement". MathWorld.£#/li#£ £#li#£Herbert H. H. Homeier: Scalar Levin-Type Sequence Transformations, Journal of Computational and Applied Mathematics, vol. 122, no. 1–2, p 81 (2000). Homeier, H. H. H. (2000). "Scalar Levin-type sequence transformations". Journal of Computational and Applied Mathematics. 122: 81. arXiv:math/0005209. Bibcode:2000JCoAM.122...81H. doi:10.1016/S0377-0427(00)00359-9., arXiv:math/0005209.£#/li#£ £#li#£Brezinski Claude and Redivo-Zaglia Michela : "The genesis and early developments of Aitken's process, Shanks transformation, the ${\displaystyle \epsilon }$ -algorithm, and related fixed point methods", Numerical Algorithms, Vol.80, No.1, (2019), pp.11-133.£#/li#£ £#li#£Delahaye J. P. : "Sequence Transformations", Springer-Verlag, Berlin, ISBN 978-3540152835 (1988).£#/li#£ £#li#£Sidi Avram : "Vector Extrapolation Methods with Applications", SIAM, ISBN 978-1-61197-495-9 (2017).£#/li#£ £#li#£Brezinski Claude, Redivo-Zaglia Michela and Saad Yousef : "Shanks Sequence Transformations and Anderson Acceleration", SIAM Review, Vol.60, No.3 (2018), pp.646–669. doi:10.1137/17M1120725 .£#/li#£ £#li#£Brezinski Claude : "Reminiscences of Peter Wynn", Numerical Algorithms, Vol.80(2019), pp.5-10.£#/li#£ £#li#£Brezinski Claude and Redivo-Zaglia Michela : "Extrapolation and Rational Approximation", Springer, ISBN 978-3-030-58417-7 (2020).£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Convergence acceleration of series£#/li#£ £#li#£GNU Scientific Library, Series Acceleration£#/li#£ £#li#£Digital Library of Mathematical Functions£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Series > Convergence £#/li#££#/ul#£




£#h3#£Acceleration£#/h3#£

In mechanics, acceleration is the rate of change of the velocity of an object with respect to time. Accelerations are vector quantities (in that they have magnitude and direction). The orientation of an object's acceleration is given by the orientation of the net force acting on that object. The magnitude of an object's acceleration, as described by Newton's Second Law, is the combined effect of two causes:

£#ul#££#li#£the net balance of all external forces acting onto that object — magnitude is directly proportional to this net resulting force;£#/li#£ £#li#£that object's mass, depending on the materials out of which it is made — magnitude is inversely proportional to the object's mass.£#/li#££#/ul#£
The SI unit for acceleration is metre per second squared (m⋅s−2, ${\displaystyle {\tfrac {\operatorname {m} }{\operatorname {s} ^{2}}}}$ ).

For example, when a vehicle starts from a standstill (zero velocity, in an inertial frame of reference) and travels in a straight line at increasing speeds, it is accelerating in the direction of travel. If the vehicle turns, an acceleration occurs toward the new direction and changes its motion vector. The acceleration of the vehicle in its current direction of motion is called a linear (or tangential during circular motions) acceleration, the reaction to which the passengers on board experience as a force pushing them back into their seats. When changing direction, the effecting acceleration is called radial (or centripetal during circular motions) acceleration, the reaction to which the passengers experience as a centrifugal force. If the speed of the vehicle decreases, this is an acceleration in the opposite direction and mathematically a negative, sometimes called deceleration or retardation, and passengers experience the reaction to deceleration as an inertial force pushing them forward. Such negative accelerations are often achieved by retrorocket burning in spacecraft. Both acceleration and deceleration are treated the same, as they are both changes in velocity. Each of these accelerations (tangential, radial, deceleration) is felt by passengers until their relative (differential) velocity are neutralized in reference to the acceleration due to change in speed.


£#h5#£Definition and properties£#/h5#£
£#h5#£Average acceleration£#/h5#£
An object's average acceleration over a period of time is its change in velocity, ${\displaystyle \Delta \mathbf {v} }$ , divided by the duration of the period, ${\displaystyle \Delta t}$ . Mathematically,


£#h5#£Instantaneous acceleration£#/h5#£
Instantaneous acceleration, meanwhile, is the limit of the average acceleration over an infinitesimal interval of time. In the terms of calculus, instantaneous acceleration is the derivative of the velocity vector with respect to time:

As acceleration is defined as the derivative of velocity, v, with respect to time t and velocity is defined as the derivative of position, x, with respect to time, acceleration can be thought of as the second derivative of x with respect to t:
(Here and elsewhere, if motion is in a straight line, vector quantities can be substituted by scalars in the equations.)

By the fundamental theorem of calculus, it can be seen that the integral of the acceleration function a(t) is the velocity function v(t); that is, the area under the curve of an acceleration vs. time (a vs. t) graph corresponds to the change of velocity.

Likewise, the integral of the jerk function j(t), the derivative of the acceleration function, can be used to find the change of acceleration at a certain time:


£#h5#£Units£#/h5#£
Acceleration has the dimensions of velocity (L/T) divided by time, i.e. L T−2. The SI unit of acceleration is the metre per second squared (m s−2); or "metre per second per second", as the velocity in metres per second changes by the acceleration value, every second.


£#h5#£Other forms£#/h5#£
An object moving in a circular motion—such as a satellite orbiting the Earth—is accelerating due to the change of direction of motion, although its speed may be constant. In this case it is said to be undergoing centripetal (directed towards the center) acceleration.

Proper acceleration, the acceleration of a body relative to a free-fall condition, is measured by an instrument called an accelerometer.

In classical mechanics, for a body with constant mass, the (vector) acceleration of the body's center of mass is proportional to the net force vector (i.e. sum of all forces) acting on it (Newton’s second law):

where F is the net force acting on the body, m is the mass of the body, and a is the center-of-mass acceleration. As speeds approach the speed of light, relativistic effects become increasingly large.
£#h5#£Tangential and centripetal acceleration£#/h5#£
The velocity of a particle moving on a curved path as a function of time can be written as:

${\displaystyle \mathbf {v} (t)=v(t){\frac {\mathbf {v} (t)}{v(t)}}=v(t)\mathbf {u} _{\mathrm {t} }(t),}$
with v(t) equal to the speed of travel along the path, and

${\displaystyle \mathbf {u} _{\mathrm {t} }={\frac {\mathbf {v} (t)}{v(t)}}\ ,}$
a unit vector tangent to the path pointing in the direction of motion at the chosen moment in time. Taking into account both the changing speed v(t) and the changing direction of ut, the acceleration of a particle moving on a curved path can be written using the chain rule of differentiation for the product of two functions of time as:

${\displaystyle {\begin{alignedat}{3}\mathbf {a} &={\frac {d\mathbf {v} }{dt}}\\&={\frac {dv}{dt}}\mathbf {u} _{\mathrm {t} }+v(t){\frac {d\mathbf {u} _{\mathrm {t} }}{dt}}\\&={\frac {dv}{dt}}\mathbf {u} _{\mathrm {t} }+{\frac {v^{2}}{r}}\mathbf {u} _{\mathrm {n} }\ ,\end{alignedat}}}$
where un is the unit (inward) normal vector to the particle's trajectory (also called the principal normal), and r is its instantaneous radius of curvature based upon the osculating circle at time t. These components are called the tangential acceleration and the normal or radial acceleration (or centripetal acceleration in circular motion, see also circular motion and centripetal force).

Geometrical analysis of three-dimensional space curves, which explains tangent, (principal) normal and binormal, is described by the Frenet–Serret formulas.


£#h5#£Special cases£#/h5#£
£#h5#£Uniform acceleration£#/h5#£
Uniform or constant acceleration is a type of motion in which the velocity of an object changes by an equal amount in every equal time period.

A frequently cited example of uniform acceleration is that of an object in free fall in a uniform gravitational field. The acceleration of a falling body in the absence of resistances to motion is dependent only on the gravitational field strength g (also called acceleration due to gravity). By Newton's Second Law the force ${\displaystyle \mathbf {F_{g}} }$ acting on a body is given by:

${\displaystyle \mathbf {F_{g}} =m\mathbf {g} }$
Because of the simple analytic properties of the case of constant acceleration, there are simple formulas relating the displacement, initial and time-dependent velocities, and acceleration to the time elapsed:

${\displaystyle \mathbf {s} (t)=\mathbf {s} _{0}+\mathbf {v} _{0}t+{\tfrac {1}{2}}\mathbf {a} t^{2}=\mathbf {s} _{0}+{\frac {\mathbf {v} _{0}+\mathbf {v} (t)}{2}}t}$
${\displaystyle \mathbf {v} (t)=\mathbf {v} _{0}+\mathbf {a} t}$
${\displaystyle {v^{2}}(t)={v_{0}}^{2}+2\mathbf {a\cdot } [\mathbf {s} (t)-\mathbf {s} _{0}]}$
where

£#ul#££#li#£ ${\displaystyle t}$ is the elapsed time,£#/li#£ £#li#£ ${\displaystyle \mathbf {s} _{0}}$ is the initial displacement from the origin,£#/li#£ £#li#£ ${\displaystyle \mathbf {s} (t)}$ is the displacement from the origin at time ${\displaystyle t}$ ,£#/li#£ £#li#£ ${\displaystyle \mathbf {v} _{0}}$ is the initial velocity,£#/li#£ £#li#£ ${\displaystyle \mathbf {v} (t)}$ is the velocity at time ${\displaystyle t}$ , and£#/li#£ £#li#£ ${\displaystyle \mathbf {a} }$ is the uniform rate of acceleration.£#/li#££#/ul#£
In particular, the motion can be resolved into two orthogonal parts, one of constant velocity and the other according to the above equations. As Galileo showed, the net result is parabolic motion, which describes, e. g., the trajectory of a projectile in a vacuum near the surface of Earth.


£#h5#£Circular motion£#/h5#£
In uniform circular motion, that is moving with constant speed along a circular path, a particle experiences an acceleration resulting from the change of the direction of the velocity vector, while its magnitude remains constant. The derivative of the location of a point on a curve with respect to time, i.e. its velocity, turns out to be always exactly tangential to the curve, respectively orthogonal to the radius in this point. Since in uniform motion the velocity in the tangential direction does not change, the acceleration must be in radial direction, pointing to the center of the circle. This acceleration constantly changes the direction of the velocity to be tangent in the neighboring point, thereby rotating the velocity vector along the circle.

£#ul#££#li#£For a given speed ${\displaystyle v}$ , the magnitude of this geometrically caused acceleration (centripetal acceleration) is inversely proportional to the radius ${\displaystyle r}$ of the circle, and increases as the square of this speed: £#/li#£ £#li#£Note that, for a given angular velocity ${\displaystyle \omega }$ , the centripetal acceleration is directly proportional to radius ${\displaystyle r}$ . This is due to the dependence of velocity ${\displaystyle v}$ on the radius ${\displaystyle r}$ . £#/li#££#/ul#£
Expressing centripetal acceleration vector in polar components, where ${\displaystyle \mathbf {r} }$ is a vector from the centre of the circle to the particle with magnitude equal to this distance, and considering the orientation of the acceleration towards the center, yields

${\displaystyle \mathbf {a_{c}} =-{\frac {v^{2}}{|\mathbf {r} |}}\cdot {\frac {\mathbf {r} }{|\mathbf {r} |}}\;.}$
As usual in rotations, the speed ${\displaystyle v}$ of a particle may be expressed as an angular speed with respect to a point at the distance ${\displaystyle r}$ as

${\displaystyle \omega ={\frac {v}{r}}.}$
Thus ${\displaystyle \mathbf {a_{c}} =-\omega ^{2}\mathbf {r} \;.}$

This acceleration and the mass of the particle determine the necessary centripetal force, directed toward the centre of the circle, as the net force acting on this particle to keep it in this uniform circular motion. The so-called 'centrifugal force', appearing to act outward on the body, is a so-called pseudo force experienced in the frame of reference of the body in circular motion, due to the body's linear momentum, a vector tangent to the circle of motion.

In a nonuniform circular motion, i.e., the speed along the curved path is changing, the acceleration has a non-zero component tangential to the curve, and is not confined to the principal normal, which directs to the center of the osculating circle, that determines the radius ${\displaystyle r}$ for the centripetal acceleration. The tangential component is given by the angular acceleration ${\displaystyle \alpha }$ , i.e., the rate of change ${\displaystyle \alpha ={\dot {\omega }}}$ of the angular speed ${\displaystyle \omega }$ times the radius ${\displaystyle r}$ . That is,

${\displaystyle a_{t}=r\alpha .}$
The sign of the tangential component of the acceleration is determined by the sign of the angular acceleration ( ${\displaystyle \alpha }$ ), and the tangent is always directed at right angles to the radius vector.


£#h5#£Relation to relativity£#/h5#£
£#h5#£Special relativity£#/h5#£
The special theory of relativity describes the behavior of objects traveling relative to other objects at speeds approaching that of light in a vacuum. Newtonian mechanics is exactly revealed to be an approximation to reality, valid to great accuracy at lower speeds. As the relevant speeds increase toward the speed of light, acceleration no longer follows classical equations.

As speeds approach that of light, the acceleration produced by a given force decreases, becoming infinitesimally small as light speed is approached; an object with mass can approach this speed asymptotically, but never reach it.


£#h5#£General relativity£#/h5#£
Unless the state of motion of an object is known, it is impossible to distinguish whether an observed force is due to gravity or to acceleration—gravity and inertial acceleration have identical effects. Albert Einstein called this the equivalence principle, and said that only observers who feel no force at all—including the force of gravity—are justified in concluding that they are not accelerating.


£#h5#£Conversions£#/h5#£
£#h5#£See also£#/h5#£
£#h5#£References£#/h5#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Acceleration Calculator Simple acceleration unit converter£#/li#£ £#li#£Acceleration Calculator Acceleration Conversion calculator converts units form meter per second square, kilometer per second square, millimeter per second square & more with metric conversion.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Klamkin, M. S. "Problem 1481." Math. Mag. 68, 307, 1995.£#/li#££#li#£Klamkin, M. S. "A Characteristic of Constant Acceleration." Solution to Problem 1481. Math. Mag. 69, 308, 1996.£#/li#££#li#£ Klamkin, M. S. "Problem 1481." Math. Mag. 68, 307, 1995. £#/li#££#li#£ Klamkin, M. S. "A Characteristic of Constant Acceleration." Solution to Problem 1481. Math. Mag. 69, 308, 1996. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Differential Geometry > General Differential Geometry £#/li#££#/ul#£




£#h3#£Accumulation Point£#/h3#£

In mathematics, a limit point, accumulation point, or cluster point of a set ${\displaystyle S}$ in a topological space ${\displaystyle X}$ is a point ${\displaystyle x}$ that can be "approximated" by points of ${\displaystyle S}$ in the sense that every neighbourhood of ${\displaystyle x}$ with respect to the topology on ${\displaystyle X}$ also contains a point of ${\displaystyle S}$ other than ${\displaystyle x}$ itself. A limit point of a set ${\displaystyle S}$ does not itself have to be an element of ${\displaystyle S.}$ There is also a closely related concept for sequences. A cluster point or accumulation point of a sequence ${\displaystyle (x_{n})_{n\in \mathbb {N} }}$ in a topological space ${\displaystyle X}$ is a point ${\displaystyle x}$ such that, for every neighbourhood ${\displaystyle V}$ of ${\displaystyle x,}$ there are infinitely many natural numbers ${\displaystyle n}$ such that ${\displaystyle x_{n}\in V.}$ This definition of a cluster or accumulation point of a sequence generalizes to nets and filters.

The similarly named notion of a limit point of a sequence (respectively, a limit point of a filter, a limit point of a net) by definition refers to a point that the sequence converges to (respectively, the filter converges to, the net converges to). Importantly, although "limit point of a set" is synonymous with "cluster/accumulation point of a set", this is not true for sequences (nor nets or filters). That is, the term "limit point of a sequence" is not synonymous with "cluster/accumulation point of a sequence".

The limit points of a set should not be confused with adherent points (also called points of closure) for which every neighbourhood of ${\displaystyle x}$ contains a point of ${\displaystyle S}$ (that is, any point belonging to closure of the set). Unlike for limit points, an adherent point of ${\displaystyle S}$ may be ${\displaystyle x}$ itself. A limit point can be characterized as an adherent point that is not an isolated point.

Limit points of a set should also not be confused with boundary points. For example, ${\displaystyle 0}$ is a boundary point (but not a limit point) of the set ${\displaystyle \{0\}}$ in ${\displaystyle \mathbb {R} }$ with standard topology. However, ${\displaystyle 0.5}$ is a limit point (though not a boundary point) of interval ${\displaystyle [0,1]}$ in ${\displaystyle \mathbb {R} }$ with standard topology (for a less trivial example of a limit point, see the first caption).

This concept profitably generalizes the notion of a limit and is the underpinning of concepts such as closed set and topological closure. Indeed, a set is closed if and only if it contains all of its limit points, and the topological closure operation can be thought of as an operation that enriches a set by uniting it with its limit points.


£#h5#£Definition£#/h5#£
£#h5#£Accumulation points of a set£#/h5#£
Let ${\displaystyle S}$ be a subset of a topological space ${\displaystyle X.}$ A point ${\displaystyle x}$ in ${\displaystyle X}$ is a limit point or cluster point or accumulation point of the set ${\displaystyle S}$ if every neighbourhood of ${\displaystyle x}$ contains at least one point of ${\displaystyle S}$ different from ${\displaystyle x}$ itself.

It does not make a difference if we restrict the condition to open neighbourhoods only. It is often convenient to use the "open neighbourhood" form of the definition to show that a point is a limit point and to use the "general neighbourhood" form of the definition to derive facts from a known limit point.

If ${\displaystyle X}$ is a ${\displaystyle T_{1}}$ space (such as a metric space), then ${\displaystyle x\in X}$ is a limit point of ${\displaystyle S}$ if and only if every neighbourhood of ${\displaystyle x}$ contains infinitely many points of ${\displaystyle S.}$ In fact, ${\displaystyle T_{1}}$ spaces are characterized by this property.

If ${\displaystyle X}$ is a Fréchet–Urysohn space (which all metric spaces and first-countable spaces are), then ${\displaystyle x\in X}$ is a limit point of ${\displaystyle S}$ if and only if there is a sequence of points in ${\displaystyle S\setminus \{x\}}$ whose limit is ${\displaystyle x.}$ In fact, Fréchet–Urysohn spaces are characterized by this property.

The set of limit points of ${\displaystyle S}$ is called the derived set of ${\displaystyle S.}$


£#h5#£Types of accumulation points£#/h5#£
If every neighbourhood of ${\displaystyle x}$ contains infinitely many points of ${\displaystyle S,}$ then ${\displaystyle x}$ is a specific type of limit point called an ω-accumulation point of ${\displaystyle S.}$

If every neighbourhood of ${\displaystyle x}$ contains uncountably many points of ${\displaystyle S,}$ then ${\displaystyle x}$ is a specific type of limit point called a condensation point of ${\displaystyle S.}$

If every neighbourhood ${\displaystyle U}$ of ${\displaystyle x}$ satisfies ${\displaystyle \left|U\cap S\right|=\left|S\right|,}$ then ${\displaystyle x}$ is a specific type of limit point called a complete accumulation point of ${\displaystyle S.}$


£#h5#£Accumulation points of sequences and nets£#/h5#£

In a topological space ${\displaystyle X,}$ a point ${\displaystyle x\in X}$ is said to be a cluster point or accumulation point of a sequence ${\displaystyle x_{\bullet }=\left(x_{n}\right)_{n=1}^{\infty }}$ if, for every neighbourhood ${\displaystyle V}$ of ${\displaystyle x,}$ there are infinitely many ${\displaystyle n\in \mathbb {N} }$ such that ${\displaystyle x_{n}\in V.}$ It is equivalent to say that for every neighbourhood ${\displaystyle V}$ of ${\displaystyle x}$ and every ${\displaystyle n_{0}\in \mathbb {N} ,}$ there is some ${\displaystyle n\geq n_{0}}$ such that ${\displaystyle x_{n}\in V.}$ If ${\displaystyle X}$ is a metric space or a first-countable space (or, more generally, a Fréchet–Urysohn space), then ${\displaystyle x}$ is a cluster point of ${\displaystyle x_{\bullet }}$ if and only if ${\displaystyle x}$ is a limit of some subsequence of ${\displaystyle x_{\bullet }.}$ The set of all cluster points of a sequence is sometimes called the limit set.

Note that there is already the notion of limit of a sequence to mean a point ${\displaystyle x}$ to which the sequence converges (that is, every neighborhood of ${\displaystyle x}$ contains all but finitely many elements of the sequence). That is why we do not use the term limit point of a sequence as a synonym for accumulation point of the sequence.

The concept of a net generalizes the idea of a sequence. A net is a function ${\displaystyle f:(P,\leq )\to X,}$ where ${\displaystyle (P,\leq )}$ is a directed set and ${\displaystyle X}$ is a topological space. A point ${\displaystyle x\in X}$ is said to be a cluster point or accumulation point of a net ${\displaystyle f}$ if, for every neighbourhood ${\displaystyle V}$ of ${\displaystyle x}$ and every ${\displaystyle p_{0}\in P,}$ there is some ${\displaystyle p\geq p_{0}}$ such that ${\displaystyle f(p)\in V,}$ equivalently, if ${\displaystyle f}$ has a subnet which converges to ${\displaystyle x.}$ Cluster points in nets encompass the idea of both condensation points and ω-accumulation points. Clustering and limit points are also defined for filters.


£#h5#£Relation between accumulation point of a sequence and accumulation point of a set£#/h5#£
Every sequence ${\displaystyle x_{\bullet }=\left(x_{n}\right)_{n=1}^{\infty }}$ in ${\displaystyle X}$ is by definition just a map ${\displaystyle x_{\bullet }:\mathbb {N} \to X}$ so that its image ${\displaystyle \operatorname {Im} x_{\bullet }:=\left\{x_{n}:n\in \mathbb {N} \right\}}$ can be defined in the usual way.

£#ul#££#li#£If there exists an element ${\displaystyle x\in X}$ that occurs infinitely many times in the sequence, ${\displaystyle x}$ is an accumulation point of the sequence. But ${\displaystyle x}$ need not be an accumulation point of the corresponding set ${\displaystyle \operatorname {Im} x_{\bullet }.}$ For example, if the sequence is the constant sequence with value ${\displaystyle x,}$ we have ${\displaystyle \operatorname {Im} x_{\bullet }=\{x\}}$ and ${\displaystyle x}$ is an isolated point of ${\displaystyle \operatorname {Im} x_{\bullet }}$ and not an accumulation point of ${\displaystyle \operatorname {Im} x_{\bullet }.}$ £#/li#££#/ul#££#ul#££#li#£If no element occurs infinitely many times in the sequence, for example if all the elements are distinct, any accumulation point of the sequence is an ${\displaystyle \omega }$ -accumulation point of the associated set ${\displaystyle \operatorname {Im} x_{\bullet }.}$ £#/li#££#/ul#£
Conversely, given a countable infinite set ${\displaystyle A\subseteq X}$ in ${\displaystyle X,}$ we can enumerate all the elements of ${\displaystyle A}$ in many ways, even with repeats, and thus associate with it many sequences ${\displaystyle x_{\bullet }}$ that will satisfy ${\displaystyle A=\operatorname {Im} x_{\bullet }.}$

£#ul#££#li#£Any ${\displaystyle \omega }$ -accumulation point of ${\displaystyle A}$ is an accumulation point of any of the corresponding sequences (because any neighborhood of the point will contain infinitely many elements of ${\displaystyle A}$ and hence also infinitely many terms in any associated sequence).£#/li#££#/ul#££#ul#££#li#£A point ${\displaystyle x\in X}$ that is not an ${\displaystyle \omega }$ -accumulation point of ${\displaystyle A}$ cannot be an accumulation point of any of the associated sequences without infinite repeats (because ${\displaystyle x}$ has a neighborhood that contains only finitely many (possibly even none) points of ${\displaystyle A}$ and that neighborhood can only contain finitely many terms of such sequences).£#/li#££#/ul#£
£#h5#£Properties£#/h5#£
Every limit of a non-constant sequence is an accumulation point of the sequence. And by definition, every limit point is an adherent point.

The closure ${\displaystyle \operatorname {cl} (S)}$ of a set ${\displaystyle S}$ is a disjoint union of its limit points ${\displaystyle L(S)}$ and isolated points ${\displaystyle I(S)}$ :

A point ${\displaystyle x\in X}$ is a limit point of ${\displaystyle S\subseteq X}$ if and only if it is in the closure of ${\displaystyle S\setminus \{x\}.}$

If we use ${\displaystyle L(S)}$ to denote the set of limit points of ${\displaystyle S,}$ then we have the following characterization of the closure of ${\displaystyle S}$ : The closure of ${\displaystyle S}$ is equal to the union of ${\displaystyle S}$ and ${\displaystyle L(S).}$ This fact is sometimes taken as the definition of closure.

A corollary of this result gives us a characterisation of closed sets: A set ${\displaystyle S}$ is closed if and only if it contains all of its limit points.

No isolated point is a limit point of any set.

A space ${\displaystyle X}$ is discrete if and only if no subset of ${\displaystyle X}$ has a limit point.

If a space ${\displaystyle X}$ has the trivial topology and ${\displaystyle S}$ is a subset of ${\displaystyle X}$ with more than one element, then all elements of ${\displaystyle X}$ are limit points of ${\displaystyle S.}$ If ${\displaystyle S}$ is a singleton, then every point of ${\displaystyle X\setminus S}$ is a limit point of ${\displaystyle S.}$


£#h5#£See also£#/h5#£ £#ul#££#li#£Adherent point – Point that belongs to the closure of some give subset of a topological space£#/li#£ £#li#£Condensation point£#/li#£ £#li#£Convergent filter£#/li#£ £#li#£Derived set (mathematics)£#/li#£ £#li#£Filters in topology – Use of filters to describe and characterize all basic topological notions and results.£#/li#£ £#li#£Isolated point – Point of a subset S around which there are no other points of S£#/li#£ £#li#£Limit of a function – Point to which functions converge in topology£#/li#£ £#li#£Limit of a sequence – Value to which tends an infinite sequence£#/li#£ £#li#£Subsequential limit – The limit of some subsequence£#/li#££#/ul#£
£#h5#£Citations£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Bourbaki, Nicolas (1989) [1966]. General Topology: Chapters 1–4 [Topologie Générale]. Éléments de mathématique. Berlin New York: Springer Science & Business Media. ISBN 978-3-540-64241-1. OCLC 18588129.£#/li#£ £#li#£Dugundji, James (1966). Topology. Boston: Allyn and Bacon. ISBN 978-0-697-06889-7. OCLC 395340485.£#/li#£ £#li#£Munkres, James R. (2000). Topology (Second ed.). Upper Saddle River, NJ: Prentice Hall, Inc. ISBN 978-0-13-181629-9. OCLC 42683260.£#/li#£ £#li#£"Limit point of a set", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Calculus > Limits £#/li#££#li#£ Calculus and Analysis > Dynamical Systems £#/li#££#/ul#£




£#h3#£AC Method£#/h3#£

In mathematics, factorization (or factorisation, see English spelling differences) or factoring consists of writing a number or another mathematical object as a product of several factors, usually smaller or simpler objects of the same kind. For example, 3 × 5 is a factorization of the integer 15, and (x – 2)(x + 2) is a factorization of the polynomial x2 – 4.

Factorization is not usually considered meaningful within number systems possessing division, such as the real or complex numbers, since any ${\displaystyle x}$ can be trivially written as ${\displaystyle (xy)\times (1/y)}$ whenever ${\displaystyle y}$ is not zero. However, a meaningful factorization for a rational number or a rational function can be obtained by writing it in lowest terms and separately factoring its numerator and denominator.

Factorization was first considered by ancient Greek mathematicians in the case of integers. They proved the fundamental theorem of arithmetic, which asserts that every positive integer may be factored into a product of prime numbers, which cannot be further factored into integers greater than 1. Moreover, this factorization is unique up to the order of the factors. Although integer factorization is a sort of inverse to multiplication, it is much more difficult algorithmically, a fact which is exploited in the RSA cryptosystem to implement public-key cryptography.

Polynomial factorization has also been studied for centuries. In elementary algebra, factoring a polynomial reduces the problem of finding its roots to finding the roots of the factors. Polynomials with coefficients in the integers or in a field possess the unique factorization property, a version of the fundamental theorem of arithmetic with prime numbers replaced by irreducible polynomials. In particular, a univariate polynomial with complex coefficients admits a unique (up to ordering) factorization into linear polynomials: this is a version of the fundamental theorem of algebra. In this case, the factorization can be done with root-finding algorithms. The case of polynomials with integer coefficients is fundamental for computer algebra. There are efficient computer algorithms for computing (complete) factorizations within the ring of polynomials with rational number coefficients (see factorization of polynomials).

A commutative ring possessing the unique factorization property is called a unique factorization domain. There are number systems, such as certain rings of algebraic integers, which are not unique factorization domains. However, rings of algebraic integers satisfy the weaker property of Dedekind domains: ideals factor uniquely into prime ideals.

Factorization may also refer to more general decompositions of a mathematical object into the product of smaller or simpler objects. For example, every function may be factored into the composition of a surjective function with an injective function. Matrices possess many kinds of matrix factorizations. For example, every matrix has a unique LUP factorization as a product of a lower triangular matrix L with all diagonal entries equal to one, an upper triangular matrix U, and a permutation matrix P; this is a matrix formulation of Gaussian elimination.


£#h5#£Integers£#/h5#£
By the fundamental theorem of arithmetic, every integer greater than 1 has a unique (up to the order of the factors) factorization into prime numbers, which are those integers which cannot be further factorized into the product of integers greater than one.

For computing the factorization of an integer n, one needs an algorithm for finding a divisor q of n or deciding that n is prime. When such a divisor is found, the repeated application of this algorithm to the factors q and n / q gives eventually the complete factorization of n.

For finding a divisor q of n, if any, it suffices to test all values of q such that 1 < q and q2 ≤ n. In fact, if r is a divisor of n such that r2 > n, then q = n / r is a divisor of n such that q2 ≤ n.

If one tests the values of q in increasing order, the first divisor that is found is necessarily a prime number, and the cofactor r = n / q cannot have any divisor smaller than q. For getting the complete factorization, it suffices thus to continue the algorithm by searching a divisor of r that is not smaller than q and not greater than √r.

There is no need to test all values of q for applying the method. In principle, it suffices to test only prime divisors. This needs to have a table of prime numbers that may be generated for example with the sieve of Eratosthenes. As the method of factorization does essentially the same work as the sieve of Eratosthenes, it is generally more efficient to test for a divisor only those numbers for which it is not immediately clear whether they are prime or not. Typically, one may proceed by testing 2, 3, 5, and the numbers > 5, whose last digit is 1, 3, 7, 9 and the sum of digits is not a multiple of 3.

This method works well for factoring small integers, but is inefficient for larger integers. For example, Pierre de Fermat was unable to discover that the 6th Fermat number

${\displaystyle 1+2^{2^{5}}=1+2^{32}=4\,294\,967\,297}$
is not a prime number. In fact, applying the above method would require more than 10000 divisions, for a number that has 10 decimal digits.

There are more efficient factoring algorithms. However they remain relatively inefficient, as, with the present state of the art, one cannot factorize, even with the more powerful computers, a number of 500 decimal digits that is the product of two randomly chosen prime numbers. This ensures the security of the RSA cryptosystem, which is widely used for secure internet communication.


£#h5#£Example£#/h5#£
For factoring n = 1386 into primes:

£#ul#££#li#£Start with division by 2: the number is even, and n = 2 · 693. Continue with 693, and 2 as a first divisor candidate.£#/li#£ £#li#£693 is odd (2 is not a divisor), but is a multiple of 3: one has 693 = 3 · 231 and n = 2 · 3 · 231. Continue with 231, and 3 as a first divisor candidate.£#/li#£ £#li#£231 is also a multiple of 3: one has 231 = 3 · 77, and thus n = 2 · 32 · 77. Continue with 77, and 3 as a first divisor candidate.£#/li#£ £#li#£77 is not a multiple of 3, since the sum of its digits is 14, not a multiple of 3. It is also not a multiple of 5 because its last digit is 7. The next odd divisor to be tested is 7. One has 77 = 7 · 11, and thus n = 2 · 32 · 7 · 11. This shows that 7 is prime (easy to test directly). Continue with 11, and 7 as a first divisor candidate.£#/li#£ £#li#£As 72 > 11, one has finished. Thus 11 is prime, and the prime factorization is£#/li#££#/ul#£
1386 = 2 · 32 · 7 · 11.

£#h5#£Expressions£#/h5#£
Manipulating expressions is the basis of algebra. Factorization is one of the most important methods for expression manipulation for several reasons. If one can put an equation in a factored form E⋅F = 0, then the problem of solving the equation splits into two independent (and generally easier) problems E = 0 and F = 0. When an expression can be factored, the factors are often much simpler, and may thus offer some insight on the problem. For example,

${\displaystyle x^{3}-ax^{2}-bx^{2}-cx^{2}+abx+acx+bcx-abc}$
having 16 multiplications, 4 subtractions and 3 additions, may be factored into the much simpler expression

${\displaystyle (x-a)(x-b)(x-c),}$
with only two multiplications and three subtractions. Moreover, the factored form immediately gives roots x = a,b,c as the roots of the polynomial.

On the other hand, factorization is not always possible, and when it is possible, the factors are not always simpler. For example, ${\displaystyle x^{10}-1}$ can be factored into two irreducible factors ${\displaystyle x-1}$ and ${\displaystyle x^{9}+x^{8}+\cdots +x^{2}+x+1}$ .

Various methods have been developed for finding factorizations; some are described below.

Solving algebraic equations may be viewed as a problem of polynomial factorization. In fact, the fundamental theorem of algebra can be stated as follows: every polynomial in x of degree n with complex coefficients may be factorized into n linear factors ${\displaystyle x-a_{i},}$ for i = 1, ..., n, where the ais are the roots of the polynomial. Even though the structure of the factorization is known in these cases, the ais generally cannot be computed in terms of radicals (nth roots), by the Abel–Ruffini theorem. In most cases, the best that can be done is computing approximate values of the roots with a root-finding algorithm.


£#h5#£History of factorization of expressions£#/h5#£
The systematic use of algebraic manipulations for simplifying expressions (more specifically equations)) may be dated to 9th century, with al-Khwarizmi's book The Compendious Book on Calculation by Completion and Balancing, which is titled with two such types of manipulation.

However, even for solving quadratic equations, the factoring method was not used before Harriot's work published in 1631, ten years after his death. In his book Artis Analyticae Praxis ad Aequationes Algebraicas Resolvendas, Harriot drew, tables for addition, subtraction, multiplication and division of monomials, binomials, and trinomials. Then, in a second section, he set up the equation aa − ba + ca = + bc, and showed that this matches the form of multiplication he had previously provided, giving the factorization (a − b)(a + c).


£#h5#£General methods£#/h5#£
The following methods apply to any expression that is a sum, or that may be transformed into a sum. Therefore, they are most often applied to polynomials, though they also may be applied when the terms of the sum are not monomials, that is, the terms of the sum are a product of variables and constants.


£#h5#£Common factor£#/h5#£
It may occur that all terms of a sum are products and that some factors are common to all terms. In this case, the distributive law allows factoring out this common factor. If there are several such common factors, it is preferable to divide out the greatest such common factor. Also, if there are integer coefficients, one may factor out the greatest common divisor of these coefficients.

For example,

${\displaystyle 6x^{3}y^{2}+8x^{4}y^{3}-10x^{5}y^{3}=2x^{3}y^{2}(3+4xy-5x^{2}y),}$
since 2 is the greatest common divisor of 6, 8, and 10, and ${\displaystyle x^{3}y^{2}}$ divides all terms.


£#h5#£Grouping£#/h5#£
Grouping terms may allow using other methods for getting a factorization.

For example, to factor

${\displaystyle 4x^{2}+20x+3xy+15y,}$
one may remark that the first two terms have a common factor x, and the last two terms have the common factor y. Thus

${\displaystyle 4x^{2}+20x+3xy+15y=(4x^{2}+20x)+(3xy+15y)=4x(x+5)+3y(x+5).}$
Then a simple inspection shows the common factor x + 5, leading to the factorization

${\displaystyle 4x^{2}+20x+3xy+15y=(4x+3y)(x+5).}$
In general, this works for sums of 4 terms that have been obtained as the product of two binomials. Although not frequently, this may work also for more complicated examples.


£#h5#£Adding and subtracting terms£#/h5#£
Sometimes, some term grouping reveals part of a recognizable pattern. It is then useful to add and subtract terms to complete the pattern.

A typical use of this is the completing the square method for getting the quadratic formula.

Another example is the factorization of ${\displaystyle x^{4}+1.}$ If one introduces the non-real square root of –1, commonly denoted i, then one has a difference of squares

${\displaystyle x^{4}+1=(x^{2}+i)(x^{2}-i).}$
However, one may also want a factorization with real number coefficients. By adding and subtracting ${\displaystyle 2x^{2},}$ and grouping three terms together, one may recognize the square of a binomial:

${\displaystyle x^{4}+1=(x^{4}+2x^{2}+1)-2x^{2}=(x^{2}+1)^{2}-\left(x{\sqrt {2}}\right)^{2}=\left(x^{2}+x{\sqrt {2}}+1\right)\left(x^{2}-x{\sqrt {2}}+1\right).}$
Subtracting and adding ${\displaystyle 2x^{2}}$ also yields the factorization:

${\displaystyle x^{4}+1=(x^{4}-2x^{2}+1)+2x^{2}=(x^{2}-1)^{2}+\left(x{\sqrt {2}}\right)^{2}=\left(x^{2}+x{\sqrt {-2}}-1\right)\left(x^{2}-x{\sqrt {-2}}-1\right).}$
These factorizations work not only over the complex numbers, but also over any field, where either –1, 2 or –2 is a square. In a finite field, the product of two non-squares is a square; this implies that the polynomial ${\displaystyle x^{4}+1,}$ which is irreducible over the integers, is reducible modulo every prime number. For example,

${\displaystyle x^{4}+1\equiv (x+1)^{4}{\pmod {2}};}$
${\displaystyle x^{4}+1\equiv (x^{2}+x-1)(x^{2}-x-1){\pmod {3}},\qquad }$ since ${\displaystyle 1^{2}\equiv -2{\pmod {3}};}$
${\displaystyle x^{4}+1\equiv (x^{2}+2)(x^{2}-2){\pmod {5}},\qquad }$ since ${\displaystyle 2^{2}\equiv -1{\pmod {5}};}$
${\displaystyle x^{4}+1\equiv (x^{2}+3x+1)(x^{2}-3x+1){\pmod {7}},\qquad }$ since ${\displaystyle 3^{2}\equiv 2{\pmod {7}}.}$

£#h5#£Recognizable patterns£#/h5#£
Many identities provide an equality between a sum and a product. The above methods may be used for letting the sum side of some identity appear in an expression, which may therefore be replaced by a product.

Below are identities whose left-hand sides are commonly used as patterns (this means that the variables E and F that appear in these identities may represent any subexpression of the expression that has to be factorized).

£#ul#££#li#£
Difference of two squares
£#/li#££#/ul#£
${\displaystyle E^{2}-F^{2}=(E+F)(E-F)}$
For example,
${\displaystyle {\begin{aligned}a^{2}+&2ab+b^{2}-x^{2}+2xy-y^{2}\\&=(a^{2}+2ab+b^{2})-(x^{2}-2xy+y^{2})\\&=(a+b)^{2}-(x-y)^{2}\\&=(a+b+x-y)(a+b-x+y).\end{aligned}}}$
£#ul#££#li#£
Sum/difference of two cubes
£#/li#££#/ul#£
${\displaystyle E^{3}+F^{3}=(E+F)(E^{2}-EF+F^{2})}$
${\displaystyle E^{3}-F^{3}=(E-F)(E^{2}+EF+F^{2})}$
£#ul#££#li#£
Difference of two fourth powers
£#/li#££#/ul#£
${\displaystyle {\begin{aligned}E^{4}-F^{4}&=(E^{2}+F^{2})(E^{2}-F^{2})\\&=(E^{2}+F^{2})(E+F)(E-F)\end{aligned}}}$
£#ul#££#li#£
Sum/difference of two nth powers
£#/li#££#/ul#£
In the following identities, the factors may often be further factorized: £#ul#££#li#£
Difference, even exponent
£#/li#££#/ul#£
${\displaystyle E^{2n}-F^{2n}=(E^{n}+F^{n})(E^{n}-F^{n})}$
£#ul#££#li#£
Difference, even or odd exponent
£#/li#££#/ul#£
${\displaystyle E^{n}-F^{n}=(E-F)(E^{n-1}+E^{n-2}F+E^{n-3}F^{2}+\cdots +EF^{n-2}+F^{n-1})}$
This is an example showing that the factors may be much larger than the sum that is factorized.
£#ul#££#li#£
Sum, odd exponent
£#/li#££#/ul#£
${\displaystyle E^{n}+F^{n}=(E+F)(E^{n-1}-E^{n-2}F+E^{n-3}F^{2}-\cdots -EF^{n-2}+F^{n-1})}$
(obtained by changing F by –F in the preceding formula)
£#ul#££#li#£
Sum, even exponent
£#/li#££#/ul#£
If the exponent is a power of two then the expression cannot, in general, be factorized without introducing complex numbers (if E and F contain complex numbers, this may be not the case). If n has an odd divisor, that is if n = pq with p odd, one may use the preceding formula (in "Sum, odd exponent") applied to ${\displaystyle (E^{q})^{p}+(F^{q})^{p}.}$
£#ul#££#li#£
Trinomials and cubic formulas
£#/li#££#/ul#£
${\displaystyle {\begin{aligned}&x^{2}+y^{2}+z^{2}+2(xy+yz+xz)=(x+y+z)^{2}\\&x^{3}+y^{3}+z^{3}-3xyz=(x+y+z)(x^{2}+y^{2}+z^{2}-xy-xz-yz)\\&x^{3}+y^{3}+z^{3}+3x^{2}(y+z)+3y^{2}(x+z)+3z^{2}(x+y)+6xyz=(x+y+z)^{3}\\&x^{4}+x^{2}y^{2}+y^{4}=(x^{2}+xy+y^{2})(x^{2}-xy+y^{2}).\end{aligned}}}$
£#ul#££#li#£
Binomial expansions
£#/li#££#/ul#£
The binomial theorem supplies patterns that can easily be recognized from the integers that appear in them
In low degree:
${\displaystyle a^{2}+2ab+b^{2}=(a+b)^{2}}$
${\displaystyle a^{2}-2ab+b^{2}=(a-b)^{2}}$
${\displaystyle a^{3}+3a^{2}b+3ab^{2}+b^{3}=(a+b)^{3}}$
${\displaystyle a^{3}-3a^{2}b+3ab^{2}-b^{3}=(a-b)^{3}}$
More generally, the coefficients of the expanded forms of ${\displaystyle (a+b)^{n}}$ and ${\displaystyle (a-b)^{n}}$ are the binomial coefficients, that appear in the nth row of Pascal's triangle.

£#h5#£Roots of unity£#/h5#£
The nth roots of unity are the complex numbers each of which is a root of the polynomial ${\displaystyle x^{n}-1.}$ They are thus the numbers

${\displaystyle e^{2ik\pi /n}=\cos {\tfrac {2\pi k}{n}}+i\sin {\tfrac {2\pi k}{n}}}$
for ${\displaystyle k=0,\ldots ,n-1.}$

It follows that for any two expressions E and F, one has:

${\displaystyle E^{n}-F^{n}=(E-F)\prod _{k=1}^{n-1}\left(E-Fe^{2ik\pi /n}\right)}$
${\displaystyle E^{n}+F^{n}=\prod _{k=0}^{n-1}\left(E-Fe^{(2k+1)i\pi /n}\right)\qquad {\text{if }}n{\text{ is even}}}$
${\displaystyle E^{n}+F^{n}=(E+F)\prod _{k=1}^{n-1}\left(E+Fe^{2ik\pi /n}\right)\qquad {\text{if }}n{\text{ is odd}}}$
If E and F are real expressions, and one wants real factors, one has to replace every pair of complex conjugate factors by its product. As the complex conjugate of ${\displaystyle e^{i\alpha }}$ is ${\displaystyle e^{-i\alpha },}$ and

${\displaystyle \left(a-be^{i\alpha }\right)\left(a-be^{-i\alpha }\right)=a^{2}-ab\left(e^{i\alpha }+e^{-i\alpha }\right)+b^{2}e^{i\alpha }e^{-i\alpha }=a^{2}-2ab\cos \,\alpha +b^{2},}$
one has the following real factorizations (one passes from one to the other by changing k into n – k or n + 1 – k, and applying the usual trigonometric formulas:

${\displaystyle {\begin{aligned}E^{2n}-F^{2n}&=(E-F)(E+F)\prod _{k=1}^{n-1}\left(E^{2}-2EF\cos \,{\tfrac {k\pi }{n}}+F^{2}\right)\\&=(E-F)(E+F)\prod _{k=1}^{n-1}\left(E^{2}+2EF\cos \,{\tfrac {k\pi }{n}}+F^{2}\right)\end{aligned}}}$
${\displaystyle {\begin{aligned}E^{2n}+F^{2n}&=\prod _{k=1}^{n}\left(E^{2}+2EF\cos \,{\tfrac {(2k-1)\pi }{2n}}+F^{2}\right)\\&=\prod _{k=1}^{n}\left(E^{2}-2EF\cos \,{\tfrac {(2k-1)\pi }{2n}}+F^{2}\right)\end{aligned}}}$
The cosines that appear in these factorizations are algebraic numbers, and may be expressed in terms of radicals (this is possible because their Galois group is cyclic); however, these radical expressions are too complicated to be used, except for low values of n. For example,

${\displaystyle a^{4}+b^{4}=(a^{2}-{\sqrt {2}}ab+b^{2})(a^{2}+{\sqrt {2}}ab+b^{2}).}$
${\displaystyle a^{5}-b^{5}=(a-b)\left(a^{2}+{\frac {1-{\sqrt {5}}}{2}}ab+b^{2}\right)\left(a^{2}+{\frac {1+{\sqrt {5}}}{2}}ab+b^{2}\right),}$
${\displaystyle a^{5}+b^{5}=(a+b)\left(a^{2}-{\frac {1-{\sqrt {5}}}{2}}ab+b^{2}\right)\left(a^{2}-{\frac {1+{\sqrt {5}}}{2}}ab+b^{2}\right),}$
Often one wants a factorization with rational coefficients. Such a factorization involves cyclotomic polynomials. To express rational factorizations of sums and differences or powers, we need a notation for the homogenization of a polynomial: if ${\displaystyle P(x)=a_{0}x^{n}+a_{i}x^{n-1}+\cdots +a_{n},}$ its homogenization is the bivariate polynomial ${\displaystyle {\overline {P}}(x,y)=a_{0}x^{n}+a_{i}x^{n-1}y+\cdots +a_{n}y^{n}.}$ Then, one has

${\displaystyle E^{n}-F^{n}=\prod _{k\mid n}{\overline {Q}}_{n}(E,F),}$
${\displaystyle E^{n}+F^{n}=\prod _{k\mid 2n,k\not \mid n}{\overline {Q}}_{n}(E,F),}$
where the products are taken over all divisors of n, or all divisors of 2n that do not divide n, and ${\displaystyle Q_{n}(x)}$ is the nth cyclotomic polynomial.

For example,

${\displaystyle a^{6}-b^{6}={\overline {Q}}_{1}(a,b){\overline {Q}}_{2}(a,b){\overline {Q}}_{3}(a,b){\overline {Q}}_{6}(a,b)=(a-b)(a+b)(a^{2}-ab+b^{2})(a^{2}+ab+b^{2}),}$
${\displaystyle a^{6}+b^{6}={\overline {Q}}_{4}(a,b){\overline {Q}}_{12}(a,b)=(a^{2}+b^{2})(a^{4}-a^{2}b^{2}+b^{4}),}$
since the divisors of 6 are 1, 2, 3, 6, and the divisors of 12 that do not divide 6 are 4 and 12.


£#h5#£Polynomials£#/h5#£
For polynomials, factorization is strongly related with the problem of solving algebraic equations. An algebraic equation has the form

${\displaystyle P(x)\ \,{\stackrel {\text{def}}{=}}\ \,a_{0}x^{n}+a_{1}x^{n-1}+\cdots +a_{n}=0,}$
where P(x) is a polynomial in x with ${\displaystyle a_{0}\neq 0.}$ A solution of this equation (also called a root of the polynomial) is a value r of x such that

${\displaystyle P(r)=0.}$
If ${\displaystyle P(x)=Q(x)R(x)}$ is a factorization of P(x) = 0 as a product of two polynomials, then the roots of P(x) are the union of the roots of Q(x) and the roots of R(x). Thus solving P(x) = 0 is reduced to the simpler problems of solving Q(x) = 0 and R(x) = 0.

Conversely, the factor theorem asserts that, if r is a root of P(x) = 0, then P(x) may be factored as

${\displaystyle P(x)=(x-r)Q(x),}$
where Q(x) is the quotient of Euclidean division of P(x) = 0 by the linear (degree one) factor x – r.

If the coefficients of P(x) are real or complex numbers, the fundamental theorem of algebra asserts that P(x) has a real or complex root. Using the factor theorem recursively, it results that

${\displaystyle P(x)=a_{0}(x-r_{1})\cdots (x-r_{n}),}$
where ${\displaystyle r_{1},\ldots ,r_{n}}$ are the real or complex roots of P, with some of them possibly repeated. This complete factorization is unique up to the order of the factors.

If the coefficients of P(x) are real, one generally wants a factorization where factors have real coefficients. In this case, the complete factorization may have some quadratic (degree two) factors. This factorization may easily be deduced from the above complete factorization. In fact, if r = a + ib is a non-real root of P(x), then its complex conjugate s = a - ib is also a root of P(x). So, the product

${\displaystyle (x-r)(x-s)=x^{2}-(r+s)x+rs=x^{2}+2ax+a^{2}+b^{2}}$
is a factor of P(x) with real coefficients. Repeating this for all non-real factors gives a factorization with linear or quadratic real factors.

For computing these real or complex factorizations, one needs the roots of the polynomial, which may not be computed exactly, and only approximated using root-finding algorithms.

In practice, most algebraic equations of interest have integer or rational coefficients, and one may want a factorization with factors of the same kind. The fundamental theorem of arithmetic may be generalized to this case, stating that polynomials with integer or rational coefficients have the unique factorization property. More precisely, every polynomial with rational coefficients may be factorized in a product

${\displaystyle P(x)=q\,P_{1}(x)\cdots P_{k}(x),}$
where q is a rational number and ${\displaystyle P_{1}(x),\ldots ,P_{k}(x)}$ are non-constant polynomials with integer coefficients that are irreducible and primitive; this means that none of the ${\displaystyle P_{i}(x)}$ may be written as the product two polynomials (with integer coefficients) that are neither 1 nor –1 (integers are considered as polynomials of degree zero). Moreover, this factorization is unique up to the order of the factors and the signs of the factors.

There are efficient algorithms for computing this factorization, which are implemented in most computer algebra systems. See Factorization of polynomials. Unfortunately, these algorithms are too complicated to use for paper-and-pencil computations. Besides the heuristics above, only a few methods are suitable for hand computations, which generally work only for polynomials of low degree, with few nonzero coefficients. The main such methods are described in next subsections.


£#h5#£Primitive-part & content factorization£#/h5#£
Every polynomial with rational coefficients, may be factorized, in a unique way, as the product of a rational number and a polynomial with integer coefficients, which is primitive (that is, the greatest common divisor of the coefficients is 1), and has a positive leading coefficient (coefficient of the term of the highest degree). For example:

${\displaystyle -10x^{2}+5x+5=(-5)\cdot (2x^{2}-x-1)}$
${\displaystyle {\frac {1}{3}}x^{5}+{\frac {7}{2}}x^{2}+2x+1={\frac {1}{6}}(2x^{5}+21x^{2}+12x+6)}$
In this factorization, the rational number is called the content, and the primitive polynomial is the primitive part. The computation of this factorization may be done as follows: firstly, reduce all coefficients to a common denominator, for getting the quotient by an integer q of a polynomial with integer coefficients. Then one divides out the greater common divisor p of the coefficients of this polynomial for getting the primitive part, the content being ${\displaystyle p/q.}$ Finally, if needed, one changes the signs of p and all coefficients of the primitive part.

This factorization may produce a result that is larger than the original polynomial (typically when there are many coprime denominators), but, even when this is the case, the primitive part is generally easier to manipulate for further factorization.


£#h5#£Using the factor theorem£#/h5#£
The factor theorem states that, if r is a root of a polynomial

${\displaystyle P(x)=a_{0}x^{n}+a_{1}x^{n-1}+\cdots +a_{n-1}x+a_{n},}$
meaning P(r) = 0, then there is a factorization

${\displaystyle P(x)=(x-r)Q(x),}$
where

${\displaystyle Q(x)=b_{0}x^{n-1}+\cdots +b_{n-2}x+b_{n-1},}$
with ${\displaystyle a_{0}=b_{0}}$ . Then polynomial long division or synthetic division give:

${\displaystyle b_{i}=a_{0}r^{i}+\cdots +a_{i-1}r+a_{i}\ {\text{ for }}\ i=1,\ldots ,n{-}1.}$
This may be useful when one knows or can guess a root of the polynomial.

For example, for ${\displaystyle P(x)=x^{3}-3x+2,}$ one may easily see that the sum of its coefficients is 0, so r = 1 is a root. As r + 0 = 1, and ${\displaystyle r^{2}+0r-3=-2,}$ one has

${\displaystyle x^{3}-3x+2=(x-1)(x^{2}+x-2).}$

£#h5#£Rational roots£#/h5#£
For polynomials with rational number coefficients, one may search for roots which are rational numbers. Primitive part-content factorization (see above) reduces the problem of searching for rational roots to the case of polynomials with integer coefficients having no non-trivial common divisor.

If ${\displaystyle x={\tfrac {p}{q}}}$ is a rational root of such a polynomial

${\displaystyle P(x)=a_{0}x^{n}+a_{1}x^{n-1}+\cdots +a_{n-1}x+a_{n},}$
the factor theorem shows that one has a factorization

${\displaystyle P(x)=(qx-p)Q(x),}$
where both factors have integer coefficients (the fact that Q has integer coefficients results from the above formula for the quotient of P(x) by ${\displaystyle x-p/q}$ ).

Comparing the coefficients of degree n and the constant coefficients in the above equality shows that, if ${\displaystyle {\tfrac {p}{q}}}$ is a rational root in reduced form, then q is a divisor of ${\displaystyle a_{0},}$ and p is a divisor of ${\displaystyle a_{n}.}$ Therefore, there is a finite number of possibilities for p and q, which can be systematically examined.

For example, if the polynomial

${\displaystyle P(x)=2x^{3}-7x^{2}+10x-6}$
has a rational root ${\displaystyle {\tfrac {p}{q}}}$ with q > 0, then p must divide 6; that is ${\displaystyle p\in \{\pm 1,\pm 2,\pm 3,\pm 6\},}$ and q must divide 2, that is ${\displaystyle q\in \{1,2\}.}$ Moreover, if x < 0, all terms of the polynomial are negative, and, therefore, a root cannot be negative. That is, one must have

${\displaystyle {\tfrac {p}{q}}\in \{1,2,3,6,{\tfrac {1}{2}},{\tfrac {3}{2}}\}.}$
A direct computation shows that only ${\displaystyle {\tfrac {3}{2}}}$ is a root, so there can be no other rational root. Applying the factor theorem leads finally to the factorization ${\displaystyle 2x^{3}-7x^{2}+10x-6=(2x-3)(x^{2}-2x+2).}$


£#h5#£Quadratic ac method£#/h5#£
The above method may be adapted for quadratic polynomials, leading to the ac method of factorization.

Consider the quadratic polynomial

${\displaystyle P(x)=ax^{2}+bx+c}$
with integer coefficients. If it has a rational root, its denominator must divide a evenly and it may be written as a possibly reducible fraction ${\displaystyle r_{1}={\tfrac {r}{a}}.}$ By Vieta's formulas, the other root ${\displaystyle r_{2}}$ is

${\displaystyle r_{2}=-{\frac {b}{a}}-r_{1}=-{\frac {b}{a}}-{\frac {r}{a}}=-{\frac {b+r}{a}}={\frac {s}{a}},}$
with ${\displaystyle s=-(b+r).}$ Thus the second root is also rational, and Vieta's second formula ${\displaystyle r_{1}r_{2}={\frac {c}{a}}}$ gives

${\displaystyle {\frac {s}{a}}{\frac {r}{a}}={\frac {c}{a}},}$
that is

${\displaystyle rs=ac\quad {\text{and}}\quad r+s=-b.}$
Checking all pairs of integers whose product is ac gives the rational roots, if any.

In summary, if ${\displaystyle ax^{2}+bx+c}$ has rational roots there are integers r and s such ${\displaystyle rs=ac}$ and ${\displaystyle r+s=-b}$ (a finite number of cases to test), and the roots are ${\displaystyle {\tfrac {r}{a}}}$ and ${\displaystyle {\tfrac {s}{a}}.}$ In other words, one has the factorization

${\displaystyle a(ax^{2}+bx+c)=(ax-r)(ax-s).}$
For example, let consider the quadratic polynomial

${\displaystyle 6x^{2}+13x+6.}$
Inspection of the factors of ac = 36 leads to 4 + 9 = 13 = b, giving the two roots

${\displaystyle r_{1}=-{\frac {4}{6}}=-{\frac {2}{3}}\quad {\text{and}}\quad r_{2}=-{\frac {9}{6}}=-{\frac {3}{2}},}$
and the factorization

${\displaystyle 6x^{2}+13x+6=6(x+{\tfrac {2}{3}})(x+{\tfrac {3}{2}})=(3x+2)(2x+3).}$

£#h5#£Using formulas for polynomial roots£#/h5#£
Any univariate quadratic polynomial ${\displaystyle ax^{2}+bx+c}$ can be factored using the quadratic formula:

${\displaystyle ax^{2}+bx+c=a(x-\alpha )(x-\beta )=a\left(x-{\frac {-b+{\sqrt {b^{2}-4ac}}}{2a}}\right)\left(x-{\frac {-b-{\sqrt {b^{2}-4ac}}}{2a}}\right),}$
where ${\displaystyle \alpha }$ and ${\displaystyle \beta }$ are the two roots of the polynomial.

If a, b, c are all real, the factors are real if and only if the discriminant ${\displaystyle b^{2}-4ac}$ is non-negative. Otherwise, the quadratic polynomial cannot be factorized into non-constant real factors.

The quadratic formula is valid when the coefficients belong to any field of characteristic different from two, and, in particular, for coefficients in a finite field with an odd number of elements.

There are also formulas for roots of cubic and quartic polynomials, which are, in general, too complicated for practical use. The Abel–Ruffini theorem shows that there are no general root formulas in terms of radicals for polynomials of degree five or higher.


£#h5#£Using relations between roots£#/h5#£
It may occur that one knows some relationship between the roots of a polynomial and its coefficients. Using this knowledge may help factoring the polynomial and finding its roots. Galois theory is based on a systematic study of the relations between roots and coefficients, that include Vieta's formulas.

Here, we consider the simpler case where two roots ${\displaystyle x_{1}}$ and ${\displaystyle x_{2}}$ of a polynomial ${\displaystyle P(x)}$ satisfy the relation

${\displaystyle x_{2}=Q(x_{1}),}$
where Q is a polynomial.

This implies that ${\displaystyle x_{1}}$ is a common root of ${\displaystyle P(Q(x))}$ and ${\displaystyle P(x).}$ It is therefore a root of the greatest common divisor of these two polynomials. It follows that this greatest common divisor is a non constant factor of ${\displaystyle P(x).}$ Euclidean algorithm for polynomials allows computing this greatest common factor.

For example, if one know or guess that: ${\displaystyle P(x)=x^{3}-5x^{2}-16x+80}$ has two roots that sum to zero, one may apply Euclidean algorithm to ${\displaystyle P(x)}$ and ${\displaystyle P(-x).}$ The first division step consists in adding ${\displaystyle P(x)}$ to ${\displaystyle P(-x),}$ giving the remainder of

${\displaystyle -10(x^{2}-16).}$
Then, dividing ${\displaystyle P(x)}$ by ${\displaystyle x^{2}-16}$ gives zero as a new remainder, and x – 5 as a quotient, leading to the complete factorization

${\displaystyle x^{3}-5x^{2}-16x+80=(x-5)(x-4)(x+4).}$

£#h5#£Unique factorization domains£#/h5#£
The integers and the polynomials over a field share the property of unique factorization, that is, every nonzero element may be factored into a product of an invertible element (a unit, ±1 in the case of integers) and a product of irreducible elements (prime numbers, in the case of integers), and this factorization is unique up to rearranging the factors and shifting units among the factors. Integral domains which share this property are called unique factorization domains (UFD).

Greatest common divisors exist in UFDs, and conversely, every integral domain in which greatest common divisors exist is an UFD. Every principal ideal domain is an UFD.

A Euclidean domain is an integral domain on which is defined a Euclidean division similar to that of integers. Every Euclidean domain is a principal ideal domain, and thus a UFD.

In a Euclidean domain, Euclidean division allows defining a Euclidean algorithm for computing greatest common divisors. However this does not imply the existence of a factorization algorithm. There is an explicit example of a field F such that there cannot exist any factorization algorithm in the Euclidean domain F[x] of the univariate polynomials over F.


£#h5#£Ideals£#/h5#£
In algebraic number theory, the study of Diophantine equations led mathematicians, during 19th century, to introduce generalizations of the integers called algebraic integers. The first ring of algebraic integers that have been considered were Gaussian integers and Eisenstein integers, which share with usual integers the property of being principal ideal domains, and have thus the unique factorization property.

Unfortunately, it soon appeared that most rings of algebraic integers are not principal and do not have unique factorization. The simplest example is ${\displaystyle \mathbb {Z} [{\sqrt {-5}}],}$ in which

${\displaystyle 9=3\cdot 3=(2+{\sqrt {-5}})(2-{\sqrt {-5}}),}$
and all these factors are irreducible.

This lack of unique factorization is a major difficulty for solving Diophantine equations. For example, many wrong proofs of Fermat's Last Theorem (probably including Fermat's "truly marvelous proof of this, which this margin is too narrow to contain") were based on the implicit supposition of unique factorization.

This difficulty was resolved by Dedekind, who proved that the rings of algebraic integers have unique factorization of ideals: in these rings, every ideal is a product of prime ideals, and this factorization is unique up the order of the factors. The integral domains that have this unique factorization property are now called Dedekind domains. They have many nice properties that make them fundamental in algebraic number theory.


£#h5#£Matrices£#/h5#£
Matrix rings are non-commutative and have no unique factorization: there are, in general, many ways of writing a matrix as a product of matrices. Thus, the factorization problem consists of finding factors of specified types. For example, the LU decomposition gives a matrix as the product of a lower triangular matrix by an upper triangular matrix. As this is not always possible, one generally considers the "LUP decomposition" having a permutation matrix as its third factor.

See Matrix decomposition for the most common types of matrix factorizations.

A logical matrix represents a binary relation, and matrix multiplication corresponds to composition of relations. Decomposition of a relation through factorization serves to profile the nature of the relation, such as a difunctional relation.


£#h5#£See also£#/h5#£ £#ul#££#li#£Euler's factorization method for integers£#/li#£ £#li#£Fermat's factorization method for integers£#/li#£ £#li#£Monoid factorisation£#/li#£ £#li#£Multiplicative partition£#/li#£ £#li#£Table of Gaussian integer factorizations£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Burnside, William Snow; Panton, Arthur William (1960) [1912], The Theory of Equations with an introduction to the theory of binary algebraic forms (Volume one), Dover£#/li#£ £#li#£Dickson, Leonard Eugene (1922), First Course in the Theory of Equations, New York: John Wiley & Sons£#/li#£ £#li#£Fite, William Benjamin (1921), College Algebra (Revised), Boston: D. C. Heath & Co.£#/li#£ £#li#£Klein, Felix (1925), Elementary Mathematics from an Advanced Standpoint; Arithmetic, Algebra, Analysis, Dover£#/li#£ £#li#£Selby, Samuel M., CRC Standard Mathematical Tables (18th ed.), The Chemical Rubber Co.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Wolfram Alpha can factorize too.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Algebra > Polynomials £#/li#££#/ul#£




£#h3#£Acos£#/h3#£

ACOS or Acos may refer to:

£#ul#££#li#£Arccosine, an inverse trigonometric function£#/li#£ £#li#£The Advanced Comprehensive Operating System mainframe computer operating system£#/li#£ £#li#£Acos District in Peru£#/li#£ £#li#£Acos Vinchos District in Peru£#/li#£ £#li#£A Crown of Swords novel£#/li#££#/ul#£
£#h5#£See also£#/h5#£ £#ul#££#li#£Aco (disambiguation)£#/li#£ £#li#£Cos (disambiguation)£#/li#£
£#li#£All pages with titles beginning with Acos£#/li#£ £#li#£All pages with titles containing Acos£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Trigonometric Functions £#/li#££#/ul#£




£#h3#£Action£#/h3#£

Action may refer to:

£#ul#££#li#£Action (narrative), a literary mode£#/li#£ £#li#£Action fiction, a type of genre fiction£#/li#£ £#li#£Action game, a genre of video game£#/li#££#/ul#£
£#h5#£Film£#/h5#£ £#ul#££#li#£Action film, a genre of film£#/li#£ £#li#£Action (1921 film), a film by John Ford£#/li#£ £#li#£Action (1980 film), a film by Tinto Brass£#/li#£ £#li#£Action 3D, a 2013 Telugu language film£#/li#£ £#li#£Action (2019 film), a Kollywood film.£#/li#££#/ul#£
£#h5#£Music£#/h5#£ £#ul#££#li#£Action (music), a characteristic of a stringed instrument£#/li#£ £#li#£Action (piano), the mechanism which drops the hammer on the string when a key is pressed£#/li#£ £#li#£The Action, a 1960s band£#/li#££#/ul#£
£#h5#£Albums£#/h5#£ £#ul#££#li#£Action (B'z album) (2007)£#/li#£ £#li#£Action! (Desmond Dekker album) (1968)£#/li#£ £#li#£Action Action Action or Action, a 1965 album by Jackie McLean£#/li#£ £#li#£Action! (Oh My God album) (2002)£#/li#£ £#li#£Action (Oscar Peterson album) (1968)£#/li#£ £#li#£Action (Punchline album) (2004)£#/li#£ £#li#£Action (Question Mark & the Mysterians album) (1967)£#/li#£ £#li#£Action (Uppermost album) (2011)£#/li#£ £#li#£Action (EP), a 2012 EP by NU'EST£#/li#£ £#li#£Action, a 1984 album by Kiddo£#/li#££#/ul#£
£#h5#£Songs£#/h5#£ £#ul#££#li#£"Action" (Freddy Cannon song) (1965), the theme song to the TV series Where the Action Is£#/li#£ £#li#£"Action" (Sweet song) (1975), covered by various artists£#/li#£ £#li#£"Action", a version of "Feeling This" by Blink-182£#/li#£ £#li#£"Action", a 1984 song by The Fits£#/li#£ £#li#£"Action", a 1960 song by Lance Fortune£#/li#£ £#li#£"Action", a 1988 song by Girlschool from Take a Bite£#/li#£ £#li#£"Action", a 1989 song by Gorky Park from Gorky Park (album)£#/li#£ £#li#£"Action", a 1988 song by Pearly Gates£#/li#£ £#li#£"Action", a 2003 song by Powerman 5000 from Transform£#/li#£ £#li#£"Action", a 1972 song by Scorpions from Lonesome Crow£#/li#£ £#li#£"Actions", a 1980 song by The Stingrays£#/li#£ £#li#£"Action", a 1994 song by Terror Fabulous featuring Nadine Sutherland£#/li#££#/ul#£
£#h5#£Literature£#/h5#£ £#ul#££#li#£Action (comics), a British comic book published in 1976–1977£#/li#£ £#li#£Action Comics, a DC Comics comic book series£#/li#£ £#li#£Action: A Book about Sex, a 2016 book by Amy Rose Spiegel£#/li#£ £#li#£Action (newspaper), a newspaper of Oswald Mosley's British Union of Fascists£#/li#££#/ul#£
£#h5#£People£#/h5#£ £#ul#££#li#£Action Bronson (born 1983), American rapper, reality television star, author, and talk show host£#/li#££#/ul#£
£#h5#£Television and radio£#/h5#£ £#ul#££#li#£Action (Canadian TV channel)£#/li#£ £#li#£Action (French TV channel)£#/li#£ £#li#£The Action Channel (US TV channel), a subsidiary of Luken Communications£#/li#£ £#li#£Action (radio), a 1945 radio program£#/li#£ £#li#£Action (TV series), a comedy series on Fox in 1999–2000£#/li#£ £#li#£CBS Action (2009–2018), now known as CBS Justice£#/li#£ £#li#£Sky Sports Action, a TV channel£#/li#££#/ul#£
£#h5#£Theatre£#/h5#£ £#ul#££#li#£Action (theatre), a principle in Western theatre practice£#/li#£ £#li#£Action (play), a 1975 play by Sam Shepard£#/li#££#/ul#£
£#h5#£Organizations£#/h5#£
£#h5#£Businesses£#/h5#£ £#ul#££#li#£Action (store), a Dutch discount store chain with branches in many European countries£#/li#£ £#li#£Action (supermarkets), an Australian supermarket chain£#/li#£ £#li#£Actions Semiconductor, a Chinese semiconductor company£#/li#£ £#li#£ACTION, an Australian public transport company£#/li#£ £#li#£The Action Network (branded as Action), an American sports betting analytics company£#/li#££#/ul#£
£#h5#£Political parties£#/h5#£ £#ul#££#li#£Action (Cyprus), a Cypriot political alliance£#/li#£ £#li#£Action (Greece), a Greek political party£#/li#£ £#li#£Action (Italian political party), an Italian political party£#/li#££#/ul#£
£#h5#£Other organizations£#/h5#£ £#ul#££#li#£ACTION (U.S. government agency), a former US government federal domestic volunteer agency£#/li#££#/ul#£
£#h5#£Science, technology, and mathematics£#/h5#£ £#ul#££#li#£Action (physics), an attribute of the dynamics of a physical system£#/li#£ £#li#£Action at a distance, an outdated term for nonlocal interaction in physics£#/li#£ £#li#£Group action (mathematics) £#ul#££#li#£Continuous group action£#/li#£ £#li#£Semigroup action£#/li#££#/ul#££#/li#£ £#li#£ Ring Action (mathematics)£#/li#£ £#li#£Action (firearms), the mechanism that manipulates cartridges and/or seals the breech£#/li#£ £#li#£Action! (programming language), for the Atari 8-bit family of microcomputers£#/li#£ £#li#£Action (UML), in the Unified Modeling Language£#/li#£ £#li#£Dudek Action, a Polish paraglider design£#/li#£ £#li#£Diia, a brand of e-governance in Ukraine£#/li#££#/ul#£
£#h5#£Other uses£#/h5#£ £#ul#££#li#£Action (philosophy), something which is done by a person£#/li#£ £#li#£Lawsuit or action£#/li#££#/ul#£
£#h5#£See also£#/h5#£ £#ul#££#li#£Action Force (disambiguation)£#/li#£ £#li#£Action Jackson (disambiguation)£#/li#£ £#li#£Action Man (disambiguation)£#/li#£ £#li#£Action theory (disambiguation)£#/li#£ £#li#£Acteon (disambiguation)£#/li#£ £#li#£Actaeon (disambiguation)£#/li#£ £#li#£Acción (disambiguation)£#/li#£ £#li#£Structural load, forces, deformations, or accelerations applied to a structure or its components£#/li#£ £#li#£All pages with titles beginning with Action £#/li#£ £#li#£All pages with titles containing action£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Algebra > Group Theory > Group Properties £#/li#££#li#£ Calculus and Analysis > Dynamical Systems £#/li#££#/ul#£




£#h3#£Adams' Method£#/h3#£

Linear multistep methods are used for the numerical solution of ordinary differential equations. Conceptually, a numerical method starts from an initial point and then takes a short step forward in time to find the next solution point. The process continues with subsequent steps to map out the solution. Single-step methods (such as Euler's method) refer to only one previous point and its derivative to determine the current value. Methods such as Runge–Kutta take some intermediate steps (for example, a half-step) to obtain a higher order method, but then discard all previous information before taking a second step. Multistep methods attempt to gain efficiency by keeping and using the information from previous steps rather than discarding it. Consequently, multistep methods refer to several previous points and derivative values. In the case of linear multistep methods, a linear combination of the previous points and derivative values is used.


£#h5#£Definitions£#/h5#£
Numerical methods for ordinary differential equations approximate solutions to initial value problems of the form

${\displaystyle y'=f(t,y),\quad y(t_{0})=y_{0}.}$
The result is approximations for the value of ${\displaystyle y(t)}$ at discrete times ${\displaystyle t_{i}}$ :

${\displaystyle y_{i}\approx y(t_{i})\quad {\text{where}}\quad t_{i}=t_{0}+ih,}$
where ${\displaystyle h}$ is the time step (sometimes referred to as ${\displaystyle \Delta t}$ ) and ${\displaystyle i}$ is an integer.

Multistep methods use information from the previous ${\displaystyle s}$ steps to calculate the next value. In particular, a linear multistep method uses a linear combination of ${\displaystyle y_{i}}$ and ${\displaystyle f(t_{i},y_{i})}$ to calculate the value of ${\displaystyle y}$ for the desired current step. Thus, a linear multistep method is a method of the form

${\displaystyle {\begin{aligned}&y_{n+s}+a_{s-1}\cdot y_{n+s-1}+a_{s-2}\cdot y_{n+s-2}+\cdots +a_{0}\cdot y_{n}\\&\qquad {}=h\cdot \left(b_{s}\cdot f(t_{n+s},y_{n+s})+b_{s-1}\cdot f(t_{n+s-1},y_{n+s-1})+\cdots +b_{0}\cdot f(t_{n},y_{n})\right)\\&\Leftrightarrow \sum _{j=0}^{s}a_{j}y_{n+j}=h\sum _{j=0}^{s}b_{j}f(t_{n+j},y_{n+j}),\end{aligned}}}$
with ${\displaystyle a_{s}=1}$ . The coefficients ${\displaystyle a_{0},\dotsc ,a_{s-1}}$ and ${\displaystyle b_{0},\dotsc ,b_{s}}$ determine the method. The designer of the method chooses the coefficients, balancing the need to get a good approximation to the true solution against the desire to get a method that is easy to apply. Often, many coefficients are zero to simplify the method.

One can distinguish between explicit and implicit methods. If ${\displaystyle b_{s}=0}$ , then the method is called "explicit", since the formula can directly compute ${\displaystyle y_{n+s}}$ . If ${\displaystyle b_{s}\neq 0}$ then the method is called "implicit", since the value of ${\displaystyle y_{n+s}}$ depends on the value of ${\displaystyle f(t_{n+s},y_{n+s})}$ , and the equation must be solved for ${\displaystyle y_{n+s}}$ . Iterative methods such as Newton's method are often used to solve the implicit formula.

Sometimes an explicit multistep method is used to "predict" the value of ${\displaystyle y_{n+s}}$ . That value is then used in an implicit formula to "correct" the value. The result is a predictor–corrector method.


£#h5#£Examples£#/h5#£
Consider for an example the problem

${\displaystyle y'=f(t,y)=y,\quad y(0)=1.}$
The exact solution is ${\displaystyle y(t)=e^{t}}$ .


£#h5#£One-step Euler£#/h5#£
A simple numerical method is Euler's method:

${\displaystyle y_{n+1}=y_{n}+hf(t_{n},y_{n}).}$
Euler's method can be viewed as an explicit multistep method for the degenerate case of one step.

This method, applied with step size ${\displaystyle h={\tfrac {1}{2}}}$ on the problem ${\displaystyle y'=y}$ , gives the following results:

${\displaystyle {\begin{aligned}y_{1}&=y_{0}+hf(t_{0},y_{0})=1+{\tfrac {1}{2}}\cdot 1=1.5,\\y_{2}&=y_{1}+hf(t_{1},y_{1})=1.5+{\tfrac {1}{2}}\cdot 1.5=2.25,\\y_{3}&=y_{2}+hf(t_{2},y_{2})=2.25+{\tfrac {1}{2}}\cdot 2.25=3.375,\\y_{4}&=y_{3}+hf(t_{3},y_{3})=3.375+{\tfrac {1}{2}}\cdot 3.375=5.0625.\end{aligned}}}$

£#h5#£Two-step Adams–Bashforth£#/h5#£
Euler's method is a one-step method. A simple multistep method is the two-step Adams–Bashforth method

${\displaystyle y_{n+2}=y_{n+1}+{\tfrac {3}{2}}hf(t_{n+1},y_{n+1})-{\tfrac {1}{2}}hf(t_{n},y_{n}).}$
This method needs two values, ${\displaystyle y_{n+1}}$ and ${\displaystyle y_{n}}$ , to compute the next value, ${\displaystyle y_{n+2}}$ . However, the initial value problem provides only one value, ${\displaystyle y_{0}=1}$ . One possibility to resolve this issue is to use the ${\displaystyle y_{1}}$ computed by Euler's method as the second value. With this choice, the Adams–Bashforth method yields (rounded to four digits):

${\displaystyle {\begin{aligned}y_{2}&=y_{1}+{\tfrac {3}{2}}hf(t_{1},y_{1})-{\tfrac {1}{2}}hf(t_{0},y_{0})=1.5+{\tfrac {3}{2}}\cdot {\tfrac {1}{2}}\cdot 1.5-{\tfrac {1}{2}}\cdot {\tfrac {1}{2}}\cdot 1=2.375,\\y_{3}&=y_{2}+{\tfrac {3}{2}}hf(t_{2},y_{2})-{\tfrac {1}{2}}hf(t_{1},y_{1})=2.375+{\tfrac {3}{2}}\cdot {\tfrac {1}{2}}\cdot 2.375-{\tfrac {1}{2}}\cdot {\tfrac {1}{2}}\cdot 1.5=3.7812,\\y_{4}&=y_{3}+{\tfrac {3}{2}}hf(t_{3},y_{3})-{\tfrac {1}{2}}hf(t_{2},y_{2})=3.7812+{\tfrac {3}{2}}\cdot {\tfrac {1}{2}}\cdot 3.7812-{\tfrac {1}{2}}\cdot {\tfrac {1}{2}}\cdot 2.375=6.0234.\end{aligned}}}$
The exact solution at ${\displaystyle t=t_{4}=2}$ is ${\displaystyle e^{2}=7.3891\ldots }$ , so the two-step Adams–Bashforth method is more accurate than Euler's method. This is always the case if the step size is small enough.


£#h5#£Families of multistep methods£#/h5#£
Three families of linear multistep methods are commonly used: Adams–Bashforth methods, Adams–Moulton methods, and the backward differentiation formulas (BDFs).


£#h5#£Adams–Bashforth methods£#/h5#£
The Adams–Bashforth methods are explicit methods. The coefficients are ${\displaystyle a_{s-1}=-1}$ and ${\displaystyle a_{s-2}=\cdots =a_{0}=0}$ , while the ${\displaystyle b_{j}}$ are chosen such that the methods have order s (this determines the methods uniquely).

The Adams–Bashforth methods with s = 1, 2, 3, 4, 5 are (Hairer, Nørsett & Wanner 1993, §III.1; Butcher 2003, p. 103):

${\displaystyle {\begin{aligned}y_{n+1}&=y_{n}+hf(t_{n},y_{n}),\qquad {\text{(This is the Euler method)}}\\y_{n+2}&=y_{n+1}+h\left({\frac {3}{2}}f(t_{n+1},y_{n+1})-{\frac {1}{2}}f(t_{n},y_{n})\right),\\y_{n+3}&=y_{n+2}+h\left({\frac {23}{12}}f(t_{n+2},y_{n+2})-{\frac {16}{12}}f(t_{n+1},y_{n+1})+{\frac {5}{12}}f(t_{n},y_{n})\right),\\y_{n+4}&=y_{n+3}+h\left({\frac {55}{24}}f(t_{n+3},y_{n+3})-{\frac {59}{24}}f(t_{n+2},y_{n+2})+{\frac {37}{24}}f(t_{n+1},y_{n+1})-{\frac {9}{24}}f(t_{n},y_{n})\right),\\y_{n+5}&=y_{n+4}+h\left({\frac {1901}{720}}f(t_{n+4},y_{n+4})-{\frac {2774}{720}}f(t_{n+3},y_{n+3})+{\frac {2616}{720}}f(t_{n+2},y_{n+2})-{\frac {1274}{720}}f(t_{n+1},y_{n+1})+{\frac {251}{720}}f(t_{n},y_{n})\right).\end{aligned}}}$
The coefficients ${\displaystyle b_{j}}$ can be determined as follows. Use polynomial interpolation to find the polynomial p of degree ${\displaystyle s-1}$ such that

${\displaystyle p(t_{n+i})=f(t_{n+i},y_{n+i}),\qquad {\text{for }}i=0,\ldots ,s-1.}$
The Lagrange formula for polynomial interpolation yields

${\displaystyle p(t)=\sum _{j=0}^{s-1}{\frac {(-1)^{s-j-1}f(t_{n+j},y_{n+j})}{j!(s-j-1)!h^{s-1}}}\prod _{i=0 \atop i\neq j}^{s-1}(t-t_{n+i}).}$
The polynomial p is locally a good approximation of the right-hand side of the differential equation ${\displaystyle y'=f(t,y)}$ that is to be solved, so consider the equation ${\displaystyle y'=p(t)}$ instead. This equation can be solved exactly; the solution is simply the integral of p. This suggests taking

${\displaystyle y_{n+s}=y_{n+s-1}+\int _{t_{n+s-1}}^{t_{n+s}}p(t)\,dt.}$
The Adams–Bashforth method arises when the formula for p is substituted. The coefficients ${\displaystyle b_{j}}$ turn out to be given by

${\displaystyle b_{s-j-1}={\frac {(-1)^{j}}{j!(s-j-1)!}}\int _{0}^{1}\prod _{i=0 \atop i\neq j}^{s-1}(u+i)\,du,\qquad {\text{for }}j=0,\ldots ,s-1.}$
Replacing ${\displaystyle f(t,y)}$ by its interpolant p incurs an error of order hs, and it follows that the s-step Adams–Bashforth method has indeed order s (Iserles 1996, §2.1)

The Adams–Bashforth methods were designed by John Couch Adams to solve a differential equation modelling capillary action due to Francis Bashforth. Bashforth (1883) published his theory and Adams' numerical method (Goldstine 1977).


£#h5#£Adams–Moulton methods£#/h5#£
The Adams–Moulton methods are similar to the Adams–Bashforth methods in that they also have ${\displaystyle a_{s-1}=-1}$ and ${\displaystyle a_{s-2}=\cdots =a_{0}=0}$ . Again the b coefficients are chosen to obtain the highest order possible. However, the Adams–Moulton methods are implicit methods. By removing the restriction that ${\displaystyle b_{s}=0}$ , an s-step Adams–Moulton method can reach order ${\displaystyle s+1}$ , while an s-step Adams–Bashforth methods has only order s.

The Adams–Moulton methods with s = 0, 1, 2, 3, 4 are (Hairer, Nørsett & Wanner 1993, §III.1; Quarteroni, Sacco & Saleri 2000):

${\displaystyle y_{n}=y_{n-1}+hf(t_{n},y_{n}),}$ This is the backward Euler method
${\displaystyle y_{n+1}=y_{n}+{\frac {1}{2}}h\left(f(t_{n+1},y_{n+1})+f(t_{n},y_{n})\right),}$ This is the trapezoidal rule
${\displaystyle {\begin{aligned}y_{n+2}&=y_{n+1}+h\left({\frac {5}{12}}f(t_{n+2},y_{n+2})+{\frac {8}{12}}f(t_{n+1},y_{n+1})-{\frac {1}{12}}f(t_{n},y_{n})\right),\\y_{n+3}&=y_{n+2}+h\left({\frac {9}{24}}f(t_{n+3},y_{n+3})+{\frac {19}{24}}f(t_{n+2},y_{n+2})-{\frac {5}{24}}f(t_{n+1},y_{n+1})+{\frac {1}{24}}f(t_{n},y_{n})\right),\\y_{n+4}&=y_{n+3}+h\left({\frac {251}{720}}f(t_{n+4},y_{n+4})+{\frac {646}{720}}f(t_{n+3},y_{n+3})-{\frac {264}{720}}f(t_{n+2},y_{n+2})+{\frac {106}{720}}f(t_{n+1},y_{n+1})-{\frac {19}{720}}f(t_{n},y_{n})\right).\end{aligned}}}$
The derivation of the Adams–Moulton methods is similar to that of the Adams–Bashforth method; however, the interpolating polynomial uses not only the points ${\displaystyle t_{n-1},\dots ,t_{n-s}}$ , as above, but also ${\displaystyle t_{n}}$ . The coefficients are given by

${\displaystyle b_{s-j}={\frac {(-1)^{j}}{j!(s-j)!}}\int _{0}^{1}\prod _{i=0 \atop i\neq j}^{s}(u+i-1)\,du,\qquad {\text{for }}j=0,\ldots ,s.}$
The Adams–Moulton methods are solely due to John Couch Adams, like the Adams–Bashforth methods. The name of Forest Ray Moulton became associated with these methods because he realized that they could be used in tandem with the Adams–Bashforth methods as a predictor-corrector pair (Moulton 1926); Milne (1926) had the same idea. Adams used Newton's method to solve the implicit equation (Hairer, Nørsett & Wanner 1993, §III.1).


£#h5#£Backward differentiation formulas (BDF)£#/h5#£
The BDF methods are implicit methods with ${\displaystyle b_{s-1}=\cdots =b_{0}=0}$ and the other coefficients chosen such that the method attains order s (the maximum possible). These methods are especially used for the solution of stiff differential equations.


£#h5#£Analysis£#/h5#£
The central concepts in the analysis of linear multistep methods, and indeed any numerical method for differential equations, are convergence, order, and stability.


£#h5#£Consistency and order£#/h5#£
The first question is whether the method is consistent: is the difference equation

${\displaystyle {\begin{aligned}&a_{s}y_{n+s}+a_{s-1}y_{n+s-1}+a_{s-2}y_{n+s-2}+\cdots +a_{0}y_{n}\\&\qquad {}=h{\bigl (}b_{s}f(t_{n+s},y_{n+s})+b_{s-1}f(t_{n+s-1},y_{n+s-1})+\cdots +b_{0}f(t_{n},y_{n}){\bigr )},\end{aligned}}}$
a good approximation of the differential equation ${\displaystyle y'=f(t,y)}$ ? More precisely, a multistep method is consistent if the local truncation error goes to zero faster than the step size h as h goes to zero, where the local truncation error is defined to be the difference between the result ${\displaystyle y_{n+s}}$ of the method, assuming that all the previous values ${\displaystyle y_{n+s-1},\ldots ,y_{n}}$ are exact, and the exact solution of the equation at time ${\displaystyle t_{n+s}}$ . A computation using Taylor series shows that a linear multistep method is consistent if and only if

${\displaystyle \sum _{k=0}^{s-1}a_{k}=-1\quad {\text{and}}\quad \sum _{k=0}^{s}b_{k}=s+\sum _{k=0}^{s-1}ka_{k}.}$
All the methods mentioned above are consistent (Hairer, Nørsett & Wanner 1993, §III.2).

If the method is consistent, then the next question is how well the difference equation defining the numerical method approximates the differential equation. A multistep method is said to have order p if the local error is of order ${\displaystyle O(h^{p+1})}$ as h goes to zero. This is equivalent to the following condition on the coefficients of the methods:

${\displaystyle \sum _{k=0}^{s-1}a_{k}=-1\quad {\text{and}}\quad q\sum _{k=0}^{s}k^{q-1}b_{k}=s^{q}+\sum _{k=0}^{s-1}k^{q}a_{k}{\text{ for }}q=1,\ldots ,p.}$
The s-step Adams–Bashforth method has order s, while the s-step Adams–Moulton method has order ${\displaystyle s+1}$ (Hairer, Nørsett & Wanner 1993, §III.2).

These conditions are often formulated using the characteristic polynomials

${\displaystyle \rho (z)=z^{s}+\sum _{k=0}^{s-1}a_{k}z^{k}\quad {\text{and}}\quad \sigma (z)=\sum _{k=0}^{s}b_{k}z^{k}.}$
In terms of these polynomials, the above condition for the method to have order p becomes

${\displaystyle \rho (\mathrm {e} ^{h})-h\sigma (\mathrm {e} ^{h})=O(h^{p+1})\quad {\text{as }}h\to 0.}$
In particular, the method is consistent if it has order at least one, which is the case if ${\displaystyle \rho (1)=0}$ and ${\displaystyle \rho '(1)=\sigma (1)}$ .


£#h5#£Stability and convergence£#/h5#£
The numerical solution of a one-step method depends on the initial condition ${\displaystyle y_{0}}$ , but the numerical solution of an s-step method depend on all the s starting values, ${\displaystyle y_{0},y_{1},\ldots ,y_{s-1}}$ . It is thus of interest whether the numerical solution is stable with respect to perturbations in the starting values. A linear multistep method is zero-stable for a certain differential equation on a given time interval, if a perturbation in the starting values of size ε causes the numerical solution over that time interval to change by no more than Kε for some value of K which does not depend on the step size h. This is called "zero-stability" because it is enough to check the condition for the differential equation ${\displaystyle y'=0}$ (Süli & Mayers 2003, p. 332).

If the roots of the characteristic polynomial ρ all have modulus less than or equal to 1 and the roots of modulus 1 are of multiplicity 1, we say that the root condition is satisfied. A linear multistep method is zero-stable if and only if the root condition is satisfied (Süli & Mayers 2003, p. 335).

Now suppose that a consistent linear multistep method is applied to a sufficiently smooth differential equation and that the starting values ${\displaystyle y_{1},\ldots ,y_{s-1}}$ all converge to the initial value ${\displaystyle y_{0}}$ as ${\displaystyle h\to 0}$ . Then, the numerical solution converges to the exact solution as ${\displaystyle h\to 0}$ if and only if the method is zero-stable. This result is known as the Dahlquist equivalence theorem, named after Germund Dahlquist; this theorem is similar in spirit to the Lax equivalence theorem for finite difference methods. Furthermore, if the method has order p, then the global error (the difference between the numerical solution and the exact solution at a fixed time) is ${\displaystyle O(h^{p})}$ (Süli & Mayers 2003, p. 340).

Furthermore, if the method is convergent, the method is said to be strongly stable if ${\displaystyle z=1}$ is the only root of modulus 1. If it is convergent and all roots of modulus 1 are not repeated, but there is more than one such root, it is said to be relatively stable. Note that 1 must be a root for the method to be convergent; thus convergent methods are always one of these two.

To assess the performance of linear multistep methods on stiff equations, consider the linear test equation y' = λy. A multistep method applied to this differential equation with step size h yields a linear recurrence relation with characteristic polynomial

${\displaystyle \pi (z;h\lambda )=(1-h\lambda \beta _{s})z^{s}+\sum _{k=0}^{s-1}(\alpha _{k}-h\lambda \beta _{k})z^{k}=\rho (z)-h\lambda \sigma (z).}$
This polynomial is called the stability polynomial of the multistep method. If all of its roots have modulus less than one then the numerical solution of the multistep method will converge to zero and the multistep method is said to be absolutely stable for that value of hλ. The method is said to be A-stable if it is absolutely stable for all hλ with negative real part. The region of absolute stability is the set of all hλ for which the multistep method is absolutely stable (Süli & Mayers 2003, pp. 347 & 348). For more details, see the section on stiff equations and multistep methods.


£#h5#£Example£#/h5#£
Consider the Adams–Bashforth three-step method

${\displaystyle y_{n+3}=y_{n+2}+h\left({23 \over 12}f(t_{n+2},y_{n+2})-{4 \over 3}f(t_{n+1},y_{n+1})+{5 \over 12}f(t_{n},y_{n})\right).}$
One characteristic polynomial is thus

${\displaystyle \rho (z)=z^{3}-z^{2}=z^{2}(z-1)\,}$
which has roots ${\displaystyle z=0,1}$ , and the conditions above are satisfied. As ${\displaystyle z=1}$ is the only root of modulus 1, the method is strongly stable.

The other characteristic polynomial is

${\displaystyle \sigma (z)={\frac {23}{12}}z^{2}-{\frac {4}{3}}z+{\frac {5}{12}}}$


£#h5#£First and second Dahlquist barriers£#/h5#£
These two results were proved by Germund Dahlquist and represent an important bound for the order of convergence and for the A-stability of a linear multistep method. The first Dahlquist barrier was proved in Dahlquist (1956) and the second in Dahlquist (1963).


£#h5#£First Dahlquist barrier£#/h5#£
The first Dahlquist barrier states that a zero-stable and linear q-step multistep method cannot attain an order of convergence greater than q + 1 if q is odd and greater than q + 2 if q is even. If the method is also explicit, then it cannot attain an order greater than q (Hairer, Nørsett & Wanner 1993, Thm III.3.5).


£#h5#£Second Dahlquist barrier£#/h5#£
The second Dahlquist barrier states that no explicit linear multistep methods are A-stable.


£#h5#£See also£#/h5#£ £#ul#££#li#£Digital energy gain£#/li#££#/ul#£
£#h5#£References£#/h5#£ £#ul#££#li#£Bashforth, Francis (1883), An Attempt to test the Theories of Capillary Action by comparing the theoretical and measured forms of drops of fluid. With an explanation of the method of integration employed in constructing the tables which give the theoretical forms of such drops, by J. C. Adams, Cambridge.£#/li#£ £#li#£Butcher, John C. (2003), Numerical Methods for Ordinary Differential Equations, John Wiley, ISBN 978-0-471-96758-3.£#/li#£ £#li#£Dahlquist, Germund (1956), "Convergence and stability in the numerical integration of ordinary differential equations", Mathematica Scandinavica, 4: 33--53.£#/li#£ £#li#£Dahlquist, Germund (1963), "A special stability problem for linear multistep methods", BIT, 3: 27–43, doi:10.1007/BF01963532, ISSN 0006-3835.£#/li#£ £#li#£Goldstine, Herman H. (1977), A History of Numerical Analysis from the 16th through the 19th Century, New York: Springer-Verlag, ISBN 978-0-387-90277-7.£#/li#£ £#li#£Hairer, Ernst; Nørsett, Syvert Paul; Wanner, Gerhard (1993), Solving ordinary differential equations I: Nonstiff problems (2nd ed.), Berlin: Springer Verlag, ISBN 978-3-540-56670-0.£#/li#£ £#li#£Hairer, Ernst; Wanner, Gerhard (1996), Solving ordinary differential equations II: Stiff and differential-algebraic problems (2nd ed.), Berlin, New York: Springer-Verlag, ISBN 978-3-540-60452-5.£#/li#£ £#li#£Iserles, Arieh (1996), A First Course in the Numerical Analysis of Differential Equations, Cambridge University Press, ISBN 978-0-521-55655-2.£#/li#£ £#li#£Milne, W. E. (1926), "Numerical integration of ordinary differential equations", American Mathematical Monthly, Mathematical Association of America, 33 (9): 455–460, doi:10.2307/2299609, JSTOR 2299609.£#/li#£ £#li#£Moulton, Forest R. (1926), New methods in exterior ballistics, University of Chicago Press.£#/li#£ £#li#£Quarteroni, Alfio; Sacco, Riccardo; Saleri, Fausto (2000), Matematica Numerica, Springer Verlag, ISBN 978-88-470-0077-3.£#/li#£ £#li#£Süli, Endre; Mayers, David (2003), An Introduction to Numerical Analysis, Cambridge University Press, ISBN 0-521-00794-1.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Adams Method". MathWorld.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Abramowitz, M. and Stegun, I. A. (Eds.). Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing. New York: Dover, p. 896, 1972.£#/li#££#li#£Bashforth, F. and Adams, J. C. Theories of Capillary Action. London: Cambridge University Press, 1883.£#/li#££#li#£Beyer, W. H. CRC Standard Mathematical Tables, 28th ed. Boca Raton, FL: CRC Press, p. 455, 1987.£#/li#££#li#£Jeffreys, H. and Jeffreys, B. S. "The Adams-Bashforth Method." §9.11 in Methods of Mathematical Physics, 3rd ed. Cambridge, England: Cambridge University Press, pp. 292-293, 1988.£#/li#££#li#£Kármán, T. von and Biot, M. A. Mathematical Methods in Engineering: An Introduction to the Mathematical Treatment of Engineering Problems. New York: McGraw-Hill, pp. 14-20, 1940.£#/li#££#li#£Press, W. H.; Flannery, B. P.; Teukolsky, S. A.; and Vetterling, W. T. Numerical Recipes in FORTRAN: The Art of Scientific Computing, 2nd ed. Cambridge, England: Cambridge University Press, p. 741, 1992.£#/li#££#li#£Whittaker, E. T. and Robinson, G. "The Numerical Solution of Differential Equations." Ch. 14 in The Calculus of Observations: A Treatise on Numerical Mathematics, 4th ed. New York: Dover, pp. 363-367, 1967.£#/li#££#li#£ Abramowitz, M. and Stegun, I. A. (Eds.). Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing. New York: Dover, p. 896, 1972. £#/li#££#li#£ Bashforth, F. and Adams, J. C. Theories of Capillary Action. London: Cambridge University Press, 1883. £#/li#££#li#£ Beyer, W. H. CRC Standard Mathematical Tables, 28th ed. Boca Raton, FL: CRC Press, p. 455, 1987. £#/li#££#li#£ Jeffreys, H. and Jeffreys, B. S. "The Adams-Bashforth Method." §9.11 in Methods of Mathematical Physics, 3rd ed. Cambridge, England: Cambridge University Press, pp. 292-293, 1988. £#/li#££#li#£ Kármán, T. von and Biot, M. A. Mathematical Methods in Engineering: An Introduction to the Mathematical Treatment of Engineering Problems. New York: McGraw-Hill, pp. 14-20, 1940. £#/li#££#li#£ Press, W. H.; Flannery, B. P.; Teukolsky, S. A.; and Vetterling, W. T. Numerical Recipes in FORTRAN: The Art of Scientific Computing, 2nd ed. Cambridge, England: Cambridge University Press, p. 741, 1992. £#/li#££#li#£ Whittaker, E. T. and Robinson, G. "The Numerical Solution of Differential Equations." Ch. 14 in The Calculus of Observations: A Treatise on Numerical Mathematics, 4th ed. New York: Dover, pp. 363-367, 1967. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Applied Mathematics > Numerical Methods > Differential Equation Solving > ODE Solving £#/li#££#/ul#£




£#h3#£Adams-Bashforth-Moulton Method£#/h3#£

Linear multistep methods are used for the numerical solution of ordinary differential equations. Conceptually, a numerical method starts from an initial point and then takes a short step forward in time to find the next solution point. The process continues with subsequent steps to map out the solution. Single-step methods (such as Euler's method) refer to only one previous point and its derivative to determine the current value. Methods such as Runge–Kutta take some intermediate steps (for example, a half-step) to obtain a higher order method, but then discard all previous information before taking a second step. Multistep methods attempt to gain efficiency by keeping and using the information from previous steps rather than discarding it. Consequently, multistep methods refer to several previous points and derivative values. In the case of linear multistep methods, a linear combination of the previous points and derivative values is used.


£#h5#£Definitions£#/h5#£
Numerical methods for ordinary differential equations approximate solutions to initial value problems of the form

${\displaystyle y'=f(t,y),\quad y(t_{0})=y_{0}.}$
The result is approximations for the value of ${\displaystyle y(t)}$ at discrete times ${\displaystyle t_{i}}$ :

${\displaystyle y_{i}\approx y(t_{i})\quad {\text{where}}\quad t_{i}=t_{0}+ih,}$
where ${\displaystyle h}$ is the time step (sometimes referred to as ${\displaystyle \Delta t}$ ) and ${\displaystyle i}$ is an integer.

Multistep methods use information from the previous ${\displaystyle s}$ steps to calculate the next value. In particular, a linear multistep method uses a linear combination of ${\displaystyle y_{i}}$ and ${\displaystyle f(t_{i},y_{i})}$ to calculate the value of ${\displaystyle y}$ for the desired current step. Thus, a linear multistep method is a method of the form

${\displaystyle {\begin{aligned}&y_{n+s}+a_{s-1}\cdot y_{n+s-1}+a_{s-2}\cdot y_{n+s-2}+\cdots +a_{0}\cdot y_{n}\\&\qquad {}=h\cdot \left(b_{s}\cdot f(t_{n+s},y_{n+s})+b_{s-1}\cdot f(t_{n+s-1},y_{n+s-1})+\cdots +b_{0}\cdot f(t_{n},y_{n})\right)\\&\Leftrightarrow \sum _{j=0}^{s}a_{j}y_{n+j}=h\sum _{j=0}^{s}b_{j}f(t_{n+j},y_{n+j}),\end{aligned}}}$
with ${\displaystyle a_{s}=1}$ . The coefficients ${\displaystyle a_{0},\dotsc ,a_{s-1}}$ and ${\displaystyle b_{0},\dotsc ,b_{s}}$ determine the method. The designer of the method chooses the coefficients, balancing the need to get a good approximation to the true solution against the desire to get a method that is easy to apply. Often, many coefficients are zero to simplify the method.

One can distinguish between explicit and implicit methods. If ${\displaystyle b_{s}=0}$ , then the method is called "explicit", since the formula can directly compute ${\displaystyle y_{n+s}}$ . If ${\displaystyle b_{s}\neq 0}$ then the method is called "implicit", since the value of ${\displaystyle y_{n+s}}$ depends on the value of ${\displaystyle f(t_{n+s},y_{n+s})}$ , and the equation must be solved for ${\displaystyle y_{n+s}}$ . Iterative methods such as Newton's method are often used to solve the implicit formula.

Sometimes an explicit multistep method is used to "predict" the value of ${\displaystyle y_{n+s}}$ . That value is then used in an implicit formula to "correct" the value. The result is a predictor–corrector method.


£#h5#£Examples£#/h5#£
Consider for an example the problem

${\displaystyle y'=f(t,y)=y,\quad y(0)=1.}$
The exact solution is ${\displaystyle y(t)=e^{t}}$ .


£#h5#£One-step Euler£#/h5#£
A simple numerical method is Euler's method:

${\displaystyle y_{n+1}=y_{n}+hf(t_{n},y_{n}).}$
Euler's method can be viewed as an explicit multistep method for the degenerate case of one step.

This method, applied with step size ${\displaystyle h={\tfrac {1}{2}}}$ on the problem ${\displaystyle y'=y}$ , gives the following results:

${\displaystyle {\begin{aligned}y_{1}&=y_{0}+hf(t_{0},y_{0})=1+{\tfrac {1}{2}}\cdot 1=1.5,\\y_{2}&=y_{1}+hf(t_{1},y_{1})=1.5+{\tfrac {1}{2}}\cdot 1.5=2.25,\\y_{3}&=y_{2}+hf(t_{2},y_{2})=2.25+{\tfrac {1}{2}}\cdot 2.25=3.375,\\y_{4}&=y_{3}+hf(t_{3},y_{3})=3.375+{\tfrac {1}{2}}\cdot 3.375=5.0625.\end{aligned}}}$

£#h5#£Two-step Adams–Bashforth£#/h5#£
Euler's method is a one-step method. A simple multistep method is the two-step Adams–Bashforth method

${\displaystyle y_{n+2}=y_{n+1}+{\tfrac {3}{2}}hf(t_{n+1},y_{n+1})-{\tfrac {1}{2}}hf(t_{n},y_{n}).}$
This method needs two values, ${\displaystyle y_{n+1}}$ and ${\displaystyle y_{n}}$ , to compute the next value, ${\displaystyle y_{n+2}}$ . However, the initial value problem provides only one value, ${\displaystyle y_{0}=1}$ . One possibility to resolve this issue is to use the ${\displaystyle y_{1}}$ computed by Euler's method as the second value. With this choice, the Adams–Bashforth method yields (rounded to four digits):

${\displaystyle {\begin{aligned}y_{2}&=y_{1}+{\tfrac {3}{2}}hf(t_{1},y_{1})-{\tfrac {1}{2}}hf(t_{0},y_{0})=1.5+{\tfrac {3}{2}}\cdot {\tfrac {1}{2}}\cdot 1.5-{\tfrac {1}{2}}\cdot {\tfrac {1}{2}}\cdot 1=2.375,\\y_{3}&=y_{2}+{\tfrac {3}{2}}hf(t_{2},y_{2})-{\tfrac {1}{2}}hf(t_{1},y_{1})=2.375+{\tfrac {3}{2}}\cdot {\tfrac {1}{2}}\cdot 2.375-{\tfrac {1}{2}}\cdot {\tfrac {1}{2}}\cdot 1.5=3.7812,\\y_{4}&=y_{3}+{\tfrac {3}{2}}hf(t_{3},y_{3})-{\tfrac {1}{2}}hf(t_{2},y_{2})=3.7812+{\tfrac {3}{2}}\cdot {\tfrac {1}{2}}\cdot 3.7812-{\tfrac {1}{2}}\cdot {\tfrac {1}{2}}\cdot 2.375=6.0234.\end{aligned}}}$
The exact solution at ${\displaystyle t=t_{4}=2}$ is ${\displaystyle e^{2}=7.3891\ldots }$ , so the two-step Adams–Bashforth method is more accurate than Euler's method. This is always the case if the step size is small enough.


£#h5#£Families of multistep methods£#/h5#£
Three families of linear multistep methods are commonly used: Adams–Bashforth methods, Adams–Moulton methods, and the backward differentiation formulas (BDFs).


£#h5#£Adams–Bashforth methods£#/h5#£
The Adams–Bashforth methods are explicit methods. The coefficients are ${\displaystyle a_{s-1}=-1}$ and ${\displaystyle a_{s-2}=\cdots =a_{0}=0}$ , while the ${\displaystyle b_{j}}$ are chosen such that the methods have order s (this determines the methods uniquely).

The Adams–Bashforth methods with s = 1, 2, 3, 4, 5 are (Hairer, Nørsett & Wanner 1993, §III.1; Butcher 2003, p. 103):

${\displaystyle {\begin{aligned}y_{n+1}&=y_{n}+hf(t_{n},y_{n}),\qquad {\text{(This is the Euler method)}}\\y_{n+2}&=y_{n+1}+h\left({\frac {3}{2}}f(t_{n+1},y_{n+1})-{\frac {1}{2}}f(t_{n},y_{n})\right),\\y_{n+3}&=y_{n+2}+h\left({\frac {23}{12}}f(t_{n+2},y_{n+2})-{\frac {16}{12}}f(t_{n+1},y_{n+1})+{\frac {5}{12}}f(t_{n},y_{n})\right),\\y_{n+4}&=y_{n+3}+h\left({\frac {55}{24}}f(t_{n+3},y_{n+3})-{\frac {59}{24}}f(t_{n+2},y_{n+2})+{\frac {37}{24}}f(t_{n+1},y_{n+1})-{\frac {9}{24}}f(t_{n},y_{n})\right),\\y_{n+5}&=y_{n+4}+h\left({\frac {1901}{720}}f(t_{n+4},y_{n+4})-{\frac {2774}{720}}f(t_{n+3},y_{n+3})+{\frac {2616}{720}}f(t_{n+2},y_{n+2})-{\frac {1274}{720}}f(t_{n+1},y_{n+1})+{\frac {251}{720}}f(t_{n},y_{n})\right).\end{aligned}}}$
The coefficients ${\displaystyle b_{j}}$ can be determined as follows. Use polynomial interpolation to find the polynomial p of degree ${\displaystyle s-1}$ such that

${\displaystyle p(t_{n+i})=f(t_{n+i},y_{n+i}),\qquad {\text{for }}i=0,\ldots ,s-1.}$
The Lagrange formula for polynomial interpolation yields

${\displaystyle p(t)=\sum _{j=0}^{s-1}{\frac {(-1)^{s-j-1}f(t_{n+j},y_{n+j})}{j!(s-j-1)!h^{s-1}}}\prod _{i=0 \atop i\neq j}^{s-1}(t-t_{n+i}).}$
The polynomial p is locally a good approximation of the right-hand side of the differential equation ${\displaystyle y'=f(t,y)}$ that is to be solved, so consider the equation ${\displaystyle y'=p(t)}$ instead. This equation can be solved exactly; the solution is simply the integral of p. This suggests taking

${\displaystyle y_{n+s}=y_{n+s-1}+\int _{t_{n+s-1}}^{t_{n+s}}p(t)\,dt.}$
The Adams–Bashforth method arises when the formula for p is substituted. The coefficients ${\displaystyle b_{j}}$ turn out to be given by

${\displaystyle b_{s-j-1}={\frac {(-1)^{j}}{j!(s-j-1)!}}\int _{0}^{1}\prod _{i=0 \atop i\neq j}^{s-1}(u+i)\,du,\qquad {\text{for }}j=0,\ldots ,s-1.}$
Replacing ${\displaystyle f(t,y)}$ by its interpolant p incurs an error of order hs, and it follows that the s-step Adams–Bashforth method has indeed order s (Iserles 1996, §2.1)

The Adams–Bashforth methods were designed by John Couch Adams to solve a differential equation modelling capillary action due to Francis Bashforth. Bashforth (1883) published his theory and Adams' numerical method (Goldstine 1977).


£#h5#£Adams–Moulton methods£#/h5#£
The Adams–Moulton methods are similar to the Adams–Bashforth methods in that they also have ${\displaystyle a_{s-1}=-1}$ and ${\displaystyle a_{s-2}=\cdots =a_{0}=0}$ . Again the b coefficients are chosen to obtain the highest order possible. However, the Adams–Moulton methods are implicit methods. By removing the restriction that ${\displaystyle b_{s}=0}$ , an s-step Adams–Moulton method can reach order ${\displaystyle s+1}$ , while an s-step Adams–Bashforth methods has only order s.

The Adams–Moulton methods with s = 0, 1, 2, 3, 4 are (Hairer, Nørsett & Wanner 1993, §III.1; Quarteroni, Sacco & Saleri 2000):

${\displaystyle y_{n}=y_{n-1}+hf(t_{n},y_{n}),}$ This is the backward Euler method
${\displaystyle y_{n+1}=y_{n}+{\frac {1}{2}}h\left(f(t_{n+1},y_{n+1})+f(t_{n},y_{n})\right),}$ This is the trapezoidal rule
${\displaystyle {\begin{aligned}y_{n+2}&=y_{n+1}+h\left({\frac {5}{12}}f(t_{n+2},y_{n+2})+{\frac {8}{12}}f(t_{n+1},y_{n+1})-{\frac {1}{12}}f(t_{n},y_{n})\right),\\y_{n+3}&=y_{n+2}+h\left({\frac {9}{24}}f(t_{n+3},y_{n+3})+{\frac {19}{24}}f(t_{n+2},y_{n+2})-{\frac {5}{24}}f(t_{n+1},y_{n+1})+{\frac {1}{24}}f(t_{n},y_{n})\right),\\y_{n+4}&=y_{n+3}+h\left({\frac {251}{720}}f(t_{n+4},y_{n+4})+{\frac {646}{720}}f(t_{n+3},y_{n+3})-{\frac {264}{720}}f(t_{n+2},y_{n+2})+{\frac {106}{720}}f(t_{n+1},y_{n+1})-{\frac {19}{720}}f(t_{n},y_{n})\right).\end{aligned}}}$
The derivation of the Adams–Moulton methods is similar to that of the Adams–Bashforth method; however, the interpolating polynomial uses not only the points ${\displaystyle t_{n-1},\dots ,t_{n-s}}$ , as above, but also ${\displaystyle t_{n}}$ . The coefficients are given by

${\displaystyle b_{s-j}={\frac {(-1)^{j}}{j!(s-j)!}}\int _{0}^{1}\prod _{i=0 \atop i\neq j}^{s}(u+i-1)\,du,\qquad {\text{for }}j=0,\ldots ,s.}$
The Adams–Moulton methods are solely due to John Couch Adams, like the Adams–Bashforth methods. The name of Forest Ray Moulton became associated with these methods because he realized that they could be used in tandem with the Adams–Bashforth methods as a predictor-corrector pair (Moulton 1926); Milne (1926) had the same idea. Adams used Newton's method to solve the implicit equation (Hairer, Nørsett & Wanner 1993, §III.1).


£#h5#£Backward differentiation formulas (BDF)£#/h5#£
The BDF methods are implicit methods with ${\displaystyle b_{s-1}=\cdots =b_{0}=0}$ and the other coefficients chosen such that the method attains order s (the maximum possible). These methods are especially used for the solution of stiff differential equations.


£#h5#£Analysis£#/h5#£
The central concepts in the analysis of linear multistep methods, and indeed any numerical method for differential equations, are convergence, order, and stability.


£#h5#£Consistency and order£#/h5#£
The first question is whether the method is consistent: is the difference equation

${\displaystyle {\begin{aligned}&a_{s}y_{n+s}+a_{s-1}y_{n+s-1}+a_{s-2}y_{n+s-2}+\cdots +a_{0}y_{n}\\&\qquad {}=h{\bigl (}b_{s}f(t_{n+s},y_{n+s})+b_{s-1}f(t_{n+s-1},y_{n+s-1})+\cdots +b_{0}f(t_{n},y_{n}){\bigr )},\end{aligned}}}$
a good approximation of the differential equation ${\displaystyle y'=f(t,y)}$ ? More precisely, a multistep method is consistent if the local truncation error goes to zero faster than the step size h as h goes to zero, where the local truncation error is defined to be the difference between the result ${\displaystyle y_{n+s}}$ of the method, assuming that all the previous values ${\displaystyle y_{n+s-1},\ldots ,y_{n}}$ are exact, and the exact solution of the equation at time ${\displaystyle t_{n+s}}$ . A computation using Taylor series shows that a linear multistep method is consistent if and only if

${\displaystyle \sum _{k=0}^{s-1}a_{k}=-1\quad {\text{and}}\quad \sum _{k=0}^{s}b_{k}=s+\sum _{k=0}^{s-1}ka_{k}.}$
All the methods mentioned above are consistent (Hairer, Nørsett & Wanner 1993, §III.2).

If the method is consistent, then the next question is how well the difference equation defining the numerical method approximates the differential equation. A multistep method is said to have order p if the local error is of order ${\displaystyle O(h^{p+1})}$ as h goes to zero. This is equivalent to the following condition on the coefficients of the methods:

${\displaystyle \sum _{k=0}^{s-1}a_{k}=-1\quad {\text{and}}\quad q\sum _{k=0}^{s}k^{q-1}b_{k}=s^{q}+\sum _{k=0}^{s-1}k^{q}a_{k}{\text{ for }}q=1,\ldots ,p.}$
The s-step Adams–Bashforth method has order s, while the s-step Adams–Moulton method has order ${\displaystyle s+1}$ (Hairer, Nørsett & Wanner 1993, §III.2).

These conditions are often formulated using the characteristic polynomials

${\displaystyle \rho (z)=z^{s}+\sum _{k=0}^{s-1}a_{k}z^{k}\quad {\text{and}}\quad \sigma (z)=\sum _{k=0}^{s}b_{k}z^{k}.}$
In terms of these polynomials, the above condition for the method to have order p becomes

${\displaystyle \rho (\mathrm {e} ^{h})-h\sigma (\mathrm {e} ^{h})=O(h^{p+1})\quad {\text{as }}h\to 0.}$
In particular, the method is consistent if it has order at least one, which is the case if ${\displaystyle \rho (1)=0}$ and ${\displaystyle \rho '(1)=\sigma (1)}$ .


£#h5#£Stability and convergence£#/h5#£
The numerical solution of a one-step method depends on the initial condition ${\displaystyle y_{0}}$ , but the numerical solution of an s-step method depend on all the s starting values, ${\displaystyle y_{0},y_{1},\ldots ,y_{s-1}}$ . It is thus of interest whether the numerical solution is stable with respect to perturbations in the starting values. A linear multistep method is zero-stable for a certain differential equation on a given time interval, if a perturbation in the starting values of size ε causes the numerical solution over that time interval to change by no more than Kε for some value of K which does not depend on the step size h. This is called "zero-stability" because it is enough to check the condition for the differential equation ${\displaystyle y'=0}$ (Süli & Mayers 2003, p. 332).

If the roots of the characteristic polynomial ρ all have modulus less than or equal to 1 and the roots of modulus 1 are of multiplicity 1, we say that the root condition is satisfied. A linear multistep method is zero-stable if and only if the root condition is satisfied (Süli & Mayers 2003, p. 335).

Now suppose that a consistent linear multistep method is applied to a sufficiently smooth differential equation and that the starting values ${\displaystyle y_{1},\ldots ,y_{s-1}}$ all converge to the initial value ${\displaystyle y_{0}}$ as ${\displaystyle h\to 0}$ . Then, the numerical solution converges to the exact solution as ${\displaystyle h\to 0}$ if and only if the method is zero-stable. This result is known as the Dahlquist equivalence theorem, named after Germund Dahlquist; this theorem is similar in spirit to the Lax equivalence theorem for finite difference methods. Furthermore, if the method has order p, then the global error (the difference between the numerical solution and the exact solution at a fixed time) is ${\displaystyle O(h^{p})}$ (Süli & Mayers 2003, p. 340).

Furthermore, if the method is convergent, the method is said to be strongly stable if ${\displaystyle z=1}$ is the only root of modulus 1. If it is convergent and all roots of modulus 1 are not repeated, but there is more than one such root, it is said to be relatively stable. Note that 1 must be a root for the method to be convergent; thus convergent methods are always one of these two.

To assess the performance of linear multistep methods on stiff equations, consider the linear test equation y' = λy. A multistep method applied to this differential equation with step size h yields a linear recurrence relation with characteristic polynomial

${\displaystyle \pi (z;h\lambda )=(1-h\lambda \beta _{s})z^{s}+\sum _{k=0}^{s-1}(\alpha _{k}-h\lambda \beta _{k})z^{k}=\rho (z)-h\lambda \sigma (z).}$
This polynomial is called the stability polynomial of the multistep method. If all of its roots have modulus less than one then the numerical solution of the multistep method will converge to zero and the multistep method is said to be absolutely stable for that value of hλ. The method is said to be A-stable if it is absolutely stable for all hλ with negative real part. The region of absolute stability is the set of all hλ for which the multistep method is absolutely stable (Süli & Mayers 2003, pp. 347 & 348). For more details, see the section on stiff equations and multistep methods.


£#h5#£Example£#/h5#£
Consider the Adams–Bashforth three-step method

${\displaystyle y_{n+3}=y_{n+2}+h\left({23 \over 12}f(t_{n+2},y_{n+2})-{4 \over 3}f(t_{n+1},y_{n+1})+{5 \over 12}f(t_{n},y_{n})\right).}$
One characteristic polynomial is thus

${\displaystyle \rho (z)=z^{3}-z^{2}=z^{2}(z-1)\,}$
which has roots ${\displaystyle z=0,1}$ , and the conditions above are satisfied. As ${\displaystyle z=1}$ is the only root of modulus 1, the method is strongly stable.

The other characteristic polynomial is

${\displaystyle \sigma (z)={\frac {23}{12}}z^{2}-{\frac {4}{3}}z+{\frac {5}{12}}}$


£#h5#£First and second Dahlquist barriers£#/h5#£
These two results were proved by Germund Dahlquist and represent an important bound for the order of convergence and for the A-stability of a linear multistep method. The first Dahlquist barrier was proved in Dahlquist (1956) and the second in Dahlquist (1963).


£#h5#£First Dahlquist barrier£#/h5#£
The first Dahlquist barrier states that a zero-stable and linear q-step multistep method cannot attain an order of convergence greater than q + 1 if q is odd and greater than q + 2 if q is even. If the method is also explicit, then it cannot attain an order greater than q (Hairer, Nørsett & Wanner 1993, Thm III.3.5).


£#h5#£Second Dahlquist barrier£#/h5#£
The second Dahlquist barrier states that no explicit linear multistep methods are A-stable.


£#h5#£See also£#/h5#£ £#ul#££#li#£Digital energy gain£#/li#££#/ul#£
£#h5#£References£#/h5#£ £#ul#££#li#£Bashforth, Francis (1883), An Attempt to test the Theories of Capillary Action by comparing the theoretical and measured forms of drops of fluid. With an explanation of the method of integration employed in constructing the tables which give the theoretical forms of such drops, by J. C. Adams, Cambridge.£#/li#£ £#li#£Butcher, John C. (2003), Numerical Methods for Ordinary Differential Equations, John Wiley, ISBN 978-0-471-96758-3.£#/li#£ £#li#£Dahlquist, Germund (1956), "Convergence and stability in the numerical integration of ordinary differential equations", Mathematica Scandinavica, 4: 33--53.£#/li#£ £#li#£Dahlquist, Germund (1963), "A special stability problem for linear multistep methods", BIT, 3: 27–43, doi:10.1007/BF01963532, ISSN 0006-3835.£#/li#£ £#li#£Goldstine, Herman H. (1977), A History of Numerical Analysis from the 16th through the 19th Century, New York: Springer-Verlag, ISBN 978-0-387-90277-7.£#/li#£ £#li#£Hairer, Ernst; Nørsett, Syvert Paul; Wanner, Gerhard (1993), Solving ordinary differential equations I: Nonstiff problems (2nd ed.), Berlin: Springer Verlag, ISBN 978-3-540-56670-0.£#/li#£ £#li#£Hairer, Ernst; Wanner, Gerhard (1996), Solving ordinary differential equations II: Stiff and differential-algebraic problems (2nd ed.), Berlin, New York: Springer-Verlag, ISBN 978-3-540-60452-5.£#/li#£ £#li#£Iserles, Arieh (1996), A First Course in the Numerical Analysis of Differential Equations, Cambridge University Press, ISBN 978-0-521-55655-2.£#/li#£ £#li#£Milne, W. E. (1926), "Numerical integration of ordinary differential equations", American Mathematical Monthly, Mathematical Association of America, 33 (9): 455–460, doi:10.2307/2299609, JSTOR 2299609.£#/li#£ £#li#£Moulton, Forest R. (1926), New methods in exterior ballistics, University of Chicago Press.£#/li#£ £#li#£Quarteroni, Alfio; Sacco, Riccardo; Saleri, Fausto (2000), Matematica Numerica, Springer Verlag, ISBN 978-88-470-0077-3.£#/li#£ £#li#£Süli, Endre; Mayers, David (2003), An Introduction to Numerical Analysis, Cambridge University Press, ISBN 0-521-00794-1.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Adams Method". MathWorld.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Applied Mathematics > Numerical Methods > Differential Equation Solving > ODE Solving £#/li#££#/ul#£




£#h3#£Additive Function£#/h3#£

In number theory, an additive function is an arithmetic function f(n) of the positive integer variable n such that whenever a and b are coprime, the function applied to the product ab is the sum of the values of the function applied to a and b:


£#h5#£Completely additive£#/h5#£
An additive function f(n) is said to be completely additive if ${\displaystyle f(ab)=f(a)+f(b)}$ holds for all positive integers a and b, even when they are not coprime. Totally additive is also used in this sense by analogy with totally multiplicative functions. If f is a completely additive function then f(1) = 0.

Every completely additive function is additive, but not vice versa.


£#h5#£Examples£#/h5#£
Examples of arithmetic functions which are completely additive are:

£#ul#££#li#£The restriction of the logarithmic function to ${\displaystyle \mathbb {N} .}$ £#/li#£ £#li#£The multiplicity of a prime factor p in n, that is the largest exponent m for which pm divides n.£#/li#£ £#li#£a0(n) – the sum of primes dividing n counting multiplicity, sometimes called sopfr(n), the potency of n or the integer logarithm of n (sequence A001414 in the OEIS). For example:£#/li#££#/ul#£
a0(4) = 2 + 2 = 4
a0(20) = a0(22 · 5) = 2 + 2 + 5 = 9
a0(27) = 3 + 3 + 3 = 9
a0(144) = a0(24 · 32) = a0(24) + a0(32) = 8 + 6 = 14
a0(2000) = a0(24 · 53) = a0(24) + a0(53) = 8 + 15 = 23
a0(2003) = 2003
a0(54,032,858,972,279) = 1240658
a0(54,032,858,972,302) = 1780417
a0(20,802,650,704,327,415) = 1240681
£#ul#££#li#£The function Ω(n), defined as the total number of prime factors of n, counting multiple factors multiple times, sometimes called the "Big Omega function" (sequence A001222 in the OEIS). For example;£#/li#££#/ul#£
Ω(1) = 0, since 1 has no prime factors
Ω(4) = 2
Ω(16) = Ω(2·2·2·2) = 4
Ω(20) = Ω(2·2·5) = 3
Ω(27) = Ω(3·3·3) = 3
Ω(144) = Ω(24 · 32) = Ω(24) + Ω(32) = 4 + 2 = 6
Ω(2000) = Ω(24 · 53) = Ω(24) + Ω(53) = 4 + 3 = 7
Ω(2001) = 3
Ω(2002) = 4
Ω(2003) = 1
Ω(54,032,858,972,279) = 3
Ω(54,032,858,972,302) = 6
Ω(20,802,650,704,327,415) = 7
Examples of arithmetic functions which are additive but not completely additive are:

£#ul#££#li#£ω(n), defined as the total number of distinct prime factors of n (sequence A001221 in the OEIS). For example:£#/li#££#/ul#£
ω(4) = 1
ω(16) = ω(24) = 1
ω(20) = ω(22 · 5) = 2
ω(27) = ω(33) = 1
ω(144) = ω(24 · 32) = ω(24) + ω(32) = 1 + 1 = 2
ω(2000) = ω(24 · 53) = ω(24) + ω(53) = 1 + 1 = 2
ω(2001) = 3
ω(2002) = 4
ω(2003) = 1
ω(54,032,858,972,279) = 3
ω(54,032,858,972,302) = 5
ω(20,802,650,704,327,415) = 5
£#ul#££#li#£a1(n) – the sum of the distinct primes dividing n, sometimes called sopf(n) (sequence A008472 in the OEIS). For example:£#/li#££#/ul#£
a1(1) = 0
a1(4) = 2
a1(20) = 2 + 5 = 7
a1(27) = 3
a1(144) = a1(24 · 32) = a1(24) + a1(32) = 2 + 3 = 5
a1(2000) = a1(24 · 53) = a1(24) + a1(53) = 2 + 5 = 7
a1(2001) = 55
a1(2002) = 33
a1(2003) = 2003
a1(54,032,858,972,279) = 1238665
a1(54,032,858,972,302) = 1780410
a1(20,802,650,704,327,415) = 1238677

£#h5#£Multiplicative functions£#/h5#£
From any additive function ${\displaystyle f(n)}$ it is possible to create a related multiplicative function ${\displaystyle g(n),}$ which is a function with the property that whenever ${\displaystyle a}$ and ${\displaystyle b}$ are coprime then:

One such example is ${\displaystyle g(n)=2^{f(n)}.}$
£#h5#£Summatory functions£#/h5#£
Given an additive function ${\displaystyle f}$ , let its summatory function be defined by ${\displaystyle {\mathcal {M}}_{f}(x):=\sum _{n\leq x}f(n)}$ . The average of ${\displaystyle f}$ is given exactly as

The summatory functions over ${\displaystyle f}$ can be expanded as ${\displaystyle {\mathcal {M}}_{f}(x)=xE(x)+O({\sqrt {x}}\cdot D(x))}$ where

The average of the function ${\displaystyle f^{2}}$ is also expressed by these functions as

There is always an absolute constant ${\displaystyle C_{f}>0}$ such that for all natural numbers ${\displaystyle x\geq 1}$ ,

Let

Suppose that ${\displaystyle f}$ is an additive function with ${\displaystyle -1\leq f(p^{\alpha })=f(p)\leq 1}$ such that as ${\displaystyle x\rightarrow \infty }$ ,

Then ${\displaystyle \nu (x;z)\sim G(z)}$ where ${\displaystyle G(z)}$ is the Gaussian distribution function

Examples of this result related to the prime omega function and the numbers of prime divisors of shifted primes include the following for fixed ${\displaystyle z\in \mathbb {R} }$ where the relations hold for ${\displaystyle x\gg 1}$ :


£#h5#£See also£#/h5#£ £#ul#££#li#£Sigma additivity£#/li#£ £#li#£Prime omega function£#/li#£ £#li#£Multiplicative function£#/li#£ £#li#£Arithmetic function£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£Further reading£#/h5#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Functions £#/li#££#/ul#£




£#h3#£Additive Polynomial£#/h3#£

In mathematics, the additive polynomials are an important topic in classical algebraic number theory.


£#h5#£Definition£#/h5#£
Let k be a field of prime characteristic p. A polynomial P(x) with coefficients in k is called an additive polynomial, or a Frobenius polynomial, if

${\displaystyle P(a+b)=P(a)+P(b)\,}$
as polynomials in a and b. It is equivalent to assume that this equality holds for all a and b in some infinite field containing k, such as its algebraic closure.

Occasionally absolutely additive is used for the condition above, and additive is used for the weaker condition that P(a + b) = P(a) + P(b) for all a and b in the field. For infinite fields the conditions are equivalent, but for finite fields they are not, and the weaker condition is the "wrong" as it does not behave well. For example, over a field of order q any multiple P of xq − x will satisfy P(a + b) = P(a) + P(b) for all a and b in the field, but will usually not be (absolutely) additive.


£#h5#£Examples£#/h5#£
The polynomial xp is additive. Indeed, for any a and b in the algebraic closure of k one has by the binomial theorem

${\displaystyle (a+b)^{p}=\sum _{n=0}^{p}{p \choose n}a^{n}b^{p-n}.}$
Since p is prime, for all n = 1, ..., p−1 the binomial coefficient ${\displaystyle \scriptstyle {p \choose n}}$ is divisible by p, which implies that

${\displaystyle (a+b)^{p}\equiv a^{p}+b^{p}\mod p}$
as polynomials in a and b.

Similarly all the polynomials of the form

${\displaystyle \tau _{p}^{n}(x)=x^{p^{n}}}$
are additive, where n is a non-negative integer.

The definition makes sense even if k is a field of characteristic zero, but in this case the only additive polynomials are those of the form ax for some a in k.


£#h5#£The ring of additive polynomials£#/h5#£
It is quite easy to prove that any linear combination of polynomials ${\displaystyle \tau _{p}^{n}(x)}$ with coefficients in k is also an additive polynomial. An interesting question is whether there are other additive polynomials except these linear combinations. The answer is that these are the only ones.

One can check that if P(x) and M(x) are additive polynomials, then so are P(x) + M(x) and P(M(x)). These imply that the additive polynomials form a ring under polynomial addition and composition. This ring is denoted

${\displaystyle k\{\tau _{p}\}.\,}$
This ring is not commutative unless k is the field ${\displaystyle \mathbb {F} _{p}=\mathbf {Z} /p\mathbf {Z} }$ (see modular arithmetic). Indeed, consider the additive polynomials ax and xp for a coefficient a in k. For them to commute under composition, we must have

${\displaystyle (ax)^{p}=ax^{p},\,}$
and hence ap − a = 0. This is false for a not a root of this equation, that is, for a outside ${\displaystyle \mathbb {F} _{p}.}$


£#h5#£The fundamental theorem of additive polynomials£#/h5#£
Let P(x) be a polynomial with coefficients in k, and ${\displaystyle \{w_{1},\dots ,w_{m}\}\subset k}$ be the set of its roots. Assuming that the roots of P(x) are distinct (that is, P(x) is separable), then P(x) is additive if and only if the set ${\displaystyle \{w_{1},\dots ,w_{m}\}}$ forms a group with the field addition.


£#h5#£See also£#/h5#£ £#ul#££#li#£Drinfeld module£#/li#£ £#li#£Additive map£#/li#££#/ul#£
£#h5#£References£#/h5#£ £#ul#££#li#£David Goss, Basic Structures of Function Field Arithmetic, 1996, Springer, Berlin. ISBN 3-540-61087-1.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Additive Polynomial". MathWorld.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Goss, D. Basic Structures of Function Field Arithmetic. Berlin: Springer-Verlag, pp. 1-33, 1996.£#/li#££#li#£ Goss, D. Basic Structures of Function Field Arithmetic. Berlin: Springer-Verlag, pp. 1-33, 1996. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Algebra > Polynomials £#/li#££#/ul#£




£#h3#£Adiabatic Invariant£#/h3#£

A property of a physical system, such as the entropy of a gas, that stays approximately constant when changes occur slowly is called an adiabatic invariant. By this it is meant that if a system is varied between two end points, as the time for the variation between the end points is increased to infinity, the variation of an adiabatic invariant between the two end points goes to zero.

In thermodynamics, an adiabatic process is a change that occurs without heat flow; it may be slow or fast. A reversible adiabatic process is an adiabatic process that occurs slowly compared to the time to reach equilibrium. In a reversible adiabatic process, the system is in equilibrium at all stages and the entropy is constant. In the 1st half of the 20th century the scientists that worked in quantum physics used the term "adiabatic" for reversible adiabatic processes and later for any gradually changing conditions which allow the system to adapt its configuration. The quantum mechanical definition is closer to the thermodynamical concept of a quasistatic process, and has no direct relation with adiabatic processes in thermodynamics.

In mechanics, an adiabatic change is a slow deformation of the Hamiltonian, where the fractional rate of change of the energy is much slower than the orbital frequency. The area enclosed by the different motions in phase space are the adiabatic invariants.

In quantum mechanics, an adiabatic change is one that occurs at a rate much slower than the difference in frequency between energy eigenstates. In this case, the energy states of the system do not make transitions, so that the quantum number is an adiabatic invariant.

The old quantum theory was formulated by equating the quantum number of a system with its classical adiabatic invariant. This determined the form of the Bohr–Sommerfeld quantization rule: the quantum number is the area in phase space of the classical orbit.


£#h5#£Thermodynamics£#/h5#£
In thermodynamics, adiabatic changes are those that do not increase the entropy. They occur slowly in comparison to the other characteristic timescales of the system of interest, and allow heat flow only between objects at the same temperature. For isolated systems, an adiabatic change allows no heat to flow in or out.


£#h5#£Adiabatic expansion of an ideal gas£#/h5#£
If a container with an ideal gas is expanded instantaneously, the temperature of the gas doesn't change at all, because none of the molecules slow down. The molecules keep their kinetic energy, but now the gas occupies a bigger volume. If the container expands slowly, however, so that the ideal gas pressure law holds at any time, gas molecules lose energy at the rate that they do work on the expanding wall. The amount of work they do is the pressure times the area of the wall times the outward displacement, which is the pressure times the change in the volume of the gas:

${\displaystyle dW=PdV={Nk_{B}T \over V}dV}$
If no heat enters the gas, the energy in the gas molecules is decreasing by the same amount. By definition, a gas is ideal when its temperature is only a function of the internal energy per particle, not the volume. So

${\displaystyle dT={1 \over NC_{v}}dE}$
Where ${\displaystyle C_{v}}$ is the specific heat at constant volume. When the change in energy is entirely due to work done on the wall, the change in temperature is given by:

${\displaystyle NC_{v}dT=-dW=-{N{k_{B}}T \over V}dV}$
This gives a differential relationship between the changes in temperature and volume, which can be integrated to find the invariant. The constant ${\displaystyle k_{B}}$ is just a unit conversion factor, which can be set equal to one:

${\displaystyle \,d(C_{v}N\log T)=-d(N\log V)}$
So

${\displaystyle \,C_{v}N\log T+N\log V}$
is an adiabatic invariant, which is related to the entropy

${\displaystyle \,S=C_{v}N\log T+N\log V-N\log N=N\log(T^{C_{v}}V/N)}$
So entropy is an adiabatic invariant. The N log(N) term makes the entropy additive, so the entropy of two volumes of gas is the sum of the entropies of each one.

In a molecular interpretation, S is the logarithm of the phase space volume of all gas states with energy E(T) and volume V.

For a monatomic ideal gas, this can easily be seen by writing down the energy,

${\displaystyle E={1 \over 2m}\sum _{k}p_{k1}^{2}+p_{k2}^{2}+p_{k3}^{2}}$
The different internal motions of the gas with total energy E define a sphere, the surface of a 3N-dimensional ball with radius ${\displaystyle \scriptstyle {\sqrt {2mE}}}$ . The volume of the sphere is

${\displaystyle {2\pi ^{3N/2}(2mE)^{{3N-1} \over 2}} \over {\Gamma (3N/2)}}$ ,
where ${\displaystyle \Gamma }$ is the Gamma function.

Since each gas molecule can be anywhere within the volume V, the volume in phase space occupied by the gas states with energy E is

${\displaystyle {2\pi ^{3N/2}(2mE)^{{3N-1} \over 2}}V^{N} \over {\Gamma (3N/2)}}$ .
Since the N gas molecules are indistinguishable, the phase space volume is divided by ${\displaystyle N!=\Gamma (N+1)}$ , the number of permutations of N molecules.

Using Stirling's approximation for the gamma function, and ignoring factors that disappear in the logarithm after taking N large,

${\displaystyle S=N{\big (}3/2\log(E)-3/2\log(3N/2)+\log(V)-\log(N){\big )}}$
${\displaystyle =N{\big (}3/2\log(\scriptstyle {\frac {2}{3}}\displaystyle E/N)+\log(V/N){\big )}}$
Since the specific heat of a monatomic gas is 3/2, this is the same as the thermodynamic formula for the entropy.


£#h5#£Wien's law – adiabatic expansion of a box of light£#/h5#£
For a box of radiation, ignoring quantum mechanics, the energy of a classical field in thermal equilibrium is infinite, since equipartition demands that each field mode has an equal energy on average and there are infinitely many modes. This is physically ridiculous, since it means that all energy leaks into high frequency electromagnetic waves over time.

Still, without quantum mechanics, there are some things that can be said about the equilibrium distribution from thermodynamics alone, because there is still a notion of adiabatic invariance that relates boxes of different size.

When a box is slowly expanded, the frequency of the light recoiling from the wall can be computed from the Doppler shift. If the wall is not moving, the light recoils at the same frequency. If the wall is moving slowly, the recoil frequency is only equal in the frame where the wall is stationary. In the frame where the wall is moving away from the light, the light coming in is bluer than the light coming out by twice the Doppler shift factor v/c.

${\displaystyle \Delta f={2v \over c}f}$
On the other hand, the energy in the light is also decreased when the wall is moving away, because the light is doing work on the wall by radiation pressure. Because the light is reflected, the pressure is equal to twice the momentum carried by light, which is E/c. The rate at which the pressure does work on the wall is found by multiplying by the velocity:

${\displaystyle \,\Delta E=v{2E \over c}}$
This means that the change in frequency of the light is equal to the work done on the wall by the radiation pressure. The light that is reflected is changed both in frequency and in energy by the same amount:

${\displaystyle {\Delta f \over f}={\Delta E \over E}}$
Since moving the wall slowly should keep a thermal distribution fixed, the probability that the light has energy E at frequency f must only be a function of E/f.

This function cannot be determined from thermodynamic reasoning alone, and Wien guessed at the form that was valid at high frequency. He supposed that the average energy in high frequency modes was suppressed by a Boltzmann-like factor. This is not the expected classical energy in the mode, which is ${\displaystyle 1/2\beta }$ by equipartition, but a new and unjustified assumption that fit the high-frequency data.

${\displaystyle \,\langle E_{f}\rangle =e^{-\beta hf}}$
When the expectation value is added over all modes in a cavity, this is Wien's distribution, and it describes the thermodynamic distribution of energy in a classical gas of photons. Wien's Law implicitly assumes that light is statistically composed of packets that change energy and frequency in the same way. The entropy of a Wien gas scales as the volume to the power N, where N is the number of packets. This led Einstein to suggest that light is composed of localizable particles with energy proportional to the frequency. Then the entropy of the Wien gas can be given a statistical interpretation as the number of possible positions that the photons can be in.


£#h5#£Classical mechanics – action variables£#/h5#£
Suppose that a Hamiltonian is slowly time varying, for example, a one-dimensional harmonic oscillator with a changing frequency.

${\displaystyle H_{t}(p,x)={p^{2} \over 2m}+{m\omega (t)^{2}x^{2} \over 2}\,}$
The action J of a classical orbit is the area enclosed by the orbit in phase space.

${\displaystyle J=\int _{0}^{T}p(t){dx \over dt}dt\,}$
Since J is an integral over a full period, it is only a function of the energy. When the Hamiltonian is constant in time and J is constant in time, the canonically conjugate variable ${\displaystyle \theta }$ increases in time at a steady rate.

${\displaystyle {d\theta \over dt}={\partial H \over \partial J}=H\,'(J)\,}$
So the constant ${\displaystyle H\,'}$ can be used to change time derivatives along the orbit to partial derivatives with respect to ${\displaystyle \theta }$ at constant J. Differentiating the integral for J with respect to J gives an identity that fixes ${\displaystyle H\,':}$

${\displaystyle {dJ \over dJ}=1=\int _{0}^{T}{\bigg (}{\partial p \over \partial J}{dx \over dt}+p{\partial \over \partial J}{dx \over dt}{\bigg )}dt=H\,'\int _{0}^{T}{\bigg (}{\partial p \over \partial J}{\partial x \over \partial \theta }-{\partial p \over \partial \theta }{\partial x \over \partial J}{\bigg )}dt\,}$
The integrand is the Poisson bracket of x and p. The Poisson bracket of two canonically conjugate quantities like x and p is equal to 1 in any canonical coordinate system. So

${\displaystyle 1=H\,'\int _{0}^{T}\{x,p\}\,dt=H\,'\,T\,}$

and ${\displaystyle H\,'}$ is the inverse period. The variable ${\displaystyle \theta }$ increases by an equal amount in each period for all values of J – it is an angle-variable.

Adiabatic invariance of J
The Hamiltonian is a function of J only, and in the simple case of the harmonic oscillator.

${\displaystyle \,H=\omega J\,}$
When H has no time dependence, J is constant. When H is slowly time varying, the rate of change of J can be computed by re-expressing the integral for J

${\displaystyle J=\int _{0}^{2\pi }p{\partial x \over \partial \theta }d\theta \,}$
The time derivative of this quantity is

${\displaystyle {dJ \over dt}=\int _{0}^{2\pi }{\bigg (}{dp \over dt}{\partial x \over \partial \theta }+p{d \over dt}{\partial x \over \partial \theta }{\bigg )}d\theta \,}$
Replacing time derivatives with theta derivatives, using ${\displaystyle d\theta =\omega dt\,}$ and setting ${\displaystyle \omega :=1\,}$ without loss of generality ( ${\displaystyle \omega }$ being a global multiplicative constant in the resulting time derivative of the action), yields

${\displaystyle {dJ \over dt}=\int _{0}^{2\pi }{\bigg (}{\partial p \over \partial \theta }{\partial x \over \partial \theta }+p{\partial \over \partial \theta }{\partial x \over \partial \theta }{\bigg )}d\theta \,}$
So as long as the coordinates J, ${\displaystyle \theta }$ do not change appreciably over one period, this expression can be integrated by parts to give zero. This means that for slow variations, there is no lowest order change in the area enclosed by the orbit. This is the adiabatic invariance theorem – the action variables are adiabatic invariants.

For a harmonic oscillator, the area in phase space of an orbit at energy E is the area of the ellipse of constant energy,

${\displaystyle E={p^{2} \over 2m}+{m\omega ^{2}x^{2} \over 2}\,}$
The x-radius of this ellipse is ${\displaystyle \scriptstyle {\sqrt {2E/\omega ^{2}m}}}$ , while the p-radius of the ellipse is ${\displaystyle \scriptstyle {\sqrt {2mE}}}$ . Multiplying, the area is ${\displaystyle 2\pi E/\omega }$ . So if a pendulum is slowly drawn in, so that the frequency changes, the energy changes by a proportional amount.


£#h5#£Old quantum theory£#/h5#£
After Planck identified that Wien's law can be extended to all frequencies, even very low ones, by interpolating with the classical equipartition law for radiation, physicists wanted to understand the quantum behavior of other systems.

The Planck radiation law quantized the motion of the field oscillators in units of energy proportional to the frequency:

${\displaystyle E=hf=\hbar \omega \,}$
The quantum can only depend on the energy/frequency by adiabatic invariance, and since the energy must be additive when putting boxes end to end, the levels must be equally spaced.

Einstein, followed by Debye, extended the domain of quantum mechanics by considering the sound modes in a solid as quantized oscillators. This model explained why the specific heat of solids approached zero at low temperatures, instead of staying fixed at ${\displaystyle 3k_{B}}$ as predicted by classical equipartition.

At the Solvay conference, the question of quantizing other motions was raised, and Lorentz pointed out a problem, known as Rayleigh–Lorentz pendulum. If you consider a quantum pendulum whose string is shortened very slowly, the quantum number of the pendulum cannot change because at no point is there a high enough frequency to cause a transition between the states. But the frequency of the pendulum changes when the string is shorter, so the quantum states change energy.

Einstein responded that for slow pulling, the frequency and energy of the pendulum both change but the ratio stays fixed. This is analogous to Wien's observation that under slow motion of the wall the energy to frequency ratio of reflected waves is constant. The conclusion was that the quantities to quantize must be adiabatic invariants.

This line of argument was extended by Sommerfeld into a general theory: the quantum number of an arbitrary mechanical system is given by the adiabatic action variable. Since the action variable in the harmonic oscillator is an integer, the general condition is:

${\displaystyle \int pdq=nh\,}$
This condition was the foundation of the old quantum theory, which was able to predict the qualitative behavior of atomic systems. The theory is inexact for small quantum numbers, since it mixes classical and quantum concepts. But it was a useful half-way step to the new quantum theory.


£#h5#£Plasma physics£#/h5#£
In plasma physics there are three adiabatic invariants of charged particle motion.


£#h5#£The first adiabatic invariant, μ£#/h5#£
The magnetic moment of a gyrating particle is

${\displaystyle \mu ={\frac {p_{\perp }^{2}}{2mB}}}$
which respects special relativity. ${\displaystyle p_{\perp }=\gamma mv_{\perp }}$ is the relativistic momentum perpendicular to the magnetic field. ${\displaystyle \mu }$ is a constant of the motion to all orders in an expansion in ${\displaystyle \omega /\omega _{c}}$ , where ${\displaystyle \omega }$ is the rate of any changes experienced by the particle, e.g., due to collisions or due to temporal or spatial variations in the magnetic field. Consequently, the magnetic moment remains nearly constant even for changes at rates approaching the gyrofrequency. When μ is constant, the perpendicular particle energy is proportional to B, so the particles can be heated by increasing B, but this is a 'one shot' deal because the field cannot be increased indefinitely. It finds applications in magnetic mirrors and magnetic bottles.

There are some important situations in which the magnetic moment is not invariant:

£#ul#££#li#£Magnetic pumping: If the collision frequency is larger than the pump frequency, μ is no longer conserved. In particular, collisions allow net heating by transferring some of the perpendicular energy to parallel energy.£#/li#£ £#li#£Cyclotron heating: If B is oscillated at the cyclotron frequency, the condition for adiabatic invariance is violated and heating is possible. In particular, the induced electric field rotates in phase with some of the particles and continuously accelerates them.£#/li#£ £#li#£Magnetic cusps: The magnetic field at the center of a cusp vanishes, so the cyclotron frequency is automatically smaller than the rate of any changes. Thus the magnetic moment is not conserved and particles are scattered relatively easily into the loss cone.£#/li#££#/ul#£
£#h5#£The second adiabatic invariant, J£#/h5#£
The longitudinal invariant of a particle trapped in a magnetic mirror,

${\displaystyle J=\int _{a}^{b}p_{\parallel }ds}$
where the integral is between the two turning points, is also an adiabatic invariant. This guarantees, for example, that a particle in the magnetosphere moving around the Earth always returns to the same line of force. The adiabatic condition is violated in transit-time magnetic pumping, where the length of a magnetic mirror is oscillated at the bounce frequency, resulting in net heating.


£#h5#£The third adiabatic invariant, ${\displaystyle \Phi }$ £#/h5#£
The total magnetic flux ${\displaystyle \Phi }$ enclosed by a drift surface is the third adiabatic invariant, associated with the periodic motion of mirror-trapped particles drifting around the axis of the system. Because this drift motion is relatively slow, ${\displaystyle \Phi }$ is often not conserved in practical applications.


£#h5#£References£#/h5#£ £#ul#££#li#£Yourgrau, Wolfgang; Stanley Mandelstam (1979). Variational Principles in Dynamics and Quantum Theory. New York: Dover. ISBN 978-0-486-63773-0. §10£#/li#£ £#li#£Pauli, Wolfgang (1973). Charles P. Enz (ed.). Pauli Lectures on Physics. Vol. 4. Cambridge, Mass: MIT Press. ISBN 978-0-262-66035-8. pp. 85–89£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£lecture notes on the second adiabatic invariant£#/li#£ £#li#£lecture notes on the third adiabatic invariant£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Dynamical Systems £#/li#££#/ul#£




£#h3#£Adjoint£#/h3#£

In mathematics, the term adjoint applies in several situations. Several of these share a similar formalism: if A is adjoint to B, then there is typically some formula of the type

(Ax, y) = (x, By).
Specifically, adjoint or adjunction may mean:

£#ul#££#li#£Adjoint of a linear map, also called its transpose£#/li#£ £#li#£Hermitian adjoint (adjoint of a linear operator) in functional analysis£#/li#£ £#li#£Adjoint endomorphism of a Lie algebra£#/li#£ £#li#£Adjoint representation of a Lie group£#/li#£ £#li#£Adjoint functors in category theory£#/li#£ £#li#£Adjunction (field theory)£#/li#£ £#li#£Adjunction formula (algebraic geometry)£#/li#£ £#li#£Adjunction space in topology£#/li#£ £#li#£Conjugate transpose of a matrix in linear algebra£#/li#£ £#li#£Adjugate matrix, related to its inverse£#/li#£ £#li#£Adjoint equation£#/li#£ £#li#£The upper and lower adjoints of a Galois connection in order theory£#/li#£ £#li#£The adjoint of a differential operator with general polynomial coefficients£#/li#£ £#li#£Kleisli adjunction£#/li#£ £#li#£Monoidal adjunction£#/li#£ £#li#£Quillen adjunction£#/li#£ £#li#£Axiom of adjunction in set theory£#/li#£ £#li#£Adjunction (rule of inference)£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Arfken, G. Mathematical Methods for Physicists, 3rd ed. Orlando, FL: Academic Press, 1985.£#/li#££#li#£Dirac, P. A. M. "Conjugate Relations." §8 in Principles of Quantum Mechanics, 4th ed. Oxford, England: Oxford University Press, pp. 26-29, 1982.£#/li#££#li#£Griffiths, D. J. Introduction to Elementary Particles. New York: Wiley, p. 220, 1987.£#/li#££#li#£ Arfken, G. Mathematical Methods for Physicists, 3rd ed. Orlando, FL: Academic Press, 1985. £#/li#££#li#£ Dirac, P. A. M. "Conjugate Relations." §8 in Principles of Quantum Mechanics, 4th ed. Oxford, England: Oxford University Press, pp. 26-29, 1982. £#/li#££#li#£ Griffiths, D. J. Introduction to Elementary Particles. New York: Wiley, p. 220, 1987. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Differential Equations > Ordinary Differential Equations £#/li#££#li#£ History and Terminology > Terminology £#/li#££#/ul#£




£#h3#£Adomian Polynomial£#/h3#£

The Adomian decomposition method (ADM) is a semi-analytical method for solving ordinary and partial nonlinear differential equations. The method was developed from the 1970s to the 1990s by George Adomian, chair of the Center for Applied Mathematics at the University of Georgia. It is further extensible to stochastic systems by using the Ito integral. The aim of this method is towards a unified theory for the solution of partial differential equations (PDE); an aim which has been superseded by the more general theory of the homotopy analysis method. The crucial aspect of the method is employment of the "Adomian polynomials" which allow for solution convergence of the nonlinear portion of the equation, without simply linearizing the system. These polynomials mathematically generalize to a Maclaurin series about an arbitrary external parameter; which gives the solution method more flexibility than direct Taylor series expansion.


£#h5#£Ordinary differential equations£#/h5#£
Adomian method is well suited to solve Cauchy problems, an important class of problems which include initial conditions problems.


£#h5#£Application to a first order nonlinear system£#/h5#£
An example of initial condition problem for an ordinary differential equation is the following:

${\displaystyle y^{\prime }(t)+y^{2}(t)=-1,}$
${\displaystyle y(0)=0.}$
To solve the problem, the highest degree differential operator (written here as L) is put on the left side, in the following way:

${\displaystyle Ly=-1-y^{2},}$
with L = d/dt and ${\displaystyle L^{-1}=\int _{0}^{t}()}$ . Now the solution is assumed to be an infinite series of contributions:

${\displaystyle y=y_{0}+y_{1}+y_{2}+y_{3}+\cdots .}$
Replacing in the previous expression, we obtain:

${\displaystyle (y_{0}+y_{1}+y_{2}+y_{3}+\cdots )=y(0)+L^{-1}[-1-(y_{0}+y_{1}+y_{2}+y_{3}+\cdots )^{2}].}$
Now we identify y0 with some explicit expression on the right, and yi, i = 1, 2, 3, ..., with some expression on the right containing terms of lower order than i. For instance:

${\displaystyle {\begin{aligned}&y_{0}&=&\ y(0)+L^{-1}(-1)&=&-t\\&y_{1}&=&-L^{-1}(y_{0}^{2})=-L^{-1}(t^{2})&=&-t^{3}/3\\&y_{2}&=&-L^{-1}(2y_{0}y_{1})&=&-2t^{5}/15\\&y_{3}&=&-L^{-1}(y_{1}^{2}+2y_{0}y_{2})&=&-17t^{7}/315.\end{aligned}}}$
In this way, any contribution can be explicitly calculated at any order. If we settle for the four first terms, the approximant is the following:

${\displaystyle {\begin{aligned}y&=y_{0}+y_{1}+y_{2}+y_{3}+\cdots \\&=-\left[t+{\frac {1}{3}}t^{3}+{\frac {2}{15}}t^{5}+{\frac {17}{315}}t^{7}+\cdots \right]\end{aligned}}}$

£#h5#£Application to Blasius equation£#/h5#£
A second example, with more complex boundary conditions is the Blasius equation for a flow in a boundary layer:

${\displaystyle {\frac {\mathrm {d} ^{3}u}{\mathrm {d} x^{3}}}+{\frac {1}{2}}u{\frac {\mathrm {d} ^{2}u}{\mathrm {d} x^{2}}}=0}$
With the following conditions at the boundaries:

${\displaystyle {\begin{aligned}u(0)&=0\\u^{\prime }(0)&=0\\u^{\prime }(x)&\to 1,\qquad x\to \infty \end{aligned}}}$
Linear and non-linear operators are now called ${\displaystyle L={\frac {\mathrm {d} ^{3}}{\mathrm {d} x^{3}}}}$ and ${\displaystyle N={\frac {1}{2}}u{\frac {\mathrm {d} ^{2}}{\mathrm {d} x^{2}}}}$ , respectively. Then, the expression becomes:

${\displaystyle Lu+Nu=0}$
and the solution may be expressed, in this case, in the following simple way:

${\displaystyle u=\alpha +\beta x+\gamma x^{2}/2-L^{-1}Nu}$
where: ${\displaystyle L^{-1}\xi (x)=\int dx\int \mathrm {d} x\int \mathrm {d} x\;\;\xi (x)}$ If:

${\displaystyle {\begin{aligned}u&=u^{0}+u^{1}+u^{2}+\cdots +u^{N}\\&=\alpha +\beta x+\gamma x^{2}/2-{\frac {1}{2}}L^{1}(u^{0}+u^{1}+u^{2}+\cdots +u^{N}){\frac {\mathrm {d} ^{2}}{\mathrm {d} x^{2}}}(u^{0}+u^{1}+u^{2}+\cdots +u^{N})\end{aligned}}}$
and:

${\displaystyle {\begin{aligned}&u^{0}&=&\alpha +\beta x+\gamma x^{2}/2\\&u^{1}&=&-{\frac {1}{2}}L^{1}(u^{0}u^{0''})&=&-L^{1}A_{0}\\&u^{2}&=&-{\frac {1}{2}}L^{1}(u^{1}u^{0''}+u^{0}u^{1''})&=&-L^{1}A_{1}\\&u^{3}&=&-{\frac {1}{2}}L^{1}(u^{2}u^{0''}+u^{1}u^{1''}+u^{0}u^{2''})&=&-L^{1}A_{2}\\&&\cdots &\end{aligned}}}$
Adomian’s polynomials to linearize the non-linear term can be obtained systematically by using the following rule:

${\displaystyle A_{n}={\frac {1}{n!}}{\frac {\mathrm {d} ^{n}}{\mathrm {d} \lambda ^{n}}}f(u(\lambda ))\mid _{\lambda =0},}$
where: ${\displaystyle {\frac {\mathrm {d} ^{n}}{\mathrm {d} \lambda ^{n}}}u(\lambda )\mid _{\lambda =0}=n!u_{n}}$

Boundary conditions must be applied, in general, at the end of each approximation. In this case, the integration constants must be grouped into three final independent constants. However, in our example, the three constants appear grouped from the beginning in the form shown in the formal solution above. After applying the two first boundary conditions we obtain the so-called Blasius series:

${\displaystyle u={\frac {\gamma }{2}}x^{2}-{\frac {\gamma ^{2}}{2}}\left({\frac {x^{5}}{5!}}\right)+{\frac {11\gamma ^{3}}{4}}\left({\frac {x^{8}}{8!}}\right)-{\frac {375\gamma ^{4}}{8}}\left({\frac {x^{11}}{11!}}\right)+\cdots }$
To obtain γ we have to apply boundary conditions at ∞, which may be done by writing the series as a Padé approximant:

${\displaystyle f(z)=\sum _{n=0}^{L+M}c_{n}z^{n}={\frac {a_{0}+a_{1}z+\cdots +a_{L}z^{L}}{b_{0}+b_{1}z+\cdots +b_{M}z^{M}}}}$
where L = M. The limit at ${\displaystyle \infty }$ of this expression is aL/bM.

If we choose b0 = 1, M linear equations for the b coefficients are obtained:

${\displaystyle \left[{\begin{array}{cccc}c_{L-M+1}&c_{L-M+2}&\cdots &c_{L}\\c_{L-M+2}&c_{L-M+3}&\cdots &c_{L+1}\\\vdots &\vdots &&\vdots \\c_{L}&c_{L+1}&\cdots &c_{L+M-1}\end{array}}\right]\left[{\begin{array}{c}b_{M}\\b_{M-1}\\\vdots \\b_{1}\end{array}}\right]=-\left[{\begin{array}{c}c_{L+1}\\c_{L+2}\\\vdots \\c_{L+M}\end{array}}\right]}$
Then, we obtain the a coefficients by means of the following sequence:

${\displaystyle {\begin{aligned}a_{0}&=c_{0}\\a_{1}&=c_{1}+b_{1}c_{0}\\a_{2}&=c_{2}+b_{1}c_{1}+b_{2}c_{0}\\&\cdots \\a_{L}&=c_{L}+\sum _{i=1}^{\min(L,m)}b_{i}c_{L-i}.\end{aligned}}}$
In our example:

${\displaystyle u'(x)=\gamma x-{\frac {\gamma ^{2}}{2}}\left({\frac {x^{4}}{4!}}\right)+{\frac {11\gamma ^{3}}{4}}\left({\frac {x^{7}}{7!}}\right)-{\frac {375\gamma ^{4}}{8}}\left({\frac {x^{10}}{10!}}\right)}$
Which when γ = 0.0408 becomes:

${\displaystyle u'(x)={\frac {0.0204+0.0379\,z-0.0059\,z^{2}-0.00004575\,z^{3}+6.357\cdot 10^{-6}z^{4}-1.291\cdot 10^{-6}z^{5}}{1-0.1429\,z-0.0000232\,z^{2}+0.0008375\,z^{3}-0.0001558\,z^{4}-1.2849\cdot 10^{-6}z^{5}}},}$
with the limit:

${\displaystyle \lim _{x\to \infty }u'(x)=1.004.}$
Which is approximately equal to 1 (from boundary condition (3)) with an accuracy of 4/1000.


£#h5#£Partial differential equations£#/h5#£
£#h5#£Application to a rectangular system with nonlinearity£#/h5#£
One of the most frequent problems in physical sciences is to obtain the solution of a (linear or nonlinear) partial differential equation which satisfies a set of functional values on a rectangular boundary. An example is the following problem:

${\displaystyle {\frac {\partial ^{2}u}{\partial x^{2}}}+{\frac {\partial ^{2}u}{\partial y^{2}}}-b{\frac {\partial u^{2}}{\partial x}}=\rho (x,y)\qquad (1)}$
with the following boundary conditions defined on a rectangle:

${\displaystyle u(x=0)=f_{1}(y)\quad {\text{and}}\quad u(x=x_{l})=f_{2}(y)\qquad {\text{(1-a)}}}$
${\displaystyle u(y=-y_{l})=g_{1}(x)\quad {\text{and}}\quad u(y=y_{l})=g_{2}(x)\qquad {\text{(1-b)}}}$
This kind of partial differential equation appears frequently coupled with others in science and engineering. For instance, in the incompressible fluid flow problem, the Navier–Stokes equations must be solved in parallel with a Poisson equation for the pressure.


£#h5#£Decomposition of the system£#/h5#£
Let us use the following notation for the problem (1):

${\displaystyle L_{x}u+L_{y}u+Nu=\rho (x,y)\qquad (2)}$
where Lx, Ly are double derivate operators and N is a non-linear operator.

The formal solution of (2) is:

${\displaystyle u=a(y)+b(y)x+L_{x}^{-1}\rho (x,y)-L_{x}^{-1}L_{y}u-L_{x}^{-1}Nu\qquad (3)}$
Expanding now u as a set of contributions to the solution we have:

${\displaystyle u=u_{0}+u_{1}+u_{2}+u_{3}+\cdots }$
By substitution in (3) and making a one-to-one correspondence between the contributions on the left side and the terms on the right side we obtain the following iterative scheme:

${\displaystyle {\begin{aligned}u_{0}&=a_{0}(y)+b_{0}(y)x+L_{x}^{-1}\rho (x,y)\\u_{1}&=a_{1}(y)+b_{1}(y)x-L_{x}^{-1}L_{y}u_{0}+b\int dxA_{0}\\&\cdots \\u_{n}&=a_{n}(y)+b_{n}(y)x-L_{x}^{-1}L_{y}u_{n-1}+b\int dxA_{n-1}\quad 0<n<\infty \end{aligned}}}$
where the couple {an(y), bn(y)} is the solution of the following system of equations:

${\displaystyle {\begin{aligned}\varphi ^{n}(x=0)&=f_{1}(y)\\\varphi ^{n}(x=x_{l})&=f_{2}(y),\end{aligned}}}$
here ${\displaystyle \varphi ^{n}\equiv \sum _{i=0}^{n}u_{i}}$ is the nth-order approximant to the solution and N u has been consistently expanded in Adomian polynomials:

${\displaystyle {\begin{aligned}Nu&=-b\partial _{x}u^{2}=-b\partial _{x}(u_{0}+u_{1}+u_{2}+u_{3}+\cdots )(u_{0}+u_{1}+u_{2}+u_{3}+\cdots )\\&=-b\partial _{x}(u_{0}u_{0}+2u_{0}u_{1}+u_{1}u_{1}+2u_{0}u_{2}+\cdots )\\&=-b\partial _{x}\sum _{n=1}^{\infty }A(n-1),\end{aligned}}}$
where ${\displaystyle A_{n}=\sum _{\nu =1}^{n}C(\nu ,n)f^{(\nu )}(u_{0})}$ and f(u) = u2 in the example (1).

Here C(ν, n) are products (or sum of products) of ν components of u whose subscripts sum up to n, divided by the factorial of the number of repeated subscripts. It is only a thumb-rule to order systematically the decomposition to be sure that all the combinations appearing are utilized sooner or later.

The ${\displaystyle \sum _{n=0}^{\infty }A_{n}}$ is equal to the sum of a generalized Taylor series about u0.

For the example (1) the Adomian polynomials are:

${\displaystyle {\begin{aligned}A_{0}&=u_{0}^{2}\\A_{1}&=2u_{0}u_{1}\\A_{2}&=u_{1}^{2}+2u_{0}u_{2}\\A_{3}&=2u_{1}u_{2}+2u_{0}u_{3}\\&\cdots \end{aligned}}}$
Other possible choices are also possible for the expression of An.


£#h5#£Series solutions£#/h5#£
Cherruault established that the series terms obtained by Adomian's method approach zero as 1/(mn)! if m is the order of the highest linear differential operator and that ${\displaystyle \lim _{n\to \infty }\varphi ^{n}=u}$ . With this method the solution can be found by systematically integrating along any of the two directions: in the x-direction we would use expression (3); in the alternative y-direction we would use the following expression:

${\displaystyle u=c(x)+d(x)y+L_{y}^{-1}\rho (x,y)-L_{y}^{-1}L_{x}u-L_{y}^{-1}Nu}$
where: c(x), d(x) is obtained from the boundary conditions at y = - yl and y = yl:

${\displaystyle {\begin{aligned}u(y=-y_{l})&=g_{1}(x)\\u(y=y_{l})&=g_{2}(x)\end{aligned}}}$
If we call the two respective solutions x-partial solution and y-partial solution, one of the most interesting consequences of the method is that the x-partial solution uses only the two boundary conditions (1-a) and the y-partial solution uses only the conditions (1-b).

Thus, one of the two sets of boundary functions {f1, f2} or {g1, g2} is redundant, and this implies that a partial differential equation with boundary conditions on a rectangle cannot have arbitrary boundary conditions on the borders, since the conditions at x = x1, x = x2 must be consistent with those imposed at y = y1 and y = y2.

An example to clarify this point is the solution of the Poisson problem with the following boundary conditions:

${\displaystyle {\begin{aligned}u(x=0)&=f_{1}(y)=0\\u(x=x_{l})&=f_{2}(y)=0\end{aligned}}}$
By using Adomian's method and a symbolic processor (such as Mathematica or Maple) it is easy to obtain the third order approximant to the solution. This approximant has an error lower than 5×10−16 in any point, as it can be proved by substitution in the initial problem and by displaying the absolute value of the residual obtained as a function of (x, y).

The solution at y = -0.25 and y = 0.25 is given by specific functions that in this case are:

${\displaystyle g_{1}(x)=0.0520833\,x-0.347222\,x^{3}+9.25186\times 10^{-17}x^{4}+0.833333\,x^{5}-0.555556\,x^{6}}$
and g2(x) = g1(x) respectively.

If a (double) integration is now performed in the y-direction using these two boundary functions the same solution will be obtained, which satisfy u(x=0, y) = 0 and u(x=0.5, y) = 0 and cannot satisfy any other condition on these borders.

Some people are surprised by these results; it seems strange that not all initial-boundary conditions must be explicitly used to solve a differential system. However, it is a well established fact that any elliptic equation has one and only one solution for any functional conditions in the four sides of a rectangle provided there is no discontinuity on the edges. The cause of the misconception is that scientists and engineers normally think in a boundary condition in terms of weak convergence in a Hilbert space (the distance to the boundary function is small enough to practical purposes). In contrast, Cauchy problems impose a point-to-point convergence to a given boundary function and to all its derivatives (and this is a quite strong condition!). For the first ones, a function satisfies a boundary condition when the area (or another functional distance) between it and the true function imposed in the boundary is so small as desired; for the second ones, however, the function must tend to the true function imposed in any and every point of the interval.

The commented Poisson problem does not have a solution for any functional boundary conditions f1, f2, g1, g2; however, given f1, f2 it is always possible to find boundary functions g1*, g2* so close to g1, g2 as desired (in the weak convergence meaning) for which the problem has solution. This property makes it possible to solve Poisson's and many other problems with arbitrary boundary conditions but never for analytic functions exactly specified on the boundaries. The reader can convince himself (herself) of the high sensitivity of PDE solutions to small changes in the boundary conditions by solving this problem integrating along the x-direction, with boundary functions slightly different even though visually not distinguishable. For instance, the solution with the boundary conditions:

${\displaystyle f_{1,2}(y)=0.00413682-0.0813801\,y^{2}+0.260416\,y^{4}-0.277778\,y^{6}}$
at x = 0 and x = 0.5, and the solution with the boundary conditions:

${\displaystyle {\begin{aligned}f_{1,2}(y)=0.00413683&-0.00040048\,y-0.0813802\,y^{2}+0.0101279\,y^{3}+0.260417\,y^{4}\\&-0.0694455\,y^{5}-0.277778\,y^{6}+0.15873\,y^{7}+\cdots \end{aligned}}}$
at x = 0 and x = 0.5, produce lateral functions with different sign convexity even though both functions are visually not distinguishable.

Solutions of elliptic problems and other partial differential equations are highly sensitive to small changes in the boundary function imposed when only two sides are used. And this sensitivity is not easily compatible with models that are supposed to represent real systems, which are described by means of measurements containing experimental errors and are normally expressed as initial-boundary value problems in a Hilbert space.


£#h5#£Improvements to the decomposition method£#/h5#£
At least three methods have been reported to obtain the boundary functions g1*, g2* that are compatible with any lateral set of conditions {f1, f2} imposed. This makes it possible to find the analytical solution of any PDE boundary problem on a closed rectangle with the required accuracy, so allowing to solve a wide range of problems that the standard Adomian's method was not able to address.

The first one perturbs the two boundary functions imposed at x = 0 and x = x1 (condition 1-a) with a Nth-order polynomial in y: p1, p2 in such a way that: f1' = f1 + p1, f2' = f2 + p2, where the norm of the two perturbation functions are smaller than the accuracy needed at the boundaries. These p1, p2 depend on a set of polynomial coefficients ci, i = 1, ..., N. Then, the Adomian method is applied and functions are obtained at the four boundaries which depend on the set of ci, i = 1, ..., N. Finally, a boundary function F(c1, c2, ..., cN) is defined as the sum of these four functions, and the distance between F(c1, c2, ..., cN) and the real boundary functions ((1-a) and (1-b)) is minimized. The problem has been reduced, in this way, to the global minimization of the function F(c1, c2, ..., cN) which has a global minimum for some combination of the parameters ci, i = 1, ..., N. This minimum may be found by means of a genetic algorithm or by using some other optimization method, as the one proposed by Cherruault (1999).

A second method to obtain analytic approximants of initial-boundary problems is to combine Adomian decomposition with spectral methods.

Finally, the third method proposed by García-Olivares is based on imposing analytic solutions at the four boundaries, but modifying the original differential operator in such a way that it is different from the original one only in a narrow region close to the boundaries, and it forces the solution to satisfy exactly analytic conditions at the four boundaries.


£#h5#£Gallery£#/h5#£

£#h5#£References£#/h5#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Adomian, G. "Linear Stochastic Operators." Ph.D. Dissertation. Los Angeles, CA: University of California, Los Angeles, 1963.£#/li#££#li#£Adomian, G. Stochastic Systems. New York: Academic Press, 1983.£#/li#££#li#£Adomian, G. "A New Approach to Nonlinear Partial Differential Equations." J. Math. Anal. Appl. 102, 420-434, 1984.£#/li#££#li#£Adomian, G. Nonlinear Stochastic Operator Equations. Orlando, FL: Academic Press, 1986.£#/li#££#li#£Adomian, G. "A Review of the Decomposition Method in Applied Mathematics." J. Math. Anal. Appl. 135, 501-544, 1988.£#/li#££#li#£Adomian, G. Nonlinear Stochastic Systems Theory and Applications to Physics. Dordrecht, Netherlands: Kluwer, 1989.£#/li#££#li#£Adomian, G. Solving Frontier Problems of Physics: The Decomposition Method. Boston, MA: Kluwer, 1994.£#/li#££#li#£Bellman, R. and Adomian, G. Partial Differential Equations: New Methods for their Treatment and Solution. Dordrecht, Netherlands: Reidel, 1985.£#/li#££#li#£Cherruault, Y. Modèles et mŽthodes mathŽmatiques pour les sciences du vivant. Paris, France: Presses Universitaires de France, 1998.£#/li#££#li#£Rach, R. "A Convenient Computational Form for the Adomian Polynomials." J. Math. Anal. Appl. 102, 415-419, 1984.£#/li#££#li#£Rach, R. C. "A New Definition of the Adomian Polynomials." Kybernetes 37, 910-955, 2008.£#/li#££#li#£Wazwaz, A. M. "A New Algorithm for Calculating Adomian Polynomials for Nonlinear Operators." Appl. Math. Comput. 111, 53-69, 2000.£#/li#££#li#£Wazwaz, A.-M. Partial Differential Equations: Methods and Applications. Lisse, Netherlands: Balkema Publishers, 2002.£#/li#££#li#£ Adomian, G. "Linear Stochastic Operators." Ph.D. Dissertation. Los Angeles, CA: University of California, Los Angeles, 1963. £#/li#££#li#£ Adomian, G. Stochastic Systems. New York: Academic Press, 1983. £#/li#££#li#£ Adomian, G. "A New Approach to Nonlinear Partial Differential Equations." J. Math. Anal. Appl. 102, 420-434, 1984. £#/li#££#li#£ Adomian, G. Nonlinear Stochastic Operator Equations. Orlando, FL: Academic Press, 1986. £#/li#££#li#£ Adomian, G. "A Review of the Decomposition Method in Applied Mathematics." J. Math. Anal. Appl. 135, 501-544, 1988. £#/li#££#li#£ Adomian, G. Nonlinear Stochastic Systems Theory and Applications to Physics. Dordrecht, Netherlands: Kluwer, 1989. £#/li#££#li#£ Adomian, G. Solving Frontier Problems of Physics: The Decomposition Method. Boston, MA: Kluwer, 1994. £#/li#££#li#£ Bellman, R. and Adomian, G. Partial Differential Equations: New Methods for their Treatment and Solution. Dordrecht, Netherlands: Reidel, 1985. £#/li#££#li#£ Cherruault, Y. Modèles et mŽthodes mathŽmatiques pour les sciences du vivant. Paris, France: Presses Universitaires de France, 1998. £#/li#££#li#£ Rach, R. "A Convenient Computational Form for the Adomian Polynomials." J. Math. Anal. Appl. 102, 415-419, 1984. £#/li#££#li#£ Rach, R. C. "A New Definition of the Adomian Polynomials." Kybernetes 37, 910-955, 2008. £#/li#££#li#£ Wazwaz, A. M. "A New Algorithm for Calculating Adomian Polynomials for Nonlinear Operators." Appl. Math. Comput. 111, 53-69, 2000. £#/li#££#li#£ Wazwaz, A.-M. Partial Differential Equations: Methods and Applications. Lisse, Netherlands: Balkema Publishers, 2002. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Algebra > Polynomials £#/li#££#li#£ Calculus and Analysis > Operator Theory £#/li#££#li#£ Calculus and Analysis > Differential Equations > Partial Differential Equations £#/li#££#/ul#£




£#h3#£Affine Complex Plane£#/h3#£

In mathematics, the complex plane is the plane formed by the complex numbers, with a Cartesian coordinate system such that the x-axis, called real axis, is formed by the real numbers, and the y-axis, called imaginary axis, is formed by the imaginary numbers.

The complex plane allows a geometric interpretation of complex numbers. Under addition, they add like vectors. The multiplication of two complex numbers can be expressed more easily in polar coordinates—the magnitude or modulus of the product is the product of the two absolute values, or moduli, and the angle or argument of the product is the sum of the two angles, or arguments. In particular, multiplication by a complex number of modulus 1 acts as a rotation.

The complex plane is sometimes known as the Argand plane or Gauss plane.


£#h5#£Notational conventions£#/h5#£
£#h5#£Complex numbers£#/h5#£
In complex analysis, the complex numbers are customarily represented by the symbol z, which can be separated into its real (x) and imaginary (y) parts:

for example: z = 4 + 5i, where x and y are real numbers, and i is the imaginary unit. In this customary notation the complex number z corresponds to the point (x, y) in the Cartesian plane.

In the Cartesian plane the point (x, y) can also be represented in polar coordinates as

In the Cartesian plane it may be assumed that the arctangent takes values from −π/2 to π/2 (in radians), and some care must be taken to define the more complete arctangent function for points (x, y) when x ≤ 0. In the complex plane these polar coordinates take the form

where

Here |z| is the absolute value or modulus of the complex number z; θ, the argument of z, is usually taken on the interval 0 ≤ θ < 2π; and the last equality (to |z|eiθ) is taken from Euler's formula. Without the constraint on the range of θ, the argument of z is multi-valued, because the complex exponential function is periodic, with period 2π i. Thus, if θ is one value of arg(z), the other values are given by arg(z) = θ + 2nπ, where n is any non-zero integer.

While seldom used explicitly, the geometric view of the complex numbers is implicitly based on its structure of a Euclidean vector space of dimension 2, where the inner product of complex numbers w and z is given by ${\displaystyle \Re (w{\overline {z}})}$ ; then for a complex number z its absolute value |z| coincides with its Euclidean norm, and its argument arg(z) with the angle turning from 1 to z.

The theory of contour integration comprises a major part of complex analysis. In this context, the direction of travel around a closed curve is important – reversing the direction in which the curve is traversed multiplies the value of the integral by −1. By convention the positive direction is counterclockwise. For example, the unit circle is traversed in the positive direction when we start at the point z = 1, then travel up and to the left through the point z = i, then down and to the left through −1, then down and to the right through −i, and finally up and to the right to z = 1, where we started.

Almost all of complex analysis is concerned with complex functions – that is, with functions that map some subset of the complex plane into some other (possibly overlapping, or even identical) subset of the complex plane. Here it is customary to speak of the domain of f(z) as lying in the z-plane, while referring to the range of f(z) as a set of points in the w-plane. In symbols we write

and often think of the function f as a transformation from the z-plane (with coordinates (x, y)) into the w-plane (with coordinates (u, v)).


£#h5#£Complex plane notation£#/h5#£
Complex plane is denoted as ${\displaystyle \mathbb {C} }$ .


£#h5#£Argand diagram£#/h5#£
Argand diagram refers to a geometric plot of complex numbers as points z = x + iy using the x-axis as the real axis and y-axis as the imaginary axis. Such plots are named after Jean-Robert Argand (1768–1822), although they were first described by Norwegian–Danish land surveyor and mathematician Caspar Wessel (1745–1818). Argand diagrams are frequently used to plot the positions of the zeros and poles of a function in the complex plane.


£#h5#£Stereographic projections£#/h5#£
It can be useful to think of the complex plane as if it occupied the surface of a sphere. Given a sphere of unit radius, place its center at the origin of the complex plane, oriented so that the equator on the sphere coincides with the unit circle in the plane, and the north pole is "above" the plane.

We can establish a one-to-one correspondence between the points on the surface of the sphere minus the north pole and the points in the complex plane as follows. Given a point in the plane, draw a straight line connecting it with the north pole on the sphere. That line will intersect the surface of the sphere in exactly one other point. The point z = 0 will be projected onto the south pole of the sphere. Since the interior of the unit circle lies inside the sphere, that entire region (|z| < 1) will be mapped onto the southern hemisphere. The unit circle itself (|z| = 1) will be mapped onto the equator, and the exterior of the unit circle (|z| > 1) will be mapped onto the northern hemisphere, minus the north pole. Clearly this procedure is reversible – given any point on the surface of the sphere that is not the north pole, we can draw a straight line connecting that point to the north pole and intersecting the flat plane in exactly one point.

Under this stereographic projection the north pole itself is not associated with any point in the complex plane. We perfect the one-to-one correspondence by adding one more point to the complex plane – the so-called point at infinity – and identifying it with the north pole on the sphere. This topological space, the complex plane plus the point at infinity, is known as the extended complex plane. We speak of a single "point at infinity" when discussing complex analysis. There are two points at infinity (positive, and negative) on the real number line, but there is only one point at infinity (the north pole) in the extended complex plane.

Imagine for a moment what will happen to the lines of latitude and longitude when they are projected from the sphere onto the flat plane. The lines of latitude are all parallel to the equator, so they will become perfect circles centered on the origin z = 0. And the lines of longitude will become straight lines passing through the origin (and also through the "point at infinity", since they pass through both the north and south poles on the sphere).

This is not the only possible yet plausible stereographic situation of the projection of a sphere onto a plane consisting of two or more values. For instance, the north pole of the sphere might be placed on top of the origin z = −1 in a plane that is tangent to the circle. The details don't really matter. Any stereographic projection of a sphere onto a plane will produce one "point at infinity", and it will map the lines of latitude and longitude on the sphere into circles and straight lines, respectively, in the plane.


£#h5#£Cutting the plane£#/h5#£
When discussing functions of a complex variable it is often convenient to think of a cut in the complex plane. This idea arises naturally in several different contexts.


£#h5#£Multi-valued relationships and branch points£#/h5#£
Consider the simple two-valued relationship

Before we can treat this relationship as a single-valued function, the range of the resulting value must be restricted somehow. When dealing with the square roots of non-negative real numbers this is easily done. For instance, we can just define

to be the non-negative real number y such that y2 = x. This idea doesn't work so well in the two-dimensional complex plane. To see why, let's think about the way the value of f(z) varies as the point z moves around the unit circle. We can write

Evidently, as z moves all the way around the circle, w only traces out one-half of the circle. So one continuous motion in the complex plane has transformed the positive square root e0 = 1 into the negative square root eiπ = −1.

This problem arises because the point z = 0 has just one square root, while every other complex number z ≠ 0 has exactly two square roots. On the real number line we could circumvent this problem by erecting a "barrier" at the single point x = 0. A bigger barrier is needed in the complex plane, to prevent any closed contour from completely encircling the branch point z = 0. This is commonly done by introducing a branch cut; in this case the "cut" might extend from the point z = 0 along the positive real axis to the point at infinity, so that the argument of the variable z in the cut plane is restricted to the range 0 ≤ arg(z) < 2π.

We can now give a complete description of w = z1⁄2. To do so we need two copies of the z-plane, each of them cut along the real axis. On one copy we define the square root of 1 to be e0 = 1, and on the other we define the square root of 1 to be eiπ = −1. We call these two copies of the complete cut plane sheets. By making a continuity argument we see that the (now single-valued) function w = z1⁄2 maps the first sheet into the upper half of the w-plane, where 0 ≤ arg(w) < π, while mapping the second sheet into the lower half of the w-plane (where π ≤ arg(w) < 2π).

The branch cut in this example doesn't have to lie along the real axis. It doesn't even have to be a straight line. Any continuous curve connecting the origin z = 0 with the point at infinity would work. In some cases the branch cut doesn't even have to pass through the point at infinity. For example, consider the relationship

Here the polynomial z2 − 1 vanishes when z = ±1, so g evidently has two branch points. We can "cut" the plane along the real axis, from −1 to 1, and obtain a sheet on which g(z) is a single-valued function. Alternatively, the cut can run from z = 1 along the positive real axis through the point at infinity, then continue "up" the negative real axis to the other branch point, z = −1.

This situation is most easily visualized by using the stereographic projection described above. On the sphere one of these cuts runs longitudinally through the southern hemisphere, connecting a point on the equator (z = −1) with another point on the equator (z = 1), and passing through the south pole (the origin, z = 0) on the way. The second version of the cut runs longitudinally through the northern hemisphere and connects the same two equatorial points by passing through the north pole (that is, the point at infinity).


£#h5#£Restricting the domain of meromorphic functions£#/h5#£
A meromorphic function is a complex function that is holomorphic and therefore analytic everywhere in its domain except at a finite, or countably infinite, number of points. The points at which such a function cannot be defined are called the poles of the meromorphic function. Sometimes all of these poles lie in a straight line. In that case mathematicians may say that the function is "holomorphic on the cut plane". Here's a simple example.

The gamma function, defined by

where γ is the Euler–Mascheroni constant, and has simple poles at 0, −1, −2, −3, ... because exactly one denominator in the infinite product vanishes when z is zero, or a negative integer. Since all its poles lie on the negative real axis, from z = 0 to the point at infinity, this function might be described as "holomorphic on the cut plane, the cut extending along the negative real axis, from 0 (inclusive) to the point at infinity."

Alternatively, Γ(z) might be described as "holomorphic in the cut plane with −π < arg(z) < π and excluding the point z = 0."

This cut is slightly different from the branch cut we've already encountered, because it actually excludes the negative real axis from the cut plane. The branch cut left the real axis connected with the cut plane on one side (0 ≤ θ), but severed it from the cut plane along the other side (θ < 2π).

Of course, it's not actually necessary to exclude the entire line segment from z = 0 to −∞ to construct a domain in which Γ(z) is holomorphic. All we really have to do is puncture the plane at a countably infinite set of points {0, −1, −2, −3, ...}. But a closed contour in the punctured plane might encircle one or more of the poles of Γ(z), giving a contour integral that is not necessarily zero, by the residue theorem. By cutting the complex plane we ensure not only that Γ(z) is holomorphic in this restricted domain – we also ensure that the contour integral of Γ over any closed curve lying in the cut plane is identically equal to zero.


£#h5#£Specifying convergence regions£#/h5#£
Many complex functions are defined by infinite series, or by continued fractions. A fundamental consideration in the analysis of these infinitely long expressions is identifying the portion of the complex plane in which they converge to a finite value. A cut in the plane may facilitate this process, as the following examples show.

Consider the function defined by the infinite series

Since z2 = (−z)2 for every complex number z, it's clear that f(z) is an even function of z, so the analysis can be restricted to one half of the complex plane. And since the series is undefined when

it makes sense to cut the plane along the entire imaginary axis and establish the convergence of this series where the real part of z is not zero before undertaking the more arduous task of examining f(z) when z is a pure imaginary number.

In this example the cut is a mere convenience, because the points at which the infinite sum is undefined are isolated, and the cut plane can be replaced with a suitably punctured plane. In some contexts the cut is necessary, and not just convenient. Consider the infinite periodic continued fraction

It can be shown that f(z) converges to a finite value if and only if z is not a negative real number such that z < −1⁄4. In other words, the convergence region for this continued fraction is the cut plane, where the cut runs along the negative real axis, from −1⁄4 to the point at infinity.


£#h5#£Gluing the cut plane back together£#/h5#£
We have already seen how the relationship

can be made into a single-valued function by splitting the domain of f into two disconnected sheets. It is also possible to "glue" those two sheets back together to form a single Riemann surface on which f(z) = z1/2 can be defined as a holomorphic function whose image is the entire w-plane (except for the point w = 0). Here's how that works.

Imagine two copies of the cut complex plane, the cuts extending along the positive real axis from z = 0 to the point at infinity. On one sheet define 0 ≤ arg(z) < 2π, so that 11/2 = e0 = 1, by definition. On the second sheet define 2π ≤ arg(z) < 4π, so that 11/2 = eiπ = −1, again by definition. Now flip the second sheet upside down, so the imaginary axis points in the opposite direction of the imaginary axis on the first sheet, with both real axes pointing in the same direction, and "glue" the two sheets together (so that the edge on the first sheet labeled "θ = 0" is connected to the edge labeled "θ < 4π" on the second sheet, and the edge on the second sheet labeled "θ = 2π" is connected to the edge labeled "θ < 2π" on the first sheet). The result is the Riemann surface domain on which f(z) = z1/2 is single-valued and holomorphic (except when z = 0).

To understand why f is single-valued in this domain, imagine a circuit around the unit circle, starting with z = 1 on the first sheet. When 0 ≤ θ < 2π we are still on the first sheet. When θ = 2π we have crossed over onto the second sheet, and are obliged to make a second complete circuit around the branch point z = 0 before returning to our starting point, where θ = 4π is equivalent to θ = 0, because of the way we glued the two sheets together. In other words, as the variable z makes two complete turns around the branch point, the image of z in the w-plane traces out just one complete circle.

Formal differentiation shows that

from which we can conclude that the derivative of f exists and is finite everywhere on the Riemann surface, except when z = 0 (that is, f is holomorphic, except when z = 0).

How can the Riemann surface for the function

also discussed above, be constructed? Once again we begin with two copies of the z-plane, but this time each one is cut along the real line segment extending from z = −1 to z = 1 – these are the two branch points of g(z). We flip one of these upside down, so the two imaginary axes point in opposite directions, and glue the corresponding edges of the two cut sheets together. We can verify that g is a single-valued function on this surface by tracing a circuit around a circle of unit radius centered at z = 1. Commencing at the point z = 2 on the first sheet we turn halfway around the circle before encountering the cut at z = 0. The cut forces us onto the second sheet, so that when z has traced out one full turn around the branch point z = 1, w has taken just one-half of a full turn, the sign of w has been reversed (since eiπ = −1), and our path has taken us to the point z = 2 on the second sheet of the surface. Continuing on through another half turn we encounter the other side of the cut, where z = 0, and finally reach our starting point (z = 2 on the first sheet) after making two full turns around the branch point.

The natural way to label θ = arg(z) in this example is to set −π < θ ≤ π on the first sheet, with π < θ ≤ 3π on the second. The imaginary axes on the two sheets point in opposite directions so that the counterclockwise sense of positive rotation is preserved as a closed contour moves from one sheet to the other (remember, the second sheet is upside down). Imagine this surface embedded in a three-dimensional space, with both sheets parallel to the xy-plane. Then there appears to be a vertical hole in the surface, where the two cuts are joined together. What if the cut is made from z = −1 down the real axis to the point at infinity, and from z = 1, up the real axis until the cut meets itself? Again a Riemann surface can be constructed, but this time the "hole" is horizontal. Topologically speaking, both versions of this Riemann surface are equivalent – they are orientable two-dimensional surfaces of genus one.


£#h5#£Use in control theory£#/h5#£
In control theory, one use of the complex plane is known as the s-plane. It is used to visualise the roots of the equation describing a system's behaviour (the characteristic equation) graphically. The equation is normally expressed as a polynomial in the parameter 's' of the Laplace transform, hence the name 's' plane. Points in the s-plane take the form ${\displaystyle s=\sigma +j\omega }$ , where 'j' is used instead of the usual 'i' to represent the imaginary component.

Another related use of the complex plane is with the Nyquist stability criterion. This is a geometric principle which allows the stability of a closed-loop feedback system to be determined by inspecting a Nyquist plot of its open-loop magnitude and phase response as a function of frequency (or loop transfer function) in the complex plane.

The z-plane is a discrete-time version of the s-plane, where z-transforms are used instead of the Laplace transformation.


£#h5#£Quadratic spaces£#/h5#£
The complex plane is associated with two distinct quadratic spaces. For a point z = x + iy in the complex plane, the squaring function z2 and the norm-squared ${\displaystyle x^{2}+y^{2}}$ are both quadratic forms. The former is frequently neglected in the wake of the latter's use in setting a metric on the complex plane. These distinct faces of the complex plane as a quadratic space arise in the construction of algebras over a field with the Cayley–Dickson process. That procedure can be applied to any field, and different results occur for the fields R and C: when R is the take-off field, then C is constructed with the quadratic form ${\displaystyle x^{2}+y^{2},}$ but the process can also begin with C and z2, and that case generates algebras that differ from those derived from R. In any case, the algebras generated are composition algebras; in this case the complex plane is the point set for two distinct composition algebras.


£#h5#£Other meanings of "complex plane"£#/h5#£
The preceding sections of this article deal with the complex plane in terms of a geometric representation of the complex numbers. Although this usage of the term "complex plane" has a long and mathematically rich history, it is by no means the only mathematical concept that can be characterized as "the complex plane". There are at least three additional possibilities.

£#li#£Two-dimensional complex vector space, a "complex plane" in the sense that it is a two-dimensional vector space whose coordinates are complex numbers. See also: Complex affine space § Two dimensions.£#/li#£ £#li#£(1 + 1)-dimensional Minkowski space, also known as the split-complex plane, is a "complex plane" in the sense that the algebraic split-complex numbers can be separated into two real components that are easily associated with the point (x, y) in the Cartesian plane.£#/li#£ £#li#£The set of dual numbers over the reals can also be placed into one-to-one correspondence with the points (x, y) of the Cartesian plane, and represent another example of a "complex plane".£#/li#£

£#h5#£See also£#/h5#£ £#ul#££#li#£Constellation diagram£#/li#£ £#li#£Riemann sphere£#/li#£ £#li#£s-plane£#/li#£ £#li#£In-phase and quadrature components£#/li#£ £#li#£Real line£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£
£#h5#£Works Cited£#/h5#£ £#ul#££#li#£Flanigan, Francis J. (1983). Complex Variables: Harmonic and Analytic Functions. Dover. ISBN 0-486-61388-7.£#/li#£ £#li#£Moretti, Gino (1964). Functions of a Complex Variable. Prentice-Hall.£#/li#£ £#li#£Wall, H. S. (1948). Analytic Theory of Continued Fractions. D. Van Nostrand Company. Reprinted (1973) by Chelsea Publishing Company ISBN 0-8284-0207-8.£#/li#£ £#li#£Whittaker, E. T.; Watson, G. N. (1927). A Course in Modern Analysis (Fourth ed.). Cambridge University Press.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Argand Diagram". MathWorld.£#/li#£ £#li#£Jean-Robert Argand, "Essai sur une manière de représenter des quantités imaginaires dans les constructions géométriques", 1806, online and analyzed on BibNum [for English version, click 'à télécharger']£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Complex Analysis > Complex Numbers £#/li#££#/ul#£




£#h3#£Affine Connection£#/h3#£

In differential geometry, an affine connection is a geometric object on a smooth manifold which connects nearby tangent spaces, so it permits tangent vector fields to be differentiated as if they were functions on the manifold with values in a fixed vector space. Connections are among the simplest methods of defining differentiation of the sections of vector bundles.

The notion of an affine connection has its roots in 19th-century geometry and tensor calculus, but was not fully developed until the early 1920s, by Élie Cartan (as part of his general theory of connections) and Hermann Weyl (who used the notion as a part of his foundations for general relativity). The terminology is due to Cartan and has its origins in the identification of tangent spaces in Euclidean space Rn by translation: the idea is that a choice of affine connection makes a manifold look infinitesimally like Euclidean space not just smoothly, but as an affine space.

On any manifold of positive dimension there are infinitely many affine connections. If the manifold is further endowed with a metric tensor then there is a natural choice of affine connection, called the Levi-Civita connection. The choice of an affine connection is equivalent to prescribing a way of differentiating vector fields which satisfies several reasonable properties (linearity and the Leibniz rule). This yields a possible definition of an affine connection as a covariant derivative or (linear) connection on the tangent bundle. A choice of affine connection is also equivalent to a notion of parallel transport, which is a method for transporting tangent vectors along curves. This also defines a parallel transport on the frame bundle. Infinitesimal parallel transport in the frame bundle yields another description of an affine connection, either as a Cartan connection for the affine group or as a principal connection on the frame bundle.

The main invariants of an affine connection are its torsion and its curvature. The torsion measures how closely the Lie bracket of vector fields can be recovered from the affine connection. Affine connections may also be used to define (affine) geodesics on a manifold, generalizing the straight lines of Euclidean space, although the geometry of those straight lines can be very different from usual Euclidean geometry; the main differences are encapsulated in the curvature of the connection.


£#h5#£Motivation and history£#/h5#£
A smooth manifold is a mathematical object which looks locally like a smooth deformation of Euclidean space Rn: for example a smooth curve or surface looks locally like a smooth deformation of a line or a plane. Smooth functions and vector fields can be defined on manifolds, just as they can on Euclidean space, and scalar functions on manifolds can be differentiated in a natural way. However, differentiation of vector fields is less straightforward: this is a simple matter in Euclidean space, because the tangent space of based vectors at a point p can be identified naturally (by translation) with the tangent space at a nearby point q. On a general manifold, there is no such natural identification between nearby tangent spaces, and so tangent vectors at nearby points cannot be compared in a well-defined way. The notion of an affine connection was introduced to remedy this problem by connecting nearby tangent spaces. The origins of this idea can be traced back to two main sources: surface theory and tensor calculus.


£#h5#£Motivation from surface theory£#/h5#£
Consider a smooth surface S in 3-dimensional Euclidean space. Near to any point, S can be approximated by its tangent plane at that point, which is an affine subspace of Euclidean space. Differential geometers in the 19th century were interested in the notion of development in which one surface was rolled along another, without slipping or twisting. In particular, the tangent plane to a point of S can be rolled on S: this should be easy to imagine when S is a surface like the 2-sphere, which is the smooth boundary of a convex region. As the tangent plane is rolled on S, the point of contact traces out a curve on S. Conversely, given a curve on S, the tangent plane can be rolled along that curve. This provides a way to identify the tangent planes at different points along the curve: in particular, a tangent vector in the tangent space at one point on the curve is identified with a unique tangent vector at any other point on the curve. These identifications are always given by affine transformations from one tangent plane to another.

This notion of parallel transport of tangent vectors, by affine transformations, along a curve has a characteristic feature: the point of contact of the tangent plane with the surface always moves with the curve under parallel translation (i.e., as the tangent plane is rolled along the surface, the point of contact moves). This generic condition is characteristic of Cartan connections. In more modern approaches, the point of contact is viewed as the origin in the tangent plane (which is then a vector space), and the movement of the origin is corrected by a translation, so that parallel transport is linear, rather than affine.

In the point of view of Cartan connections, however, the affine subspaces of Euclidean space are model surfaces — they are the simplest surfaces in Euclidean 3-space, and are homogeneous under the affine group of the plane — and every smooth surface has a unique model surface tangent to it at each point. These model surfaces are Klein geometries in the sense of Felix Klein's Erlangen programme. More generally, an n-dimensional affine space is a Klein geometry for the affine group Aff(n), the stabilizer of a point being the general linear group GL(n). An affine n-manifold is then a manifold which looks infinitesimally like n-dimensional affine space.


£#h5#£Motivation from tensor calculus£#/h5#£
The second motivation for affine connections comes from the notion of a covariant derivative of vector fields. Before the advent of coordinate-independent methods, it was necessary to work with vector fields by embedding their respective Euclidean vectors into an atlas. These components can be differentiated, but the derivatives do not transform in a manageable way under changes of coordinates. Correction terms were introduced by Elwin Bruno Christoffel (following ideas of Bernhard Riemann) in the 1870s so that the (corrected) derivative of one vector field along another transformed covariantly under coordinate transformations — these correction terms subsequently came to be known as Christoffel symbols.

This idea was developed into the theory of absolute differential calculus (now known as tensor calculus) by Gregorio Ricci-Curbastro and his student Tullio Levi-Civita between 1880 and the turn of the 20th century.

Tensor calculus really came to life, however, with the advent of Albert Einstein's theory of general relativity in 1915. A few years after this, Levi-Civita formalized the unique connection associated to a Riemannian metric, now known as the Levi-Civita connection. More general affine connections were then studied around 1920, by Hermann Weyl, who developed a detailed mathematical foundation for general relativity, and Élie Cartan, who made the link with the geometrical ideas coming from surface theory.


£#h5#£Approaches£#/h5#£
The complex history has led to the development of widely varying approaches to and generalizations of the affine connection concept.

The most popular approach is probably the definition motivated by covariant derivatives. On the one hand, the ideas of Weyl were taken up by physicists in the form of gauge theory and gauge covariant derivatives. On the other hand, the notion of covariant differentiation was abstracted by Jean-Louis Koszul, who defined (linear or Koszul) connections on vector bundles. In this language, an affine connection is simply a covariant derivative or (linear) connection on the tangent bundle.

However, this approach does not explain the geometry behind affine connections nor how they acquired their name. The term really has its origins in the identification of tangent spaces in Euclidean space by translation: this property means that Euclidean n-space is an affine space. (Alternatively, Euclidean space is a principal homogeneous space or torsor under the group of translations, which is a subgroup of the affine group.) As mentioned in the introduction, there are several ways to make this precise: one uses the fact that an affine connection defines a notion of parallel transport of vector fields along a curve. This also defines a parallel transport on the frame bundle. Infinitesimal parallel transport in the frame bundle yields another description of an affine connection, either as a Cartan connection for the affine group Aff(n) or as a principal GL(n) connection on the frame bundle.


£#h5#£Formal definition as a differential operator£#/h5#£
Let M be a smooth manifold and let Γ(TM) be the space of vector fields on M, that is, the space of smooth sections of the tangent bundle TM. Then an affine connection on M is a bilinear map

${\displaystyle {\begin{aligned}\Gamma (\mathrm {T} M)\times \Gamma (\mathrm {T} M)&\rightarrow \Gamma (\mathrm {T} M)\\(X,Y)&\mapsto \nabla _{X}Y\,,\end{aligned}}}$
such that for all f in the set of smooth functions on M, written C∞(M, R), and all vector fields X, Y on M:

£#li#£∇fXY = f ∇XY, that is, ∇ is C∞(M, R)-linear in the first variable;£#/li#£ £#li#£∇X(fY) = ∂X f Y + f ∇XY, where ∂X denotes the directional derivative; that is, ∇ satisfies Leibniz rule in the second variable.£#/li#£

£#h5#£Elementary properties£#/h5#£ £#ul#££#li#£It follows from property 1 above that the value of ∇XY at a point x ∈ M depends only on the value of X at x and not on the value of X on M − {x}. It also follows from property 2 above that the value of ∇XY at a point x ∈ M depends only on the value of Y on a neighbourhood of x.£#/li#£ £#li#£If ∇1, ∇2 are affine connections then the value at x of ∇1
XY − ∇2
XY may be written Γx(Xx, Yx) where is bilinear and depends smoothly on x (i.e., it defines a smooth bundle homomorphism). Conversely if ∇ is an affine connection and Γ is such a smooth bilinear bundle homomorphism (called a connection form on M) then ∇ + Γ is an affine connection.£#/li#£ £#li#£If M is an open subset of Rn, then the tangent bundle of M is the trivial bundle M × Rn. In this situation there is a canonical affine connection d on M: any vector field Y is given by a smooth function V from M to Rn; then dXY is the vector field corresponding to the smooth function dV(X) = ∂XY from M to Rn. Any other affine connection ∇ on M may therefore be written ∇ = d + Γ, where Γ is a connection form on M.£#/li#£ £#li#£More generally, a local trivialization of the tangent bundle is a bundle isomorphism between the restriction of TM to an open subset U of M, and U × Rn. The restriction of an affine connection ∇ to U may then be written in the form d + Γ where Γ is a connection form on U.£#/li#££#/ul#£
£#h5#£Parallel transport for affine connections£#/h5#£
Comparison of tangent vectors at different points on a manifold is generally not a well-defined process. An affine connection provides one way to remedy this using the notion of parallel transport, and indeed this can be used to give a definition of an affine connection.

Let M be a manifold with an affine connection ∇. Then a vector field X is said to be parallel if ∇X = 0 in the sense that for any vector field Y, ∇YX = 0. Intuitively speaking, parallel vectors have all their derivatives equal to zero and are therefore in some sense constant. By evaluating a parallel vector field at two points x and y, an identification between a tangent vector at x and one at y is obtained. Such tangent vectors are said to be parallel transports of each other.

Nonzero parallel vector fields do not, in general, exist, because the equation ∇X = 0 is a partial differential equation which is overdetermined: the integrability condition for this equation is the vanishing of the curvature of ∇ (see below). However, if this equation is restricted to a curve from x to y it becomes an ordinary differential equation. There is then a unique solution for any initial value of X at x.

More precisely, if γ : I → M a smooth curve parametrized by an interval [a, b] and ξ ∈ TxM, where x = γ(a), then a vector field X along γ (and in particular, the value of this vector field at y = γ(b)) is called the parallel transport of ξ along γ if

£#li#£∇γ′(t)X = 0, for all t ∈ [a, b]£#/li#£ £#li#£Xγ(a) = ξ.£#/li#£
Formally, the first condition means that X is parallel with respect to the pullback connection on the pullback bundle γ ∗ TM. However, in a local trivialization it is a first-order system of linear ordinary differential equations, which has a unique solution for any initial condition given by the second condition (for instance, by the Picard–Lindelöf theorem).

Thus parallel transport provides a way of moving tangent vectors along a curve using the affine connection to keep them "pointing in the same direction" in an intuitive sense, and this provides a linear isomorphism between the tangent spaces at the two ends of the curve. The isomorphism obtained in this way will in general depend on the choice of the curve: if it does not, then parallel transport along every curve can be used to define parallel vector fields on M, which can only happen if the curvature of ∇ is zero.

A linear isomorphism is determined by its action on an ordered basis or frame. Hence parallel transport can also be characterized as a way of transporting elements of the (tangent) frame bundle GL(M) along a curve. In other words, the affine connection provides a lift of any curve γ in M to a curve γ̃ in GL(M).


£#h5#£Formal definition on the frame bundle£#/h5#£
An affine connection may also be defined as a principal GL(n) connection ω on the frame bundle FM or GL(M) of a manifold M. In more detail, ω is a smooth map from the tangent bundle T(FM) of the frame bundle to the space of n × n matrices (which is the Lie algebra gl(n) of the Lie group GL(n) of invertible n × n matrices) satisfying two properties:

£#li#£ω is equivariant with respect to the action of GL(n) on T(FM) and gl(n);£#/li#£ £#li#£ω(Xξ) = ξ for any ξ in gl(n), where Xξ is the vector field on FM corresponding to ξ.£#/li#£
Such a connection ω immediately defines a covariant derivative not only on the tangent bundle, but on vector bundles associated to any group representation of GL(n), including bundles of tensors and tensor densities. Conversely, an affine connection on the tangent bundle determines an affine connection on the frame bundle, for instance, by requiring that ω vanishes on tangent vectors to the lifts of curves to the frame bundle defined by parallel transport.

The frame bundle also comes equipped with a solder form θ : T(FM) → Rn which is horizontal in the sense that it vanishes on vertical vectors such as the point values of the vector fields Xξ: indeed θ is defined first by projecting a tangent vector (to FM at a frame f) to M, then by taking the components of this tangent vector on M with respect to the frame f. Note that θ is also GL(n)-equivariant (where GL(n) acts on Rn by matrix multiplication).

The pair (θ, ω) defines a bundle isomorphism of T(FM) with the trivial bundle FM × aff(n), where aff(n) is the Cartesian product of Rn and gl(n) (viewed as the Lie algebra of the affine group, which is actually a semidirect product – see below).


£#h5#£Affine connections as Cartan connections£#/h5#£
Affine connections can be defined within Cartan's general framework. In the modern approach, this is closely related to the definition of affine connections on the frame bundle. Indeed, in one formulation, a Cartan connection is an absolute parallelism of a principal bundle satisfying suitable properties. From this point of view the aff(n)-valued one-form (θ, ω) : T(FM) → aff(n) on the frame bundle (of an affine manifold) is a Cartan connection. However, Cartan's original approach was different from this in a number of ways:

£#ul#££#li#£the concept of frame bundles or principal bundles did not exist;£#/li#£ £#li#£a connection was viewed in terms of parallel transport between infinitesimally nearby points;£#/li#£ £#li#£this parallel transport was affine, rather than linear;£#/li#£ £#li#£the objects being transported were not tangent vectors in the modern sense, but elements of an affine space with a marked point, which the Cartan connection ultimately identifies with the tangent space.£#/li#££#/ul#£
£#h5#£Explanations and historical intuition£#/h5#£
The points just raised are easiest to explain in reverse, starting from the motivation provided by surface theory. In this situation, although the planes being rolled over the surface are tangent planes in a naive sense, the notion of a tangent space is really an infinitesimal notion, whereas the planes, as affine subspaces of R3, are infinite in extent. However these affine planes all have a marked point, the point of contact with the surface, and they are tangent to the surface at this point. The confusion therefore arises because an affine space with a marked point can be identified with its tangent space at that point. However, the parallel transport defined by rolling does not fix this origin: it is affine rather than linear; the linear parallel transport can be recovered by applying a translation.

Abstracting this idea, an affine manifold should therefore be an n-manifold M with an affine space Ax, of dimension n, attached to each x ∈ M at a marked point ax ∈ Ax, together with a method for transporting elements of these affine spaces along any curve C in M. This method is required to satisfy several properties:

£#li#£for any two points x, y on C, parallel transport is an affine transformation from Ax to Ay;£#/li#£ £#li#£parallel transport is defined infinitesimally in the sense that it is differentiable at any point on C and depends only on the tangent vector to C at that point;£#/li#£ £#li#£the derivative of the parallel transport at x determines a linear isomorphism from TxM to TaxAx.£#/li#£
These last two points are quite hard to make precise, so affine connections are more often defined infinitesimally. To motivate this, it suffices to consider how affine frames of reference transform infinitesimally with respect to parallel transport. (This is the origin of Cartan's method of moving frames.) An affine frame at a point consists of a list (p, e1,… en), where p ∈ Ax and the ei form a basis of Tp(Ax). The affine connection is then given symbolically by a first order differential system

${\displaystyle (*){\begin{cases}\mathrm {d} {p}&=\theta ^{1}\mathbf {e} _{1}+\cdots +\theta ^{n}\mathbf {e} _{n}\\\mathrm {d} \mathbf {e} _{i}&=\omega _{i}^{1}\mathbf {e} _{1}+\cdots +\omega _{i}^{n}\mathbf {e} _{n}\end{cases}}\quad i=1,2,\ldots ,n}$
defined by a collection of one-forms (θ j, ω j
i). Geometrically, an affine frame undergoes a displacement travelling along a curve γ from γ(t) to γ(t + δt) given (approximately, or infinitesimally) by

${\displaystyle {\begin{aligned}p(\gamma (t+\delta t))-p(\gamma (t))&=\left(\theta ^{1}\left(\gamma '(t)\right)\mathbf {e} _{1}+\cdots +\theta ^{n}\left(\gamma '(t)\right)\mathbf {e} _{n}\right)\mathrm {\delta } t\\\mathbf {e} _{i}(\gamma (t+\delta t))-\mathbf {e} _{i}(\gamma (t))&=\left(\omega _{i}^{1}\left(\gamma '(t)\right)\mathbf {e} _{1}+\cdots +\omega _{i}^{n}\left(\gamma '(t)\right)\mathbf {e} _{n}\right)\delta t\,.\end{aligned}}}$
Furthermore, the affine spaces Ax are required to be tangent to M in the informal sense that the displacement of ax along γ can be identified (approximately or infinitesimally) with the tangent vector γ′(t) to γ at x = γ(t) (which is the infinitesimal displacement of x). Since

${\displaystyle a_{x}(\gamma (t+\delta t))-a_{x}(\gamma (t))=\theta \left(\gamma '(t)\right)\delta t\,,}$
where θ is defined by θ(X) = θ1(X)e1 + … + θn(X)en, this identification is given by θ, so the requirement is that θ should be a linear isomorphism at each point.

The tangential affine space Ax is thus identified intuitively with an infinitesimal affine neighborhood of x.

The modern point of view makes all this intuition more precise using principal bundles (the essential idea is to replace a frame or a variable frame by the space of all frames and functions on this space). It also draws on the inspiration of Felix Klein's Erlangen programme, in which a geometry is defined to be a homogeneous space. Affine space is a geometry in this sense, and is equipped with a flat Cartan connection. Thus a general affine manifold is viewed as curved deformation of the flat model geometry of affine space.


£#h5#£Affine space as the flat model geometry£#/h5#£
£#h5#£Definition of an affine space£#/h5#£
Informally, an affine space is a vector space without a fixed choice of origin. It describes the geometry of points and free vectors in space. As a consequence of the lack of origin, points in affine space cannot be added together as this requires a choice of origin with which to form the parallelogram law for vector addition. However, a vector v may be added to a point p by placing the initial point of the vector at p and then transporting p to the terminal point. The operation thus described p → p + v is the translation of p along v. In technical terms, affine n-space is a set An equipped with a free transitive action of the vector group Rn on it through this operation of translation of points: An is thus a principal homogeneous space for the vector group Rn.

The general linear group GL(n) is the group of transformations of Rn which preserve the linear structure of Rn in the sense that T(av + bw) = aT(v) + bT(w). By analogy, the affine group Aff(n) is the group of transformations of An preserving the affine structure. Thus φ ∈ Aff(n) must preserve translations in the sense that

${\displaystyle \varphi (p+v)=\varphi (p)+T(v)}$
where T is a general linear transformation. The map sending φ ∈ Aff(n) to T ∈ GL(n) is a group homomorphism. Its kernel is the group of translations Rn. The stabilizer of any point p in A can thus be identified with GL(n) using this projection: this realises the affine group as a semidirect product of GL(n) and Rn, and affine space as the homogeneous space Aff(n)/GL(n).


£#h5#£Affine frames and the flat affine connection£#/h5#£
An affine frame for A consists of a point p ∈ A and a basis (e1,… en) of the vector space TpA = Rn. The general linear group GL(n) acts freely on the set FA of all affine frames by fixing p and transforming the basis (e1,… en) in the usual way, and the map π sending an affine frame (p; e1,… en) to p is the quotient map. Thus FA is a principal GL(n)-bundle over A. The action of GL(n) extends naturally to a free transitive action of the affine group Aff(n) on FA, so that FA is an Aff(n)-torsor, and the choice of a reference frame identifies FA → A with the principal bundle Aff(n) → Aff(n)/GL(n).

On FA there is a collection of n + 1 functions defined by

${\displaystyle \pi (p;\mathbf {e} _{1},\dots ,\mathbf {e} _{n})=p}$
(as before) and

${\displaystyle \varepsilon _{i}(p;\mathbf {e} _{1},\dots ,\mathbf {e} _{n})=\mathbf {e} _{i}\,.}$
After choosing a basepoint for A, these are all functions with values in Rn, so it is possible to take their exterior derivatives to obtain differential 1-forms with values in Rn. Since the functions εi yield a basis for Rn at each point of FA, these 1-forms must be expressible as sums of the form

${\displaystyle {\begin{aligned}\mathrm {d} \pi &=\theta ^{1}\varepsilon _{1}+\cdots +\theta ^{n}\varepsilon _{n}\\\mathrm {d} \varepsilon _{i}&=\omega _{i}^{1}\varepsilon _{1}+\cdots +\omega _{i}^{n}\varepsilon _{n}\end{aligned}}}$
for some collection (θ i, ω k
j)1 ≤ i, j, k ≤ n of real-valued one-forms on Aff(n). This system of one-forms on the principal bundle FA → A defines the affine connection on A.

Taking the exterior derivative a second time, and using the fact that d2 = 0 as well as the linear independence of the εi, the following relations are obtained:

${\displaystyle {\begin{aligned}\mathrm {d} \theta ^{j}-\sum _{i}\omega _{i}^{j}\wedge \theta ^{i}&=0\\\mathrm {d} \omega _{i}^{j}-\sum _{k}\omega _{k}^{j}\wedge \omega _{i}^{k}&=0\,.\end{aligned}}}$
These are the Maurer–Cartan equations for the Lie group Aff(n) (identified with FA by the choice of a reference frame). Furthermore:

£#ul#££#li#£the Pfaffian system θ j = 0 (for all j) is integrable, and its integral manifolds are the fibres of the principal bundle Aff(n) → A.£#/li#£ £#li#£the Pfaffian system ω j
i = 0 (for all i, j) is also integrable, and its integral manifolds define parallel transport in FA.£#/li#££#/ul#£
Thus the forms (ω j
i) define a flat principal connection on FA → A.

For a strict comparison with the motivation, one should actually define parallel transport in a principal Aff(n)-bundle over A. This can be done by pulling back FA by the smooth map φ : Rn × A → A defined by translation. Then the composite φ′ ∗ FA → FA → A is a principal Aff(n)-bundle over A, and the forms (θ i, ω k
j) pull back to give a flat principal Aff(n)-connection on this bundle.


£#h5#£General affine geometries: formal definitions£#/h5#£
An affine space, as with essentially any smooth Klein geometry, is a manifold equipped with a flat Cartan connection. More general affine manifolds or affine geometries are obtained easily by dropping the flatness condition expressed by the Maurer-Cartan equations. There are several ways to approach the definition and two will be given. Both definitions are facilitated by the realisation that 1-forms (θ i, ω k
j) in the flat model fit together to give a 1-form with values in the Lie algebra aff(n) of the affine group Aff(n).

In these definitions, M is a smooth n-manifold and A = Aff(n)/GL(n) is an affine space of the same dimension.


£#h5#£Definition via absolute parallelism£#/h5#£
Let M be a manifold, and P a principal GL(n)-bundle over M. Then an affine connection is a 1-form η on P with values in aff(n) satisfying the following properties

£#li#£η is equivariant with respect to the action of GL(n) on P and aff(n);£#/li#£ £#li#£η(Xξ) = ξ for all ξ in the Lie algebra gl(n) of all n × n matrices;£#/li#£ £#li#£η is a linear isomorphism of each tangent space of P with aff(n).£#/li#£
The last condition means that η is an absolute parallelism on P, i.e., it identifies the tangent bundle of P with a trivial bundle (in this case P × aff(n)). The pair (P, η) defines the structure of an affine geometry on M, making it into an affine manifold.

The affine Lie algebra aff(n) splits as a semidirect product of Rn and gl(n) and so η may be written as a pair (θ, ω) where θ takes values in Rn and ω takes values in gl(n). Conditions 1 and 2 are equivalent to ω being a principal GL(n)-connection and θ being a horizontal equivariant 1-form, which induces a bundle homomorphism from TM to the associated bundle P ×GL(n) Rn. Condition 3 is equivalent to the fact that this bundle homomorphism is an isomorphism. (However, this decomposition is a consequence of the rather special structure of the affine group.) Since P is the frame bundle of P ×GL(n) Rn, it follows that θ provides a bundle isomorphism between P and the frame bundle FM of M; this recovers the definition of an affine connection as a principal GL(n)-connection on FM.

The 1-forms arising in the flat model are just the components of θ and ω.


£#h5#£Definition as a principal affine connection£#/h5#£
An affine connection on M is a principal Aff(n)-bundle Q over M, together with a principal GL(n)-subbundle P of Q and a principal Aff(n)-connection α (a 1-form on Q with values in aff(n)) which satisfies the following (generic) Cartan condition. The Rn component of pullback of α to P is a horizontal equivariant 1-form and so defines a bundle homomorphism from TM to P ×GL(n) Rn: this is required to be an isomorphism.


£#h5#£Relation to the motivation£#/h5#£
Since Aff(n) acts on A, there is, associated to the principal bundle Q, a bundle A = Q ×Aff(n) A, which is a fiber bundle over M whose fiber at x in M is an affine space Ax. A section a of A (defining a marked point ax in Ax for each x ∈ M) determines a principal GL(n)-subbundle P of Q (as the bundle of stabilizers of these marked points) and vice versa. The principal connection α defines an Ehresmann connection on this bundle, hence a notion of parallel transport. The Cartan condition ensures that the distinguished section a always moves under parallel transport.


£#h5#£Further properties£#/h5#£
£#h5#£Curvature and torsion£#/h5#£
Curvature and torsion are the main invariants of an affine connection. As there are many equivalent ways to define the notion of an affine connection, so there are many different ways to define curvature and torsion.

From the Cartan connection point of view, the curvature is the failure of the affine connection η to satisfy the Maurer–Cartan equation

${\displaystyle \mathrm {d} \eta +{\tfrac {1}{2}}[\eta \wedge \eta ]=0,}$
where the second term on the left hand side is the wedge product using the Lie bracket in aff(n) to contract the values. By expanding η into the pair (θ, ω) and using the structure of the Lie algebra aff(n), this left hand side can be expanded into the two formulae

${\displaystyle \mathrm {d} \theta +\omega \wedge \theta \quad {\text{and}}\quad \mathrm {d} \omega +\omega \wedge \omega \,,}$
where the wedge products are evaluated using matrix multiplication. The first expression is called the torsion of the connection, and the second is also called the curvature.

These expressions are differential 2-forms on the total space of a frame bundle. However, they are horizontal and equivariant, and hence define tensorial objects. These can be defined directly from the induced covariant derivative ∇ on TM as follows.

The torsion is given by the formula

${\displaystyle T^{\nabla }(X,Y)=\nabla _{X}Y-\nabla _{Y}X-[X,Y].}$
If the torsion vanishes, the connection is said to be torsion-free or symmetric.

The curvature is given by the formula

${\displaystyle R_{X,Y}^{\nabla }Z=\nabla _{X}\nabla _{Y}Z-\nabla _{Y}\nabla _{X}Z-\nabla _{[X,Y]}Z.}$
Note that [X, Y] is the Lie bracket of vector fields

${\displaystyle [X,Y]=\left(X^{j}\partial _{j}Y^{i}-Y^{j}\partial _{j}X^{i}\right)\partial _{i}}$
in Einstein notation. This is independent of coordinate system choice and

${\displaystyle \partial _{i}=\left({\frac {\partial }{\partial \xi ^{i}}}\right)_{p}\,,}$
the tangent vector at point p of the ith coordinate curve. The ∂i are a natural basis for the tangent space at point p, and the X i the corresponding coordinates for the vector field X = X i ∂i.

When both curvature and torsion vanish, the connection defines a pre-Lie algebra structure on the space of global sections of the tangent bundle.


£#h5#£The Levi-Civita connection£#/h5#£
If (M, g) is a Riemannian manifold then there is a unique affine connection ∇ on M with the following two properties:

£#ul#££#li#£the connection is torsion-free, i.e., T∇ is zero, so that ∇XY − ∇YX = [X, Y];£#/li#£ £#li#£parallel transport is an isometry, i.e., the inner products (defined using g) between tangent vectors are preserved.£#/li#££#/ul#£
This connection is called the Levi-Civita connection.

The term "symmetric" is often used instead of torsion-free for the first property. The second condition means that the connection is a metric connection in the sense that the Riemannian metric g is parallel: ∇g = 0. For a torsion-free connection, the condition is equivalent to the identity X g(Y, Z) = g(∇XY, Z) + g(Y, ∇X Z), "compatibility with the metric". In local coordinates the components of the form are called Christoffel symbols: because of the uniqueness of the Levi-Civita connection, there is a formula for these components in terms of the components of g.


£#h5#£Geodesics£#/h5#£
Since straight lines are a concept in affine geometry, affine connections define a generalized notion of (parametrized) straight lines on any affine manifold, called affine geodesics. Abstractly, a parametric curve γ : I → M is a straight line if its tangent vector remains parallel and equipollent with itself when it is transported along γ. From the linear point of view, an affine connection M distinguishes the affine geodesics in the following way: a smooth curve γ : I → M is an affine geodesic if γ̇ is parallel transported along γ, that is

${\displaystyle \tau _{t}^{s}{\dot {\gamma }}(s)={\dot {\gamma }}(t)}$
where τs
t : TγsM → TγtM is the parallel transport map defining the connection.

In terms of the infinitesimal connection ∇, the derivative of this equation implies

${\displaystyle \nabla _{{\dot {\gamma }}(t)}{\dot {\gamma }}(t)=0}$
for all t ∈ I.

Conversely, any solution of this differential equation yields a curve whose tangent vector is parallel transported along the curve. For every x ∈ M and every X ∈ TxM, there exists a unique affine geodesic γ : I → M with γ(0) = x and γ̇(0) = X and where I is the maximal open interval in R, containing 0, on which the geodesic is defined. This follows from the Picard–Lindelöf theorem, and allows for the definition of an exponential map associated to the affine connection.

In particular, when M is a (pseudo-)Riemannian manifold and ∇ is the Levi-Civita connection, then the affine geodesics are the usual geodesics of Riemannian geometry and are the locally distance minimizing curves.

The geodesics defined here are sometimes called affinely parametrized, since a given straight line in M determines a parametric curve γ through the line up to a choice of affine reparametrization γ(t) → γ(at + b), where a and b are constants. The tangent vector to an affine geodesic is parallel and equipollent along itself. An unparametrized geodesic, or one which is merely parallel along itself without necessarily being equipollent, need only satisfy

${\displaystyle \nabla _{\dot {\gamma }}{\dot {\gamma }}=k{\dot {\gamma }}}$
for some function k defined along γ. Unparametrized geodesics are often studied from the point of view of projective connections.


£#h5#£Development£#/h5#£
An affine connection defines a notion of development of curves. Intuitively, development captures the notion that if xt is a curve in M, then the affine tangent space at x0 may be rolled along the curve. As it does so, the marked point of contact between the tangent space and the manifold traces out a curve Ct in this affine space: the development of xt.

In formal terms, let τ0
t : TxtM → Tx0M be the linear parallel transport map associated to the affine connection. Then the development Ct is the curve in Tx0M starts off at 0 and is parallel to the tangent of xt for all time t:

${\displaystyle {\dot {C}}_{t}=\tau _{t}^{0}{\dot {x}}_{t}\,,\quad C_{0}=0.}$
In particular, xt is a geodesic if and only if its development is an affinely parametrized straight line in Tx0M.


£#h5#£Surface theory revisited£#/h5#£
If M is a surface in R3, it is easy to see that M has a natural affine connection. From the linear connection point of view, the covariant derivative of a vector field is defined by differentiating the vector field, viewed as a map from M to R3, and then projecting the result orthogonally back onto the tangent spaces of M. It is easy to see that this affine connection is torsion-free. Furthermore, it is a metric connection with respect to the Riemannian metric on M induced by the inner product on R3, hence it is the Levi-Civita connection of this metric.


£#h5#£Example: the unit sphere in Euclidean space£#/h5#£
Let ⟨ , ⟩ be the usual scalar product on R3, and let S2 be the unit sphere. The tangent space to S2 at a point x is naturally identified with the vector subspace of R3 consisting of all vectors orthogonal to x. It follows that a vector field Y on S2 can be seen as a map Y : S2 → R3 which satisfies

${\displaystyle \langle Y_{x},x\rangle =0\,,\quad \forall x\in \mathbf {S} ^{2}.}$
Denote as dY the differential (Jacobian matrix) of such a map. Then we have:

Lemma. The formula
${\displaystyle (\nabla _{Z}Y)_{x}=\mathrm {d} Y_{x}(Z_{x})+\langle Z_{x},Y_{x}\rangle x}$
defines an affine connection on S2 with vanishing torsion.
Proof. It is straightforward to prove that ∇ satisfies the Leibniz identity and is C∞(S2) linear in the first variable. So all that needs to be proved here is that the map above does indeed define a tangent vector field. That is, we need to prove that for all x in S2
${\displaystyle {\bigl \langle }(\nabla _{Z}Y)_{x},x{\bigr \rangle }=0\,.\qquad {\text{(Eq.1)}}}$
Consider the map
${\displaystyle {\begin{aligned}f:\mathbf {S} ^{2}&\to \mathbf {R} \\x&\mapsto \langle Y_{x},x\rangle \,.\end{aligned}}}$
The map f is constant, hence its differential vanishes. In particular
${\displaystyle \mathrm {d} f_{x}(Z_{x})={\bigl \langle }(\mathrm {d} Y)_{x}(Z_{x}),x(\gamma '(t)){\bigr \rangle }+\langle Y_{x},Z_{x}\rangle =0\,.}$
Equation 1 above follows. Q.E.D.

£#h5#£See also£#/h5#£ £#ul#££#li#£Atlas (topology)£#/li#£ £#li#£Connection (mathematics)£#/li#£ £#li#£Connection (fibred manifold)£#/li#£ £#li#£Connection (affine bundle)£#/li#£ £#li#£Differentiable manifold£#/li#£ £#li#£Differential geometry£#/li#£ £#li#£Introduction to the mathematics of general relativity£#/li#£ £#li#£Levi-Civita connection£#/li#£ £#li#£List of formulas in Riemannian geometry£#/li#£ £#li#£Riemannian geometry£#/li#££#/ul#£
£#h5#£Notes£#/h5#£



£#h5#£Citations£#/h5#£
£#h5#£References£#/h5#£
£#h5#£Bibliography£#/h5#£
£#h5#£Primary historical references£#/h5#£
£#h5#£Secondary references£#/h5#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Differential Geometry > Tensor Analysis £#/li#££#li#£ Calculus and Analysis > Differential Geometry > General Differential Geometry £#/li#££#/ul#£




£#h3#£Affine Curvature£#/h3#£

Special affine curvature, also known as the equiaffine curvature or affine curvature, is a particular type of curvature that is defined on a plane curve that remains unchanged under a special affine transformation (an affine transformation that preserves area). The curves of constant equiaffine curvature k are precisely all non-singular plane conics. Those with k > 0 are ellipses, those with k = 0 are parabolae, and those with k < 0 are hyperbolae.

The usual Euclidean curvature of a curve at a point is the curvature of its osculating circle, the unique circle making second order contact (having three point contact) with the curve at the point. In the same way, the special affine curvature of a curve at a point P is the special affine curvature of its hyperosculating conic, which is the unique conic making fourth order contact (having five point contact) with the curve at P. In other words it is the limiting position of the (unique) conic through P and four points P1, P2, P3, P4 on the curve, as each of the points approaches P:

${\displaystyle P_{1},P_{2},P_{3},P_{4}\to P.}$
In some contexts, the affine curvature refers to a differential invariant κ of the general affine group, which may readily obtained from the special affine curvature k by κ = k−3/2dk/ds, where s is the special affine arc length. Where the general affine group is not used, the special affine curvature k is sometimes also called the affine curvature.


£#h5#£Formal definition£#/h5#£
£#h5#£Special affine arclength£#/h5#£
To define the special affine curvature, it is necessary first to define the special affine arclength (also called the equiaffine arclength). Consider an affine plane curve β(t). Choose coordinates for the affine plane such that the area of the parallelogram spanned by two vectors a = (a1, a2) and b = (b1, b2) is given by the determinant

${\displaystyle \det {\begin{bmatrix}a&b\end{bmatrix}}=a_{1}b_{2}-a_{2}b_{1}.}$
In particular, the determinant

${\displaystyle \det {\begin{bmatrix}{\dfrac {d\beta }{dt}}&{\dfrac {d^{2}\beta }{dt^{2}}}\end{bmatrix}}}$
is a well-defined invariant of the special affine group, and gives the signed area of the parallelogram spanned by the velocity and acceleration of the curve β. Consider a reparameterization of the curve β, say with a new parameter s related to t by means of a regular reparameterization s = s(t). This determinant undergoes then a transformation of the following sort, by the chain rule:

${\displaystyle {\begin{aligned}\det {\begin{bmatrix}{\dfrac {d\beta }{dt}}&{\dfrac {d^{2}\beta }{dt^{2}}}\end{bmatrix}}&=\det {\begin{bmatrix}{\dfrac {d\beta }{ds}}{\dfrac {ds}{dt}}&\left({\dfrac {d^{2}\beta }{ds^{2}}}\left({\dfrac {ds}{dt}}\right)^{2}+{\dfrac {d\beta }{ds}}{\dfrac {d^{2}s}{dt^{2}}}\right)\end{bmatrix}}\\&=\left({\frac {ds}{dt}}\right)^{3}\det {\begin{bmatrix}{\dfrac {d\beta }{ds}}&{\dfrac {d^{2}\beta }{ds^{2}}}\end{bmatrix}}.\end{aligned}}}$
The reparameterization can be chosen so that

${\displaystyle \det {\begin{bmatrix}{\dfrac {d\beta }{ds}}&{\dfrac {d^{2}\beta }{ds^{2}}}\end{bmatrix}}=1}$
provided the velocity and acceleration, dβ/dt and d2β/dt2 are linearly independent. Existence and uniqueness of such a parameterization follows by integration:

${\displaystyle s(t)=\int _{a}^{t}{\sqrt[{3}]{\det {\begin{bmatrix}{\dfrac {d\beta }{dt}}&{\dfrac {d^{2}\beta }{dt^{2}}}\end{bmatrix}}}}\,\,dt.}$
This integral is called the special affine arclength, and a curve carrying this parameterization is said to be parameterized with respect to its special affine arclength.


£#h5#£Special affine curvature£#/h5#£
Suppose that β(s) is a curve parameterized with its special affine arclength. Then the special affine curvature (or equiaffine curvature) is given by

${\displaystyle k(s)=\det {\begin{bmatrix}\beta ''(s)&\beta '''(s)\end{bmatrix}}.}$
Here β′ denotes the derivative of β with respect to s.

More generally, for a plane curve with arbitrary parameterization

${\displaystyle t\mapsto {\bigl (}x(t),y(t){\bigr )},}$
the special affine curvature is:

${\displaystyle {\begin{aligned}k(t)&={\frac {x''y'''-x'''y''}{\left(x'y''-x''y'\right)^{\frac {5}{3}}}}-{\frac {1}{2}}\left({\frac {1}{\left(x'y''-x''y'\right)^{\frac {2}{3}}}}\right)''\\[6px]&={\frac {4\left(x''y'''-x'''y''\right)+\left(x'y''''-x''''y'\right)}{3\left(x'y''-x''y'\right)^{\frac {5}{3}}}}-{\frac {5\left(x'y'''-x'''y'\right)^{2}}{9\left(x'y''-x''y'\right)^{\frac {8}{3}}}}\end{aligned}}}$
provided the first and second derivatives of the curve are linearly independent. In the special case of a graph y = y(x), these formulas reduce to

${\displaystyle k=-{\frac {1}{2}}\left({\frac {1}{\left(y''\right)^{\frac {2}{3}}}}\right)''={\frac {y''''}{3\left(y''\right)^{\frac {5}{3}}}}-{\frac {5\left(y'''\right)^{2}}{9\left(y''\right)^{\frac {8}{3}}}}}$
where the prime denotes differentiation with respect to x.


£#h5#£Affine curvature£#/h5#£
Suppose as above that β(s) is a curve parameterized by special affine arclength. There are a pair of invariants of the curve that are invariant under the full general affine group — the group of all affine motions of the plane, not just those that are area-preserving. The first of these is

${\displaystyle \sigma =\int {\sqrt {k(s)}}\,ds,}$
sometimes called the affine arclength (although this risks confusion with the special affine arclength described above). The second is referred to as the affine curvature:

${\displaystyle \kappa =k^{-{\frac {3}{2}}}{\frac {dk}{ds}}.}$

£#h5#£Conics£#/h5#£
Suppose that β(s) is a curve parameterized by special affine arclength with constant affine curvature k. Let

${\displaystyle C_{\beta }(s)={\begin{bmatrix}\beta '(s)&\beta ''(s)\end{bmatrix}}.}$
Note that det(Cβ) = 1 since β is assumed to carry the special affine arclength parameterization, and that

${\displaystyle k=\det \left(C_{\beta }'\right).\,}$
It follows from the form of Cβ that

${\displaystyle C_{\beta }'=C_{\beta }{\begin{bmatrix}0&-k\\1&0\end{bmatrix}}.}$
By applying a suitable special affine transformation, we can arrange that Cβ(0) = I is the identity matrix. Since k is constant, it follows that Cβ is given by the matrix exponential

${\displaystyle {\begin{aligned}C_{\beta }(s)&=\exp \left\{s\cdot {\begin{bmatrix}0&-k\\1&0\end{bmatrix}}\right\}\\&={\begin{bmatrix}\cos {\sqrt {k}}\,s&{\sqrt {k}}\sin {\sqrt {k}}\,s\\-{\frac {1}{\sqrt {k}}}\sin {\sqrt {k}}\,s&\cos {\sqrt {k}}\,s\end{bmatrix}}.\end{aligned}}}$
The three cases are now as follows.

k = 0
If the curvature vanishes identically, then upon passing to a limit,
${\displaystyle C_{\beta }(s)={\begin{bmatrix}1&0\\s&1\end{bmatrix}}}$
so β′(s) = (1, s), and so integration gives
${\displaystyle \beta (s)=\left(s,{\frac {s^{2}}{2}}\right)\,}$
up to an overall constant translation, which is the special affine parameterization of the parabola y = x2/2.
k > 0
If the special affine curvature is positive, then it follows that
${\displaystyle \beta '(s)=\left(\cos {\sqrt {k}}\,s,{\frac {1}{\sqrt {k}}}\sin {\sqrt {k}}\,s\right)}$
so that
${\displaystyle \beta (s)=\left({\frac {1}{\sqrt {k}}}\sin {\sqrt {k}}\,s,-{\frac {1}{k}}\cos {\sqrt {k}}\,s\right)}$
up to a translation, which is the special affine parameterization of the ellipse kx2 + k2y2 = 1.
k < 0
If k is negative, then the trigonometric functions in Cβ give way to hyperbolic functions:
${\displaystyle C_{\beta }(s)={\begin{bmatrix}\cosh {\sqrt {|k|}}\,s&{\sqrt {|k|}}\sinh {\sqrt {|k|}}\,s\\{\frac {1}{\sqrt {|k|}}}\sinh {\sqrt {|k|}}\,s&\cosh {\sqrt {|k|}}\,s\end{bmatrix}}.}$
Thus
${\displaystyle \beta (s)=\left({\frac {1}{\sqrt {|k|}}}\sinh {\sqrt {|k|}}\,s,{\frac {1}{|k|}}\cosh {\sqrt {|k|}}\,s\right)}$
up to a translation, which is the special affine parameterization of the hyperbola
${\displaystyle -|k|x^{2}+|k|^{2}y^{2}=1.}$

£#h5#£Characterization up to affine congruence£#/h5#£
The special affine curvature of an immersed curve is the only (local) invariant of the curve in the following sense:

£#ul#££#li#£If two curves have the same special affine curvature at every point, then one curve is obtained from the other by means of a special affine transformation.£#/li#££#/ul#£
In fact, a slightly stronger statement holds:

£#ul#££#li#£Given any continuous function k : [a, b] → R, there exists a curve β whose first and second derivatives are linearly independent, such that the special affine curvature of β relative to the special affine parameterization is equal to the given function k. The curve β is uniquely determined up to a special affine transformation.£#/li#££#/ul#£
This is analogous to the fundamental theorem of curves in the classical Euclidean differential geometry of curves, in which the complete classification of plane curves up to Euclidean motion depends on a single function κ, the curvature of the curve. It follows essentially by applying the Picard–Lindelöf theorem to the system

${\displaystyle C_{\beta }'=C_{\beta }{\begin{bmatrix}0&-k\\1&0\end{bmatrix}}}$
where Cβ = [β′ β″]. An alternative approach, rooted in the theory of moving frames, is to apply the existence of a primitive for the Darboux derivative.


£#h5#£Derivation of the curvature by affine invariance£#/h5#£
The special affine curvature can be derived explicitly by techniques of invariant theory. For simplicity, suppose that an affine plane curve is given in the form of a graph y = y(x). The special affine group acts on the Cartesian plane via transformations of the form

${\displaystyle {\begin{aligned}x&\mapsto ax+by+\alpha \\y&\mapsto cx+dy+\beta ,\end{aligned}}}$
with ad − bc = 1. The following vector fields span the Lie algebra of infinitesimal generators of the special affine group:

${\displaystyle {\begin{aligned}T_{1}&=\partial _{x},&\quad T_{2}&=\partial _{y}\\X_{1}&=x\partial _{y},&\quad X_{2}&=y\partial _{x},&H&=x\partial _{x}-y\partial _{y}.\end{aligned}}}$
An affine transformation not only acts on points, but also on the tangent lines to graphs of the form y = y(x). That is, there is an action of the special affine group on triples of coordinates (x, y, y′). The group action is generated by vector fields

${\displaystyle T_{1}^{(1)},T_{2}^{(1)},X_{1}^{(1)},X_{2}^{(1)},H^{(1)}}$
defined on the space of three variables (x, y, y′). These vector fields can be determined by the following two requirements:

£#ul#££#li#£Under the projection onto the xy-plane, they must to project to the corresponding original generators of the action T1, T2, X1, X2, H, respectively.£#/li#£ £#li#£The vectors must preserve up to scale the contact structure of the jet space£#/li#££#/ul#£
${\displaystyle \theta _{1}=dy-y'\,dx.}$
Concretely, this means that the generators X(1) must satisfy
${\displaystyle L_{X^{(1)}}\theta _{1}\equiv 0{\pmod {\theta _{1}}}}$
where L is the Lie derivative.
Similarly, the action of the group can be extended to the space of any number of derivatives (x, y, y′, y″,…, y(k)).

The prolonged vector fields generating the action of the special affine group must then inductively satisfy, for each generator X ∈ {T1, T2, X1, X2, H}:

£#ul#££#li#£The projection of X(k) onto the space of variables (x, y, y′,…, y(k−1)) is X(k−1).£#/li#£ £#li#£X(k) preserves the contact ideal:£#/li#££#/ul#£
${\displaystyle L_{X^{(k)}}\theta _{k}\equiv 0{\pmod {\theta _{1},\dots ,\theta _{k}}}}$
where
${\displaystyle \theta _{i}=dy^{(i-1)}-y^{(i)}dx.}$
Carrying out the inductive construction up to order 4 gives

${\displaystyle {\begin{aligned}T_{1}^{(4)}&=\partial _{x},\qquad T_{2}^{(4)}=\partial _{y}\\X_{1}^{(4)}&=x\partial _{y}+\partial _{y'}\\X_{2}^{(4)}&=y\partial _{x}-y'^{2}\partial _{y'}-3y'y''\partial _{y''}-\left(3y''^{2}+4y'y'''\right)\partial _{y'''}-{\bigl (}10y''y'''+5y'y''''{\bigr )}\partial _{y''''}\\H^{(4)}&=x\partial _{x}-y\partial _{y}-2y'\partial _{y'}-3y''\partial _{y''}-4y'''\partial _{y'''}-5y''''\partial _{y''''}.\end{aligned}}}$
The special affine curvature

${\displaystyle k={\frac {y''''}{3\left(y''\right)^{\frac {5}{3}}}}-{\frac {5\left(y'''\right)^{2}}{9\left(y''\right)^{\frac {8}{3}}}}}$
does not depend explicitly on x, y, or y′, and so satisfies

${\displaystyle T_{1}^{(4)}k=T_{2}^{(4)}k=X_{1}^{(4)}k=0.}$
The vector field H acts diagonally as a modified homogeneity operator, and it is readily verified that H(4)k = 0. Finally,

${\displaystyle X_{2}^{(4)}k={\tfrac {1}{2}}\left[H,X_{1}\right]^{(4)}k={\tfrac {1}{2}}\left[H^{(4)},X_{1}^{(4)}\right]k=0.}$
The five vector fields

${\displaystyle T_{1}^{(4)},T_{2}^{(4)},X_{1}^{(4)},X_{2}^{(4)},H^{(4)}}$
form an involutive distribution on (an open subset of) R6 so that, by the Frobenius integration theorem, they integrate locally to give a foliation of R6 by five-dimensional leaves. Concretely, each leaf is a local orbit of the special affine group. The function k parameterizes these leaves.


£#h5#£Human motor system£#/h5#£
Human curvilinear 2-dimensional drawing movements tend to follow the equiaffine parametrization. This is more commonly known as the two thirds power law, according to which the hand's speed is proportional to the Euclidean curvature raised to the minus third power. Namely,

${\displaystyle v=\gamma \kappa ^{-{\frac {1}{3}}},}$
where v is the speed of the hand, κ is the Euclidean curvature and γ is a constant termed the velocity gain factor.


£#h5#£See also£#/h5#£ £#ul#££#li#£Affine geometry of curves£#/li#£ £#li#£Affine sphere£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£Sources£#/h5#£ £#ul#££#li#£Blaschke, Wilhelm (1923), Affine Differentialgeometrie, Vorlesungen über Differentialgeometrie und geometrische Grundlagen von Einsteins Relativitätstheorie (in German), vol. II, Berlin: Springer-Verlag OHG£#/li#£ £#li#£Guggenheimer, Heinrich (1977), Differential Geometry, New York: Dover Publications, ISBN 978-0-486-63433-3£#/li#£ £#li#£Shirokov, A.P. (2001a) [1994], "Affine curvature", Encyclopedia of Mathematics, EMS Press£#/li#£ £#li#£Shirokov, A.P. (2001b) [1994], "Affine differential geometry", Encyclopedia of Mathematics, EMS Press£#/li#£ £#li#£Spivak, Michael (1999), A Comprehensive introduction to differential geometry (Volume 2), Houston, TX: Publish or Perish, ISBN 978-0-914098-71-3£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Geometry > Projective Geometry > General Projective Geometry £#/li#££#li#£ Calculus and Analysis > Differential Geometry > Differential Geometry of Curves £#/li#££#/ul#£




£#h3#£Affine Space£#/h3#£

In mathematics, an affine space is a geometric structure that generalizes some of the properties of Euclidean spaces in such a way that these are independent of the concepts of distance and measure of angles, keeping only the properties related to parallelism and ratio of lengths for parallel line segments.

In an affine space, there is no distinguished point that serves as an origin. Hence, no vector has a fixed origin and no vector can be uniquely associated to a point. In an affine space, there are instead displacement vectors, also called translation vectors or simply translations, between two points of the space. Thus it makes sense to subtract two points of the space, giving a translation vector, but it does not make sense to add two points of the space. Likewise, it makes sense to add a displacement vector to a point of an affine space, resulting in a new point translated from the starting point by that vector.

Any vector space may be viewed as an affine space; this amounts to forgetting the special role played by the zero vector. In this case, the elements of the vector space may be viewed either as points of the affine space or as displacement vectors or translations. When considered as a point, the zero vector is called the origin. Adding a fixed vector to the elements of a linear subspace of a vector space produces an affine subspace. One commonly says that this affine subspace has been obtained by translating (away from the origin) the linear subspace by the translation vector. In finite dimensions, such an affine subspace is the solution set of an inhomogeneous linear system. The displacement vectors for that affine space are the solutions of the corresponding homogeneous linear system, which is a linear subspace. Linear subspaces, in contrast, always contain the origin of the vector space.

The dimension of an affine space is defined as the dimension of the vector space of its translations. An affine space of dimension one is an affine line. An affine space of dimension 2 is an affine plane. An affine subspace of dimension n – 1 in an affine space or a vector space of dimension n is an affine hyperplane.


£#h5#£Informal description£#/h5#£
The following characterization may be easier to understand than the usual formal definition: an affine space is what is left of a vector space after one has forgotten which point is the origin (or, in the words of the French mathematician Marcel Berger, "An affine space is nothing more than a vector space whose origin we try to forget about, by adding translations to the linear maps"). Imagine that Alice knows that a certain point is the actual origin, but Bob believes that another point—call it p—is the origin. Two vectors, a and b, are to be added. Bob draws an arrow from point p to point a and another arrow from point p to point b, and completes the parallelogram to find what Bob thinks is a + b, but Alice knows that he has actually computed

p + (a − p) + (b − p).
Similarly, Alice and Bob may evaluate any linear combination of a and b, or of any finite set of vectors, and will generally get different answers. However, if the sum of the coefficients in a linear combination is 1, then Alice and Bob will arrive at the same answer.

If Alice travels to

λa + (1 − λ)b
then Bob can similarly travel to

p + λ(a − p) + (1 − λ)(b − p) = λa + (1 − λ)b.
Under this condition, for all coefficients λ + (1 − λ) = 1, Alice and Bob describe the same point with the same linear combination, despite using different origins.

While only Alice knows the "linear structure", both Alice and Bob know the "affine structure"—i.e. the values of affine combinations, defined as linear combinations in which the sum of the coefficients is 1. A set with an affine structure is an affine space.


£#h5#£Definition£#/h5#£
An affine space is a set A together with a vector space ${\displaystyle {\overrightarrow {A}}}$ , and a transitive and free action of the additive group of ${\displaystyle {\overrightarrow {A}}}$ on the set A. The elements of the affine space A are called points. The vector space ${\displaystyle {\overrightarrow {A}}}$ is said to be associated to the affine space, and its elements are called vectors, translations, or sometimes free vectors.

Explicitly, the definition above means that the action is a mapping, generally denoted as an addition,

${\displaystyle {\begin{aligned}A\times {\overrightarrow {A}}&\to A\\(a,v)\;&\mapsto a+v,\end{aligned}}}$
that has the following properties.

£#li#£Right identity:
${\displaystyle \forall a\in A,\;a+0=a}$ , where 0 is the zero vector in ${\displaystyle {\overrightarrow {A}}}$
£#/li#£ £#li#£Associativity:
${\displaystyle \forall v,w\in {\overrightarrow {A}},\forall a\in A,\;(a+v)+w=a+(v+w)}$ (here the last + is the addition in ${\displaystyle {\overrightarrow {A}}}$ )
£#/li#£ £#li#£Free and transitive action:
For every ${\displaystyle a\in A}$ , the mapping ${\displaystyle {\overrightarrow {A}}\to A\colon v\mapsto a+v}$ is a bijection.
£#/li#£
The first two properties are simply defining properties of a (right) group action. The third property characterizes free and transitive actions, the onto character coming from transitivity, and then the injective character follows from the action being free. There is a fourth property that follows from 1, 2 above:

Existence of one-to-one translations£#/li#£
For all ${\displaystyle v\in {\overrightarrow {A}}}$ , the mapping ${\displaystyle A\to A\colon a\mapsto a+v}$ is a bijection.
Property 3 is often used in the following equivalent form.

Subtraction:£#/li#£
For every a, b in A, there exists a unique ${\displaystyle v\in {\overrightarrow {A}}}$ , denoted b – a, such that ${\displaystyle b=a+v}$ .
Another way to express the definition is that an affine space is a principal homogeneous space for the action of the additive group of a vector space. Homogeneous spaces are by definition endowed with a transitive group action, and for a principal homogeneous space such a transitive action is by definition free.


£#h5#£Subtraction and Weyl's axioms£#/h5#£
The properties of the group action allows for the definition of subtraction for any given ordered pair (b, a) of points in A, producing a vector of ${\displaystyle {\overrightarrow {A}}}$ . This vector, denoted ${\displaystyle b-a}$ or ${\displaystyle {\overrightarrow {ab}}}$ , is defined to be the unique vector in ${\displaystyle {\overrightarrow {A}}}$ such that

${\displaystyle a+(b-a)=b.}$
Existence follows from the transitivity of the action, and uniqueness follows because the action is free.

This subtraction has the two following properties, called Weyl's axioms:

£#li#£ ${\displaystyle \forall a\in A,\;\forall v\in {\overrightarrow {A}}}$ , there is a unique point ${\displaystyle b\in A}$ such that ${\displaystyle b-a=v.}$ £#/li#£ £#li#£ ${\displaystyle \forall a,b,c\in A,\;(c-b)+(b-a)=c-a.}$ £#/li#£
In Euclidean geometry, the second Weyl's axiom is commonly called the parallelogram rule.

Affine spaces can be equivalently defined as a point set A, together with a vector space ${\displaystyle {\overrightarrow {A}}}$ , and a subtraction satisfying Weyl's axioms. In this case, the addition of a vector to a point is defined from the first Weyl's axioms.


£#h5#£Affine subspaces and parallelism£#/h5#£
An affine subspace (also called, in some contexts, a linear variety, a flat, or, over the real numbers, a linear manifold) B of an affine space A is a subset of A such that, given a point ${\displaystyle a\in B}$ , the set of vectors ${\displaystyle {\overrightarrow {B}}=\{b-a\mid b\in B\}}$ is a linear subspace of ${\displaystyle {\overrightarrow {A}}}$ . This property, which does not depend on the choice of a, implies that B is an affine space, which has ${\displaystyle {\overrightarrow {B}}}$ as its associated vector space.

The affine subspaces of A are the subsets of A of the form

${\displaystyle a+V=\{a+w:w\in V\},}$
where a is a point of A, and V a linear subspace of ${\displaystyle {\overrightarrow {A}}}$ .

The linear subspace associated with an affine subspace is often called its direction, and two subspaces that share the same direction are said to be parallel.

This implies the following generalization of Playfair's axiom: Given a direction V, for any point a of A there is one and only one affine subspace of direction V, which passes through a, namely the subspace a + V.

Every translation ${\displaystyle A\to A:a\mapsto a+v}$ maps any affine subspace to a parallel subspace.

The term parallel is also used for two affine subspaces such that the direction of one is included in the direction of the other.


£#h5#£Affine map£#/h5#£
Given two affine spaces A and B whose associated vector spaces are ${\displaystyle {\overrightarrow {A}}}$ and ${\displaystyle {\overrightarrow {B}}}$ , an affine map or affine homomorphism from A to B is a map

${\displaystyle f:A\to B}$
such that

${\displaystyle {\begin{aligned}{\overrightarrow {f}}:{\overrightarrow {A}}&\to {\overrightarrow {B}}\\b-a&\mapsto f(b)-f(a)\end{aligned}}}$
is a well defined linear map. By ${\displaystyle f}$ being well defined is meant that b – a = d – c implies f(b) – f(a) = f(d) – f(c).

This implies that, for a point ${\displaystyle a\in A}$ and a vector ${\displaystyle v\in {\overrightarrow {A}}}$ , one has

${\displaystyle f(a+v)=f(a)+{\overrightarrow {f}}(v).}$
Therefore, since for any given b in A, b = a + v for a unique v, f is completely defined by its value on a single point and the associated linear map ${\displaystyle {\overrightarrow {f}}}$ .


£#h5#£Endomorphisms£#/h5#£
An affine transformation or endomorphism of an affine space ${\displaystyle A}$ is an affine map from that space to itself. One important family of examples is the translations: given a vector ${\displaystyle {\overrightarrow {v}}}$ , the translation map ${\displaystyle T_{\overrightarrow {v}}:A\rightarrow A}$ that sends ${\displaystyle a\mapsto a+{\overrightarrow {v}}}$ for every ${\displaystyle a}$ in ${\displaystyle A}$ is an affine map. Another important family of examples are the linear maps centred at an origin: given a point ${\displaystyle b}$ and a linear map ${\displaystyle M}$ , one may define an affine map ${\displaystyle L_{M,b}:A\rightarrow A}$ by

for every ${\displaystyle a}$ in ${\displaystyle A}$ .
After making a choice of origin ${\displaystyle b}$ , any affine map may be written uniquely as a combination of a translation and a linear map centred at ${\displaystyle b}$ .


£#h5#£Vector spaces as affine spaces£#/h5#£
Every vector space V may be considered as an affine space over itself. This means that every element of V may be considered either as a point or as a vector. This affine space is sometimes denoted (V, V) for emphasizing the double role of the elements of V. When considered as a point, the zero vector is commonly denoted o (or O, when upper-case letters are used for points) and called the origin.

If A is another affine space over the same vector space (that is ${\displaystyle V={\overrightarrow {A}}}$ ) the choice of any point a in A defines a unique affine isomorphism, which is the identity of V and maps a to o. In other words, the choice of an origin a in A allows us to identify A and (V, V) up to a canonical isomorphism. The counterpart of this property is that the affine space A may be identified with the vector space V in which "the place of the origin has been forgotten".


£#h5#£Relation to Euclidean spaces£#/h5#£
£#h5#£Definition of Euclidean spaces£#/h5#£
Euclidean spaces (including the one-dimensional line, two-dimensional plane, and three-dimensional space commonly studied in elementary geometry, as well as higher-dimensional analogues) are affine spaces.

Indeed, in most modern definitions, a Euclidean space is defined to be an affine space, such that the associated vector space is a real inner product space of finite dimension, that is a vector space over the reals with a positive-definite quadratic form q(x). The inner product of two vectors x and y is the value of the symmetric bilinear form

${\displaystyle x\cdot y={\frac {1}{2}}(q(x+y)-q(x)-q(y)).}$
The usual Euclidean distance between two points A and B is

${\displaystyle d(A,B)={\sqrt {q(B-A)}}.}$
In older definition of Euclidean spaces through synthetic geometry, vectors are defined as equivalence classes of ordered pairs of points under equipollence (the pairs (A, B) and (C, D) are equipollent if the points A, B, D, C (in this order) form a parallelogram). It is straightforward to verify that the vectors form a vector space, the square of the Euclidean distance is a quadratic form on the space of vectors, and the two definitions of Euclidean spaces are equivalent.


£#h5#£Affine properties£#/h5#£
In Euclidean geometry, the common phrase "affine property" refers to a property that can be proved in affine spaces, that is, it can be proved without using the quadratic form and its associated inner product. In other words, an affine property is a property that does not involve lengths and angles. Typical examples are parallelism, and the definition of a tangent. A non-example is the definition of a normal.

Equivalently, an affine property is a property that is invariant under affine transformations of the Euclidean space.


£#h5#£Affine combinations and barycenter£#/h5#£
Let a1, ..., an be a collection of n points in an affine space, and ${\displaystyle \lambda _{1},\dots ,\lambda _{n}}$ be n elements of the ground field.

Suppose that ${\displaystyle \lambda _{1}+\dots +\lambda _{n}=0}$ . For any two points o and o' one has

${\displaystyle \lambda _{1}{\overrightarrow {oa_{1}}}+\dots +\lambda _{n}{\overrightarrow {oa_{n}}}=\lambda _{1}{\overrightarrow {o'a_{1}}}+\dots +\lambda _{n}{\overrightarrow {o'a_{n}}}.}$
Thus this sum is independent of the choice of the origin, and the resulting vector may be denoted

${\displaystyle \lambda _{1}a_{1}+\dots +\lambda _{n}a_{n}.}$
When ${\displaystyle n=2,\lambda _{1}=1,\lambda _{2}=-1}$ , one retrieves the definition of the subtraction of points.

Now suppose instead that the field elements satisfy ${\displaystyle \lambda _{1}+\dots +\lambda _{n}=1}$ . For some choice of an origin o, denote by ${\displaystyle g}$ the unique point such that

${\displaystyle \lambda _{1}{\overrightarrow {oa_{1}}}+\dots +\lambda _{n}{\overrightarrow {oa_{n}}}={\overrightarrow {og}}.}$
One can show that ${\displaystyle g}$ is independent from the choice of o. Therefore, if

${\displaystyle \lambda _{1}+\dots +\lambda _{n}=1,}$
one may write

${\displaystyle g=\lambda _{1}a_{1}+\dots +\lambda _{n}a_{n}.}$
The point ${\displaystyle g}$ is called the barycenter of the ${\displaystyle a_{i}}$ for the weights ${\displaystyle \lambda _{i}}$ . One says also that ${\displaystyle g}$ is an affine combination of the ${\displaystyle a_{i}}$ with coefficients ${\displaystyle \lambda _{i}}$ .


£#h5#£Examples£#/h5#£ £#ul#££#li#£When children find the answers to sums such as 4 + 3 or 4 − 2 by counting right or left on a number line, they are treating the number line as a one-dimensional affine space.£#/li#£ £#li#£The space of energies is an affine space for ${\displaystyle \mathbb {R} }$ , since it is often not meaningful to talk about absolute energy, but it is meaningful to talk about energy differences. The vacuum energy when it is defined picks out a canonical origin.£#/li#£ £#li#£Physical space is often modelled as an affine space for ${\displaystyle \mathbb {R} ^{3}}$ in non-relativistic settings and ${\displaystyle \mathbb {R} ^{1,3}}$ in the relativistic setting. To distinguish them from the vector space these are sometimes called Euclidean spaces ${\displaystyle {\text{E}}(3)}$ and ${\displaystyle {\text{E}}(1,3)}$ .£#/li#£ £#li#£Any coset of a subspace V of a vector space is an affine space over that subspace.£#/li#£ £#li#£If T is a matrix and b lies in its column space, the set of solutions of the equation Tx = b is an affine space over the subspace of solutions of Tx = 0.£#/li#£ £#li#£The solutions of an inhomogeneous linear differential equation form an affine space over the solutions of the corresponding homogeneous linear equation.£#/li#£ £#li#£Generalizing all of the above, if T : V → W is a linear mapping and y lies in its image, the set of solutions x ∈ V to the equation Tx = y is a coset of the kernel of T , and is therefore an affine space over Ker T .£#/li#£ £#li#£The space of (linear) complementary subspaces of a vector subspace V in a vector space W is an affine space, over Hom(W/V, V). That is, if 0 → V → W → X → 0 is a short exact sequence of vector spaces, then the space of all splittings of the exact sequence naturally carries the structure of an affine space over Hom(X, V).£#/li#£ £#li#£The space of connections (viewed from the vector bundle ${\displaystyle E\xrightarrow {\pi } M}$ , where ${\displaystyle M}$ is a smooth manifold) is an affine space for the vector space of ${\displaystyle {\text{End}}(E)}$ valued 1-forms. The space of connections (viewed from the principal bundle ${\displaystyle P\xrightarrow {\pi } M}$ ) is an affine space for the vector space of ${\displaystyle {\text{ad}}(P)}$ -valued 1-forms, where ${\displaystyle {\text{ad}}(P)}$ is the associated adjoint bundle.£#/li#££#/ul#£
£#h5#£Affine span and bases£#/h5#£
For any subset X of an affine space A, there is a smallest affine subspace that contains it, called the affine span of X. It is the intersection of all affine subspaces containing X, and its direction is the intersection of the directions of the affine subspaces that contain X.

The affine span of X is the set of all (finite) affine combinations of points of X, and its direction is the linear span of the x − y for x and y in X. If one chooses a particular point x0, the direction of the affine span of X is also the linear span of the x – x0 for x in X.

One says also that the affine span of X is generated by X and that X is a generating set of its affine span.

A set X of points of an affine space is said to be affinely independent or, simply, independent, if the affine span of any strict subset of X is a strict subset of the affine span of X. An affine basis or barycentric frame (see § Barycentric coordinates, below) of an affine space is a generating set that is also independent (that is a minimal generating set).

Recall that the dimension of an affine space is the dimension of its associated vector space. The bases of an affine space of finite dimension n are the independent subsets of n + 1 elements, or, equivalently, the generating subsets of n + 1 elements. Equivalently, {x0, ..., xn} is an affine basis of an affine space if and only if {x1 − x0, ..., xn − x0} is a linear basis of the associated vector space.


£#h5#£Coordinates£#/h5#£
There are two strongly related kinds of coordinate systems that may be defined on affine spaces.


£#h5#£Barycentric coordinates£#/h5#£
Let A be an affine space of dimension n over a field k, and ${\displaystyle \{x_{0},\dots ,x_{n}\}}$ be an affine basis of A. The properties of an affine basis imply that for every x in A there is a unique (n + 1)-tuple ${\displaystyle (\lambda _{0},\dots ,\lambda _{n})}$ of elements of k such that

${\displaystyle \lambda _{0}+\dots +\lambda _{n}=1}$
and

${\displaystyle x=\lambda _{0}x_{0}+\dots +\lambda _{n}x_{n}.}$
The ${\displaystyle \lambda _{i}}$ are called the barycentric coordinates of x over the affine basis ${\displaystyle \{x_{0},\dots ,x_{n}\}}$ . If the xi are viewed as bodies that have weights (or masses) ${\displaystyle \lambda _{i}}$ , the point x is thus the barycenter of the xi, and this explains the origin of the term barycentric coordinates.

The barycentric coordinates define an affine isomorphism between the affine space A and the affine subspace of kn + 1 defined by the equation ${\displaystyle \lambda _{0}+\dots +\lambda _{n}=1}$ .

For affine spaces of infinite dimension, the same definition applies, using only finite sums. This means that for each point, only a finite number of coordinates are non-zero.


£#h5#£Affine coordinates£#/h5#£
An affine frame of an affine space consists of a point, called the origin, and a linear basis of the associated vector space. More precisely, for an affine space A with associated vector space ${\displaystyle {\overrightarrow {A}}}$ , the origin o belongs to A, and the linear basis is a basis (v1, ..., vn) of ${\displaystyle {\overrightarrow {A}}}$ (for simplicity of the notation, we consider only the case of finite dimension, the general case is similar).

For each point p of A, there is a unique sequence ${\displaystyle \lambda _{1},\dots ,\lambda _{n}}$ of elements of the ground field such that

${\displaystyle p=o+\lambda _{1}v_{1}+\dots +\lambda _{n}v_{n},}$
or equivalently

${\displaystyle {\overrightarrow {op}}=\lambda _{1}v_{1}+\dots +\lambda _{n}v_{n}.}$
The ${\displaystyle \lambda _{i}}$ are called the affine coordinates of p over the affine frame (o, v1, ..., vn).

Example: In Euclidean geometry, Cartesian coordinates are affine coordinates relative to an orthonormal frame, that is an affine frame (o, v1, ..., vn) such that (v1, ..., vn) is an orthonormal basis.


£#h5#£Relationship between barycentric and affine coordinates£#/h5#£
Barycentric coordinates and affine coordinates are strongly related, and may be considered as equivalent.

In fact, given a barycentric frame

${\displaystyle (x_{0},\dots ,x_{n}),}$
one deduces immediately the affine frame

${\displaystyle (x_{0},{\overrightarrow {x_{0}x_{1}}},\dots ,{\overrightarrow {x_{0}x_{n}}})=\left(x_{0},x_{1}-x_{0},\dots ,x_{n}-x_{0}\right),}$
and, if

${\displaystyle \left(\lambda _{0},\lambda _{1},\dots ,\lambda _{n}\right)}$
are the barycentric coordinates of a point over the barycentric frame, then the affine coordinates of the same point over the affine frame are

${\displaystyle \left(\lambda _{1},\dots ,\lambda _{n}\right).}$
Conversely, if

${\displaystyle \left(o,v_{1},\dots ,v_{n}\right)}$
is an affine frame, then

${\displaystyle \left(o,o+v_{1},\dots ,o+v_{n}\right)}$
is a barycentric frame. If

${\displaystyle \left(\lambda _{1},\dots ,\lambda _{n}\right)}$
are the affine coordinates of a point over the affine frame, then its barycentric coordinates over the barycentric frame are

${\displaystyle \left(1-\lambda _{1}-\dots -\lambda _{n},\lambda _{1},\dots ,\lambda _{n}\right).}$
Therefore, barycentric and affine coordinates are almost equivalent. In most applications, affine coordinates are preferred, as involving less coordinates that are independent. However, in the situations where the important points of the studied problem are affinely independent, barycentric coordinates may lead to simpler computation, as in the following example.


£#h5#£Example of the triangle£#/h5#£
The vertices of a non-flat triangle form an affine basis of the Euclidean plane. The barycentric coordinates allows easy characterization of the elements of the triangle that do not involve angles or distance:

The vertices are the points of barycentric coordinates (1, 0, 0), (0, 1, 0) and (0, 0, 1). The lines supporting the edges are the points that have a zero coordinate. The edges themselves are the points that have a zero coordinate and two nonnegative coordinates. The interior of the triangle are the points whose coordinates are all positive. The medians are the points that have two equal coordinates, and the centroid is the point of coordinates (1/3, 1/3, 1/3).


£#h5#£Change of coordinates£#/h5#£
£#h5#£Case of affine coordinates£#/h5#£
£#h5#£Case of barycentric coordinates£#/h5#£
£#h5#£Properties of affine homomorphisms£#/h5#£
£#h5#£Matrix representation£#/h5#£
£#h5#£Image and fibers£#/h5#£
Let

${\displaystyle f\colon E\to F}$
be an affine homomorphism, with

${\displaystyle {\overrightarrow {f}}\colon {\overrightarrow {E}}\to {\overrightarrow {F}}}$
as associated linear map.

The image of f is the affine subspace f(E) of F, which has ${\displaystyle {\overrightarrow {f}}({\overrightarrow {E}})}$ as associated vector space. As an affine space does not have a zero element, an affine homomorphism does not have a kernel. However, for any point x of f(E), the inverse image f–1(x) of x is an affine subspace of E, of direction ${\displaystyle {\overrightarrow {f}}^{-1}({\overrightarrow {F}})}$ . This affine subspace is called the fiber of x.


£#h5#£Projection£#/h5#£
An important example is the projection parallel to some direction onto an affine subspace. The importance of this example lies in the fact that Euclidean spaces are affine spaces, and that these kinds of projections are fundamental in Euclidean geometry.

More precisely, given an affine space E with associated vector space ${\displaystyle {\overrightarrow {E}}}$ , let F be an affine subspace of direction ${\displaystyle {\overrightarrow {F}}}$ , and D be a complementary subspace of ${\displaystyle {\overrightarrow {F}}}$ in ${\displaystyle {\overrightarrow {E}}}$ (this means that every vector of ${\displaystyle {\overrightarrow {E}}}$ may be decomposed in a unique way as the sum of an element of ${\displaystyle {\overrightarrow {F}}}$ and an element of D). For every point x of E, its projection to F parallel to D is the unique point p(x) in F such that

${\displaystyle p(x)-x\in D.}$
This is an affine homomorphism whose associated linear map ${\displaystyle {\overrightarrow {p}}}$ is defined by

${\displaystyle {\overrightarrow {p}}(x-y)=p(x)-p(y),}$
for x and y in E.

The image of this projection is F, and its fibers are the subspaces of direction D.


£#h5#£Quotient space£#/h5#£
Although kernels are not defined for affine spaces, quotient spaces are defined. This results from the fact that "belonging to the same fiber of an affine homomorphism" is an equivalence relation.

Let E be an affine space, and D be a linear subspace of the associated vector space ${\displaystyle {\overrightarrow {E}}}$ . The quotient E/D of E by D is the quotient of E by the equivalence relation such that x and y are equivalent if

${\displaystyle x-y\in D.}$
This quotient is an affine space, which has ${\displaystyle {\overrightarrow {E}}/D}$ as associated vector space.

For every affine homomorphism ${\displaystyle E\to F}$ , the image is isomorphic to the quotient of E by the kernel of the associated linear map. This is the first isomorphism theorem for affine spaces.


£#h5#£Axioms£#/h5#£
Affine spaces are usually studied by analytic geometry using coordinates, or equivalently vector spaces. They can also be studied as synthetic geometry by writing down axioms, though this approach is much less common. There are several different systems of axioms for affine space.

Coxeter (1969, p. 192) axiomatizes the special case of affine geometry over the reals as ordered geometry together with an affine form of Desargues's theorem and an axiom stating that in a plane there is at most one line through a given point not meeting a given line.

Affine planes satisfy the following axioms (Cameron 1991, chapter 2): (in which two lines are called parallel if they are equal or disjoint):

£#ul#££#li#£Any two distinct points lie on a unique line.£#/li#£ £#li#£Given a point and line there is a unique line which contains the point and is parallel to the line£#/li#£ £#li#£There exist three non-collinear points.£#/li#££#/ul#£
As well as affine planes over fields (or division rings), there are also many non-Desarguesian planes satisfying these axioms. (Cameron 1991, chapter 3) gives axioms for higher-dimensional affine spaces.

Purely axiomatic affine geometry is more general than affine spaces and is treated in a separate article.


£#h5#£Relation to projective spaces£#/h5#£
Affine spaces are contained in projective spaces. For example, an affine plane can be obtained from any projective plane by removing one line and all the points on it, and conversely any affine plane can be used to construct a projective plane as a closure by adding a line at infinity whose points correspond to equivalence classes of parallel lines. Similar constructions hold in higher dimensions.

Further, transformations of projective space that preserve affine space (equivalently, that leave the hyperplane at infinity invariant as a set) yield transformations of affine space. Conversely, any affine linear transformation extends uniquely to a projective linear transformation, so the affine group is a subgroup of the projective group. For instance, Möbius transformations (transformations of the complex projective line, or Riemann sphere) are affine (transformations of the complex plane) if and only if they fix the point at infinity.


£#h5#£Affine algebraic geometry£#/h5#£
In algebraic geometry, an affine variety (or, more generally, an affine algebraic set) is defined as the subset of an affine space that is the set of the common zeros of a set of so-called polynomial functions over the affine space. For defining a polynomial function over the affine space, one has to choose an affine frame. Then, a polynomial function is a function such that the image of any point is the value of some multivariate polynomial function of the coordinates of the point. As a change of affine coordinates may be expressed by linear functions (more precisely affine functions) of the coordinates, this definition is independent of a particular choice of coordinates.

The choice of a system of affine coordinates for an affine space ${\displaystyle \mathbb {A} _{k}^{n}}$ of dimension n over a field k induces an affine isomorphism between ${\displaystyle \mathbb {A} _{k}^{n}}$ and the affine coordinate space kn. This explains why, for simplification, many textbooks write ${\displaystyle \mathbb {A} _{k}^{n}=k^{n}}$ , and introduce affine algebraic varieties as the common zeros of polynomial functions over kn.

As the whole affine space is the set of the common zeros of the zero polynomial, affine spaces are affine algebraic varieties.


£#h5#£Ring of polynomial functions£#/h5#£
By the definition above, the choice of an affine frame of an affine space ${\displaystyle \mathbb {A} _{k}^{n}}$ allows one to identify the polynomial functions on ${\displaystyle \mathbb {A} _{k}^{n}}$ with polynomials in n variables, the ith variable representing the function that maps a point to its ith coordinate. It follows that the set of polynomial functions over ${\displaystyle \mathbb {A} _{k}^{n}}$ is a k-algebra, denoted ${\displaystyle k\left[\mathbb {A} _{k}^{n}\right]}$ , which is isomorphic to the polynomial ring ${\displaystyle k\left[X_{1},\dots ,X_{n}\right]}$ .

When one changes coordinates, the isomorphism between ${\displaystyle k\left[\mathbb {A} _{k}^{n}\right]}$ and ${\displaystyle k[X_{1},\dots ,X_{n}]}$ changes accordingly, and this induces an automorphism of ${\displaystyle k\left[X_{1},\dots ,X_{n}\right]}$ , which maps each indeterminate to a polynomial of degree one. It follows that the total degree defines a filtration of ${\displaystyle k\left[\mathbb {A} _{k}^{n}\right]}$ , which is independent from the choice of coordinates. The total degree defines also a graduation, but it depends on the choice of coordinates, as a change of affine coordinates may map indeterminates on non-homogeneous polynomials.


£#h5#£Zariski topology£#/h5#£
Affine spaces over topological fields, such as the real or the complex numbers, have a natural topology. The Zariski topology, which is defined for affine spaces over any field, allows use of topological methods in any case. Zariski topology is the unique topology on an affine space whose closed sets are affine algebraic sets (that is sets of the common zeros of polynomials functions over the affine set). As, over a topological field, polynomial functions are continuous, every Zariski closed set is closed for the usual topology, if any. In other words, over a topological field, Zariski topology is coarser than the natural topology.

There is a natural injective function from an affine space into the set of prime ideals (that is the spectrum) of its ring of polynomial functions. When affine coordinates have been chosen, this function maps the point of coordinates ${\displaystyle \left(a_{1},\dots ,a_{n}\right)}$ to the maximal ideal ${\displaystyle \left\langle X_{1}-a_{1},\dots ,X_{n}-a_{n}\right\rangle }$ . This function is a homeomorphism (for the Zariski topology of the affine space and of the spectrum of the ring of polynomial functions) of the affine space onto the image of the function.

The case of an algebraically closed ground field is especially important in algebraic geometry, because, in this case, the homeomorphism above is a map between the affine space and the set of all maximal ideals of the ring of functions (this is Hilbert's Nullstellensatz).

This is the starting idea of scheme theory of Grothendieck, which consists, for studying algebraic varieties, of considering as "points", not only the points of the affine space, but also all the prime ideals of the spectrum. This allows gluing together algebraic varieties in a similar way as, for manifolds, charts are glued together for building a manifold.


£#h5#£Cohomology£#/h5#£
Like all affine varieties, local data on an affine space can always be patched together globally: the cohomology of affine space is trivial. More precisely, ${\displaystyle H^{i}\left(\mathbb {A} _{k}^{n},\mathbf {F} \right)=0}$ for all coherent sheaves F, and integers ${\displaystyle i>0}$ . This property is also enjoyed by all other affine varieties. But also all of the étale cohomology groups on affine space are trivial. In particular, every line bundle is trivial. More generally, the Quillen–Suslin theorem implies that every algebraic vector bundle over an affine space is trivial.


£#h5#£See also£#/h5#£ £#ul#££#li#£Affine hull – Smallest affine subspace that contains a subset£#/li#£ £#li#£Complex affine space – Affine space over the complex numbers£#/li#£ £#li#£Exotic affine space – Real affine space of even dimension that is not isomorphic to a complex affine space£#/li#£ £#li#£Space (mathematics) – Mathematical set with some added structure£#/li#£ £#li#£Barycentric coordinate system£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Berger, Marcel (1984), "Affine spaces", Problems in Geometry, Springer-Verlag, ISBN 978-0-387-90971-4£#/li#£ £#li#£Berger, Marcel (1987), Geometry I, Berlin: Springer, ISBN 3-540-11658-3£#/li#£ £#li#£Cameron, Peter J. (1991), Projective and polar spaces, QMW Maths Notes, vol. 13, London: Queen Mary and Westfield College School of Mathematical Sciences, MR 1153019£#/li#£ £#li#£Coxeter, Harold Scott MacDonald (1969), Introduction to Geometry (2nd ed.), New York: John Wiley & Sons, ISBN 978-0-471-50458-0, MR 0123930£#/li#£ £#li#£Dolgachev, I.V.; Shirokov, A.P. (2001) [1994], "Affine space", Encyclopedia of Mathematics, EMS Press£#/li#£ £#li#£Hartshorne, Robin (1977). Algebraic Geometry. Springer-Verlag. ISBN 978-0-387-90244-9. Zbl 0367.14001.£#/li#£ £#li#£Nomizu, K.; Sasaki, S. (1994), Affine Differential Geometry (New ed.), Cambridge University Press, ISBN 978-0-521-44177-3£#/li#£ £#li#£Snapper, Ernst; Troyer, Robert J. (1989), Metric Affine Geometry (Dover edition, first published in 1989 ed.), Dover Publications, ISBN 0-486-66108-3£#/li#£ £#li#£Reventós Tarrida, Agustí (2011), "Affine spaces", Affine Maps, Euclidean Motions and Quadrics, Springer, ISBN 978-0-85729-709-9£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > General Analysis £#/li#££#/ul#£




£#h3#£Affine Tensor£#/h3#£

£#h5#£ References £#/h5#£ £#ul#££#li#£Goldstein, H. Classical Mechanics, 2nd ed. Reading, MA: Addison-Wesley, p. 580, 1980.£#/li#££#li#£Kay, D. Schaum's Outline of Tensor Calculus. New York: McGraw-Hill, 1988.£#/li#££#li#£Lovelock, D. and Rund, H. Tensors, Differential Forms, and Variational Principles. New York: Dover, 1989.£#/li#££#li#£ Goldstein, H. Classical Mechanics, 2nd ed. Reading, MA: Addison-Wesley, p. 580, 1980. £#/li#££#li#£ Kay, D. Schaum's Outline of Tensor Calculus. New York: McGraw-Hill, 1988. £#/li#££#li#£ Lovelock, D. and Rund, H. Tensors, Differential Forms, and Variational Principles. New York: Dover, 1989. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Differential Geometry > Tensor Analysis £#/li#££#/ul#£




£#h3#£Affix£#/h3#£

In linguistics, an affix is a morpheme that is attached to a word stem to form a new word or word form. Affixes may be derivational, like English -ness and pre-, or inflectional, like English plural -s and past tense -ed. They are bound morphemes by definition; prefixes and suffixes may be separable affixes. Affixation is the linguistic process that speakers use to form different words by adding morphemes at the beginning (prefixation), the middle (infixation) or the end (suffixation) of words.


£#h5#£Positional categories of affixes£#/h5#£
Prefix and suffix may be subsumed under the term adfix, in contrast to infix.

When marking text for interlinear glossing, as in the third column in the chart above, simple affixes such as prefixes and suffixes are separated from the stem with hyphens. Affixes which disrupt the stem, or which themselves are discontinuous, are often marked off with angle brackets. Reduplication is often shown with a tilde. Affixes which cannot be segmented are marked with a back slash.


£#h5#£Lexical affixes£#/h5#£
Lexical affixes (or semantic affixes) are bound elements that appear as affixes, but function as incorporated nouns within verbs and as elements of nouns. In other words, they are similar to word roots/stems in function but similar to affixes in form. Although similar to incorporated nouns, lexical affixes differ in that they never occur as freestanding nouns, i.e. they always appear as affixes.

Lexical affixes are relatively rare. The Wakashan, Salishan, and Chimakuan languages all have lexical suffixes — the presence of these is an areal feature of the Pacific Northwest of North America.

The lexical suffixes of these languages often show little to no resemblance to free nouns with similar meanings. Compare the lexical suffixes and free nouns of Northern Straits Saanich written in the Saanich orthography and in Americanist notation:

Lexical suffixes, when compared with free nouns, often have a more generic or general meaning. For instance, one of these languages may have a lexical suffix that means water in a general sense, but it may not have any noun equivalent referring to water in general and instead have several nouns with a more specific meaning (such "saltwater", "whitewater", etc.). In other cases, the lexical suffixes have become grammaticalized to various degrees.

Some linguists have claimed that these lexical suffixes provide only adverbial or adjectival notions to verbs. Other linguists disagree arguing that they may additionally be syntactic arguments just as free nouns are and, thus, equating lexical suffixes with incorporated nouns. Gerdts (2003) gives examples of lexical suffixes in the Halkomelem language (the word order here is verb–subject–object):

In sentence (1), the verb "wash" is šak’ʷətəs where šak’ʷ- is the root and -ət and -əs are inflectional suffixes. The subject "the woman" is łə słeniʔ and the object "the baby" is łə qeq. In this sentence, "the baby" is a free noun. (The niʔ here is an auxiliary, which can be ignored for explanatory purposes.)

In sentence (2), "baby" does not appear as a free noun. Instead it appears as the lexical suffix -əyəł which is affixed to the verb root šk’ʷ- (which has changed slightly in pronunciation, but this can also be ignored here). Note how the lexical suffix is neither "the baby" (definite) nor "a baby" (indefinite); such referential changes are routine with incorporated nouns.


£#h5#£Orthographic affixes£#/h5#£
In orthography, the terms for affixes may be used for the smaller elements of conjunct characters. For example, Maya glyphs are generally compounds of a main sign and smaller affixes joined at its margins. These are called prefixes, superfixes, postfixes, and subfixes according to their position to the left, on top, to the right, or at the bottom of the main glyph. A small glyph placed inside another is called an infix. Similar terminology is found with the conjunct consonants of the Indic alphabets. For example, the Tibetan alphabet utilizes prefix, suffix, superfix, and subfix consonant letters.


£#h5#£See also£#/h5#£
£#h5#£References£#/h5#£
£#h5#£Bibliography£#/h5#£ £#ul#££#li#£Gerdts, Donna B. (2003). "The morphosyntax of Halkomelem lexical suffixes". International Journal of American Linguistics. 69 (4): 345–356. doi:10.1086/382736. S2CID 143721330.£#/li#£ £#li#£Montler, Timothy. (1986). An outline of the morphology and phonology of Saanich, North Straits Salish. Occasional Papers in Linguistics (No. 4). Missoula, MT: University of Montana Linguistics Laboratory.£#/li#£ £#li#£Montler, Timothy. (1991). Saanich, North Straits Salish classified word list. Canadian Ethnology service paper (No. 119); Mercury series. Hull, Quebec: Canadian Museum of Civilization. ISBN 0-660-12908-6£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£ Media related to Affixes at Wikimedia Commons£#/li#£ £#li#£Comprehensive and searchable affix dictionary reference£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Whittaker, E. T. and Watson, G. N. A Course in Modern Analysis, 4th ed. Cambridge, England: Cambridge University Press, 1990.£#/li#££#li#£ Whittaker, E. T. and Watson, G. N. A Course in Modern Analysis, 4th ed. Cambridge, England: Cambridge University Press, 1990. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Complex Analysis > Complex Numbers £#/li#££#/ul#£




£#h3#£AGM£#/h3#£

AGM or agm may refer to:


£#h5#£Military£#/h5#£ £#ul#££#li#£Air-to-ground missile, a missile designed to be launched from military aircraft£#/li#£ £#li#£Artillery Gun Module, an air-portable self-propelled howitzer£#/li#£ £#li#£Missile Range Instrumentation Ship (US Navy hull classification symbol), a special type of ship for launching and tracking missiles and rockets£#/li#££#/ul#£
£#h5#£Organisations£#/h5#£ £#ul#££#li#£Active Gaming Media, a game localization company based in Japan£#/li#£ £#li#£Apollo Global Management, an American private equity firm£#/li#£ £#li#£Art Gallery of Mississauga, an art gallery in Canada£#/li#£ £#li#£Federal Agricultural Mortgage Corporation (NYSE symbol), a US loan and mortgage company£#/li#££#/ul#£
£#h5#£Science and technology£#/h5#£ £#ul#££#li#£AGM postulates, a set of conditions describing knowledge and belief revision£#/li#£ £#li#£Absorbent glass mat, a technology used in some models of VRLA battery£#/li#£ £#li#£Aorta-gonad-mesonephros, a part of chicken, mouse, and human embryos£#/li#£ £#li#£Arithmetic–geometric mean, a function of two positive numbers that is between the arithmetic mean and the geometric mean£#/li#££#/ul#£
£#h5#£Other uses£#/h5#£ £#ul#££#li#£Angaataha language (ISO 639-3 code)£#/li#£ £#li#£Annual general meeting, a meeting of the general membership of an organization£#/li#£ £#li#£Award of Garden Merit, by the British Royal Horticultural Society£#/li#£ £#li#£Tasiilaq Heliport (IATA airport code), Greenland£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Arithmetic-Geometric Mean £#/li#££#/ul#£




£#h3#£Ahlfors-Bers Theorem£#/h3#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Topology > Manifolds £#/li#££#/ul#£




£#h3#£Ahlfors Five Island Theorem£#/h3#£

£#h5#£ References £#/h5#£ £#ul#££#li#£Ahlfors, L. "Sur les fonctions inverses des fonctions méromorphes." Comptes Rendus Acad. Sci. Paris 194, 1145-1147, 1932. Reprinted in Lars Valerian Ahlfors: Collected Papers Volume 1, 1929-1955 (Ed. R. M. Shortt). Boston, MA: Birkhäuser, 149-151, 1982.£#/li#££#li#£Ahlfors, L. "Über die Kreise die von einer Riemannschen Fläche schlicht überdeckt werden." Comm. Math. Helv. 5, 28-38, 1933. Reprinted in Lars Valerian Ahlfors: Collected Papers Volume 1, 1929-1955 (Ed. R. M. Shortt). Boston, MA: Birkhäuser, 163-173, 1982.£#/li#££#li#£Bergweiler, W. "Iteration of Meromorphic Functions." Bull. Amer. Math. Soc. (N. S.) 29, 151-188, 1993.£#/li#££#li#£Hayman, W. K. Meromorphic Functions. Oxford, England: Oxford University Press, 1964.£#/li#££#li#£Nevanlinna, R. Analytic Functions. New York: Springer-Verlag, 1970.£#/li#££#li#£ Ahlfors, L. "Sur les fonctions inverses des fonctions méromorphes." Comptes Rendus Acad. Sci. Paris 194, 1145-1147, 1932. Reprinted in Lars Valerian Ahlfors: Collected Papers Volume 1, 1929-1955 (Ed. R. M. Shortt). Boston, MA: Birkhäuser, 149-151, 1982. £#/li#££#li#£ Ahlfors, L. "Über die Kreise die von einer Riemannschen Fläche schlicht überdeckt werden." Comm. Math. Helv. 5, 28-38, 1933. Reprinted in Lars Valerian Ahlfors: Collected Papers Volume 1, 1929-1955 (Ed. R. M. Shortt). Boston, MA: Birkhäuser, 163-173, 1982. £#/li#££#li#£ Bergweiler, W. "Iteration of Meromorphic Functions." Bull. Amer. Math. Soc. (N. S.) 29, 151-188, 1993. £#/li#££#li#£ Hayman, W. K. Meromorphic Functions. Oxford, England: Oxford University Press, 1964. £#/li#££#li#£ Nevanlinna, R. Analytic Functions. New York: Springer-Verlag, 1970. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Complex Analysis > Conformal Mapping £#/li#££#/ul#£




£#h3#£Ahmed's Integral£#/h3#£

£#h5#£ References £#/h5#£ £#ul#££#li#£Ahmed, Z. "Definitely An Integral." Amer. Math. Monthly 109, 670-671, 2002.£#/li#££#li#£Borwein, J.; Bailey, D.; and Girgensohn, R. "Ahmed's Integral Problem." §1.6 in Experimentation in Mathematics: Computational Paths to Discovery. Wellesley, MA: A K Peters, pp. 17-20, 2004.£#/li#££#li#£Sloane, N. J. A. Sequences A096615, A098459, and A102521 in "The On-Line Encyclopedia of Integer Sequences."£#/li#££#li#£ Ahmed, Z. "Definitely An Integral." Amer. Math. Monthly 109, 670-671, 2002. £#/li#££#li#£ Borwein, J.; Bailey, D.; and Girgensohn, R. "Ahmed's Integral Problem." §1.6 in Experimentation in Mathematics: Computational Paths to Discovery. Wellesley, MA: A K Peters, pp. 17-20, 2004. £#/li#££#li#£ Sloane, N. J. A. Sequences A096615, A098459, and A102521 in "The On-Line Encyclopedia of Integer Sequences." £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Calculus > Integrals > Definite Integrals £#/li#££#/ul#£




£#h3#£Airy-Fock Functions£#/h3#£

£#h5#£ References £#/h5#£ £#ul#££#li#£Babich, V. M. and Kirpichnikova, N. Ya. The Boundary Layer Method in Diffraction Problems. New York: Springer-Verlag, 1979.£#/li#££#li#£Babich, M. and Buldyrev, V. S. Asymptotic Methods in Short-Wavelength Diffraction Theory. Alpha Science, 2008.£#/li#££#li#£Fock, V. A. Electromagnetic Diffraction and Propagation Problems. Oxford, England: Pergamon Press, 1965.£#/li#££#li#£Fok, V. A. Tables of Airy Functions. Moscow, 1946.£#/li#££#li#£Hazewinkel, M. (Managing Ed.). Encyclopaedia of Mathematics: An Updated and Annotated Translation of the Soviet "Mathematical Encyclopaedia." Dordrecht, Netherlands: Reidel, p. 65, 1988.£#/li#££#li#£Kiselev, A. P.; Yarovoĭ, V. O.; and Vsemirnova, E. A. "Polarization Anomalies of Elastic Waves. Caustic and Penumbra." Zap. Nauchn. Sem. St.-Petersburg. Otdel. Mat. Inst. Steklov. (POMI) 297, 2003. Published in Mat. Vopr. Teor. Rasprostr. Voln. 32, 136-153 and 275-27. Translation in J. Math. Sci. (N. Y.) 127, 2413-2423, 2005.£#/li#££#li#£ Babich, V. M. and Kirpichnikova, N. Ya. The Boundary Layer Method in Diffraction Problems. New York: Springer-Verlag, 1979. £#/li#££#li#£ Babich, M. and Buldyrev, V. S. Asymptotic Methods in Short-Wavelength Diffraction Theory. Alpha Science, 2008. £#/li#££#li#£ Fock, V. A. Electromagnetic Diffraction and Propagation Problems. Oxford, England: Pergamon Press, 1965. £#/li#££#li#£ Fok, V. A. Tables of Airy Functions. Moscow, 1946. £#/li#££#li#£ Hazewinkel, M. (Managing Ed.). Encyclopaedia of Mathematics: An Updated and Annotated Translation of the Soviet "Mathematical Encyclopaedia." Dordrecht, Netherlands: Reidel, p. 65, 1988. £#/li#££#li#£ Kiselev, A. P.; Yarovoĭ, V. O.; and Vsemirnova, E. A. "Polarization Anomalies of Elastic Waves. Caustic and Penumbra." Zap. Nauchn. Sem. St.-Petersburg. Otdel. Mat. Inst. Steklov. (POMI) 297, 2003. Published in Mat. Vopr. Teor. Rasprostr. Voln. 32, 136-153 and 275-27. Translation in J. Math. Sci. (N. Y.) 127, 2413-2423, 2005. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Bessel Functions £#/li#££#/ul#£




£#h3#£Airy Differential Equation£#/h3#£

In the physical sciences, the Airy function (or Airy function of the first kind) Ai(x) is a special function named after the British astronomer George Biddell Airy (1801–1892). The function Ai(x) and the related function Bi(x), are linearly independent solutions to the differential equation

known as the Airy equation or the Stokes equation. This is the simplest second-order linear differential equation with a turning point (a point where the character of the solutions changes from oscillatory to exponential).
£#h5#£Definitions£#/h5#£
For real values of x, the Airy function of the first kind can be defined by the improper Riemann integral:

which converges by Dirichlet's test. For any real number ${\displaystyle x}$ there is positive real number ${\displaystyle M}$ such that function ${\textstyle {\dfrac {t^{3}}{3}}+xt}$ is increasing, unbounded and convex with continuous and unbounded derivative on interval ${\displaystyle [M,\infty )}$ . The convergence of the integral on this interval can be proven by Dirichlet's test after substitution ${\textstyle u={\dfrac {t^{3}}{3}}+xt}$ .
y = Ai(x) satisfies the Airy equation

This equation has two linearly independent solutions. Up to scalar multiplication, Ai(x) is the solution subject to the condition y → 0 as x → ∞. The standard choice for the other solution is the Airy function of the second kind, denoted Bi(x). It is defined as the solution with the same amplitude of oscillation as Ai(x) as x → −∞ which differs in phase by π/2:

£#h5#£Properties£#/h5#£
The values of Ai(x) and Bi(x) and their derivatives at x = 0 are given by

Here, Γ denotes the Gamma function. It follows that the Wronskian of Ai(x) and Bi(x) is 1/π.
When x is positive, Ai(x) is positive, convex, and decreasing exponentially to zero, while Bi(x) is positive, convex, and increasing exponentially. When x is negative, Ai(x) and Bi(x) oscillate around zero with ever-increasing frequency and ever-decreasing amplitude. This is supported by the asymptotic formulae below for the Airy functions.

The Airy functions are orthogonal in the sense that

again using an improper Riemann integral.
£#h5#£Asymptotic formulae£#/h5#£
As explained below, the Airy functions can be extended to the complex plane, giving entire functions. The asymptotic behaviour of the Airy functions as |z| goes to infinity at a constant value of arg(z) depends on arg(z): this is called the Stokes phenomenon. For |arg(z)| < π we have the following asymptotic formula for Ai(z):

and a similar one for Bi(z), but only applicable when |arg(z)| < π/3:
A more accurate formula for Ai(z) and a formula for Bi(z) when π/3 < |arg(z)| < π or, equivalently, for Ai(−z) and Bi(−z) when |arg(z)| < 2π/3 but not zero, are:

When |arg(z)| = 0 these are good approximations but are not asymptotic because the ratio between Ai(−z) or Bi(−z) and the above approximation goes to infinity whenever the sine or cosine goes to zero. Asymptotic expansions for these limits are also available. These are listed in (Abramowitz and Stegun, 1983) and (Olver, 1974).

One is also able to obtain asymptotic expressions for the derivatives Ai'(z) and Bi'(z). Similarly to before, when |arg(z)| < π:

When |arg(z)| < π/3 we have:

Similarly, an expression for Ai'(−z) and Bi'(−z) when |arg(z)| < 2π/3 but not zero, are


£#h5#£Complex arguments£#/h5#£
We can extend the definition of the Airy function to the complex plane by

where the integral is over a path C starting at the point at infinity with argument −π/3 and ending at the point at infinity with argument π/3. Alternatively, we can use the differential equation y′′ − xy = 0 to extend Ai(x) and Bi(x) to entire functions on the complex plane.
The asymptotic formula for Ai(x) is still valid in the complex plane if the principal value of x2/3 is taken and x is bounded away from the negative real axis. The formula for Bi(x) is valid provided x is in the sector {x ∈ C : |arg(x)| < (π/3) − δ} for some positive δ. Finally, the formulae for Ai(−x) and Bi(−x) are valid if x is in the sector {x ∈ C : |arg(x)| < (2π/3) − δ}.

It follows from the asymptotic behaviour of the Airy functions that both Ai(x) and Bi(x) have an infinity of zeros on the negative real axis. The function Ai(x) has no other zeros in the complex plane, while the function Bi(x) also has infinitely many zeros in the sector {z ∈ C : π/3 < |arg(z)| < π/2}.


£#h5#£Plots£#/h5#£
£#h5#£Relation to other special functions£#/h5#£
For positive arguments, the Airy functions are related to the modified Bessel functions:

Here, I±1/3 and K1/3 are solutions of
The first derivative of the Airy function is

Functions K1/3 and K2/3 can be represented in terms of rapidly convergent integrals (see also modified Bessel functions )

For negative arguments, the Airy function are related to the Bessel functions:

Here, J±1/3 are solutions of
The Scorer's functions Hi(x) and -Gi(x) solve the equation y′′ − xy = 1/π. They can also be expressed in terms of the Airy functions:


£#h5#£Fourier transform£#/h5#£
Using the definition of the Airy function Ai(x), it is straightforward to show its Fourier transform is given by


£#h5#£Applications£#/h5#£
£#h5#£Quantum mechanics£#/h5#£
The Airy function is the solution to the time-independent Schrödinger equation for a particle confined within a triangular potential well and for a particle in a one-dimensional constant force field. For the same reason, it also serves to provide uniform semiclassical approximations near a turning point in the WKB approximation, when the potential may be locally approximated by a linear function of position. The triangular potential well solution is directly relevant for the understanding of electrons trapped in semiconductor heterojunctions.


£#h5#£Optics£#/h5#£
A transversally asymmetric optical beam, where the electric field profile is given by Airy function, has the interesting property that of its maximum intensity accelerates towards one side instead of propagating over straight line as is the case in symmetric beams. This is at expense of the low-intensity tail being spread in the opposite direction, so the overall momentum of the beam is of course conserved.


£#h5#£Caustics£#/h5#£
The Airy function underlies the form of the intensity near an optical directional caustic, such as that of the rainbow. Historically, this was the mathematical problem that led Airy to develop this special function.


£#h5#£History£#/h5#£
The Airy function is named after the British astronomer and physicist George Biddell Airy (1801–1892), who encountered it in his early study of optics in physics (Airy 1838). The notation Ai(x) was introduced by Harold Jeffreys. Airy had become the British Astronomer Royal in 1835, and he held that post until his retirement in 1881.


£#h5#£See also£#/h5#£ £#ul#££#li#£The proof of Witten's conjecture used a matrix-valued generalization of the Airy function.£#/li#£ £#li#£Airy zeta function£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Abramowitz, Milton; Stegun, Irene Ann, eds. (1983) [June 1964]. "Chapter 10". Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables. Applied Mathematics Series. Vol. 55 (Ninth reprint with additional corrections of tenth original printing with corrections (December 1972); first ed.). Washington D.C.; New York: United States Department of Commerce, National Bureau of Standards; Dover Publications. p. 448. ISBN 978-0-486-61272-0. LCCN 64-60036. MR 0167642. LCCN 65-12253.£#/li#£ £#li#£Airy (1838), "On the intensity of light in the neighbourhood of a caustic", Transactions of the Cambridge Philosophical Society, University Press, 6: 379–402, Bibcode:1838TCaPS...6..379A£#/li#£ £#li#£Frank William John Olver (1974). Asymptotics and Special Functions, Chapter 11. Academic Press, New York.£#/li#£ £#li#£Press, WH; Teukolsky, SA; Vetterling, WT; Flannery, BP (2007), "Section 6.6.3. Airy Functions", Numerical Recipes: The Art of Scientific Computing (3rd ed.), New York: Cambridge University Press, ISBN 978-0-521-88068-8£#/li#£ £#li#£Vallée, Olivier; Soares, Manuel (2004), Airy functions and applications to physics, London: Imperial College Press, ISBN 978-1-86094-478-9, MR 2114198, archived from the original on 2010-01-13, retrieved 2010-05-14£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Airy functions", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#£ £#li#£Weisstein, Eric W. "Airy Functions". MathWorld.£#/li#£ £#li#£Wolfram function pages for Ai and Bi functions. Includes formulas, function evaluator, and plotting calculator.£#/li#£ £#li#£Olver, F. W. J. (2010), "Airy and related functions", in Olver, Frank W. J.; Lozier, Daniel M.; Boisvert, Ronald F.; Clark, Charles W. (eds.), NIST Handbook of Mathematical Functions, Cambridge University Press, ISBN 978-0-521-19225-5, MR 2723248£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Abramowitz, M. and Stegun, I. A. (Eds.). "Airy Functions." §10.4.1 in Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing. New York: Dover, pp. 446-452, 1972.£#/li#££#li#£Zwillinger, D. (Ed.). CRC Standard Mathematical Tables and Formulae. Boca Raton, FL: CRC Press, p. 413, 1995.£#/li#££#li#£Zwillinger, D. Handbook of Differential Equations, 3rd ed. Boston, MA: Academic Press, p. 121, 1997.£#/li#££#li#£ Abramowitz, M. and Stegun, I. A. (Eds.). "Airy Functions." §10.4.1 in Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing. New York: Dover, pp. 446-452, 1972. £#/li#££#li#£ Zwillinger, D. (Ed.). CRC Standard Mathematical Tables and Formulae. Boca Raton, FL: CRC Press, p. 413, 1995. £#/li#££#li#£ Zwillinger, D. Handbook of Differential Equations, 3rd ed. Boston, MA: Academic Press, p. 121, 1997. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Differential Equations > Ordinary Differential Equations £#/li#££#/ul#£




£#h3#£Airy Functions£#/h3#£

In the physical sciences, the Airy function (or Airy function of the first kind) Ai(x) is a special function named after the British astronomer George Biddell Airy (1801–1892). The function Ai(x) and the related function Bi(x), are linearly independent solutions to the differential equation

known as the Airy equation or the Stokes equation. This is the simplest second-order linear differential equation with a turning point (a point where the character of the solutions changes from oscillatory to exponential).
£#h5#£Definitions£#/h5#£
For real values of x, the Airy function of the first kind can be defined by the improper Riemann integral:

which converges by Dirichlet's test. For any real number ${\displaystyle x}$ there is positive real number ${\displaystyle M}$ such that function ${\textstyle {\dfrac {t^{3}}{3}}+xt}$ is increasing, unbounded and convex with continuous and unbounded derivative on interval ${\displaystyle [M,\infty )}$ . The convergence of the integral on this interval can be proven by Dirichlet's test after substitution ${\textstyle u={\dfrac {t^{3}}{3}}+xt}$ .
y = Ai(x) satisfies the Airy equation

This equation has two linearly independent solutions. Up to scalar multiplication, Ai(x) is the solution subject to the condition y → 0 as x → ∞. The standard choice for the other solution is the Airy function of the second kind, denoted Bi(x). It is defined as the solution with the same amplitude of oscillation as Ai(x) as x → −∞ which differs in phase by π/2:

£#h5#£Properties£#/h5#£
The values of Ai(x) and Bi(x) and their derivatives at x = 0 are given by

Here, Γ denotes the Gamma function. It follows that the Wronskian of Ai(x) and Bi(x) is 1/π.
When x is positive, Ai(x) is positive, convex, and decreasing exponentially to zero, while Bi(x) is positive, convex, and increasing exponentially. When x is negative, Ai(x) and Bi(x) oscillate around zero with ever-increasing frequency and ever-decreasing amplitude. This is supported by the asymptotic formulae below for the Airy functions.

The Airy functions are orthogonal in the sense that

again using an improper Riemann integral.
£#h5#£Asymptotic formulae£#/h5#£
As explained below, the Airy functions can be extended to the complex plane, giving entire functions. The asymptotic behaviour of the Airy functions as |z| goes to infinity at a constant value of arg(z) depends on arg(z): this is called the Stokes phenomenon. For |arg(z)| < π we have the following asymptotic formula for Ai(z):

and a similar one for Bi(z), but only applicable when |arg(z)| < π/3:
A more accurate formula for Ai(z) and a formula for Bi(z) when π/3 < |arg(z)| < π or, equivalently, for Ai(−z) and Bi(−z) when |arg(z)| < 2π/3 but not zero, are:

When |arg(z)| = 0 these are good approximations but are not asymptotic because the ratio between Ai(−z) or Bi(−z) and the above approximation goes to infinity whenever the sine or cosine goes to zero. Asymptotic expansions for these limits are also available. These are listed in (Abramowitz and Stegun, 1983) and (Olver, 1974).

One is also able to obtain asymptotic expressions for the derivatives Ai'(z) and Bi'(z). Similarly to before, when |arg(z)| < π:

When |arg(z)| < π/3 we have:

Similarly, an expression for Ai'(−z) and Bi'(−z) when |arg(z)| < 2π/3 but not zero, are


£#h5#£Complex arguments£#/h5#£
We can extend the definition of the Airy function to the complex plane by

where the integral is over a path C starting at the point at infinity with argument −π/3 and ending at the point at infinity with argument π/3. Alternatively, we can use the differential equation y′′ − xy = 0 to extend Ai(x) and Bi(x) to entire functions on the complex plane.
The asymptotic formula for Ai(x) is still valid in the complex plane if the principal value of x2/3 is taken and x is bounded away from the negative real axis. The formula for Bi(x) is valid provided x is in the sector {x ∈ C : |arg(x)| < (π/3) − δ} for some positive δ. Finally, the formulae for Ai(−x) and Bi(−x) are valid if x is in the sector {x ∈ C : |arg(x)| < (2π/3) − δ}.

It follows from the asymptotic behaviour of the Airy functions that both Ai(x) and Bi(x) have an infinity of zeros on the negative real axis. The function Ai(x) has no other zeros in the complex plane, while the function Bi(x) also has infinitely many zeros in the sector {z ∈ C : π/3 < |arg(z)| < π/2}.


£#h5#£Plots£#/h5#£
£#h5#£Relation to other special functions£#/h5#£
For positive arguments, the Airy functions are related to the modified Bessel functions:

Here, I±1/3 and K1/3 are solutions of
The first derivative of the Airy function is

Functions K1/3 and K2/3 can be represented in terms of rapidly convergent integrals (see also modified Bessel functions )

For negative arguments, the Airy function are related to the Bessel functions:

Here, J±1/3 are solutions of
The Scorer's functions Hi(x) and -Gi(x) solve the equation y′′ − xy = 1/π. They can also be expressed in terms of the Airy functions:


£#h5#£Fourier transform£#/h5#£
Using the definition of the Airy function Ai(x), it is straightforward to show its Fourier transform is given by


£#h5#£Applications£#/h5#£
£#h5#£Quantum mechanics£#/h5#£
The Airy function is the solution to the time-independent Schrödinger equation for a particle confined within a triangular potential well and for a particle in a one-dimensional constant force field. For the same reason, it also serves to provide uniform semiclassical approximations near a turning point in the WKB approximation, when the potential may be locally approximated by a linear function of position. The triangular potential well solution is directly relevant for the understanding of electrons trapped in semiconductor heterojunctions.


£#h5#£Optics£#/h5#£
A transversally asymmetric optical beam, where the electric field profile is given by Airy function, has the interesting property that of its maximum intensity accelerates towards one side instead of propagating over straight line as is the case in symmetric beams. This is at expense of the low-intensity tail being spread in the opposite direction, so the overall momentum of the beam is of course conserved.


£#h5#£Caustics£#/h5#£
The Airy function underlies the form of the intensity near an optical directional caustic, such as that of the rainbow. Historically, this was the mathematical problem that led Airy to develop this special function.


£#h5#£History£#/h5#£
The Airy function is named after the British astronomer and physicist George Biddell Airy (1801–1892), who encountered it in his early study of optics in physics (Airy 1838). The notation Ai(x) was introduced by Harold Jeffreys. Airy had become the British Astronomer Royal in 1835, and he held that post until his retirement in 1881.


£#h5#£See also£#/h5#£ £#ul#££#li#£The proof of Witten's conjecture used a matrix-valued generalization of the Airy function.£#/li#£ £#li#£Airy zeta function£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Abramowitz, Milton; Stegun, Irene Ann, eds. (1983) [June 1964]. "Chapter 10". Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables. Applied Mathematics Series. Vol. 55 (Ninth reprint with additional corrections of tenth original printing with corrections (December 1972); first ed.). Washington D.C.; New York: United States Department of Commerce, National Bureau of Standards; Dover Publications. p. 448. ISBN 978-0-486-61272-0. LCCN 64-60036. MR 0167642. LCCN 65-12253.£#/li#£ £#li#£Airy (1838), "On the intensity of light in the neighbourhood of a caustic", Transactions of the Cambridge Philosophical Society, University Press, 6: 379–402, Bibcode:1838TCaPS...6..379A£#/li#£ £#li#£Frank William John Olver (1974). Asymptotics and Special Functions, Chapter 11. Academic Press, New York.£#/li#£ £#li#£Press, WH; Teukolsky, SA; Vetterling, WT; Flannery, BP (2007), "Section 6.6.3. Airy Functions", Numerical Recipes: The Art of Scientific Computing (3rd ed.), New York: Cambridge University Press, ISBN 978-0-521-88068-8£#/li#£ £#li#£Vallée, Olivier; Soares, Manuel (2004), Airy functions and applications to physics, London: Imperial College Press, ISBN 978-1-86094-478-9, MR 2114198, archived from the original on 2010-01-13, retrieved 2010-05-14£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Airy functions", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#£ £#li#£Weisstein, Eric W. "Airy Functions". MathWorld.£#/li#£ £#li#£Wolfram function pages for Ai and Bi functions. Includes formulas, function evaluator, and plotting calculator.£#/li#£ £#li#£Olver, F. W. J. (2010), "Airy and related functions", in Olver, Frank W. J.; Lozier, Daniel M.; Boisvert, Ronald F.; Clark, Charles W. (eds.), NIST Handbook of Mathematical Functions, Cambridge University Press, ISBN 978-0-521-19225-5, MR 2723248£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Abramowitz, M. and Stegun, I. A. (Eds.). "Airy Functions." §10.4 in Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing. New York: Dover, pp. 446-452, 1972.£#/li#££#li#£Banderier, C.; Flajolet, P.; Schaeffer, G.; and Soria, M. "Planar Maps and Airy Phenomena." In Automata, Languages and Programming. Proceedings of the 27th International Colloquium (ICALP 2000) held at the University of Geneva, Geneva, July 9-15, 2000 (Ed. U. Montanari, J. D. P. Rolim, and E. Welzl). Berlin: Springer, pp. 388-402, 2000.£#/li#££#li#£Press, W. H.; Flannery, B. P.; Teukolsky, S. A.; and Vetterling, W. T. "Bessel Functions of Fractional Order, Airy Functions, Spherical Bessel Functions." §6.7 in Numerical Recipes in FORTRAN: The Art of Scientific Computing, 2nd ed. Cambridge, England: Cambridge University Press, pp. 234-245, 1992.£#/li#££#li#£Sloane, N. J. A. Sequences A096714 and A096715 in "The On-Line Encyclopedia of Integer Sequences."£#/li#££#li#£Spanier, J. and Oldham, K. B. "The Airy Functions Ai(x) and Bi(x)." Ch. 56 in An Atlas of Functions. Washington, DC: Hemisphere, pp. 555-562, 1987.£#/li#££#li#£Watson, G. N. A Treatise on the Theory of Bessel Functions, 2nd ed. Cambridge, England: Cambridge University Press, 1966.£#/li#££#li#£ Abramowitz, M. and Stegun, I. A. (Eds.). "Airy Functions." §10.4 in Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing. New York: Dover, pp. 446-452, 1972. £#/li#££#li#£ Banderier, C.; Flajolet, P.; Schaeffer, G.; and Soria, M. "Planar Maps and Airy Phenomena." In Automata, Languages and Programming. Proceedings of the 27th International Colloquium (ICALP 2000) held at the University of Geneva, Geneva, July 9-15, 2000 (Ed. U. Montanari, J. D. P. Rolim, and E. Welzl). Berlin: Springer, pp. 388-402, 2000. £#/li#££#li#£ Press, W. H.; Flannery, B. P.; Teukolsky, S. A.; and Vetterling, W. T. "Bessel Functions of Fractional Order, Airy Functions, Spherical Bessel Functions." §6.7 in Numerical Recipes in FORTRAN: The Art of Scientific Computing, 2nd ed. Cambridge, England: Cambridge University Press, pp. 234-245, 1992. £#/li#££#li#£ Sloane, N. J. A. Sequences A096714 and A096715 in "The On-Line Encyclopedia of Integer Sequences." £#/li#££#li#£ Spanier, J. and Oldham, K. B. "The Airy Functions Ai() and Bi()." Ch. 56 in An Atlas of Functions. Washington, DC: Hemisphere, pp. 555-562, 1987. £#/li#££#li#£ Watson, G. N. A Treatise on the Theory of Bessel Functions, 2nd ed. Cambridge, England: Cambridge University Press, 1966. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Bessel Functions £#/li#££#li#£ Calculus and Analysis > Calculus > Integrals > Definite Integrals £#/li#££#li#£ Calculus and Analysis > Complex Analysis > Entire Functions £#/li#££#li#£ Interactive Entries > webMathematica Examples £#/li#££#/ul#£




£#h3#£Airy Function Zeros£#/h3#£

In the physical sciences, the Airy function (or Airy function of the first kind) Ai(x) is a special function named after the British astronomer George Biddell Airy (1801–1892). The function Ai(x) and the related function Bi(x), are linearly independent solutions to the differential equation

known as the Airy equation or the Stokes equation. This is the simplest second-order linear differential equation with a turning point (a point where the character of the solutions changes from oscillatory to exponential).
£#h5#£Definitions£#/h5#£
For real values of x, the Airy function of the first kind can be defined by the improper Riemann integral:

which converges by Dirichlet's test. For any real number ${\displaystyle x}$ there is positive real number ${\displaystyle M}$ such that function ${\textstyle {\dfrac {t^{3}}{3}}+xt}$ is increasing, unbounded and convex with continuous and unbounded derivative on interval ${\displaystyle [M,\infty )}$ . The convergence of the integral on this interval can be proven by Dirichlet's test after substitution ${\textstyle u={\dfrac {t^{3}}{3}}+xt}$ .
y = Ai(x) satisfies the Airy equation

This equation has two linearly independent solutions. Up to scalar multiplication, Ai(x) is the solution subject to the condition y → 0 as x → ∞. The standard choice for the other solution is the Airy function of the second kind, denoted Bi(x). It is defined as the solution with the same amplitude of oscillation as Ai(x) as x → −∞ which differs in phase by π/2:

£#h5#£Properties£#/h5#£
The values of Ai(x) and Bi(x) and their derivatives at x = 0 are given by

Here, Γ denotes the Gamma function. It follows that the Wronskian of Ai(x) and Bi(x) is 1/π.
When x is positive, Ai(x) is positive, convex, and decreasing exponentially to zero, while Bi(x) is positive, convex, and increasing exponentially. When x is negative, Ai(x) and Bi(x) oscillate around zero with ever-increasing frequency and ever-decreasing amplitude. This is supported by the asymptotic formulae below for the Airy functions.

The Airy functions are orthogonal in the sense that

again using an improper Riemann integral.
£#h5#£Asymptotic formulae£#/h5#£
As explained below, the Airy functions can be extended to the complex plane, giving entire functions. The asymptotic behaviour of the Airy functions as |z| goes to infinity at a constant value of arg(z) depends on arg(z): this is called the Stokes phenomenon. For |arg(z)| < π we have the following asymptotic formula for Ai(z):

and a similar one for Bi(z), but only applicable when |arg(z)| < π/3:
A more accurate formula for Ai(z) and a formula for Bi(z) when π/3 < |arg(z)| < π or, equivalently, for Ai(−z) and Bi(−z) when |arg(z)| < 2π/3 but not zero, are:

When |arg(z)| = 0 these are good approximations but are not asymptotic because the ratio between Ai(−z) or Bi(−z) and the above approximation goes to infinity whenever the sine or cosine goes to zero. Asymptotic expansions for these limits are also available. These are listed in (Abramowitz and Stegun, 1983) and (Olver, 1974).

One is also able to obtain asymptotic expressions for the derivatives Ai'(z) and Bi'(z). Similarly to before, when |arg(z)| < π:

When |arg(z)| < π/3 we have:

Similarly, an expression for Ai'(−z) and Bi'(−z) when |arg(z)| < 2π/3 but not zero, are


£#h5#£Complex arguments£#/h5#£
We can extend the definition of the Airy function to the complex plane by

where the integral is over a path C starting at the point at infinity with argument −π/3 and ending at the point at infinity with argument π/3. Alternatively, we can use the differential equation y′′ − xy = 0 to extend Ai(x) and Bi(x) to entire functions on the complex plane.
The asymptotic formula for Ai(x) is still valid in the complex plane if the principal value of x2/3 is taken and x is bounded away from the negative real axis. The formula for Bi(x) is valid provided x is in the sector {x ∈ C : |arg(x)| < (π/3) − δ} for some positive δ. Finally, the formulae for Ai(−x) and Bi(−x) are valid if x is in the sector {x ∈ C : |arg(x)| < (2π/3) − δ}.

It follows from the asymptotic behaviour of the Airy functions that both Ai(x) and Bi(x) have an infinity of zeros on the negative real axis. The function Ai(x) has no other zeros in the complex plane, while the function Bi(x) also has infinitely many zeros in the sector {z ∈ C : π/3 < |arg(z)| < π/2}.


£#h5#£Plots£#/h5#£
£#h5#£Relation to other special functions£#/h5#£
For positive arguments, the Airy functions are related to the modified Bessel functions:

Here, I±1/3 and K1/3 are solutions of
The first derivative of the Airy function is

Functions K1/3 and K2/3 can be represented in terms of rapidly convergent integrals (see also modified Bessel functions )

For negative arguments, the Airy function are related to the Bessel functions:

Here, J±1/3 are solutions of
The Scorer's functions Hi(x) and -Gi(x) solve the equation y′′ − xy = 1/π. They can also be expressed in terms of the Airy functions:


£#h5#£Fourier transform£#/h5#£
Using the definition of the Airy function Ai(x), it is straightforward to show its Fourier transform is given by


£#h5#£Applications£#/h5#£
£#h5#£Quantum mechanics£#/h5#£
The Airy function is the solution to the time-independent Schrödinger equation for a particle confined within a triangular potential well and for a particle in a one-dimensional constant force field. For the same reason, it also serves to provide uniform semiclassical approximations near a turning point in the WKB approximation, when the potential may be locally approximated by a linear function of position. The triangular potential well solution is directly relevant for the understanding of electrons trapped in semiconductor heterojunctions.


£#h5#£Optics£#/h5#£
A transversally asymmetric optical beam, where the electric field profile is given by Airy function, has the interesting property that of its maximum intensity accelerates towards one side instead of propagating over straight line as is the case in symmetric beams. This is at expense of the low-intensity tail being spread in the opposite direction, so the overall momentum of the beam is of course conserved.


£#h5#£Caustics£#/h5#£
The Airy function underlies the form of the intensity near an optical directional caustic, such as that of the rainbow. Historically, this was the mathematical problem that led Airy to develop this special function.


£#h5#£History£#/h5#£
The Airy function is named after the British astronomer and physicist George Biddell Airy (1801–1892), who encountered it in his early study of optics in physics (Airy 1838). The notation Ai(x) was introduced by Harold Jeffreys. Airy had become the British Astronomer Royal in 1835, and he held that post until his retirement in 1881.


£#h5#£See also£#/h5#£ £#ul#££#li#£The proof of Witten's conjecture used a matrix-valued generalization of the Airy function.£#/li#£ £#li#£Airy zeta function£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Abramowitz, Milton; Stegun, Irene Ann, eds. (1983) [June 1964]. "Chapter 10". Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables. Applied Mathematics Series. Vol. 55 (Ninth reprint with additional corrections of tenth original printing with corrections (December 1972); first ed.). Washington D.C.; New York: United States Department of Commerce, National Bureau of Standards; Dover Publications. p. 448. ISBN 978-0-486-61272-0. LCCN 64-60036. MR 0167642. LCCN 65-12253.£#/li#£ £#li#£Airy (1838), "On the intensity of light in the neighbourhood of a caustic", Transactions of the Cambridge Philosophical Society, University Press, 6: 379–402, Bibcode:1838TCaPS...6..379A£#/li#£ £#li#£Frank William John Olver (1974). Asymptotics and Special Functions, Chapter 11. Academic Press, New York.£#/li#£ £#li#£Press, WH; Teukolsky, SA; Vetterling, WT; Flannery, BP (2007), "Section 6.6.3. Airy Functions", Numerical Recipes: The Art of Scientific Computing (3rd ed.), New York: Cambridge University Press, ISBN 978-0-521-88068-8£#/li#£ £#li#£Vallée, Olivier; Soares, Manuel (2004), Airy functions and applications to physics, London: Imperial College Press, ISBN 978-1-86094-478-9, MR 2114198, archived from the original on 2010-01-13, retrieved 2010-05-14£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Airy functions", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#£ £#li#£Weisstein, Eric W. "Airy Functions". MathWorld.£#/li#£ £#li#£Wolfram function pages for Ai and Bi functions. Includes formulas, function evaluator, and plotting calculator.£#/li#£ £#li#£Olver, F. W. J. (2010), "Airy and related functions", in Olver, Frank W. J.; Lozier, Daniel M.; Boisvert, Ronald F.; Clark, Charles W. (eds.), NIST Handbook of Mathematical Functions, Cambridge University Press, ISBN 978-0-521-19225-5, MR 2723248£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Abramowitz, M. and Stegun, I. A. (Eds.). "Airy Functions." §10.4 in Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing. New York: Dover, pp. 446-452, 1972.£#/li#££#li#£Fabijonas, B. R. "Algorithm 838: Airy Functions." ACM Trans. Math. Software 30, 491-501, 2004.£#/li#££#li#£Fabijonas, B. R.; Lozier, D. W.; and Olver, F. W. J. "Computation of Complex Airy Functions and Their Zeros Using Asymptotics and the Differential Equation." ACM Trans. Math. Software 30, 471-490, 2004.£#/li#££#li#£Sherry, M. "The Zeros and Maxima of the Airy Function and Its First Derivative to 25 Significant Figures." Air Force Cambridge Research Center. Tech. Rep. AFCRC-TR-59-135, ASTIA AD214568. April, 1959.£#/li#££#li#£ Abramowitz, M. and Stegun, I. A. (Eds.). "Airy Functions." §10.4 in Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing. New York: Dover, pp. 446-452, 1972. £#/li#££#li#£ Fabijonas, B. R. "Algorithm 838: Airy Functions." ACM Trans. Math. Software 30, 491-501, 2004. £#/li#££#li#£ Fabijonas, B. R.; Lozier, D. W.; and Olver, F. W. J. "Computation of Complex Airy Functions and Their Zeros Using Asymptotics and the Differential Equation." ACM Trans. Math. Software 30, 471-490, 2004. £#/li#££#li#£ Sherry, M. "The Zeros and Maxima of the Airy Function and Its First Derivative to 25 Significant Figures." Air Force Cambridge Research Center. Tech. Rep. AFCRC-TR-59-135, ASTIA AD214568. April, 1959. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Bessel Functions £#/li#££#li#£ Calculus and Analysis > Roots £#/li#££#/ul#£




£#h3#£Airy Zeta Function£#/h3#£

In mathematics, the Airy zeta function, studied by Crandall (1996), is a function analogous to the Riemann zeta function and related to the zeros of the Airy function.


£#h5#£Definition£#/h5#£
The Airy function

${\displaystyle \mathrm {Ai} (x)={\frac {1}{\pi }}\int _{0}^{\infty }\cos \left({\tfrac {1}{3}}t^{3}+xt\right)\,dt,}$
is positive for positive x, but oscillates for negative values of x. The Airy zeros are the values ${\displaystyle \{a_{i}\}_{i=1}^{\infty }}$ at which ${\displaystyle {\text{Ai}}(a_{i})=0}$ , ordered by increasing magnitude: ${\displaystyle |a_{1}|<|a_{2}|<\cdots }$ .

The Airy zeta function is the function defined from this sequence of zeros by the series

${\displaystyle \zeta _{\mathrm {Ai} }(s)=\sum _{i=1}^{\infty }{\frac {1}{|a_{i}|^{s}}}.}$
This series converges when the real part of s is greater than 3/2, and may be extended by analytic continuation to other values of s.


£#h5#£Evaluation at integers£#/h5#£
Like the Riemann zeta function, whose value ${\displaystyle \zeta (2)=\pi ^{2}/6}$ is the solution to the Basel problem, the Airy zeta function may be exactly evaluated at s = 2:

${\displaystyle \zeta _{\mathrm {Ai} }(2)=\sum _{i=1}^{\infty }{\frac {1}{a_{i}^{2}}}={\frac {3^{5/3}\Gamma ^{4}({\frac {2}{3}})}{4\pi ^{2}}},}$
where ${\displaystyle \Gamma }$ is the gamma function, a continuous variant of the factorial. Similar evaluations are also possible for larger integer values of s.

It is conjectured that the analytic continuation of the Airy zeta function evaluates at 1 to

${\displaystyle \zeta _{\mathrm {Ai} }(1)=-{\frac {\Gamma ({\frac {2}{3}})}{\Gamma ({\frac {4}{3}}){\sqrt[{3}]{9}}}}.}$

£#h5#£References£#/h5#£ £#ul#££#li#£Crandall, Richard E. (1996), "On the quantum zeta function", Journal of Physics A: Mathematical and General, 29 (21): 6795–6816, Bibcode:1996JPhA...29.6795C, doi:10.1088/0305-4470/29/21/014, ISSN 0305-4470, MR 1421901£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Airy Zeta Function". MathWorld.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Borwein, J.; Bailey, D.; and Girgensohn, R. Experimentation in Mathematics: Computational Paths to Discovery. Wellesley, MA: A K Peters, pp. 61-62, 2004.£#/li#££#li#£Crandall, R. E. "On the Quantum Zeta Function." J. Phys. A: Math. General 29, 6795-6816, 1996.£#/li#££#li#£Sloane, N. J. A. Sequences A096631 and A096632 in "The On-Line Encyclopedia of Integer Sequences."£#/li#££#li#£ Borwein, J.; Bailey, D.; and Girgensohn, R. Experimentation in Mathematics: Computational Paths to Discovery. Wellesley, MA: A K Peters, pp. 61-62, 2004. £#/li#££#li#£ Crandall, R. E. "On the Quantum Zeta Function." J. Phys. A: Math. General 29, 6795-6816, 1996. £#/li#££#li#£ Sloane, N. J. A. Sequences A096631 and A096632 in "The On-Line Encyclopedia of Integer Sequences." £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Miscellaneous Special Functions £#/li#££#li#£ Calculus and Analysis > Calculus > Integrals > Definite Integrals £#/li#££#/ul#£




£#h3#£Aitken's Delta-Squared Process£#/h3#£

In numerical analysis, Aitken's delta-squared process or Aitken Extrapolation is a series acceleration method, used for accelerating the rate of convergence of a sequence. It is named after Alexander Aitken, who introduced this method in 1926. Its early form was known to Seki Kōwa (end of 17th century) and was found for rectification of the circle, i.e. the calculation of π. It is most useful for accelerating the convergence of a sequence that is converging linearly.


£#h5#£Definition£#/h5#£
Given a sequence ${\displaystyle X={(x_{n})}_{n\in \mathbb {N} }}$ , one associates with this sequence the new sequence

${\displaystyle AX={\left({\frac {x_{n}\,x_{n+2}-x_{n+1}^{2}}{x_{n}+x_{n+2}-2\,x_{n+1}}}\right)}_{n\in \mathbb {Z} ^{*}},}$
which can, with improved numerical stability, also be written as

${\displaystyle (AX)_{n}=x_{n}-{\frac {(\Delta x_{n})^{2}}{\Delta ^{2}x_{n}}},}$
or equivalently as

${\displaystyle (AX)_{n}=x_{n+2}-{\frac {(\Delta x_{n+1})^{2}}{\Delta ^{2}x_{n}}}=x_{n+2}-{\frac {(x_{n+2}-x_{n+1})^{2}}{(x_{n+2}-x_{n+1})-(x_{n+1}-x_{n})}}}$
where

${\displaystyle \Delta x_{n}={(x_{n+1}-x_{n})},\ \Delta x_{n+1}={(x_{n+2}-x_{n+1})},}$
and

${\displaystyle \Delta ^{2}x_{n}=x_{n}-2x_{n+1}+x_{n+2}=\Delta x_{n+1}-\Delta x_{n},\ }$
for ${\displaystyle n=0,1,2,3,\dots \,}$

Obviously, ${\displaystyle AX}$ is ill-defined if ${\displaystyle \Delta ^{2}x}$ contains a zero element, or equivalently, if the sequence of first differences has a repeating term.

From a theoretical point of view, if that occurs only for a finite number of indices, one could easily agree to consider the sequence ${\displaystyle AX}$ restricted to indices ${\displaystyle n>n_{0}}$ with a sufficiently large ${\displaystyle n_{0}}$ . From a practical point of view, one does in general rather consider only the first few terms of the sequence, which usually provide the needed precision. Moreover, when numerically computing the sequence, one has to take care to stop the computation when rounding errors in the denominator become too large, where the Δ² operation may cancel too many significant digits. (It would be better for numerical calculation to use ${\displaystyle \Delta x_{n+1}-\Delta x_{n}\ =(x_{n+2}-x_{n+1})-(x_{n+1}-x_{n})\ }$ rather than ${\displaystyle x_{n}-2x_{n+1}+x_{n+2}\ }$ .)


£#h5#£Properties£#/h5#£
Aitken's delta-squared process is a method of acceleration of convergence, and a particular case of a nonlinear sequence transformation.

${\displaystyle \{x_{n}\}_{n\in \mathbb {N} }}$ will converge linearly to ${\displaystyle \ell }$ if there exists a number μ ∈ (0, 1) such that

${\displaystyle \lim _{n\to \infty }{\frac {|x_{n+1}-\ell |}{|x_{n}-\ell |}}=\mu .}$
Aitken's method will accelerate the sequence ${\displaystyle x_{n}}$ if ${\displaystyle \lim _{n\to \infty }{\frac {(Ax)_{n}-\ell }{x_{n}-\ell }}=0.}$

${\displaystyle A}$ is not a linear operator, but a constant term drops out, viz: ${\displaystyle A[x-\ell ]=Ax-\ell }$ , if ${\displaystyle \ell }$ is a constant. This is clear from the expression of ${\displaystyle Ax}$ in terms of the finite difference operator ${\displaystyle \Delta }$ .

Although the new process does not in general converge quadratically, it can be shown that for a fixed point process, that is, for an iterated function sequence ${\displaystyle x_{n+1}=f(x_{n})}$ for some function ${\displaystyle f}$ , converging to a fixed point, the convergence is quadratic. In this case, the technique is known as Steffensen's method.

Empirically, the A-operation eliminates the "most important error term". One can check this by considering a sequence of the form ${\displaystyle x_{n}=\ell +a^{n}+b^{n}}$ , where ${\displaystyle 0<b<a<1}$ : The sequence ${\displaystyle Ax}$ will then go to the limit like ${\displaystyle b^{n}}$ goes to zero.

Geometrically, the graph of an exponential function ${\displaystyle f(t)}$ that satisfies ${\displaystyle f(n)=x_{n}}$ , ${\displaystyle f(n+1)=x_{n+1}}$ and ${\displaystyle f(n+2)=x_{n+2}}$ has an horizontal asymptote at ${\displaystyle {\frac {x_{n}x_{n+2}-x_{n+1}^{2}}{x_{n}-2x_{n+1}+x_{n+2}}}}$ (if ${\displaystyle x_{n}-2x_{n+1}+x_{n+2}\neq 0}$ ).

One can also show that if ${\displaystyle x}$ goes to its limit ${\displaystyle \ell }$ at a rate strictly greater than 1, ${\displaystyle Ax}$ does not have a better rate of convergence. (In practice, one rarely has e.g. quadratic convergence which would mean over 30 resp. 100 correct decimal places after 5 resp. 7 iterations (starting with 1 correct digit); usually no acceleration is needed in that case.)

In practice, ${\displaystyle Ax}$ converges much faster to the limit than ${\displaystyle x}$ does, as demonstrated by the example calculations below. Usually, it is much cheaper to calculate ${\displaystyle Ax}$ (involving only calculation of differences, one multiplication and one division) than to calculate many more terms of the sequence ${\displaystyle x}$ . Care must be taken, however, to avoid introducing errors due to insufficient precision when calculating the differences in the numerator and denominator of the expression.


£#h5#£Example calculations£#/h5#£
Example 1: The value of ${\displaystyle {\sqrt {2}}\approx 1.4142136}$ can be approximated by assuming an initial value for ${\displaystyle a_{0}}$ and iterating the following:

${\displaystyle a_{n+1}={\frac {a_{n}+{\frac {2}{a_{n}}}}{2}}.}$
Starting with ${\displaystyle a_{0}=1:}$

It is worth noting here that Aitken's method does not save two iteration steps; computation of the first three Ax values required the first five x values. Also, the second Ax value is decidedly inferior to the 4th x value, mostly due to the fact that Aitken's process assumes linear, rather than quadratic, convergence.

Example 2: The value of ${\displaystyle {\frac {\pi }{4}}}$ may be calculated as an infinite sum:

${\displaystyle {\frac {\pi }{4}}=\sum _{n=0}^{\infty }{\frac {(-1)^{n}}{2n+1}}\approx 0.785398}$
In this example, Aitken's method is applied to a sublinearly converging series, accelerating convergence considerably. It is still sublinear, but much faster than the original convergence: the first Ax value, whose computation required the first three x values, is closer to the limit than the eighth x value.


£#h5#£Example pseudocode for Aitken extrapolation£#/h5#£
The following is an example of using the Aitken extrapolation to help find the limit of the sequence ${\displaystyle x_{n+1}=f(x_{n})}$ when given some initial ${\displaystyle x_{0},}$ where the limit of this sequence is assumed to be a fixed point ${\displaystyle f}$ (say ${\displaystyle \alpha =f(\alpha )}$ ). For instance, if the sequence is given by ${\displaystyle x_{n+1}={\frac {1}{2}}\left(x_{n}+{\frac {2}{x_{n}}}\right)}$ with starting point ${\displaystyle x_{0}=1,}$ then the function will be ${\displaystyle f(x):={\frac {1}{2}}\left(x+{\frac {2}{x}}\right),}$ which has ${\displaystyle \alpha :={\sqrt {2}}}$ as a fixed point (see Methods of computing square roots); it is this fixed point whose value will be approximated.

This pseudo code also computes the Aitken approximation to ${\displaystyle f^{\prime }(\alpha )}$ . The Aitken extrapolates will be denoted by aitkenX. During the computation of the extrapolate, it is important to check if the denominator becomes too small, which could happen if we already have a large amount of accuracy; without this check, a large amount of error could be introduced by the division. This small number will be denoted by epsilon. Because the binary representation of the fixed point could be infinite (or at least too large to fit in the available memory), the calculation will stop once the approximation is within tolerance of the true value.


£#h5#£See also£#/h5#£ £#ul#££#li#£Rate of convergence£#/li#£ £#li#£Limit of a sequence£#/li#£ £#li#£Fixed point iteration£#/li#£ £#li#£Richardson extrapolation£#/li#£ £#li#£Sequence transformation£#/li#£ £#li#£Shanks transformation£#/li#£ £#li#£Steffensen's method£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£William H. Press, et al., Numerical Recipes in C, (1987) Cambridge University Press, ISBN 0-521-43108-5 (See section 5.1)£#/li#£ £#li#£Abramowitz and Stegun, Handbook of Mathematical Functions, section 3.9.7£#/li#£ £#li#£Kendall E. Atkinson, An Introduction to Numerical Analysis, (1989) John Wiley & Sons, Inc, ISBN 0-471-62489-6£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Abramowitz, M. and Stegun, I. A. (Eds.). Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing. New York: Dover, p. 18, 1972.£#/li#££#li#£Press, W. H.; Flannery, B. P.; Teukolsky, S. A.; and Vetterling, W. T. Numerical Recipes in FORTRAN: The Art of Scientific Computing, 2nd ed. Cambridge, England: Cambridge University Press, p. 160, 1992.£#/li#££#li#£ Abramowitz, M. and Stegun, I. A. (Eds.). Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing. New York: Dover, p. 18, 1972. £#/li#££#li#£ Press, W. H.; Flannery, B. P.; Teukolsky, S. A.; and Vetterling, W. T. Numerical Recipes in FORTRAN: The Art of Scientific Computing, 2nd ed. Cambridge, England: Cambridge University Press, p. 160, 1992. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Series > Convergence £#/li#££#/ul#£




£#h3#£Algebraic Branch Point£#/h3#£

£#h5#£ References £#/h5#£ £#ul#££#li#£Knopp, K. Theory of Functions Parts I and II, Two Volumes Bound as One, Part I. New York: Dover, Part II, pp. 142-143, 1996.£#/li#££#li#£ Knopp, K. Theory of Functions Parts I and II, Two Volumes Bound as One, Part I. New York: Dover, Part II, pp. 142-143, 1996. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Complex Analysis > General Complex Analysis £#/li#££#/ul#£




£#h3#£Algebraic Conjugate Space£#/h3#£

£#h5#£ References £#/h5#£ £#ul#££#li#£Kreyszig, E. Introductory Functional Analysis with Applications. New York: Wiley, 1978.£#/li#££#li#£ Kreyszig, E. Introductory Functional Analysis with Applications. New York: Wiley, 1978. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Functional Analysis £#/li#££#/ul#£




£#h3#£Algebraic Element£#/h3#£

In mathematics, if L is a field extension of K, then an element a of L is called an algebraic element over K, or just algebraic over K, if there exists some non-zero polynomial g(x) with coefficients in K such that g(a) = 0. Elements of L which are not algebraic over K are called transcendental over K.

These notions generalize the algebraic numbers and the transcendental numbers (where the field extension is C/Q, C being the field of complex numbers and Q being the field of rational numbers).


£#h5#£Examples£#/h5#£ £#ul#££#li#£The square root of 2 is algebraic over Q, since it is the root of the polynomial g(x) = x2 − 2 whose coefficients are rational.£#/li#£ £#li#£Pi is transcendental over Q but algebraic over the field of real numbers R: it is the root of g(x) = x − π, whose coefficients (1 and −π) are both real, but not of any polynomial with only rational coefficients. (The definition of the term transcendental number uses C/Q, not C/R.)£#/li#££#/ul#£
£#h5#£Properties£#/h5#£
The following conditions are equivalent for an element ${\displaystyle a}$ of ${\displaystyle L}$ :

£#ul#££#li#£ ${\displaystyle a}$ is algebraic over ${\displaystyle K}$ ,£#/li#£ £#li#£the field extension ${\displaystyle K(a)/K}$ is algebraic, i.e. every element of ${\displaystyle K(a)}$ is algebraic over ${\displaystyle K}$ (here ${\displaystyle K(a)}$ denotes the smallest subfield of ${\displaystyle L}$ containing ${\displaystyle K}$ and ${\displaystyle a}$ ),£#/li#£ £#li#£the field extension ${\displaystyle K(a)/K}$ has finite degree, i.e. the dimension of ${\displaystyle K(a)}$ as a ${\displaystyle K}$ -vector space is finite,£#/li#£ £#li#£ ${\displaystyle K[a]=K(a)}$ , where ${\displaystyle K[a]}$ is the set of all elements of ${\displaystyle L}$ that can be written in the form ${\displaystyle g(a)}$ with a polynomial ${\displaystyle g}$ whose coefficients lie in ${\displaystyle K}$ .£#/li#££#/ul#£
To make this more explicit, consider the polynomial evaluation ${\displaystyle \varepsilon _{a}:K[X]\rightarrow K(a),\,P\mapsto P(a)}$ . This is a homomorphism and its kernel is ${\displaystyle \{P\in K[X]\mid P(a)=0\}}$ . If ${\displaystyle a}$ is algebraic, this ideal contains non-zero polynomials, but as ${\displaystyle K[X]}$ is a euclidean domain, it contains a unique polynomial ${\displaystyle p}$ with minimal degree and leading coefficient ${\displaystyle 1}$ , which then also generates the ideal and must be irreducible. The polynomial ${\displaystyle p}$ is called the minimal polynomial of ${\displaystyle a}$ and it encodes many important properties of ${\displaystyle a}$ . Hence the ring isomorphism ${\displaystyle K[X]/(p)\rightarrow \mathrm {im} (\varepsilon _{a})}$ obtained by the homomorphism theorem is an isomorphism of fields, where we can then observe that ${\displaystyle \mathrm {im} (\varepsilon _{a})=K(a)}$ . Otherwise, ${\displaystyle \varepsilon _{a}}$ is injective and hence we obtain a field isomorphism ${\displaystyle K(X)\rightarrow K(a)}$ , where ${\displaystyle K(X)}$ is the field of fractions of ${\displaystyle K[X]}$ , i.e. the field of rational functions on ${\displaystyle K}$ , by the universal property of the field of fractions. We can conclude that in any case, we find an isomorphism ${\displaystyle K(a)\cong K[X]/(p)}$ or ${\displaystyle K(a)\cong K(X)}$ . Investigating this construction yields the desired results.

This characterization can be used to show that the sum, difference, product and quotient of algebraic elements over ${\displaystyle K}$ are again algebraic over ${\displaystyle K}$ . For if ${\displaystyle a}$ and ${\displaystyle b}$ are both algebraic, then ${\displaystyle (K(a))(b)}$ is finite. As it contains the aforementioned combinations of ${\displaystyle a}$ and ${\displaystyle b}$ , adjoining one of them to ${\displaystyle K}$ also yields a finite extension, and therefore these elements are algebraic as well. Thus set of all elements of ${\displaystyle L}$ which are algebraic over ${\displaystyle K}$ is a field that sits in between ${\displaystyle L}$ and ${\displaystyle K}$ .

Fields that do not allow any algebraic elements over them (except their own elements) are called algebraically closed. The field of complex numbers is an example. If ${\displaystyle L}$ is algebraically closed, then the field of algebraic elements of ${\displaystyle L}$ over ${\displaystyle K}$ is algebraically closed, which can again be directly shown using the characterisation of simple algebraic extensions above. An example for this is the field of algebraic numbers.


£#h5#£See also£#/h5#£ £#ul#££#li#£Algebraic independence£#/li#££#/ul#£
£#h5#£References£#/h5#£ £#ul#££#li#£Lang, Serge (2002), Algebra, Graduate Texts in Mathematics, vol. 211 (Revised third ed.), New York: Springer-Verlag, ISBN 978-0-387-95385-4, MR 1878556, Zbl 0984.00001£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Algebra > Field Theory £#/li#££#li#£ Algebra > Polynomials £#/li#££#/ul#£




£#h3#£Algebraic Function£#/h3#£

In mathematics, an algebraic function is a function that can be defined as the root of a polynomial equation. Quite often algebraic functions are algebraic expressions using a finite number of terms, involving only the algebraic operations addition, subtraction, multiplication, division, and raising to a fractional power. Examples of such functions are:

£#ul#££#li#£ ${\displaystyle f(x)=1/x}$ £#/li#£ £#li#£ ${\displaystyle f(x)={\sqrt {x}}}$ £#/li#£ £#li#£ ${\displaystyle f(x)={\frac {\sqrt {1+x^{3}}}{x^{3/7}-{\sqrt {7}}x^{1/3}}}}$ £#/li#££#/ul#£
Some algebraic functions, however, cannot be expressed by such finite expressions (this is the Abel–Ruffini theorem). This is the case, for example, for the Bring radical, which is the function implicitly defined by

${\displaystyle f(x)^{5}+f(x)+x=0}$ .
In more precise terms, an algebraic function of degree n in one variable x is a function ${\displaystyle y=f(x),}$ that is continuous in its domain and satisfies a polynomial equation

${\displaystyle a_{n}(x)y^{n}+a_{n-1}(x)y^{n-1}+\cdots +a_{0}(x)=0}$
where the coefficients ai(x) are polynomial functions of x, with integer coefficients. It can be shown that the same class of functions is obtained if algebraic numbers are accepted for the coefficients of the ai(x)'s. If transcendental numbers occur in the coefficients the function is, in general, not algebraic, but it is algebraic over the field generated by these coefficients.

The value of an algebraic function at a rational number, and more generally, at an algebraic number is always an algebraic number. Sometimes, coefficients ${\displaystyle a_{i}(x)}$ that are polynomial over a ring R are considered, and one then talks about "functions algebraic over R".

A function which is not algebraic is called a transcendental function, as it is for example the case of ${\displaystyle \exp x,\tan x,\ln x,\Gamma (x)}$ . A composition of transcendental functions can give an algebraic function: ${\displaystyle f(x)=\cos \arcsin x={\sqrt {1-x^{2}}}}$ .

As a polynomial equation of degree n has up to n roots (and exactly n roots over an algebraically closed field, such as the complex numbers), a polynomial equation does not implicitly define a single function, but up to n functions, sometimes also called branches. Consider for example the equation of the unit circle: ${\displaystyle y^{2}+x^{2}=1.\,}$ This determines y, except only up to an overall sign; accordingly, it has two branches: ${\displaystyle y=\pm {\sqrt {1-x^{2}}}.\,}$

An algebraic function in m variables is similarly defined as a function ${\displaystyle y=f(x_{1},\dots ,x_{m})}$ which solves a polynomial equation in m + 1 variables:

${\displaystyle p(y,x_{1},x_{2},\dots ,x_{m})=0.}$
It is normally assumed that p should be an irreducible polynomial. The existence of an algebraic function is then guaranteed by the implicit function theorem.

Formally, an algebraic function in m variables over the field K is an element of the algebraic closure of the field of rational functions K(x1, ..., xm).


£#h5#£Algebraic functions in one variable£#/h5#£
£#h5#£Introduction and overview£#/h5#£
The informal definition of an algebraic function provides a number of clues about their properties. To gain an intuitive understanding, it may be helpful to regard algebraic functions as functions which can be formed by the usual algebraic operations: addition, multiplication, division, and taking an nth root. This is something of an oversimplification; because of the fundamental theorem of Galois theory, algebraic functions need not be expressible by radicals.

First, note that any polynomial function ${\displaystyle y=p(x)}$ is an algebraic function, since it is simply the solution y to the equation

${\displaystyle y-p(x)=0.\,}$
More generally, any rational function ${\displaystyle y={\frac {p(x)}{q(x)}}}$ is algebraic, being the solution to

${\displaystyle q(x)y-p(x)=0.}$
Moreover, the nth root of any polynomial ${\textstyle y={\sqrt[{n}]{p(x)}}}$ is an algebraic function, solving the equation

${\displaystyle y^{n}-p(x)=0.}$
Surprisingly, the inverse function of an algebraic function is an algebraic function. For supposing that y is a solution to

${\displaystyle a_{n}(x)y^{n}+\cdots +a_{0}(x)=0,}$
for each value of x, then x is also a solution of this equation for each value of y. Indeed, interchanging the roles of x and y and gathering terms,

${\displaystyle b_{m}(y)x^{m}+b_{m-1}(y)x^{m-1}+\cdots +b_{0}(y)=0.}$
Writing x as a function of y gives the inverse function, also an algebraic function.

However, not every function has an inverse. For example, y = x2 fails the horizontal line test: it fails to be one-to-one. The inverse is the algebraic "function" ${\displaystyle x=\pm {\sqrt {y}}}$ . Another way to understand this, is that the set of branches of the polynomial equation defining our algebraic function is the graph of an algebraic curve.


£#h5#£The role of complex numbers£#/h5#£
From an algebraic perspective, complex numbers enter quite naturally into the study of algebraic functions. First of all, by the fundamental theorem of algebra, the complex numbers are an algebraically closed field. Hence any polynomial relation p(y, x) = 0 is guaranteed to have at least one solution (and in general a number of solutions not exceeding the degree of p in y) for y at each point x, provided we allow y to assume complex as well as real values. Thus, problems to do with the domain of an algebraic function can safely be minimized.

Furthermore, even if one is ultimately interested in real algebraic functions, there may be no means to express the function in terms of addition, multiplication, division and taking nth roots without resorting to complex numbers (see casus irreducibilis). For example, consider the algebraic function determined by the equation

${\displaystyle y^{3}-xy+1=0.\,}$
Using the cubic formula, we get

${\displaystyle y=-{\frac {2x}{\sqrt[{3}]{-108+12{\sqrt {81-12x^{3}}}}}}+{\frac {\sqrt[{3}]{-108+12{\sqrt {81-12x^{3}}}}}{6}}.}$
For ${\displaystyle x\leq {\frac {3}{\sqrt[{3}]{4}}},}$ the square root is real and the cubic root is thus well defined, providing the unique real root. On the other hand, for ${\displaystyle x>{\frac {3}{\sqrt[{3}]{4}}},}$ the square root is not real, and one has to choose, for the square root, either non-real square root. Thus the cubic root has to be chosen among three non-real numbers. If the same choices are done in the two terms of the formula, the three choices for the cubic root provide the three branches shown, in the accompanying image.

It may be proven that there is no way to express this function in terms of nth roots using real numbers only, even though the resulting function is real-valued on the domain of the graph shown.

On a more significant theoretical level, using complex numbers allows one to use the powerful techniques of complex analysis to discuss algebraic functions. In particular, the argument principle can be used to show that any algebraic function is in fact an analytic function, at least in the multiple-valued sense.

Formally, let p(x, y) be a complex polynomial in the complex variables x and y. Suppose that x0 ∈ C is such that the polynomial p(x0, y) of y has n distinct zeros. We shall show that the algebraic function is analytic in a neighborhood of x0. Choose a system of n non-overlapping discs Δi containing each of these zeros. Then by the argument principle

${\displaystyle {\frac {1}{2\pi i}}\oint _{\partial \Delta _{i}}{\frac {p_{y}(x_{0},y)}{p(x_{0},y)}}\,dy=1.}$
By continuity, this also holds for all x in a neighborhood of x0. In particular, p(x, y) has only one root in Δi, given by the residue theorem:

${\displaystyle f_{i}(x)={\frac {1}{2\pi i}}\oint _{\partial \Delta _{i}}y{\frac {p_{y}(x,y)}{p(x,y)}}\,dy}$
which is an analytic function.


£#h5#£Monodromy£#/h5#£
Note that the foregoing proof of analyticity derived an expression for a system of n different function elements fi (x), provided that x is not a critical point of p(x, y). A critical point is a point where the number of distinct zeros is smaller than the degree of p, and this occurs only where the highest degree term of p vanishes, and where the discriminant vanishes. Hence there are only finitely many such points c1, ..., cm.

A close analysis of the properties of the function elements fi near the critical points can be used to show that the monodromy cover is ramified over the critical points (and possibly the point at infinity). Thus the holomorphic extension of the fi has at worst algebraic poles and ordinary algebraic branchings over the critical points.

Note that, away from the critical points, we have

${\displaystyle p(x,y)=a_{n}(x)(y-f_{1}(x))(y-f_{2}(x))\cdots (y-f_{n}(x))}$
since the fi are by definition the distinct zeros of p. The monodromy group acts by permuting the factors, and thus forms the monodromy representation of the Galois group of p. (The monodromy action on the universal covering space is related but different notion in the theory of Riemann surfaces.)


£#h5#£History£#/h5#£
The ideas surrounding algebraic functions go back at least as far as René Descartes. The first discussion of algebraic functions appears to have been in Edward Waring's 1794 An Essay on the Principles of Human Knowledge in which he writes:

let a quantity denoting the ordinate, be an algebraic function of the abscissa x, by the common methods of division and extraction of roots, reduce it into an infinite series ascending or descending according to the dimensions of x, and then find the integral of each of the resulting terms.

£#h5#£See also£#/h5#£ £#ul#££#li#£Algebraic expression£#/li#£ £#li#£Analytic function£#/li#£ £#li#£Complex function£#/li#£ £#li#£Elementary function£#/li#£ £#li#£Function (mathematics)£#/li#£ £#li#£Generalized function£#/li#£ £#li#£List of special functions and eponyms£#/li#£ £#li#£List of types of functions£#/li#£ £#li#£Polynomial£#/li#£ £#li#£Rational function£#/li#£ £#li#£Special functions£#/li#£ £#li#£Transcendental function£#/li#££#/ul#£
£#h5#£References£#/h5#£ £#ul#££#li#£Ahlfors, Lars (1979). Complex Analysis. McGraw Hill.£#/li#£ £#li#£van der Waerden, B.L. (1931). Modern Algebra, Volume II. Springer.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Definition of "Algebraic function" in the Encyclopedia of Math£#/li#£ £#li#£Weisstein, Eric W. "Algebraic Function". MathWorld.£#/li#£ £#li#£Algebraic Function at PlanetMath.£#/li#£ £#li#£Definition of "Algebraic function" Archived 2020-10-26 at the Wayback Machine in David J. Darling's Internet Encyclopedia of Science£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Flajolet, P. and Sedgewick, R. "Analytic Combinatorics: Functional Equations, Rational and Algebraic Functions." http://www.inria.fr/RRRT/RR-4103.html.£#/li#££#li#£Knopp, K. "Algebraic Functions." Ch. 5 in Theory of Functions Parts I and II, Two Volumes Bound as One, Part II. New York: Dover, pp. 119-134, 1996.£#/li#££#li#£Koch, H. "Algebraic Functions of One Variable." Ch. 6 in Number Theory: Algebraic Numbers and Functions. Providence, RI: Amer. Math. Soc., pp. 141-170, 2000.£#/li#££#li#£ Flajolet, P. and Sedgewick, R. "Analytic Combinatorics: Functional Equations, Rational and Algebraic Functions." http://www.inria.fr/RRRT/RR-4103.html. £#/li#££#li#£ Knopp, K. "Algebraic Functions." Ch. 5 in Theory of Functions Parts I and II, Two Volumes Bound as One, Part II. New York: Dover, pp. 119-134, 1996. £#/li#££#li#£ Koch, H. "Algebraic Functions of One Variable." Ch. 6 in Number Theory: Algebraic Numbers and Functions. Providence, RI: Amer. Math. Soc., pp. 141-170, 2000. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > General Analysis £#/li#££#/ul#£




£#h3#£Algebraic Manifold£#/h3#£

In mathematics, an algebraic manifold is an algebraic variety which is also a manifold. As such, algebraic manifolds are a generalisation of the concept of smooth curves and surfaces defined by polynomials. An example is the sphere, which can be defined as the zero set of the polynomial x2 + y2 + z2 – 1, and hence is an algebraic variety.

For an algebraic manifold, the ground field will be the real numbers or complex numbers; in the case of the real numbers, the manifold of real points is sometimes called a Nash manifold.

Every sufficiently small local patch of an algebraic manifold is isomorphic to km where k is the ground field. Equivalently the variety is smooth (free from singular points). The Riemann sphere is one example of a complex algebraic manifold, since it is the complex projective line.


£#h5#£Examples£#/h5#£ £#ul#££#li#£Elliptic curves£#/li#£ £#li#£Grassmannian£#/li#££#/ul#£
£#h5#£See also£#/h5#£ £#ul#££#li#£Algebraic geometry and analytic geometry£#/li#££#/ul#£
£#h5#£References£#/h5#£ £#ul#££#li#£Nash, John Forbes (1952). "Real algebraic manifolds". Annals of Mathematics. 56 (3): 405–21. doi:10.2307/1969649. MR 0050928. (See also Proc. Internat. Congr. Math., 1950, (AMS, 1952), pp. 516–517.)£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£K-Algebraic manifold at PlanetMath£#/li#£ £#li#£Algebraic manifold at Mathworld£#/li#£ £#li#£Lecture notes on algebraic manifolds£#/li#££#/ul#£



£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Topology > Manifolds £#/li#££#/ul#£




£#h3#£Aliasing£#/h3#£

In signal processing and related disciplines, aliasing is an effect that causes different signals to become indistinguishable (or aliases of one another) when sampled. It also often refers to the distortion or artifact that results when a signal reconstructed from samples is different from the original continuous signal.

Aliasing can occur in signals sampled in time, for instance digital audio, or the stroboscopic effect, and is referred to as temporal aliasing. It can also occur in spatially sampled signals (e.g. moiré patterns in digital images); this type of aliasing is called spatial aliasing.

Aliasing is generally avoided by applying low-pass filters or anti-aliasing filters (AAF) to the input signal before sampling and when converting a signal from a higher to a lower sampling rate. Suitable reconstruction filtering should then be used when restoring the sampled signal to the continuous domain or converting a signal from a lower to a higher sampling rate. For spatial anti-aliasing, the types of anti-aliasing include fast sample anti-aliasing (FSAA), multisample anti-aliasing, and supersampling.


£#h5#£Description£#/h5#£
When a digital image is viewed, a reconstruction is performed by a display or printer device, and by the eyes and the brain. If the image data is processed in some way during sampling or reconstruction, the reconstructed image will differ from the original image, and an alias is seen.

An example of spatial aliasing is the moiré pattern observed in a poorly pixelized image of a brick wall. Spatial anti-aliasing techniques avoid such poor pixelizations. Aliasing can be caused either by the sampling stage or the reconstruction stage; these may be distinguished by calling sampling aliasing prealiasing and reconstruction aliasing postaliasing.

Temporal aliasing is a major concern in the sampling of video and audio signals. Music, for instance, may contain high-frequency components that are inaudible to humans. If a piece of music is sampled at 32,000 samples per second (Hz), any frequency components at or above 16,000 Hz (the Nyquist frequency for this sampling rate) will cause aliasing when the music is reproduced by a digital-to-analog converter (DAC). The high frequencies in the analog signal will appear as lower frequencies (wrong alias) in the recorded digital sample and, hence, cannot be reproduced by the DAC. To prevent this, an anti-aliasing filter is used to remove components above the Nyquist frequency prior to sampling.

In video or cinematography, temporal aliasing results from the limited frame rate, and causes the wagon-wheel effect, whereby a spoked wheel appears to rotate too slowly or even backwards. Aliasing has changed its apparent frequency of rotation. A reversal of direction can be described as a negative frequency. Temporal aliasing frequencies in video and cinematography are determined by the frame rate of the camera, but the relative intensity of the aliased frequencies is determined by the shutter timing (exposure time) or the use of a temporal aliasing reduction filter during filming.

Like the video camera, most sampling schemes are periodic; that is, they have a characteristic sampling frequency in time or in space. Digital cameras provide a certain number of samples (pixels) per degree or per radian, or samples per mm in the focal plane of the camera. Audio signals are sampled (digitized) with an analog-to-digital converter, which produces a constant number of samples per second. Some of the most dramatic and subtle examples of aliasing occur when the signal being sampled also has periodic content.


£#h5#£Bandlimited functions£#/h5#£
Actual signals have a finite duration and their frequency content, as defined by the Fourier transform, has no upper bound. Some amount of aliasing always occurs when such functions are sampled. Functions whose frequency content is bounded (bandlimited) have an infinite duration in the time domain. If sampled at a high enough rate, determined by the bandwidth, the original function can, in theory, be perfectly reconstructed from the infinite set of samples.


£#h5#£Bandpass signals£#/h5#£
Sometimes aliasing is used intentionally on signals with no low-frequency content, called bandpass signals. Undersampling, which creates low-frequency aliases, can produce the same result, with less effort, as frequency-shifting the signal to lower frequencies before sampling at the lower rate. Some digital channelizers exploit aliasing in this way for computational efficiency.  (See Sampling (signal processing), Nyquist rate (relative to sampling), and Filter bank.)


£#h5#£Sampling sinusoidal functions£#/h5#£
Sinusoids are an important type of periodic function, because realistic signals are often modeled as the summation of many sinusoids of different frequencies and different amplitudes (for example, with a Fourier series or transform). Understanding what aliasing does to the individual sinusoids is useful in understanding what happens to their sum.

When sampling a function at frequency fs (intervals 1/fs), the following functions of time (t) yield identical sets of samples: {sin(2π( f+Nfs) t + φ), N = 0, ±1, ±2, ±3,...}. A frequency spectrum of the samples produces equally strong responses at all those frequencies. Without collateral information, the frequency of the original function is ambiguous. So the functions and their frequencies are said to be aliases of each other. Noting the trigonometric identity:

${\displaystyle \sin(2\pi (f+Nf_{\rm {s}})t+\phi )=\left\{{\begin{array}{ll}+\sin(2\pi (f+Nf_{\rm {s}})t+\phi ),&f+Nf_{\rm {s}}\geq 0\\-\sin(2\pi |f+Nf_{\rm {s}}|t-\phi ),&f+Nf_{\rm {s}}<0\\\end{array}}\right.}$
we can write all the alias frequencies as positive values:  ${\displaystyle f_{_{N}}(f)\triangleq \left|f+Nf_{\rm {s}}\right|}$ . For example, a snapshot of the lower right frame of Fig.2 shows a component at the actual frequency ${\displaystyle f}$ and another component at alias ${\displaystyle f_{_{-1}}(f)}$ . As ${\displaystyle f}$ increases during the animation, ${\displaystyle f_{_{-1}}(f)}$ decreases. The point at which they are equal ${\displaystyle (f=f_{s}/2)}$ is an axis of symmetry called the folding frequency, also known as Nyquist frequency.

Aliasing matters when one attempts to reconstruct the original waveform from its samples. The most common reconstruction technique produces the smallest of the ${\displaystyle f_{_{N}}(f)}$ frequencies. So it is usually important that ${\displaystyle f_{0}(f)}$ be the unique minimum.  A necessary and sufficient condition for that is ${\displaystyle f_{s}/2>|f|,}$ called the Nyquist condition. The lower left frame of Fig.2 depicts the typical reconstruction result of the available samples. Until ${\displaystyle f}$ exceeds the Nyquist frequency, the reconstruction matches the actual waveform (upper left frame). After that, it is the low frequency alias of the upper frame.


£#h5#£Folding£#/h5#£
The figures below offer additional depictions of aliasing, due to sampling. A graph of amplitude vs frequency (not time) for a single sinusoid at frequency  0.6 fs  and some of its aliases at  0.4 fs,  1.4 fs,  and  1.6 fs  would look like the 4 black dots in Fig.3. The red lines depict the paths (loci) of the 4 dots if we were to adjust the frequency and amplitude of the sinusoid along the solid red segment (between  fs/2  and  fs).  No matter what function we choose to change the amplitude vs frequency, the graph will exhibit symmetry between 0 and  fs.  Folding is often observed in practice when viewing the frequency spectrum of real-valued samples, such as Fig.4..


£#h5#£Complex sinusoids£#/h5#£
Complex sinusoids are waveforms whose samples are complex numbers, and the concept of negative frequency is necessary to distinguish them. In that case, the frequencies of the aliases are given by just:  fN( f ) = f + N fs.  Therefore, as  f  increases from  0  to  fs,   f−1( f )  also increases (from  –fs  to 0).  Consequently, complex sinusoids do not exhibit folding.


£#h5#£Sample frequency£#/h5#£
When the condition  fs/2 > f   is met for the highest frequency component of the original signal, then it is met for all the frequency components, a condition called the Nyquist criterion. That is typically approximated by filtering the original signal to attenuate high frequency components before it is sampled. These attenuated high frequency components still generate low-frequency aliases, but typically at low enough amplitudes that they do not cause problems. A filter chosen in anticipation of a certain sample frequency is called an anti-aliasing filter.

The filtered signal can subsequently be reconstructed, by interpolation algorithms, without significant additional distortion. Most sampled signals are not simply stored and reconstructed. But the fidelity of a theoretical reconstruction (via the Whittaker–Shannon interpolation formula) is a customary measure of the effectiveness of sampling.


£#h5#£Historical usage£#/h5#£
Historically the term aliasing evolved from radio engineering because of the action of superheterodyne receivers. When the receiver shifts multiple signals down to lower frequencies, from RF to IF by heterodyning, an unwanted signal, from an RF frequency equally far from the local oscillator (LO) frequency as the desired signal, but on the wrong side of the LO, can end up at the same IF frequency as the wanted one. If it is strong enough it can interfere with reception of the desired signal. This unwanted signal is known as an image or alias of the desired signal.


£#h5#£Angular aliasing£#/h5#£
Aliasing occurs whenever the use of discrete elements to capture or produce a continuous signal causes frequency ambiguity.

Spatial aliasing, particular of angular frequency, can occur when reproducing a light field or sound field with discrete elements, as in 3D displays or wave field synthesis of sound.

This aliasing is visible in images such as posters with lenticular printing: if they have low angular resolution, then as one moves past them, say from left-to-right, the 2D image does not initially change (so it appears to move left), then as one moves to the next angular image, the image suddenly changes (so it jumps right) – and the frequency and amplitude of this side-to-side movement corresponds to the angular resolution of the image (and, for frequency, the speed of the viewer's lateral movement), which is the angular aliasing of the 4D light field.

The lack of parallax on viewer movement in 2D images and in 3-D film produced by stereoscopic glasses (in 3D films the effect is called "yawing", as the image appears to rotate on its axis) can similarly be seen as loss of angular resolution, all angular frequencies being aliased to 0 (constant).


£#h5#£More examples£#/h5#£
£#h5#£Audio example£#/h5#£
The qualitative effects of aliasing can be heard in the following audio demonstration. Six sawtooth waves are played in succession, with the first two sawtooths having a fundamental frequency of 440 Hz (A4), the second two having fundamental frequency of 880 Hz (A5), and the final two at 1,760 Hz (A6). The sawtooths alternate between bandlimited (non-aliased) sawtooths and aliased sawtooths and the sampling rate is 22,05 kHz. The bandlimited sawtooths are synthesized from the sawtooth waveform's Fourier series such that no harmonics above the Nyquist frequency are present.

The aliasing distortion in the lower frequencies is increasingly obvious with higher fundamental frequencies, and while the bandlimited sawtooth is still clear at 1,760 Hz, the aliased sawtooth is degraded and harsh with a buzzing audible at frequencies lower than the fundamental.


£#h5#£Direction finding£#/h5#£
A form of spatial aliasing can also occur in antenna arrays or microphone arrays used to estimate the direction of arrival of a wave signal, as in geophysical exploration by seismic waves. Waves must be sampled more densely than two points per wavelength, or the wave arrival direction becomes ambiguous.


£#h5#£See also£#/h5#£ £#ul#££#li#£Brillouin zone£#/li#£ £#li#£Glossary of video terms£#/li#£ £#li#£Jaggies£#/li#£ £#li#£Kell factor£#/li#£ £#li#£Sinc filter£#/li#£ £#li#£Sinc function£#/li#£ £#li#£Stroboscopic effect£#/li#£ £#li#£Wagon-wheel effect£#/li#£ £#li#£Nyquist–Shannon sampling theorem § Critical frequency£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£
£#h5#£Further reading£#/h5#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Aliasing by a sampling oscilloscope on YouTube by Tektronix Application Engineer£#/li#£ £#li#£Anti-Aliasing Filter Primer by La Vida Leica, discusses its purpose and effect on recorded images£#/li#£ £#li#£Interactive examples demonstrating the aliasing effect£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Blackman, R. B. and Tukey, J. W. "Aliasing" §12 in The Measurement of Power Spectra, From the Point of View of Communications Engineering. New York: Dover, pp. 31-33, 1959.£#/li#££#li#£Roberts, S. §in Lecture 7-The Discrete Fourier Transform. http://www.robots.ox.ac.uk/~sjrob/Teaching/SP/l7.pdf.£#/li#££#li#£ Blackman, R. B. and Tukey, J. W. "Aliasing" §12 in The Measurement of Power Spectra, From the Point of View of Communications Engineering. New York: Dover, pp. 31-33, 1959. £#/li#££#li#£ Roberts, S. §in Lecture 7-The Discrete Fourier Transform. http://www.robots.ox.ac.uk/~sjrob/Teaching/SP/l7.pdf. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Integral Transforms > Fourier Transforms £#/li#££#li#£ Calculus and Analysis > Integral Transforms > Apodization Functions £#/li#££#/ul#£




£#h3#£All-Poles Model£#/h3#£

The principle of maximum entropy states that the probability distribution which best represents the current state of knowledge about a system is the one with largest entropy, in the context of precisely stated prior data (such as a proposition that expresses testable information).

Another way of stating this: Take precisely stated prior data or testable information about a probability distribution function. Consider the set of all trial probability distributions that would encode the prior data. According to this principle, the distribution with maximal information entropy is the best choice.

Since the distribution with the maximum entropy is the one that makes the fewest assumptions about the true distribution of data, the principle of maximum entropy can be seen as an application of Occam's razor.


£#h5#£History£#/h5#£
The principle was first expounded by E. T. Jaynes in two papers in 1957 where he emphasized a natural correspondence between statistical mechanics and information theory. In particular, Jaynes offered a new and very general rationale why the Gibbsian method of statistical mechanics works. He argued that the entropy of statistical mechanics and the information entropy of information theory are basically the same thing. Consequently, statistical mechanics should be seen just as a particular application of a general tool of logical inference and information theory.


£#h5#£Overview£#/h5#£
In most practical cases, the stated prior data or testable information is given by a set of conserved quantities (average values of some moment functions), associated with the probability distribution in question. This is the way the maximum entropy principle is most often used in statistical thermodynamics. Another possibility is to prescribe some symmetries of the probability distribution. The equivalence between conserved quantities and corresponding symmetry groups implies a similar equivalence for these two ways of specifying the testable information in the maximum entropy method.

The maximum entropy principle is also needed to guarantee the uniqueness and consistency of probability assignments obtained by different methods, statistical mechanics and logical inference in particular.

The maximum entropy principle makes explicit our freedom in using different forms of prior data. As a special case, a uniform prior probability density (Laplace's principle of indifference, sometimes called the principle of insufficient reason), may be adopted. Thus, the maximum entropy principle is not merely an alternative way to view the usual methods of inference of classical statistics, but represents a significant conceptual generalization of those methods.

However these statements do not imply that thermodynamical systems need not be shown to be ergodic to justify treatment as a statistical ensemble.

In ordinary language, the principle of maximum entropy can be said to express a claim of epistemic modesty, or of maximum ignorance. The selected distribution is the one that makes the least claim to being informed beyond the stated prior data, that is to say the one that admits the most ignorance beyond the stated prior data.


£#h5#£Testable information£#/h5#£
The principle of maximum entropy is useful explicitly only when applied to testable information. Testable information is a statement about a probability distribution whose truth or falsity is well-defined. For example, the statements

the expectation of the variable ${\displaystyle x}$ is 2.87
and

${\displaystyle p_{2}+p_{3}>0.6}$
(where ${\displaystyle p_{2}}$ and ${\displaystyle p_{3}}$ are probabilities of events) are statements of testable information.

Given testable information, the maximum entropy procedure consists of seeking the probability distribution which maximizes information entropy, subject to the constraints of the information. This constrained optimization problem is typically solved using the method of Lagrange multipliers.

Entropy maximization with no testable information respects the universal "constraint" that the sum of the probabilities is one. Under this constraint, the maximum entropy discrete probability distribution is the uniform distribution,

${\displaystyle p_{i}={\frac {1}{n}}\ {\rm {for\ all}}\ i\in \{\,1,\dots ,n\,\}.}$

£#h5#£Applications£#/h5#£
The principle of maximum entropy is commonly applied in two ways to inferential problems:


£#h5#£Prior probabilities£#/h5#£
The principle of maximum entropy is often used to obtain prior probability distributions for Bayesian inference. Jaynes was a strong advocate of this approach, claiming the maximum entropy distribution represented the least informative distribution. A large amount of literature is now dedicated to the elicitation of maximum entropy priors and links with channel coding.


£#h5#£Posterior probabilities£#/h5#£
Maximum entropy is a sufficient updating rule for radical probabilism. Richard Jeffrey's probability kinematics is a special case of maximum entropy inference. However, maximum entropy is not a generalisation of all such sufficient updating rules.


£#h5#£Maximum entropy models£#/h5#£
Alternatively, the principle is often invoked for model specification: in this case the observed data itself is assumed to be the testable information. Such models are widely used in natural language processing. An example of such a model is logistic regression, which corresponds to the maximum entropy classifier for independent observations.


£#h5#£Probability density estimation£#/h5#£
One of the main applications of the maximum entropy principle is in discrete and continuous density estimation. Similar to support vector machine estimators, the maximum entropy principle may require the solution to a quadratic programming problem, and thus provide a sparse mixture model as the optimal density estimator. One important advantage of the method is its ability to incorporate prior information in the density estimation.


£#h5#£General solution for the maximum entropy distribution with linear constraints£#/h5#£
£#h5#£Discrete case£#/h5#£
We have some testable information I about a quantity x taking values in {x1, x2,..., xn}. We assume this information has the form of m constraints on the expectations of the functions fk; that is, we require our probability distribution to satisfy the moment inequality/equality constraints:

${\displaystyle \sum _{i=1}^{n}\Pr(x_{i})f_{k}(x_{i})\geq F_{k}\qquad k=1,\ldots ,m.}$
where the ${\displaystyle F_{k}}$ are observables. We also require the probability density to sum to one, which may be viewed as a primitive constraint on the identity function and an observable equal to 1 giving the constraint

${\displaystyle \sum _{i=1}^{n}\Pr(x_{i})=1.}$
The probability distribution with maximum information entropy subject to these inequality/equality constraints is of the form:

${\displaystyle \Pr(x_{i})={\frac {1}{Z(\lambda _{1},\ldots ,\lambda _{m})}}\exp \left[\lambda _{1}f_{1}(x_{i})+\cdots +\lambda _{m}f_{m}(x_{i})\right],}$
for some ${\displaystyle \lambda _{1},\ldots ,\lambda _{m}}$ . It is sometimes called the Gibbs distribution. The normalization constant is determined by:

${\displaystyle Z(\lambda _{1},\ldots ,\lambda _{m})=\sum _{i=1}^{n}\exp \left[\lambda _{1}f_{1}(x_{i})+\cdots +\lambda _{m}f_{m}(x_{i})\right],}$
and is conventionally called the partition function. (The Pitman–Koopman theorem states that the necessary and sufficient condition for a sampling distribution to admit sufficient statistics of bounded dimension is that it have the general form of a maximum entropy distribution.)

The λk parameters are Lagrange multipliers. In the case of equality constraints their values are determined from the solution of the nonlinear equations

${\displaystyle F_{k}={\frac {\partial }{\partial \lambda _{k}}}\log Z(\lambda _{1},\ldots ,\lambda _{m}).}$
In the case of inequality constraints, the Lagrange multipliers are determined from the solution of a convex optimization program with linear constraints. In both cases, there is no closed form solution, and the computation of the Lagrange multipliers usually requires numerical methods.


£#h5#£Continuous case£#/h5#£
For continuous distributions, the Shannon entropy cannot be used, as it is only defined for discrete probability spaces. Instead Edwin Jaynes (1963, 1968, 2003) gave the following formula, which is closely related to the relative entropy (see also differential entropy).

${\displaystyle H_{c}=-\int p(x)\log {\frac {p(x)}{q(x)}}\,dx}$
where q(x), which Jaynes called the "invariant measure", is proportional to the limiting density of discrete points. For now, we shall assume that q is known; we will discuss it further after the solution equations are given.

A closely related quantity, the relative entropy, is usually defined as the Kullback–Leibler divergence of p from q (although it is sometimes, confusingly, defined as the negative of this). The inference principle of minimizing this, due to Kullback, is known as the Principle of Minimum Discrimination Information.

We have some testable information I about a quantity x which takes values in some interval of the real numbers (all integrals below are over this interval). We assume this information has the form of m constraints on the expectations of the functions fk, i.e. we require our probability density function to satisfy the inequality (or purely equality) moment constraints:

${\displaystyle \int p(x)f_{k}(x)\,dx\geq F_{k}\qquad k=1,\dotsc ,m.}$
where the ${\displaystyle F_{k}}$ are observables. We also require the probability density to integrate to one, which may be viewed as a primitive constraint on the identity function and an observable equal to 1 giving the constraint

${\displaystyle \int p(x)\,dx=1.}$
The probability density function with maximum Hc subject to these constraints is:

${\displaystyle p(x)={\frac {1}{Z(\lambda _{1},\dotsc ,\lambda _{m})}}q(x)\exp \left[\lambda _{1}f_{1}(x)+\dotsb +\lambda _{m}f_{m}(x)\right]}$
with the partition function determined by

${\displaystyle Z(\lambda _{1},\dotsc ,\lambda _{m})=\int q(x)\exp \left[\lambda _{1}f_{1}(x)+\dotsb +\lambda _{m}f_{m}(x)\right]\,dx.}$
As in the discrete case, in the case where all moment constraints are equalities, the values of the ${\displaystyle \lambda _{k}}$ parameters are determined by the system of nonlinear equations:

${\displaystyle F_{k}={\frac {\partial }{\partial \lambda _{k}}}\log Z(\lambda _{1},\dotsc ,\lambda _{m}).}$
In the case with inequality moment constraints the Lagrange multipliers are determined from the solution of a convex optimization program.

The invariant measure function q(x) can be best understood by supposing that x is known to take values only in the bounded interval (a, b), and that no other information is given. Then the maximum entropy probability density function is

${\displaystyle p(x)=A\cdot q(x),\qquad a<x<b}$
where A is a normalization constant. The invariant measure function is actually the prior density function encoding 'lack of relevant information'. It cannot be determined by the principle of maximum entropy, and must be determined by some other logical method, such as the principle of transformation groups or marginalization theory.


£#h5#£Examples£#/h5#£
For several examples of maximum entropy distributions, see the article on maximum entropy probability distributions.


£#h5#£Justifications for the principle of maximum entropy£#/h5#£
Proponents of the principle of maximum entropy justify its use in assigning probabilities in several ways, including the following two arguments. These arguments take the use of Bayesian probability as given, and are thus subject to the same postulates.


£#h5#£Information entropy as a measure of 'uninformativeness'£#/h5#£
Consider a discrete probability distribution among ${\displaystyle m}$ mutually exclusive propositions. The most informative distribution would occur when one of the propositions was known to be true. In that case, the information entropy would be equal to zero. The least informative distribution would occur when there is no reason to favor any one of the propositions over the others. In that case, the only reasonable probability distribution would be uniform, and then the information entropy would be equal to its maximum possible value, ${\displaystyle \log m}$ . The information entropy can therefore be seen as a numerical measure which describes how uninformative a particular probability distribution is, ranging from zero (completely informative) to ${\displaystyle \log m}$ (completely uninformative).

By choosing to use the distribution with the maximum entropy allowed by our information, the argument goes, we are choosing the most uninformative distribution possible. To choose a distribution with lower entropy would be to assume information we do not possess. Thus the maximum entropy distribution is the only reasonable distribution. The dependence of the solution on the dominating measure represented by ${\displaystyle m(x)}$ is however a source of criticisms of the approach since this dominating measure is in fact arbitrary.


£#h5#£The Wallis derivation£#/h5#£
The following argument is the result of a suggestion made by Graham Wallis to E. T. Jaynes in 1962. It is essentially the same mathematical argument used for the Maxwell–Boltzmann statistics in statistical mechanics, although the conceptual emphasis is quite different. It has the advantage of being strictly combinatorial in nature, making no reference to information entropy as a measure of 'uncertainty', 'uninformativeness', or any other imprecisely defined concept. The information entropy function is not assumed a priori, but rather is found in the course of the argument; and the argument leads naturally to the procedure of maximizing the information entropy, rather than treating it in some other way.

Suppose an individual wishes to make a probability assignment among ${\displaystyle m}$ mutually exclusive propositions. He has some testable information, but is not sure how to go about including this information in his probability assessment. He therefore conceives of the following random experiment. He will distribute ${\displaystyle N}$ quanta of probability (each worth ${\displaystyle 1/N}$ ) at random among the ${\displaystyle m}$ possibilities. (One might imagine that he will throw ${\displaystyle N}$ balls into ${\displaystyle m}$ buckets while blindfolded. In order to be as fair as possible, each throw is to be independent of any other, and every bucket is to be the same size.) Once the experiment is done, he will check if the probability assignment thus obtained is consistent with his information. (For this step to be successful, the information must be a constraint given by an open set in the space of probability measures). If it is inconsistent, he will reject it and try again. If it is consistent, his assessment will be

${\displaystyle p_{i}={\frac {n_{i}}{N}}}$
where ${\displaystyle p_{i}}$ is the probability of the ${\displaystyle i}$ th proposition, while ni is the number of quanta that were assigned to the ${\displaystyle i}$ th proposition (i.e. the number of balls that ended up in bucket ${\displaystyle i}$ ).

Now, in order to reduce the 'graininess' of the probability assignment, it will be necessary to use quite a large number of quanta of probability. Rather than actually carry out, and possibly have to repeat, the rather long random experiment, the protagonist decides to simply calculate and use the most probable result. The probability of any particular result is the multinomial distribution,

${\displaystyle Pr(\mathbf {p} )=W\cdot m^{-N}}$
where

${\displaystyle W={\frac {N!}{n_{1}!\,n_{2}!\,\dotsb \,n_{m}!}}}$
is sometimes known as the multiplicity of the outcome.

The most probable result is the one which maximizes the multiplicity ${\displaystyle W}$ . Rather than maximizing ${\displaystyle W}$ directly, the protagonist could equivalently maximize any monotonic increasing function of ${\displaystyle W}$ . He decides to maximize

${\displaystyle {\begin{aligned}{\frac {1}{N}}\log W&={\frac {1}{N}}\log {\frac {N!}{n_{1}!\,n_{2}!\,\dotsb \,n_{m}!}}\\[6pt]&={\frac {1}{N}}\log {\frac {N!}{(Np_{1})!\,(Np_{2})!\,\dotsb \,(Np_{m})!}}\\[6pt]&={\frac {1}{N}}\left(\log N!-\sum _{i=1}^{m}\log((Np_{i})!)\right).\end{aligned}}}$
At this point, in order to simplify the expression, the protagonist takes the limit as ${\displaystyle N\to \infty }$ , i.e. as the probability levels go from grainy discrete values to smooth continuous values. Using Stirling's approximation, he finds

${\displaystyle {\begin{aligned}\lim _{N\to \infty }\left({\frac {1}{N}}\log W\right)&={\frac {1}{N}}\left(N\log N-\sum _{i=1}^{m}Np_{i}\log(Np_{i})\right)\\[6pt]&=\log N-\sum _{i=1}^{m}p_{i}\log(Np_{i})\\[6pt]&=\log N-\log N\sum _{i=1}^{m}p_{i}-\sum _{i=1}^{m}p_{i}\log p_{i}\\[6pt]&=\left(1-\sum _{i=1}^{m}p_{i}\right)\log N-\sum _{i=1}^{m}p_{i}\log p_{i}\\[6pt]&=-\sum _{i=1}^{m}p_{i}\log p_{i}\\[6pt]&=H(\mathbf {p} ).\end{aligned}}}$
All that remains for the protagonist to do is to maximize entropy under the constraints of his testable information. He has found that the maximum entropy distribution is the most probable of all "fair" random distributions, in the limit as the probability levels go from discrete to continuous.


£#h5#£Compatibility with Bayes' theorem£#/h5#£
Giffin and Caticha (2007) state that Bayes' theorem and the principle of maximum entropy are completely compatible and can be seen as special cases of the "method of maximum relative entropy". They state that this method reproduces every aspect of orthodox Bayesian inference methods. In addition this new method opens the door to tackling problems that could not be addressed by either the maximal entropy principle or orthodox Bayesian methods individually. Moreover, recent contributions (Lazar 2003, and Schennach 2005) show that frequentist relative-entropy-based inference approaches (such as empirical likelihood and exponentially tilted empirical likelihood – see e.g. Owen 2001 and Kitamura 2006) can be combined with prior information to perform Bayesian posterior analysis.

Jaynes stated Bayes' theorem was a way to calculate a probability, while maximum entropy was a way to assign a prior probability distribution.

It is however, possible in concept to solve for a posterior distribution directly from a stated prior distribution using the principle of minimum cross entropy (or the Principle of Maximum Entropy being a special case of using a uniform distribution as the given prior), independently of any Bayesian considerations by treating the problem formally as a constrained optimisation problem, the Entropy functional being the objective function. For the case of given average values as testable information (averaged over the sought after probability distribution), the sought after distribution is formally the Gibbs (or Boltzmann) distribution the parameters of which must be solved for in order to achieve minimum cross entropy and satisfy the given testable information.


£#h5#£Relevance to physics£#/h5#£
The principle of maximum entropy bears a relation to a key assumption of kinetic theory of gases known as molecular chaos or Stosszahlansatz. This asserts that the distribution function characterizing particles entering a collision can be factorized. Though this statement can be understood as a strictly physical hypothesis, it can also be interpreted as a heuristic hypothesis regarding the most probable configuration of particles before colliding.


£#h5#£See also£#/h5#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Bajkova, A. T. (1992). "The generalization of maximum entropy method for reconstruction of complex functions". Astronomical and Astrophysical Transactions. 1 (4): 313–320. Bibcode:1992A&AT....1..313B. doi:10.1080/10556799208230532.£#/li#£ £#li#£Fornalski, K.W.; Parzych, G.; Pylak, M.; Satuła, D.; Dobrzyński, L. (2010). "Application of Bayesian reasoning and the Maximum Entropy Method to some reconstruction problems" (PDF). Acta Physica Polonica A. 117 (6): 892–899. Bibcode:2010AcPPA.117..892F. doi:10.12693/APhysPolA.117.892.£#/li#£ £#li#£Giffin, A. and Caticha, A., 2007, Updating Probabilities with Data and Moments£#/li#£ £#li#£Guiasu, S.; Shenitzer, A. (1985). "The principle of maximum entropy". The Mathematical Intelligencer. 7 (1): 42–48. doi:10.1007/bf03023004. S2CID 53059968.£#/li#£ £#li#£Harremoës, P.; Topsøe (2001). "Maximum entropy fundamentals". Entropy. 3 (3): 191–226. Bibcode:2001Entrp...3..191H. doi:10.3390/e3030191.£#/li#£ £#li#£Jaynes, E. T. (1963). "Information Theory and Statistical Mechanics". In Ford, K. (ed.). Statistical Physics. New York: Benjamin. p. 181.£#/li#£ £#li#£Jaynes, E. T., 1986 (new version online 1996), "Monkeys, kangaroos and N", in Maximum-Entropy and Bayesian Methods in Applied Statistics, J. H. Justice (ed.), Cambridge University Press, Cambridge, p. 26.£#/li#£ £#li#£Kapur, J. N.; and Kesavan, H. K., 1992, Entropy Optimization Principles with Applications, Boston: Academic Press. ISBN 0-12-397670-7£#/li#£ £#li#£Kitamura, Y., 2006, Empirical Likelihood Methods in Econometrics: Theory and Practice, Cowles Foundation Discussion Papers 1569, Cowles Foundation, Yale University.£#/li#£ £#li#£Lazar, N (2003). "Bayesian empirical likelihood". Biometrika. 90 (2): 319–326. doi:10.1093/biomet/90.2.319.£#/li#£ £#li#£Owen, A. B., 2001, Empirical Likelihood, Chapman and Hall/CRC. ISBN 1-58-488071-6.£#/li#£ £#li#£Schennach, S. M. (2005). "Bayesian exponentially tilted empirical likelihood". Biometrika. 92 (1): 31–46. doi:10.1093/biomet/92.1.31.£#/li#£ £#li#£Uffink, Jos (1995). "Can the Maximum Entropy Principle be explained as a consistency requirement?" (PDF). Studies in History and Philosophy of Modern Physics. 26B (3): 223–261. Bibcode:1995SHPMP..26..223U. CiteSeerX 10.1.1.27.6392. doi:10.1016/1355-2198(95)00015-1. hdl:1874/2649. Archived from the original (PDF) on 2006-06-03.£#/li#££#/ul#£
£#h5#£Further reading£#/h5#£ £#ul#££#li#£Boyd, Stephen; Lieven Vandenberghe (2004). Convex Optimization (PDF). Cambridge University Press. p. 362. ISBN 0-521-83378-7. Retrieved 2008-08-24.£#/li#£ £#li#£Ratnaparkhi A. (1997) "A simple introduction to maximum entropy models for natural language processing" Technical Report 97-08, Institute for Research in Cognitive Science, University of Pennsylvania. An easy-to-read introduction to maximum entropy methods in the context of natural language processing.£#/li#£ £#li#£Tang, A.; Jackson, D.; Hobbs, J.; Chen, W.; Smith, J. L.; Patel, H.; Prieto, A.; Petrusca, D.; Grivich, M. I.; Sher, A.; Hottowy, P.; Dabrowski, W.; Litke, A. M.; Beggs, J. M. (2008). "A Maximum Entropy Model Applied to Spatial and Temporal Correlations from Cortical Networks in Vitro". Journal of Neuroscience. 28 (2): 505–518. doi:10.1523/JNEUROSCI.3359-07.2008. PMC 6670549. PMID 18184793. Open access article containing pointers to various papers and software implementations of Maximum Entropy Model on the net.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Integral Transforms > Convolution £#/li#££#/ul#£




£#h3#£Almost Complex Structure£#/h3#£

In mathematics, an almost complex manifold is a smooth manifold equipped with a smooth linear complex structure on each tangent space. Every complex manifold is an almost complex manifold, but there are almost complex manifolds that are not complex manifolds. Almost complex structures have important applications in symplectic geometry.

The concept is due to Charles Ehresmann and Heinz Hopf in the 1940s.


£#h5#£Formal definition£#/h5#£
Let M be a smooth manifold. An almost complex structure J on M is a linear complex structure (that is, a linear map which squares to −1) on each tangent space of the manifold, which varies smoothly on the manifold. In other words, we have a smooth tensor field J of degree (1, 1) such that ${\displaystyle J^{2}=-1}$ when regarded as a vector bundle isomorphism ${\displaystyle J\colon TM\to TM}$ on the tangent bundle. A manifold equipped with an almost complex structure is called an almost complex manifold.

If M admits an almost complex structure, it must be even-dimensional. This can be seen as follows. Suppose M is n-dimensional, and let J : TM → TM be an almost complex structure. If J2 = −1 then (det J)2 = (−1)n. But if M is a real manifold, then det J is a real number – thus n must be even if M has an almost complex structure. One can show that it must be orientable as well.

An easy exercise in linear algebra shows that any even dimensional vector space admits a linear complex structure. Therefore, an even dimensional manifold always admits a (1, 1)-rank tensor pointwise (which is just a linear transformation on each tangent space) such that Jp2 = −1 at each point p. Only when this local tensor can be patched together to be defined globally does the pointwise linear complex structure yield an almost complex structure, which is then uniquely determined. The possibility of this patching, and therefore existence of an almost complex structure on a manifold M is equivalent to a reduction of the structure group of the tangent bundle from GL(2n, R) to GL(n, C). The existence question is then a purely algebraic topological one and is fairly well understood.


£#h5#£Examples£#/h5#£
For every integer n, the flat space R2n admits an almost complex structure. An example for such an almost complex structure is (1 ≤ i, j ≤ 2n): ${\displaystyle J_{ij}=-\delta _{i,j-1}}$ for even i, ${\displaystyle J_{ij}=\delta _{i,j+1}}$ for odd i.

The only spheres which admit almost complex structures are S2 and S6 (Borel & Serre (1953)). In particular, S4 cannot be given an almost complex structure (Ehresmann and Hopf). In the case of S2, the almost complex structure comes from an honest complex structure on the Riemann sphere. The 6-sphere, S6, when considered as the set of unit norm imaginary octonions, inherits an almost complex structure from the octonion multiplication; the question of whether it has a complex structure is known as the Hopf problem, after Heinz Hopf.


£#h5#£Differential topology of almost complex manifolds£#/h5#£
Just as a complex structure on a vector space V allows a decomposition of VC into V+ and V− (the eigenspaces of J corresponding to +i and −i, respectively), so an almost complex structure on M allows a decomposition of the complexified tangent bundle TMC (which is the vector bundle of complexified tangent spaces at each point) into TM+ and TM−. A section of TM+ is called a vector field of type (1, 0), while a section of TM− is a vector field of type (0, 1). Thus J corresponds to multiplication by i on the (1, 0)-vector fields of the complexified tangent bundle, and multiplication by −i on the (0, 1)-vector fields.

Just as we build differential forms out of exterior powers of the cotangent bundle, we can build exterior powers of the complexified cotangent bundle (which is canonically isomorphic to the bundle of dual spaces of the complexified tangent bundle). The almost complex structure induces the decomposition of each space of r-forms

${\displaystyle \Omega ^{r}(M)^{\mathbf {C} }=\bigoplus _{p+q=r}\Omega ^{(p,q)}(M).\,}$
In other words, each Ωr(M)C admits a decomposition into a sum of Ω(p, q)(M), with r = p + q.

As with any direct sum, there is a canonical projection πp,q from Ωr(M)C to Ω(p,q). We also have the exterior derivative d which maps Ωr(M)C to Ωr+1(M)C. Thus we may use the almost complex structure to refine the action of the exterior derivative to the forms of definite type

${\displaystyle \partial =\pi _{p+1,q}\circ d}$
${\displaystyle {\overline {\partial }}=\pi _{p,q+1}\circ d}$
so that ${\displaystyle \partial }$ is a map which increases the holomorphic part of the type by one (takes forms of type (p, q) to forms of type (p+1, q)), and ${\displaystyle {\overline {\partial }}}$ is a map which increases the antiholomorphic part of the type by one. These operators are called the Dolbeault operators.

Since the sum of all the projections must be the identity map, we note that the exterior derivative can be written

${\displaystyle d=\sum _{r+s=p+q+1}\pi _{r,s}\circ d=\partial +{\overline {\partial }}+\cdots .}$

£#h5#£Integrable almost complex structures£#/h5#£
Every complex manifold is itself an almost complex manifold. In local holomorphic coordinates ${\displaystyle z^{\mu }=x^{\mu }+iy^{\mu }}$ one can define the maps

${\displaystyle J{\frac {\partial }{\partial x^{\mu }}}={\frac {\partial }{\partial y^{\mu }}}\qquad J{\frac {\partial }{\partial y^{\mu }}}=-{\frac {\partial }{\partial x^{\mu }}}}$
(just like a counterclockwise rotation of π/2) or

${\displaystyle J{\frac {\partial }{\partial z^{\mu }}}=i{\frac {\partial }{\partial z^{\mu }}}\qquad J{\frac {\partial }{\partial {\bar {z}}^{\mu }}}=-i{\frac {\partial }{\partial {\bar {z}}^{\mu }}}.}$
One easily checks that this map defines an almost complex structure. Thus any complex structure on a manifold yields an almost complex structure, which is said to be 'induced' by the complex structure, and the complex structure is said to be 'compatible with' the almost complex structure.

The converse question, whether the almost complex structure implies the existence of a complex structure is much less trivial, and not true in general. On an arbitrary almost complex manifold one can always find coordinates for which the almost complex structure takes the above canonical form at any given point p. In general, however, it is not possible to find coordinates so that J takes the canonical form on an entire neighborhood of p. Such coordinates, if they exist, are called 'local holomorphic coordinates for J'. If M admits local holomorphic coordinates for J around every point then these patch together to form a holomorphic atlas for M giving it a complex structure, which moreover induces J. J is then said to be 'integrable'. If J is induced by a complex structure, then it is induced by a unique complex structure.

Given any linear map A on each tangent space of M; i.e., A is a tensor field of rank (1, 1), then the Nijenhuis tensor is a tensor field of rank (1,2) given by

${\displaystyle N_{A}(X,Y)=-A^{2}[X,Y]+A([AX,Y]+[X,AY])-[AX,AY].\,}$
or, for the usual case of an almost complex structure A=J such that ${\displaystyle J^{2}=-Id}$ ,

${\displaystyle N_{J}(X,Y)=[X,Y]+J([JX,Y]+[X,JY])-[JX,JY].\,}$
The individual expressions on the right depend on the choice of the smooth vector fields X and Y, but the left side actually depends only on the pointwise values of X and Y, which is why NA is a tensor. This is also clear from the component formula

${\displaystyle -(N_{A})_{ij}^{k}=A_{i}^{m}\partial _{m}A_{j}^{k}-A_{j}^{m}\partial _{m}A_{i}^{k}-A_{m}^{k}(\partial _{i}A_{j}^{m}-\partial _{j}A_{i}^{m}).}$
In terms of the Frölicher–Nijenhuis bracket, which generalizes the Lie bracket of vector fields, the Nijenhuis tensor NA is just one-half of [A, A].

The Newlander–Nirenberg theorem states that an almost complex structure J is integrable if and only if NJ = 0. The compatible complex structure is unique, as discussed above. Since the existence of an integrable almost complex structure is equivalent to the existence of a complex structure, this is sometimes taken as the definition of a complex structure.

There are several other criteria which are equivalent to the vanishing of the Nijenhuis tensor, and which therefore furnish methods for checking the integrability of an almost complex structure (and in fact each of these can be found in the literature):

£#ul#££#li#£The Lie bracket of any two (1, 0)-vector fields is again of type (1, 0)£#/li#£ £#li#£ ${\displaystyle d=\partial +{\bar {\partial }}}$ £#/li#£ £#li#£ ${\displaystyle {\bar {\partial }}^{2}=0.}$ £#/li#££#/ul#£
Any of these conditions implies the existence of a unique compatible complex structure.

The existence of an almost complex structure is a topological question and is relatively easy to answer, as discussed above. The existence of an integrable almost complex structure, on the other hand, is a much more difficult analytic question. For example, it is still not known whether S6 admits an integrable almost complex structure, despite a long history of ultimately unverified claims. Smoothness issues are important. For real-analytic J, the Newlander–Nirenberg theorem follows from the Frobenius theorem; for C∞ (and less smooth) J, analysis is required (with more difficult techniques as the regularity hypothesis weakens).


£#h5#£Compatible triples£#/h5#£
Suppose M is equipped with a symplectic form ω, a Riemannian metric g, and an almost complex structure J. Since ω and g are nondegenerate, each induces a bundle isomorphism TM → T*M, where the first map, denoted φω, is given by the interior product φω(u) = iuω = ω(u, •) and the other, denoted φg, is given by the analogous operation for g. With this understood, the three structures (g, ω, J) form a compatible triple when each structure can be specified by the two others as follows:

£#ul#££#li#£g(u, v) = ω(u, Jv)£#/li#£ £#li#£ω(u, v) = g(Ju, v)£#/li#£ £#li#£J(u) = (φg)−1(φω(u)).£#/li#££#/ul#£
In each of these equations, the two structures on the right hand side are called compatible when the corresponding construction yields a structure of the type specified. For example, ω and J are compatible if and only if ω(•, J•) is a Riemannian metric. The bundle on M whose sections are the almost complex structures compatible to ω has contractible fibres: the complex structures on the tangent fibres compatible with the restriction to the symplectic forms.

Using elementary properties of the symplectic form ω, one can show that a compatible almost complex structure J is an almost Kähler structure for the Riemannian metric ω(u, Jv). Also, if J is integrable, then (M, ω, J) is a Kähler manifold.

These triples are related to the 2 out of 3 property of the unitary group.


£#h5#£Generalized almost complex structure£#/h5#£
Nigel Hitchin introduced the notion of a generalized almost complex structure on the manifold M, which was elaborated in the doctoral dissertations of his students Marco Gualtieri and Gil Cavalcanti. An ordinary almost complex structure is a choice of a half-dimensional subspace of each fiber of the complexified tangent bundle TM. A generalized almost complex structure is a choice of a half-dimensional isotropic subspace of each fiber of the direct sum of the complexified tangent and cotangent bundles. In both cases one demands that the direct sum of the subbundle and its complex conjugate yield the original bundle.

An almost complex structure integrates to a complex structure if the half-dimensional subspace is closed under the Lie bracket. A generalized almost complex structure integrates to a generalized complex structure if the subspace is closed under the Courant bracket. If furthermore this half-dimensional space is the annihilator of a nowhere vanishing pure spinor then M is a generalized Calabi–Yau manifold.


£#h5#£See also£#/h5#£ £#ul#££#li#£Almost quaternionic manifold£#/li#£ £#li#£Chern class – Characteristic classes on algebraic vector bundles£#/li#£ £#li#£Frölicher–Nijenhuis bracket£#/li#£ £#li#£Kähler manifold – Manifold with Riemannian, complex and symplectic structure£#/li#£ £#li#£Poisson manifold – Mathematical structure in differential geometry£#/li#£ £#li#£Rizza manifold£#/li#£ £#li#£Symplectic manifold – Type of manifold in differential geometry£#/li#££#/ul#£
£#h5#£References£#/h5#£ £#ul#££#li#£Newlander, August; Nirenberg, Louis (1957). "Complex analytic coordinates in almost complex manifolds". Annals of Mathematics. Second Series. 65 (3): 391–404. doi:10.2307/1970051. ISSN 0003-486X. JSTOR 1970051. MR 0088770.£#/li#£ £#li#£Cannas da Silva, Ana (2001). Lectures on Symplectic Geometry. Springer. ISBN 3-540-42195-5. Information on compatible triples, Kähler and Hermitian manifolds, etc.£#/li#£ £#li#£Wells, Raymond O. (1980). Differential Analysis on Complex Manifolds. New York: Springer-Verlag. ISBN 0-387-90419-0. Short section which introduces standard basic material.£#/li#£ £#li#£Rubei, Elena (2014). Algebraic Geometry, a concise dictionary. Berlin/Boston: Walter De Gruyter. ISBN 978-3-11-031622-3.£#/li#£ £#li#£Borel, Armand; Serre, Jean-Pierre (1953). "Groupes de Lie et puissances réduites de Steenrod". American Journal of Mathematics. 75 (3): 409–448. doi:10.2307/2372495. JSTOR 2372495. MR 0058213.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Complex Analysis > General Complex Analysis £#/li#££#/ul#£




£#h3#£Almost Periodic Function£#/h3#£

In mathematics, an almost periodic function is, loosely speaking, a function of a real number that is periodic to within any desired level of accuracy, given suitably long, well-distributed "almost-periods". The concept was first studied by Harald Bohr and later generalized by Vyacheslav Stepanov, Hermann Weyl and Abram Samoilovitch Besicovitch, amongst others. There is also a notion of almost periodic functions on locally compact abelian groups, first studied by John von Neumann.

Almost periodicity is a property of dynamical systems that appear to retrace their paths through phase space, but not exactly. An example would be a planetary system, with planets in orbits moving with periods that are not commensurable (i.e., with a period vector that is not proportional to a vector of integers). A theorem of Kronecker from diophantine approximation can be used to show that any particular configuration that occurs once, will recur to within any specified accuracy: if we wait long enough we can observe the planets all return to within a second of arc to the positions they once were in.


£#h5#£Motivation£#/h5#£
There are several inequivalent definitions of almost periodic functions. The first was given by Harald Bohr. His interest was initially in finite Dirichlet series. In fact by truncating the series for the Riemann zeta function ζ(s) to make it finite, one gets finite sums of terms of the type

${\displaystyle e^{(\sigma +it)\log n}\,}$
with s written as (σ + it) – the sum of its real part σ and imaginary part it. Fixing σ, so restricting attention to a single vertical line in the complex plane, we can see this also as

${\displaystyle n^{\sigma }e^{(\log n)it}.\,}$
Taking a finite sum of such terms avoids difficulties of analytic continuation to the region σ < 1. Here the 'frequencies' log n will not all be commensurable (they are as linearly independent over the rational numbers as the integers n are multiplicatively independent – which comes down to their prime factorizations).

With this initial motivation to consider types of trigonometric polynomial with independent frequencies, mathematical analysis was applied to discuss the closure of this set of basic functions, in various norms.

The theory was developed using other norms by Besicovitch, Stepanov, Weyl, von Neumann, Turing, Bochner and others in the 1920s and 1930s.


£#h5#£Uniform or Bohr or Bochner almost periodic functions£#/h5#£
Bohr (1925) defined the uniformly almost-periodic functions as the closure of the trigonometric polynomials with respect to the uniform norm

${\displaystyle \|f\|_{\infty }=\sup _{x}|f(x)|}$
(on bounded functions f on R). In other words, a function f is uniformly almost periodic if for every ε > 0 there is a finite linear combination of sine and cosine waves that is of distance less than ε from f with respect to the uniform norm. Bohr proved that this definition was equivalent to the existence of a relatively dense set of ε almost-periods, for all ε > 0: that is, translations T(ε) = T of the variable t making

${\displaystyle \left|f(t+T)-f(t)\right|<\varepsilon .}$
An alternative definition due to Bochner (1926) is equivalent to that of Bohr and is relatively simple to state:

A function f is almost periodic if every sequence {ƒ(t + Tn)} of translations of f has a subsequence that converges uniformly for t in (−∞, +∞).

The Bohr almost periodic functions are essentially the same as continuous functions on the Bohr compactification of the reals.


£#h5#£Stepanov almost periodic functions£#/h5#£
The space Sp of Stepanov almost periodic functions (for p ≥ 1) was introduced by V.V. Stepanov (1925). It contains the space of Bohr almost periodic functions. It is the closure of the trigonometric polynomials under the norm

${\displaystyle \|f\|_{S,r,p}=\sup _{x}\left({1 \over r}\int _{x}^{x+r}|f(s)|^{p}\,ds\right)^{1/p}}$
for any fixed positive value of r; for different values of r these norms give the same topology and so the same space of almost periodic functions (though the norm on this space depends on the choice of r).


£#h5#£Weyl almost periodic functions£#/h5#£
The space Wp of Weyl almost periodic functions (for p ≥ 1) was introduced by Weyl (1927). It contains the space Sp of Stepanov almost periodic functions. It is the closure of the trigonometric polynomials under the seminorm

${\displaystyle \|f\|_{W,p}=\lim _{r\to \infty }\|f\|_{S,r,p}}$
Warning: there are nonzero functions ƒ with ||ƒ||W,p = 0, such as any bounded function of compact support, so to get a Banach space one has to quotient out by these functions.


£#h5#£Besicovitch almost periodic functions£#/h5#£
The space Bp of Besicovitch almost periodic functions was introduced by Besicovitch (1926). It is the closure of the trigonometric polynomials under the seminorm

${\displaystyle \|f\|_{B,p}=\limsup _{x\to \infty }\left({1 \over 2x}\int _{-x}^{x}|f(s)|^{p}\,ds\right)^{1/p}}$
Warning: there are nonzero functions ƒ with ||ƒ||B,p = 0, such as any bounded function of compact support, so to get a Banach space one has to quotient out by these functions.

The Besicovitch almost periodic functions in B2 have an expansion (not necessarily convergent) as

${\displaystyle \sum a_{n}e^{i\lambda _{n}t}}$
with Σa2
n finite and λn real. Conversely every such series is the expansion of some Besicovitch periodic function (which is not unique).

The space Bp of Besicovitch almost periodic functions (for p ≥ 1) contains the space Wp of Weyl almost periodic functions. If one quotients out a subspace of "null" functions, it can be identified with the space of Lp functions on the Bohr compactification of the reals.


£#h5#£Almost periodic functions on a locally compact abelian group£#/h5#£
With these theoretical developments and the advent of abstract methods (the Peter–Weyl theorem, Pontryagin duality and Banach algebras) a general theory became possible. The general idea of almost-periodicity in relation to a locally compact abelian group G becomes that of a function F in L∞(G), such that its translates by G form a relatively compact set. Equivalently, the space of almost periodic functions is the norm closure of the finite linear combinations of characters of G. If G is compact the almost periodic functions are the same as the continuous functions.

The Bohr compactification of G is the compact abelian group of all possibly discontinuous characters of the dual group of G, and is a compact group containing G as a dense subgroup. The space of uniform almost periodic functions on G can be identified with the space of all continuous functions on the Bohr compactification of G. More generally the Bohr compactification can be defined for any topological group G, and the spaces of continuous or Lp functions on the Bohr compactification can be considered as almost periodic functions on G. For locally compact connected groups G the map from G to its Bohr compactification is injective if and only if G is a central extension of a compact group, or equivalently the product of a compact group and a finite-dimensional vector space.


£#h5#£Quasiperiodic signals in audio and music synthesis£#/h5#£
In speech processing, audio signal processing, and music synthesis, a quasiperiodic signal, sometimes called a quasiharmonic signal, is a waveform that is virtually periodic microscopically, but not necessarily periodic macroscopically. This does not give a quasiperiodic function in the sense of the Wikipedia article of that name, but something more akin to an almost periodic function, being a nearly periodic function where any one period is virtually identical to its adjacent periods but not necessarily similar to periods much farther away in time. This is the case for musical tones (after the initial attack transient) where all partials or overtones are harmonic (that is all overtones are at frequencies that are an integer multiple of a fundamental frequency of the tone).

When a signal ${\displaystyle x(t)\ }$ is fully periodic with period ${\displaystyle P\ }$ , then the signal exactly satisfies

${\displaystyle x(t)=x(t+P)\qquad \forall t\in \mathbb {R} }$
or

${\displaystyle {\Big |}x(t)-x(t+P){\Big |}=0\qquad \forall t\in \mathbb {R} .\ }$
The Fourier series representation would be

${\displaystyle x(t)=a_{0}+\sum _{n=1}^{\infty }{\big [}a_{n}\cos(2\pi nf_{0}t)-b_{n}\sin(2\pi nf_{0}t){\big ]}}$
or

${\displaystyle x(t)=a_{0}+\sum _{n=1}^{\infty }r_{n}\cos(2\pi nf_{0}t+\varphi _{n})}$
where ${\displaystyle f_{0}={\frac {1}{P}}}$ is the fundamental frequency and the Fourier coefficients are

${\displaystyle a_{0}={\frac {1}{P}}\int _{t_{0}}^{t_{0}+P}x(t)\,dt\ }$
${\displaystyle a_{n}=r_{n}\cos \left(\varphi _{n}\right)={\frac {2}{P}}\int _{t_{0}}^{t_{0}+P}x(t)\cos(2\pi nf_{0}t)\,dt\qquad n\geq 1}$
${\displaystyle b_{n}=r_{n}\sin \left(\varphi _{n}\right)=-{\frac {2}{P}}\int _{t_{0}}^{t_{0}+P}x(t)\sin(2\pi nf_{0}t)\,dt\ }$
where ${\displaystyle t_{0}\ }$ can be any time: ${\displaystyle -\infty <t_{0}<+\infty \ }$ .
The fundamental frequency ${\displaystyle f_{0}\ }$ , and Fourier coefficients ${\displaystyle a_{n}\ }$ , ${\displaystyle b_{n}\ }$ , ${\displaystyle r_{n}\ }$ , or ${\displaystyle \varphi _{n}\ }$ , are constants, i.e. they are not functions of time. The harmonic frequencies are exact integer multiples of the fundamental frequency.

When ${\displaystyle x(t)\ }$ is quasiperiodic then

${\displaystyle x(t)\approx x{\big (}t+P(t){\big )}\ }$
or

${\displaystyle {\Big |}x(t)-x{\big (}t+P(t){\big )}{\Big |}<\varepsilon \ }$
where

${\displaystyle 0<\epsilon \ll {\big \Vert }x{\big \Vert }={\sqrt {\overline {x^{2}}}}={\sqrt {\lim _{\tau \to \infty }{\frac {1}{\tau }}\int _{-\tau /2}^{\tau /2}x^{2}(t)\,dt}}.\ }$
Now the Fourier series representation would be

${\displaystyle x(t)=a_{0}(t)\ +\ \sum _{n=1}^{\infty }\left[a_{n}(t)\cos \left(2\pi n\int _{0}^{t}f_{0}(\tau )\,d\tau \right)-b_{n}(t)\sin \left(2\pi n\int _{0}^{t}f_{0}(\tau )\,d\tau \right)\right]}$
or

${\displaystyle x(t)=a_{0}(t)\ +\ \sum _{n=1}^{\infty }r_{n}(t)\cos \left(2\pi n\int _{0}^{t}f_{0}(\tau )\,d\tau +\varphi _{n}(t)\right)}$
or

${\displaystyle x(t)=a_{0}(t)+\sum _{n=1}^{\infty }r_{n}(t)\cos \left(2\pi \int _{0}^{t}f_{n}(\tau )\,d\tau +\varphi _{n}(0)\right)}$
where ${\displaystyle f_{0}(t)={\frac {1}{P(t)}}}$ is the possibly time-varying fundamental frequency and the time-varying Fourier coefficients are

${\displaystyle a_{0}(t)={\frac {1}{P(t)}}\int _{t-P(t)/2}^{t+P(t)/2}x(\tau )\,d\tau \ }$
${\displaystyle a_{n}(t)=r_{n}(t)\cos {\big (}\varphi _{n}(t){\big )}={\frac {2}{P(t)}}\int _{t-P(t)/2}^{t+P(t)/2}x(\tau )\cos {\big (}2\pi nf_{0}(t)\tau {\big )}\,d\tau \qquad n\geq 1}$
${\displaystyle b_{n}(t)=r_{n}(t)\sin {\big (}\varphi _{n}(t){\big )}=-{\frac {2}{P(t)}}\int _{t-P(t)/2}^{t+P(t)/2}x(\tau )\sin {\big (}2\pi nf_{0}(t)\tau {\big )}\,d\tau \ }$
and the instantaneous frequency for each partial is

${\displaystyle f_{n}(t)=nf_{0}(t)+{\frac {1}{2\pi }}\varphi _{n}^{\prime }(t).\,}$
Whereas in this quasiperiodic case, the fundamental frequency ${\displaystyle f_{0}(t)\ }$ , the harmonic frequencies ${\displaystyle f_{n}(t)\ }$ , and the Fourier coefficients ${\displaystyle a_{n}(t)\ }$ , ${\displaystyle b_{n}(t)\ }$ , ${\displaystyle r_{n}(t)\ }$ , or ${\displaystyle \varphi _{n}(t)\ }$ are not necessarily constant, and are functions of time albeit slowly varying functions of time. Stated differently these functions of time are bandlimited to much less than the fundamental frequency for ${\displaystyle x(t)\ }$ to be considered to be quasiperiodic.

The partial frequencies ${\displaystyle f_{n}(t)\ }$ are very nearly harmonic but not necessarily exactly so. The time-derivative of ${\displaystyle \varphi _{n}(t)\ }$ , that is ${\displaystyle \varphi _{n}^{\prime }(t)\ }$ , has the effect of detuning the partials from their exact integer harmonic value ${\displaystyle nf_{0}(t)\ }$ . A rapidly changing ${\displaystyle \varphi _{n}(t)\ }$ means that the instantaneous frequency for that partial is severely detuned from the integer harmonic value which would mean that ${\displaystyle x(t)\ }$ is not quasiperiodic.


£#h5#£See also£#/h5#£ £#ul#££#li#£Quasiperiodic function£#/li#£ £#li#£Aperiodic function£#/li#£ £#li#£Quasiperiodic tiling£#/li#£ £#li#£Fourier series£#/li#£ £#li#£Additive synthesis£#/li#£ £#li#£Harmonic series (music)£#/li#£ £#li#£Computer music£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£Bibliography£#/h5#£ £#ul#££#li#£Amerio, Luigi; Prouse, Giovanni (1971), Almost-periodic functions and functional equations, The University Series in Higher Mathematics, New York–Cincinnati–Toronto–London–Melbourne: Van Nostrand Reinhold, pp. viii+184, ISBN 0-442-20295-4, MR 0275061, Zbl 0215.15701.£#/li#£ £#li#£A.S. Besicovitch, "Almost periodic functions", Cambridge Univ. Press (1932)£#/li#£ £#li#£Bochner, S. (1926), "Beitrage zur Theorie der fastperiodischen Funktionen", Math. Annalen, 96: 119–147, doi:10.1007/BF01209156£#/li#£ £#li#£S. Bochner and J. von Neumann, "Almost Periodic Function in a Group II", Trans. Amer. Math. Soc., 37 no. 1 (1935) pp. 21–50£#/li#£ £#li#£H. Bohr, "Almost-periodic functions", Chelsea, reprint (1947)£#/li#£ £#li#£Bredikhina, E.A. (2001) [1994], "Almost-periodic functions", Encyclopedia of Mathematics, EMS Press£#/li#£ £#li#£Bredikhina, E.A. (2001) [1994], "Besicovitch almost periodic functions", Encyclopedia of Mathematics, EMS Press£#/li#£ £#li#£Bredikhina, E.A. (2001) [1994], "Bohr almost periodic functions", Encyclopedia of Mathematics, EMS Press£#/li#£ £#li#£Bredikhina, E.A. (2001) [1994], "Stepanov almost periodic functions", Encyclopedia of Mathematics, EMS Press£#/li#£ £#li#£Bredikhina, E.A. (2001) [1994], "Weyl almost periodic functions", Encyclopedia of Mathematics, EMS Press£#/li#£ £#li#£J. von Neumann, "Almost Periodic Functions in a Group I", Trans. Amer. Math. Soc., 36 no. 3 (1934) pp. 445–492£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Almost periodic function (equivalent definition)". PlanetMath.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Bohr, H. Almost Periodic Functions. New York: Chelsea, 1947.£#/li#££#li#£Besicovitch, A. S. Almost Periodic Functions. New York: Dover, 1954.£#/li#££#li#£Corduneanu, C. Almost Periodic Functions. New York: Wiley Interscience, 1961.£#/li#££#li#£Krasnosel'skii, M. A.; Burd, V. Sh.; and Kolesov, Yu. S. Nonlinear Almost Periodic Oscillations. New York: Wiley, 1973.£#/li#££#li#£Levitan, B. M. Almost-Periodic Functions. Moscow, 1953.£#/li#££#li#£Montgomery, H. L. "Harmonic Analysis as Found in Analytic Number Theory." In Twentieth Century Harmonic Analysis--A Celebration. Proceedings of the NATO Advanced Study Institute Held in Il Ciocco, July 2-15, 2000 (Ed. J. S. Byrnes). Dordrecht, Netherlands: Kluwer, pp. 271-293, 2001.£#/li#££#li#£ Bohr, H. Almost Periodic Functions. New York: Chelsea, 1947. £#/li#££#li#£ Besicovitch, A. S. Almost Periodic Functions. New York: Dover, 1954. £#/li#££#li#£ Corduneanu, C. Almost Periodic Functions. New York: Wiley Interscience, 1961. £#/li#££#li#£ Krasnosel'skii, M. A.; Burd, V. Sh.; and Kolesov, Yu. S. Nonlinear Almost Periodic Oscillations. New York: Wiley, 1973. £#/li#££#li#£ Levitan, B. M. Almost-Periodic Functions. Moscow, 1953. £#/li#££#li#£ Montgomery, H. L. "Harmonic Analysis as Found in Analytic Number Theory." In Twentieth Century Harmonic Analysis--A Celebration. Proceedings of the NATO Advanced Study Institute Held in Il Ciocco, July 2-15, 2000 (Ed. J. S. Byrnes). Dordrecht, Netherlands: Kluwer, pp. 271-293, 2001. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Functions £#/li#££#/ul#£




£#h3#£Almost Surely£#/h3#£

In probability theory, an event is said to happen almost surely (sometimes abbreviated as a.s.) if it happens with probability 1 (or Lebesgue measure 1). In other words, the set of possible exceptions may be non-empty, but it has probability 0. The concept is analogous to the concept of "almost everywhere" in measure theory.

In probability experiments on a finite sample space, there is often no difference between almost surely and surely (since having a probability of 1 often entails including all the sample points). However, this distinction becomes important when the sample space is an infinite set, because an infinite set can have non-empty subsets of probability 0.

Some examples of the use of this concept include the strong and uniform versions of the law of large numbers, and the continuity of the paths of Brownian motion.

The terms almost certainly (a.c.) and almost always (a.a.) are also used. Almost never describes the opposite of almost surely: an event that happens with probability zero happens almost never.


£#h5#£Formal definition£#/h5#£
Let ${\displaystyle (\Omega ,{\mathcal {F}},P)}$ be a probability space. An event ${\displaystyle E\in {\mathcal {F}}}$ happens almost surely if ${\displaystyle P(E)=1}$ . Equivalently, ${\displaystyle E}$ happens almost surely if the probability of ${\displaystyle E}$ not occurring is zero: ${\displaystyle P(E^{C})=0}$ . More generally, any event ${\displaystyle E\subseteq \Omega }$ (not necessarily in ${\displaystyle {\mathcal {F}}}$ ) happens almost surely if ${\displaystyle E^{C}}$ is contained in a null set: a subset ${\displaystyle N}$ in ${\displaystyle {\mathcal {F}}}$ such that ${\displaystyle P(N)=0}$ . The notion of almost sureness depends on the probability measure ${\displaystyle P}$ . If it is necessary to emphasize this dependence, it is customary to say that the event ${\displaystyle E}$ occurs P-almost surely, or almost surely ${\displaystyle \left(\!P\right)}$ .


£#h5#£Illustrative examples£#/h5#£
In general, an event can happen "almost surely", even if the probability space in question includes outcomes which do not belong to the event—as the following examples illustrate.


£#h5#£Throwing a dart£#/h5#£
Imagine throwing a dart at a unit square (a square with an area of 1) so that the dart always hits an exact point in the square, in such a way that each point in the square is equally likely to be hit. Since the square has area 1, the probability that the dart will hit any particular subregion of the square is equal to the area of that subregion. For example, the probability that the dart will hit the right half of the square is 0.5, since the right half has area 0.5.

Next, consider the event that the dart hits exactly a point in the diagonals of the unit square. Since the area of the diagonals of the square is 0, the probability that the dart will land exactly on a diagonal is 0. That is, the dart will almost never land on a diagonal (equivalently, it will almost surely not land on a diagonal), even though the set of points on the diagonals is not empty, and a point on a diagonal is no less possible than any other point.


£#h5#£Tossing a coin repeatedly£#/h5#£
Consider the case where a (possibly biased) coin is tossed, corresponding to the probability space ${\displaystyle (\{H,T\},2^{\{H,T\}},P)}$ , where the event ${\displaystyle \{H\}}$ occurs if a head is flipped, and ${\displaystyle \{T\}}$ if a tail is flipped. For this particular coin, it is assumed that the probability of flipping a head is ${\displaystyle P(H)=p\in (0,1)}$ , from which it follows that the complement event, that of flipping a tail, has probability ${\displaystyle P(T)=1-p}$ .

Now, suppose an experiment were conducted where the coin is tossed repeatedly, with outcomes ${\displaystyle \omega _{1},\omega _{2},\ldots }$ and the assumption that each flip's outcome is independent of all the others (i.e., they are independent and identically distributed;i.i.d). Define the sequence of random variables on the coin toss space, ${\displaystyle (X_{i})_{i\in \mathbb {N} }}$ where ${\displaystyle X_{i}(\omega )=\omega _{i}}$ . i.e. each ${\displaystyle X_{i}}$ records the outcome of the ${\displaystyle i}$ th flip.

In this case, any infinite sequence of heads and tails is a possible outcome of the experiment. However, any particular infinite sequence of heads and tails has probability 0 of being the exact outcome of the (infinite) experiment. This is because the i.i.d. assumption implies that the probability of flipping all heads over ${\displaystyle n}$ flips is simply ${\displaystyle P(X_{i}=H,\ i=1,2,\dots ,n)=\left(P(X_{1}=H)\right)^{n}=p^{n}}$ . Letting ${\displaystyle n\rightarrow \infty }$ yields 0, since ${\displaystyle p\in (0,1)}$ by assumption. The result is the same no matter how much we bias the coin towards heads, so long as we constrain ${\displaystyle p}$ to be strictly between 0 and 1. In fact, the same result even holds in non-standard analysis—where infinitesimal probabilities are not allowed.

Moreover, the event "the sequence of tosses contains at least one ${\displaystyle T}$ " will also happen almost surely (i.e., with probability 1). But if instead of an infinite number of flips, flipping stops after some finite time, say 1,000,000 flips, then the probability of getting an all-heads sequence, ${\displaystyle p^{1,000,000}}$ , would no longer be 0, while the probability of getting at least one tails, ${\displaystyle 1-p^{1,000,000}}$ , would no longer be 1 (i.e., the event is no longer almost sure).


£#h5#£Asymptotically almost surely£#/h5#£
In asymptotic analysis, a property is said to hold asymptotically almost surely (a.a.s.) if over a sequence of sets, the probability converges to 1. For instance, in number theory, a large number is asymptotically almost surely composite, by the prime number theorem; and in random graph theory, the statement " ${\displaystyle G(n,p_{n})}$ is connected" (where ${\displaystyle G(n,p)}$ denotes the graphs on ${\displaystyle n}$ vertices with edge probability ${\displaystyle p}$ ) is true a.a.s. when, for some ${\displaystyle \varepsilon >0}$

${\displaystyle p_{n}>{\frac {(1+\varepsilon )\ln n}{n}}.}$    
In number theory, this is referred to as "almost all", as in "almost all numbers are composite". Similarly, in graph theory, this is sometimes referred to as "almost surely".


£#h5#£See also£#/h5#£ £#ul#££#li#£Almost£#/li#£ £#li#£Almost everywhere, the corresponding concept in measure theory£#/li#£ £#li#£Convergence of random variables, for "almost sure convergence"£#/li#£ £#li#£Cromwell's rule, which says that probabilities should almost never be set as zero or one£#/li#£ £#li#£Degenerate distribution, for "almost surely constant"£#/li#£ £#li#£Infinite monkey theorem, a theorem using the aforementioned terms£#/li#£ £#li#£List of mathematical jargon£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Rogers, L. C. G.; Williams, David (2000). Diffusions, Markov Processes, and Martingales. Vol. 1: Foundations. Cambridge University Press. ISBN 978-0521775946.£#/li#£ £#li#£Williams, David (1991). Probability with Martingales. Cambridge Mathematical Textbooks. Cambridge University Press. ISBN 978-0521406055.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Probability and Statistics > Descriptive Statistics £#/li#££#li#£ Calculus and Analysis > Measure Theory £#/li#££#li#£ History and Terminology > Terminology £#/li#££#/ul#£




£#h3#£Alpha Function£#/h3#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Named Integrals £#/li#££#li#£ Calculus and Analysis > Calculus > Integrals > Definite Integrals £#/li#££#/ul#£




£#h3#£Alternating Bilinear Form£#/h3#£

In mathematics, a bilinear form on a vector space V (the elements of which are called vectors) over a field K (the elements of which are called scalars) is a bilinear map V × V → K. In other words, a bilinear form is a function B : V × V → K that is linear in each argument separately:

£#ul#££#li#£B(u + v, w) = B(u, w) + B(v, w)     and     B(λu, v) = λB(u, v)£#/li#£ £#li#£B(u, v + w) = B(u, v) + B(u, w)     and     B(u, λv) = λB(u, v)£#/li#££#/ul#£
The dot product on ${\displaystyle \mathbb {R} ^{n}}$ is an example of a bilinear form.

The definition of a bilinear form can be extended to include modules over a ring, with linear maps replaced by module homomorphisms.

When K is the field of complex numbers C, one is often more interested in sesquilinear forms, which are similar to bilinear forms but are conjugate linear in one argument.


£#h5#£Coordinate representation£#/h5#£
Let V ≅ Kn be an n-dimensional vector space with basis {e1, …, en}.

The n × n matrix A, defined by Aij = B(ei, ej) is called the matrix of the bilinear form on the basis {e1, …, en}.

If the n × 1 matrix x represents a vector v with respect to this basis, and analogously, y represents another vector w, then:

A bilinear form has different matrices on different bases. However, the matrices of a bilinear form on different bases are all congruent. More precisely, if {f1, …, fn} is another basis of V, then

where the ${\displaystyle S_{i,j}}$ form an invertible matrix S. Then, the matrix of the bilinear form on the new basis is STAS.
£#h5#£Maps to the dual space£#/h5#£
Every bilinear form B on V defines a pair of linear maps from V to its dual space V∗. Define B1, B2: V → V∗ by

This is often denoted as

where the dot ( ⋅ ) indicates the slot into which the argument for the resulting linear functional is to be placed (see Currying).

For a finite-dimensional vector space V, if either of B1 or B2 is an isomorphism, then both are, and the bilinear form B is said to be nondegenerate. More concretely, for a finite-dimensional vector space, non-degenerate means that every non-zero element pairs non-trivially with some other element:

${\displaystyle B(x,y)=0}$ for all ${\displaystyle y\in V}$ implies that x = 0 and
${\displaystyle B(x,y)=0}$ for all ${\displaystyle x\in V}$ implies that y = 0.
The corresponding notion for a module over a commutative ring is that a bilinear form is unimodular if V → V∗ is an isomorphism. Given a finitely generated module over a commutative ring, the pairing may be injective (hence "nondegenerate" in the above sense) but not unimodular. For example, over the integers, the pairing B(x, y) = 2xy is nondegenerate but not unimodular, as the induced map from V = Z to V∗ = Z is multiplication by 2.

If V is finite-dimensional then one can identify V with its double dual V∗∗. One can then show that B2 is the transpose of the linear map B1 (if V is infinite-dimensional then B2 is the transpose of B1 restricted to the image of V in V∗∗). Given B one can define the transpose of B to be the bilinear form given by

The left radical and right radical of the form B are the kernels of B1 and B2 respectively; they are the vectors orthogonal to the whole space on the left and on the right.

If V is finite-dimensional then the rank of B1 is equal to the rank of B2. If this number is equal to dim(V) then B1 and B2 are linear isomorphisms from V to V∗. In this case B is nondegenerate. By the rank–nullity theorem, this is equivalent to the condition that the left and equivalently right radicals be trivial. For finite-dimensional spaces, this is often taken as the definition of nondegeneracy:

Given any linear map A : V → V∗ one can obtain a bilinear form B on V via

This form will be nondegenerate if and only if A is an isomorphism.

If V is finite-dimensional then, relative to some basis for V, a bilinear form is degenerate if and only if the determinant of the associated matrix is zero. Likewise, a nondegenerate form is one for which the determinant of the associated matrix is non-zero (the matrix is non-singular). These statements are independent of the chosen basis. For a module over a commutative ring, a unimodular form is one for which the determinant of the associate matrix is a unit (for example 1), hence the term; note that a form whose matrix determinant is non-zero but not a unit will be nondegenerate but not unimodular, for example B(x, y) = 2xy over the integers.


£#h5#£Symmetric, skew-symmetric and alternating forms£#/h5#£
We define a bilinear form to be

£#ul#££#li#£symmetric if B(v, w) = B(w, v) for all v, w in V;£#/li#£ £#li#£alternating if B(v, v) = 0 for all v in V;£#/li#£ £#li#£skew-symmetric or antisymmetric if B(v, w) = −B(w, v) for all v, w in V;
Proposition
Every alternating form is skew-symmetric.
Proof
This can be seen by expanding B(v + w, v + w).
£#/li#££#/ul#£
If the characteristic of K is not 2 then the converse is also true: every skew-symmetric form is alternating. If, however, char(K) = 2 then a skew-symmetric form is the same as a symmetric form and there exist symmetric/skew-symmetric forms that are not alternating.

A bilinear form is symmetric (respectively skew-symmetric) if and only if its coordinate matrix (relative to any basis) is symmetric (respectively skew-symmetric). A bilinear form is alternating if and only if its coordinate matrix is skew-symmetric and the diagonal entries are all zero (which follows from skew-symmetry when char(K) ≠ 2).

A bilinear form is symmetric if and only if the maps B1, B2: V → V∗ are equal, and skew-symmetric if and only if they are negatives of one another. If char(K) ≠ 2 then one can decompose a bilinear form into a symmetric and a skew-symmetric part as follows

where tB is the transpose of B (defined above).
£#h5#£Derived quadratic form£#/h5#£
For any bilinear form B : V × V → K, there exists an associated quadratic form Q : V → K defined by Q : V → K : v ↦ B(v, v).

When char(K) ≠ 2, the quadratic form Q is determined by the symmetric part of the bilinear form B and is independent of the antisymmetric part. In this case there is a one-to-one correspondence between the symmetric part of the bilinear form and the quadratic form, and it makes sense to speak of the symmetric bilinear form associated with a quadratic form.

When char(K) = 2 and dim V > 1, this correspondence between quadratic forms and symmetric bilinear forms breaks down.


£#h5#£Reflexivity and orthogonality£#/h5#£
A bilinear form B is reflexive if and only if it is either symmetric or alternating. In the absence of reflexivity we have to distinguish left and right orthogonality. In a reflexive space the left and right radicals agree and are termed the kernel or the radical of the bilinear form: the subspace of all vectors orthogonal with every other vector. A vector v, with matrix representation x, is in the radical of a bilinear form with matrix representation A, if and only if Ax = 0 ⇔ xTA = 0. The radical is always a subspace of V. It is trivial if and only if the matrix A is nonsingular, and thus if and only if the bilinear form is nondegenerate.

Suppose W is a subspace. Define the orthogonal complement

For a non-degenerate form on a finite-dimensional space, the map V/W → W⊥ is bijective, and the dimension of W⊥ is dim(V) − dim(W).


£#h5#£Different spaces£#/h5#£
Much of the theory is available for a bilinear mapping from two vector spaces over the same base field to that field

Here we still have induced linear mappings from V to W∗, and from W to V∗. It may happen that these mappings are isomorphisms; assuming finite dimensions, if one is an isomorphism, the other must be. When this occurs, B is said to be a perfect pairing.

In finite dimensions, this is equivalent to the pairing being nondegenerate (the spaces necessarily having the same dimensions). For modules (instead of vector spaces), just as how a nondegenerate form is weaker than a unimodular form, a nondegenerate pairing is a weaker notion than a perfect pairing. A pairing can be nondegenerate without being a perfect pairing, for instance Z × Z → Z via (x, y) ↦ 2xy is nondegenerate, but induces multiplication by 2 on the map Z → Z∗.

Terminology varies in coverage of bilinear forms. For example, F. Reese Harvey discusses "eight types of inner product". To define them he uses diagonal matrices Aij having only +1 or −1 for non-zero elements. Some of the "inner products" are symplectic forms and some are sesquilinear forms or Hermitian forms. Rather than a general field K, the instances with real numbers R, complex numbers C, and quaternions H are spelled out. The bilinear form

is called the real symmetric case and labeled R(p, q), where p + q = n. Then he articulates the connection to traditional terminology:
Some of the real symmetric cases are very important. The positive definite case R(n, 0) is called Euclidean space, while the case of a single minus, R(n−1, 1) is called Lorentzian space. If n = 4, then Lorentzian space is also called Minkowski space or Minkowski spacetime. The special case R(p, p) will be referred to as the split-case.


£#h5#£Relation to tensor products£#/h5#£
By the universal property of the tensor product, there is a canonical correspondence between bilinear forms on V and linear maps V ⊗ V → K. If B is a bilinear form on V the corresponding linear map is given by

In the other direction, if F : V ⊗ V → K is a linear map the corresponding bilinear form is given by composing F with the bilinear map V × V → V ⊗ V that sends (v, w) to v⊗w.

The set of all linear maps V ⊗ V → K is the dual space of V ⊗ V, so bilinear forms may be thought of as elements of (V ⊗ V)∗ which (when V is finite-dimensional) is canonically isomorphic to V∗ ⊗ V∗.

Likewise, symmetric bilinear forms may be thought of as elements of Sym2(V∗) (the second symmetric power of V∗), and alternating bilinear forms as elements of Λ2V∗ (the second exterior power of V∗).


£#h5#£On normed vector spaces£#/h5#£
Definition: A bilinear form on a normed vector space (V, ‖⋅‖) is bounded, if there is a constant C such that for all u, v ∈ V,

Definition: A bilinear form on a normed vector space (V, ‖⋅‖) is elliptic, or coercive, if there is a constant c > 0 such that for all u ∈ V,


£#h5#£Generalization to modules£#/h5#£
Given a ring R and a right R-module M and its dual module M∗, a mapping B : M∗ × M → R is called a bilinear form if

for all u, v ∈ M∗, all x, y ∈ M and all α, β ∈ R.

The mapping ⟨⋅,⋅⟩ : M∗ × M → R : (u, x) ↦ u(x) is known as the natural pairing, also called the canonical bilinear form on M∗ × M.

A linear map S : M∗ → M∗ : u ↦ S(u) induces the bilinear form B : M∗ × M → R : (u, x) ↦ ⟨S(u), x⟩, and a linear map T : M → M : x ↦ T(x) induces the bilinear form B : M∗ × M → R : (u, x) ↦ ⟨u, T(x)⟩.

Conversely, a bilinear form B : M∗ × M → R induces the R-linear maps S : M∗ → M∗ : u ↦ (x ↦ B(u, x)) and T′ : M → M∗∗ : x ↦ (u ↦ B(u, x)). Here, M∗∗ denotes the double dual of M.


£#h5#£See also£#/h5#£
£#h5#£Citations£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Adkins, William A.; Weintraub, Steven H. (1992), Algebra: An Approach via Module Theory, Graduate Texts in Mathematics, vol. 136, Springer-Verlag, ISBN 3-540-97839-9, Zbl 0768.00003£#/li#£ £#li#£Bourbaki, N. (1970), Algebra, Springer£#/li#£ £#li#£Cooperstein, Bruce (2010), "Ch 8: Bilinear Forms and Maps", Advanced Linear Algebra, CRC Press, pp. 249–88, ISBN 978-1-4398-2966-0£#/li#£ £#li#£Grove, Larry C. (1997), Groups and characters, Wiley-Interscience, ISBN 978-0-471-16340-4£#/li#£ £#li#£Halmos, Paul R. (1974), Finite-dimensional vector spaces, Undergraduate Texts in Mathematics, Berlin, New York: Springer-Verlag, ISBN 978-0-387-90093-3, Zbl 0288.15002£#/li#£ £#li#£Harvey, F. Reese (1990), "Chapter 2: The Eight Types of Inner Product Spaces", Spinors and calibrations, Academic Press, pp. 19–40, ISBN 0-12-329650-1£#/li#£ £#li#£Popov, V. L. (1987), "Bilinear form", in Hazewinkel, M. (ed.), Encyclopedia of Mathematics, vol. 1, Kluwer Academic Publishers, pp. 390–392. Also: Bilinear form, p. 390, at Google Books£#/li#£ £#li#£Jacobson, Nathan (2009), Basic Algebra, vol. I (2nd ed.), ISBN 978-0-486-47189-1£#/li#£ £#li#£Milnor, J.; Husemoller, D. (1973), Symmetric Bilinear Forms, Ergebnisse der Mathematik und ihrer Grenzgebiete, vol. 73, Springer-Verlag, ISBN 3-540-06009-X, Zbl 0292.10016£#/li#£ £#li#£Porteous, Ian R. (1995), Clifford Algebras and the Classical Groups, Cambridge Studies in Advanced Mathematics, vol. 50, Cambridge University Press, ISBN 978-0-521-55177-9£#/li#£ £#li#£Shafarevich, I. R.; A. O. Remizov (2012), Linear Algebra and Geometry, Springer, ISBN 978-3-642-30993-9£#/li#£ £#li#£Shilov, Georgi E. (1977), Silverman, Richard A. (ed.), Linear Algebra, Dover, ISBN 0-486-63518-X£#/li#£ £#li#£Zhelobenko, Dmitriĭ Petrovich (2006), Principal Structures and Methods of Representation Theory, Translations of Mathematical Monographs, American Mathematical Society, ISBN 0-8218-3731-1£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Bilinear form", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#£ £#li#£"Bilinear form". PlanetMath.£#/li#££#/ul#£
This article incorporates material from Unimodular on PlanetMath, which is licensed under the Creative Commons Attribution/Share-Alike License.


£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Differential Forms £#/li#££#/ul#£




£#h3#£Alternating Factorial£#/h3#£

In mathematics, an alternating factorial is the absolute value of the alternating sum of the first n factorials of positive integers.

This is the same as their sum, with the odd-indexed factorials multiplied by −1 if n is even, and the even-indexed factorials multiplied by −1 if n is odd, resulting in an alternation of signs of the summands (or alternation of addition and subtraction operators, if preferred). To put it algebraically,

${\displaystyle \operatorname {af} (n)=\sum _{i=1}^{n}(-1)^{n-i}i!}$
or with the recurrence relation

${\displaystyle \operatorname {af} (n)=n!-\operatorname {af} (n-1)}$
in which af(1) = 1.

The first few alternating factorials are

1, 1, 5, 19, 101, 619, 4421, 35899, 326981, 3301819, 36614981, 442386619, 5784634181, 81393657019 (sequence A005165 in the OEIS)
For example, the third alternating factorial is 1! – 2! + 3!. The fourth alternating factorial is −1! + 2! − 3! + 4! = 19. Regardless of the parity of n, the last (nth) summand, n!, is given a positive sign, the (n – 1)th summand is given a negative sign, and the signs of the lower-indexed summands are alternated accordingly.

This pattern of alternation ensures the resulting sums are all positive integers. Changing the rule so that either the odd- or even-indexed summands are given negative signs (regardless of the parity of n) changes the signs of the resulting sums but not their absolute values.

Miodrag Zivković proved in 1999 that there are only a finite number of alternating factorials that are also prime numbers, since 3612703 divides af(3612702) and therefore divides af(n) for all n ≥ 3612702. As of 2006, the known primes and probable primes are af(n) for (sequence A001272 in the OEIS)

n = 3, 4, 5, 6, 7, 8, 10, 15, 19, 41, 59, 61, 105, 160, 661, 2653, 3069, 3943, 4053, 4998, 8275, 9158, 11164
Only the values up to n = 661 have been proved prime in 2006. af(661) is approximately 7.818097272875 × 101578.


£#h5#£References£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Alternating Factorial". MathWorld.£#/li#£ £#li#£Yves Gallot, Is the number of primes ${\displaystyle {1 \over 2}\sum _{i=0}^{n-1}i!}$ finite?£#/li#£ £#li#£Paul Jobling, Guy's problem B43: search for primes of form n!-(n-1)!+(n-2)!-(n-3)!+...+/-1!£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Balatov, S. "Alternating Factorials." Jul. 19, 2017. http://www.mersenneforum.org/showpost.php?p=463778&postcount=7.£#/li#££#li#£Guy, R. K. "Equal Products of Factorials," "Alternating Sums of Factorials," and "Equations Involving Factorial n." §B23, B43, and D25 in Unsolved Problems in Number Theory, 2nd ed. New York: Springer-Verlag, pp. 80, 100, and 193-194, 1994.£#/li#££#li#£Jobling, P. "Guy's Problem B43: Search for Primes of Form n!-(n-1)!+(n-2)!-(n-3)!+...+/-1!." 25 Nov 2004. http://listserv.nodak.edu/scripts/wa.exe?A1=ind0411&L=nmbrthry#4.£#/li#££#li#£Rodenkirch, M. "Alternating Factorials." Dec. 15, 2017. http://www.mersenneforum.org/showthread.php?p=474083#post474083.£#/li#££#li#£Sloane, N. J. A. Sequences A001272, A005165/M3892 in "The On-Line Encyclopedia of Integer Sequences."£#/li#££#li#£Živković, M. "The Number of Primes sum_(i=1)^(n)(-1)^(n-i)i! is Finite." Math. Comput. 68, 403-409, 1999.£#/li#££#li#£ Balatov, S. "Alternating Factorials." Jul. 19, 2017. http://www.mersenneforum.org/showpost.php?p=463778&postcount=7. £#/li#££#li#£ Guy, R. K. "Equal Products of Factorials," "Alternating Sums of Factorials," and "Equations Involving Factorial ." §B23, B43, and D25 in Unsolved Problems in Number Theory, 2nd ed. New York: Springer-Verlag, pp. 80, 100, and 193-194, 1994. £#/li#££#li#£ Jobling, P. "Guy's Problem B43: Search for Primes of Form n!-(n-1)!+(n-2)!-(n-3)!+...+/-1!." 25 Nov 2004. http://listserv.nodak.edu/scripts/wa.exe?A1=ind0411&L=nmbrthry#4. £#/li#££#li#£ Rodenkirch, M. "Alternating Factorials." Dec. 15, 2017. http://www.mersenneforum.org/showthread.php?p=474083#post474083. £#/li#££#li#£ Sloane, N. J. A. Sequences A001272, A005165/M3892 in "The On-Line Encyclopedia of Integer Sequences." £#/li#££#li#£ Živković, M. "The Number of Primes is Finite." Math. Comput. 68, 403-409, 1999. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Algebra > Sums £#/li#££#li#£ Calculus and Analysis > Special Functions > Factorials £#/li#££#li#£ History and Terminology > Wolfram Language Commands £#/li#££#/ul#£




£#h3#£Alternating Harmonic Series£#/h3#£

In mathematics, the harmonic series is the infinite series formed by summing all positive unit fractions:

The first ${\displaystyle n}$ terms of the series sum to approximately ${\displaystyle \ln n+\gamma }$ , where ${\displaystyle \ln }$ is the natural logarithm and ${\displaystyle \gamma \approx 0.577}$ is the Euler–Mascheroni constant. Because the logarithm has arbitrarily large values, the harmonic series does not have a finite limit: it is a divergent series. Its divergence was proven in the 14th century by Nicole Oresme using a precursor to the Cauchy condensation test for the convergence of infinite series. It can also be proven to diverge by comparing the sum to an integral, according to the integral test for convergence.

Applications of the harmonic series and its partial sums include Euler's proof that there are infinitely many prime numbers, the analysis of the coupon collector's problem on how many random trials are needed to provide a complete range of responses, the connected components of random graphs, the block-stacking problem on how far over the edge of a table a stack of blocks can be cantilevered, and the average case analysis of the quicksort algorithm.


£#h5#£History£#/h5#£
The name of the harmonic series derives from the concept of overtones or harmonics in music: the wavelengths of the overtones of a vibrating string are ${\displaystyle {\tfrac {1}{2}}}$ , ${\displaystyle {\tfrac {1}{3}}}$ , ${\displaystyle {\tfrac {1}{4}}}$ , etc., of the string's fundamental wavelength. Every term of the harmonic series after the first is the harmonic mean of the neighboring terms, so the terms form a harmonic progression; the phrases harmonic mean and harmonic progression likewise derive from music. Beyond music, harmonic sequences have also had a certain popularity with architects. This was so particularly in the Baroque period, when architects used them to establish the proportions of floor plans, of elevations, and to establish harmonic relationships between both interior and exterior architectural details of churches and palaces.

The divergence of the harmonic series was first proven in 1350 by Nicole Oresme. Oresme's work, and the contemporaneous work of Richard Swineshead on a different series, marked the first appearance of infinite series other than the geometric series in mathematics. However, this achievement fell into obscurity. Additional proofs were published in the 17th century by Pietro Mengoli and by Jacob Bernoulli. Bernoulli credited his brother Johann Bernoulli for finding the proof, and it was later included in Johann Bernoulli's collected works.

The partial sums of the harmonic series were named harmonic numbers, and given their usual notation ${\displaystyle H_{n}}$ , in 1968 by Donald Knuth.


£#h5#£Definition and divergence£#/h5#£
The harmonic series is the infinite series

in which the terms are all of the positive unit fractions. It is a divergent series: as more terms of the series are included in partial sums of the series, the values of these partial sums grow arbitrarily large, beyond any finite limit. Because it is a divergent series, it should be interpreted as a formal sum, an abstract mathematical expression combining the unit fractions, rather than as something that can be evaluated to a numeric value. There are many different proofs of the divergence of the harmonic series, surveyed in a 2006 paper by S. J. Kifowit and T. A. Stamps. Two of the best-known are listed below.
£#h5#£Comparison test£#/h5#£
One way to prove divergence is to compare the harmonic series with another divergent series, where each denominator is replaced with the next-largest power of two:

Grouping equal terms shows that the second series diverges (because every grouping of convergent series is only convergent): Because each term of the harmonic series is greater than or equal to the corresponding term of the second series (and the terms are all positive), it follows (by the comparison test) that the harmonic series diverges as well. The same argument proves more strongly that, for every positive integer ${\displaystyle k}$ , This is the original proof given by Nicole Oresme in around 1350. The Cauchy condensation test is a generalization of this argument.
£#h5#£Integral test£#/h5#£
It is possible to prove that the harmonic series diverges by comparing its sum with an improper integral. Specifically, consider the arrangement of rectangles shown in the figure to the right. Each rectangle is 1 unit wide and ${\displaystyle {\tfrac {1}{n}}}$ units high, so if the harmonic series converged then the total area of the rectangles would be the sum of the harmonic series. The curve ${\displaystyle y={\tfrac {1}{x}}}$ stays entirely below the upper boundary of the rectangles, so the area under the curve (in the range of ${\displaystyle x}$ from one to infinity that is covered by rectangles) would be less than the area of the union of the rectangles. However, the area under the curve is given by a divergent improper integral,

Because this integral does not converge, the sum cannot converge either.
Replacing each rectangle by the next one in the sequence would produce a sequence of rectangles whose boundary lies below the curve rather than above it. This shows that the partial sums of the harmonic series differ from the integral by an amount that is bounded above and below by the unit area of the first rectangle:

Generalizing this argument, any infinite sum of values of a monotone decreasing positive function of ${\displaystyle n}$ (like the harmonic series) has partial sums that are within a bounded distance of the values of the corresponding integrals. Therefore, the sum converges if and only if the integral over the same range of the same function converges. When this equivalence is used to check the convergence of a sum by replacing it with an easier integral, it is known as the integral test for convergence.
£#h5#£Partial sums£#/h5#£
Adding the first ${\displaystyle n}$ terms of the harmonic series produces a partial sum, called a harmonic number and denoted ${\displaystyle H_{n}}$ :


£#h5#£Growth rate£#/h5#£
These numbers grow very slowly, with logarithmic growth, as can be seen from the integral test. More precisely,

where ${\displaystyle \gamma \approx 0.5772}$ is the Euler–Mascheroni constant and ${\displaystyle 0\leq \varepsilon _{n}\leq 1/8n^{2}}$ which approaches 0 as ${\displaystyle n}$ goes to infinity.
£#h5#£Divisibility£#/h5#£
No harmonic numbers are integers, except for ${\displaystyle H_{1}=1}$ . One way to prove that ${\displaystyle H_{n}}$ is not an integer is to consider the highest power of two ${\displaystyle 2^{k}}$ in the range from 1 to ${\displaystyle n}$ . If ${\displaystyle M}$ is the least common multiple of the numbers from 1 to ${\displaystyle n}$ , then ${\displaystyle H_{k}}$ can be rewritten as a sum of fractions with equal denominators

in which only one of the numerators, ${\displaystyle M/2^{k}}$ , is odd and the rest are even, and (when ${\displaystyle n>1}$ ) ${\displaystyle M}$ is itself even. Therefore, the result is a fraction with an odd numerator and an even denominator, which cannot be an integer. More strongly, any sequence of consecutive integers has a unique member divisible by a greater power of two than all the other sequence members, from which it follows by the same argument that no two harmonic numbers differ by an integer.
Another proof that the harmonic numbers are not integers observes that the denominator of ${\displaystyle H_{n}}$ must be divisible by all prime numbers greater than ${\displaystyle n/2}$ , and uses Bertrand's postulate to prove that this set of primes is non-empty. The same argument implies more strongly that, except for ${\displaystyle H_{1}=1}$ , ${\displaystyle H_{2}=1.5}$ , and ${\displaystyle H_{6}=2.45}$ , no harmonic number can have a terminating decimal representation. It has been conjectured that every prime number divides the numerators of only a finite subset of the harmonic numbers, but this remains unproven.


£#h5#£Interpolation£#/h5#£
The digamma function is defined as the logarithmic derivative of the gamma function

Just as the gamma function provides a continuous interpolation of the factorials, the digamma function provides a continuous interpolation of the harmonic numbers, in the sense that ${\displaystyle \psi (n)=H_{n-1}-\gamma }$ . This equation can be used to extend the definition to harmonic numbers with rational indices.
£#h5#£Applications£#/h5#£
Many well-known mathematical problems have solutions involving the harmonic series and its partial sums.


£#h5#£Crossing a desert£#/h5#£
The jeep problem or desert-crossing problem is included in a 9th-century problem collection by Alcuin, Propositiones ad Acuendos Juvenes (formulated in terms of camels rather than jeeps), but with an incorrect solution. The problem asks how far into the desert a jeep can travel and return, starting from a base with ${\displaystyle n}$ loads of fuel, by carrying some of the fuel into the desert and leaving it in depots. The optimal solution involves placing depots spaced at distances ${\displaystyle {\tfrac {r}{2n}},{\tfrac {r}{2(n-1)}},{\tfrac {r}{2(n-2)}},\dots }$ from the starting point and each other, where ${\displaystyle r}$ is the range of distance that the jeep can travel with a single load of fuel. On each trip out and back from the base, the jeep places one more depot, refueling at the other depots along the way, and placing as much fuel as it can in the newly placed depot while still leaving enough for itself to return to the previous depots and the base. Therefore, the total distance reached on the ${\displaystyle n}$ th trip is

where ${\displaystyle H_{n}}$ is the ${\displaystyle n}$ th harmonic number. The divergence of the harmonic series implies that crossings of any length are possible with enough fuel.
For instance, for Alcuin's version of the problem, ${\displaystyle r=30}$ : a camel can carry 30 measures of grain and can travel one leuca while eating a single measure, where a leuca is a unit of distance roughly equal to 2.3 kilometres (1.4 mi). The problem has ${\displaystyle n=3}$ : there are 90 measures of grain, enough to supply three trips. For the standard formulation of the desert-crossing problem, it would be possible for the camel to travel ${\displaystyle {\tfrac {30}{2}}{\bigl (}{\tfrac {1}{3}}+{\tfrac {1}{2}}+{\tfrac {1}{1}})=27.5}$ leucas and return, by placing a grain storage depot 5 leucas from the base on the first trip and 12.5 leucas from the base on the second trip. However, Alcuin instead asks a slightly different question, how much grain can be transported a distance of 30 leucas without a final return trip, and either strands some camels in the desert or fails to account for the amount of grain consumed by a camel on its return trips.


£#h5#£Stacking blocks£#/h5#£
In the block-stacking problem, one must place a pile of ${\displaystyle n}$ identical rectangular blocks, one per layer, so that they hang as far as possible over the edge of a table without falling. The top block can be placed with ${\displaystyle {\tfrac {1}{2}}}$ of its length extending beyond the next lower block. If it is placed in this way, the next block down needs to be placed with at most ${\displaystyle {\tfrac {1}{2}}\cdot {\tfrac {1}{2}}}$ of its length extending beyond the next lower block, so that the center of mass of the top two block is supported and they do not topple. The third block needs to be placed with at most ${\displaystyle {\tfrac {1}{2}}\cdot {\tfrac {1}{3}}}$ of its length extending beyond the next lower block, and so on. In this way, it is possible to place the ${\displaystyle n}$ blocks in such a way that they extend ${\displaystyle {\tfrac {1}{2}}H_{n}}$ lengths beyond the table, where ${\displaystyle H_{n}}$ is the ${\displaystyle n}$ th harmonic number. The divergence of the harmonic series implies that there is no limit on how far beyond the table the block stack can extend. For stacks with one block per layer, no better solution is possible, but significantly more overhang can be achieved using stacks with more than one block per layer.


£#h5#£Counting primes and divisors£#/h5#£
In 1737, Leonhard Euler observed that, as a formal sum, the harmonic series is equal to an Euler product in which each term comes from a prime number:

where ${\displaystyle \mathbb {P} }$ denotes the set of prime numbers. The left equality comes from applying the distributive law to the product and recognizing the resulting terms as the prime factorizations of the terms in the harmonic series, and the right equality uses the standard formula for a geometric series. The product is divergent, just like the sum, but if it converged one could take logarithms and obtain Here, each logarithm is replaced by its Taylor series, and the constant ${\displaystyle K}$ on the right is the evaluation of the convergent series of terms with exponent greater than one. It follows from these manipulations that the sum of reciprocals of primes, on the right hand of this equality, must diverge, for if it converged these steps could be reversed to show that the harmonic series also converges, which it does not. An immediate corollary is that there are infinitely many prime numbers, because a finite sum cannot diverge. Although Euler's work is not considered adequately rigorous by the standards of modern mathematics, it can be made rigorous by taking more care with limits and error bounds. Euler's conclusion that the partial sums of reciprocals of primes grow as a double logarithm of the number of terms has been confirmed by later mathematicians as one of Mertens' theorems, and can be seen as a precursor to the prime number theorem.
Another problem in number theory closely related to the harmonic series concerns the average number of divisors of the numbers in a range from 1 to ${\displaystyle n}$ , formalized as the average order of the divisor function,

The operation of rounding each term in the harmonic series to the next smaller integer multiple of ${\displaystyle {\tfrac {1}{n}}}$ causes this average to differ from the harmonic numbers by a small constant, and Peter Gustav Lejeune Dirichlet showed more precisely that the average number of divisors is ${\displaystyle \ln n+2\gamma -1+O(1/{\sqrt {n}})}$ (expressed in big O notation). Bounding the final error term more precisely remains an open problem, known as Dirichlet's divisor problem.
£#h5#£Collecting coupons£#/h5#£
Several common games or recreations involve repeating a random selection from a set of items until all possible choices have been selected; these include the collection of trading cards and the completion of parkrun bingo, in which the goal is to obtain all 60 possible numbers of seconds in the times from a sequence of running events. More serious applications of this problem include sampling all variations of a manufactured product for its quality control, and the connectivity of random graphs. In situations of this form, once there are ${\displaystyle k}$ items remaining to be collected out of a total of ${\displaystyle n}$ equally-likely items, the probability of collecting a new item in a single random choice is ${\displaystyle k/n}$ and the expected number of random choices needed until a new item is collected is ${\displaystyle n/k}$ . Summing over all values of ${\displaystyle k}$ from ${\displaystyle n}$ down to 1 shows that the total expected number of random choices needed to collect all items is ${\displaystyle nH_{n}}$ , where ${\displaystyle H_{n}}$ is the ${\displaystyle n}$ th harmonic number.


£#h5#£Analyzing algorithms£#/h5#£
The quicksort algorithm for sorting a set of items can be analyzed using the harmonic numbers. The algorithm operates by choosing one item as a "pivot", comparing it to all the others, and recursively sorting the two subsets of items whose comparison places them before the pivot and after the pivot. In either its average-case complexity (with the assumption that all input permutations are equally likely) or in its expected time analysis of worst-case inputs with a random choice of pivot, all of the items are equally likely to be chosen as the pivot. For such cases, one can compute the probability that two items are ever compared with each other, throughout the recursion, as a function of the number of other items that separate them in the final sorted order. If items ${\displaystyle x}$ and ${\displaystyle y}$ are separated by ${\displaystyle k}$ other items, then the algorithm will make a comparison between ${\displaystyle x}$ and ${\displaystyle y}$ only when, as the recursion progresses, it picks ${\displaystyle x}$ or ${\displaystyle y}$ as a pivot before picking any of the other ${\displaystyle k}$ items between them. Because each of these ${\displaystyle k+2}$ items is equally likely to be chosen first, this happens with probability ${\displaystyle {\tfrac {2}{k+2}}}$ . The total expected number of comparisons, which controls the total running time of the algorithm, can then be calculated by summing these probabilities over all pairs, giving

The divergence of the harmonic series corresponds in this application to the fact that, in the comparison model of sorting used for quicksort, it is not possible to sort in linear time.
£#h5#£Related series£#/h5#£
£#h5#£Alternating harmonic series£#/h5#£
The series

is known as the alternating harmonic series. It is conditionally convergent by the alternating series test, but not absolutely convergent. Its sum is the natural logarithm of 2.
Using alternating signs with only odd unit fractions produces a related series, the Leibniz formula for π


£#h5#£Riemann zeta function£#/h5#£
The Riemann zeta function is defined for real ${\displaystyle x>1}$ by the convergent series

which for ${\displaystyle x=1}$ would be the harmonic series. It can be extended by analytic continuation to a holomorphic function on all complex numbers except ${\displaystyle x=1}$ , where the extended function has a simple pole. Other important values of the zeta function include ${\displaystyle \zeta (2)=\pi ^{2}/6}$ , the solution to the Basel problem, Apéry's constant ${\displaystyle \zeta (3)}$ , proved by Roger Apéry to be an irrational number, and the "critical line" of complex numbers with real part ${\displaystyle {\tfrac {1}{2}}}$ , conjectured by the Riemann hypothesis to be the only values other than negative integers where the function can be zero.
£#h5#£Random harmonic series£#/h5#£
The random harmonic series is

where the values ${\displaystyle s_{n}}$ are independent and identically distributed random variables that take the two values ${\displaystyle +1}$ and ${\displaystyle -1}$ with equal probability ${\displaystyle {\tfrac {1}{2}}}$ . It converges with probability 1, as can be seen by using the Kolmogorov three-series theorem or of the closely related Kolmogorov maximal inequality. The sum of the series is a random variable whose probability density function is close to ${\displaystyle {\tfrac {1}{4}}}$ for values between ${\displaystyle -1}$ and ${\displaystyle 1}$ , and decreases to near-zero for values greater than ${\displaystyle 3}$ or less than ${\displaystyle -3}$ . Intermediate between these ranges, at the values ${\displaystyle \pm 2}$ , the probability density is ${\displaystyle {\tfrac {1}{8}}-\varepsilon }$ for a nonzero but very small value ${\displaystyle \varepsilon <10^{-42}}$ .
£#h5#£Depleted harmonic series£#/h5#£
The depleted harmonic series where all of the terms in which the digit 9 appears anywhere in the denominator are removed can be shown to converge to the value 22.92067661926415034816.... In fact, when all the terms containing any particular string of digits (in any base) are removed, the series converges.


£#h5#£References£#/h5#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Harmonic Series". MathWorld.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Havil, J. Gamma: Exploring Euler's Constant. Princeton, NJ: Princeton University Press, p. 33, 2003.£#/li#££#li#£ Havil, J. Gamma: Exploring Euler's Constant. Princeton, NJ: Princeton University Press, p. 33, 2003. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Riemann Zeta Function £#/li#££#/ul#£




£#h3#£Alternating Series£#/h3#£

In mathematical analysis, the alternating series test is the method used to show that an alternating series is convergent when its terms (1) decrease in absolute value, and (2) approach zero in the limit. The test was used by Gottfried Leibniz and is sometimes known as Leibniz's test, Leibniz's rule, or the Leibniz criterion. The test is only sufficient, not necessary, so some convergent alternating series may fail the first part of the test.


£#h5#£Formal Statement£#/h5#£
£#h5#£Alternating series test£#/h5#£
A series of the form

${\displaystyle \sum _{n=0}^{\infty }(-1)^{n}a_{n}=a_{0}-a_{1}+a_{2}-a_{3}+\cdots \!}$
where either all an are positive or all an are negative, is called an alternating series.

The alternating series test guarantees that an alternating series converges if the following two conditions are met:

£#li#£ ${\displaystyle |a_{n}|}$ decreases monotonically[1], i.e., ${\displaystyle |a_{n+1}|\leq |a_{n}|}$ , and£#/li#£ £#li#£ ${\displaystyle \lim _{n\to \infty }a_{n}=0}$ £#/li#£

£#h5#£Alternating series estimation theorem£#/h5#£
Moreover, let L denote the sum of the series, then the partial sum

${\displaystyle S_{k}=\sum _{n=0}^{k}(-1)^{n}a_{n}\!}$
approximates L with error bounded by the next omitted term:

${\displaystyle \left|S_{k}-L\right\vert \leq \left|S_{k}-S_{k+1}\right\vert =a_{k+1}.\!}$

£#h5#£Proof£#/h5#£
Suppose we are given a series of the form ${\displaystyle \sum _{n=1}^{\infty }(-1)^{n-1}a_{n}\!}$ , where ${\displaystyle \lim _{n\rightarrow \infty }a_{n}=0}$ and ${\displaystyle a_{n}\geq a_{n+1}}$ for all natural numbers n. (The case ${\displaystyle \sum _{n=1}^{\infty }(-1)^{n}a_{n}\!}$ follows by taking the negative.)


£#h5#£Proof of the alternating series test£#/h5#£
We will prove that both the partial sums ${\displaystyle S_{2m+1}=\sum _{n=1}^{2m+1}(-1)^{n-1}a_{n}}$ with odd number of terms, and ${\displaystyle S_{2m}=\sum _{n=1}^{2m}(-1)^{n-1}a_{n}}$ with even number of terms, converge to the same number L. Thus the usual partial sum ${\displaystyle S_{k}=\sum _{n=1}^{k}(-1)^{n-1}a_{n}}$ also converges to L.

The odd partial sums decrease monotonically:

${\displaystyle S_{2(m+1)+1}=S_{2m+1}-a_{2m+2}+a_{2m+3}\leq S_{2m+1}}$
while the even partial sums increase monotonically:

${\displaystyle S_{2(m+1)}=S_{2m}+a_{2m+1}-a_{2m+2}\geq S_{2m}}$
both because an decreases monotonically with n.

Moreover, since an are positive, ${\displaystyle S_{2m+1}-S_{2m}=a_{2m+1}\geq 0}$ . Thus we can collect these facts to form the following suggestive inequality:

${\displaystyle a_{1}-a_{2}=S_{2}\leq S_{2m}\leq S_{2m+1}\leq S_{1}=a_{1}.}$
Now, note that a1 − a2 is a lower bound of the monotonically decreasing sequence S2m+1, the monotone convergence theorem then implies that this sequence converges as m approaches infinity. Similarly, the sequence of even partial sum converges too.

Finally, they must converge to the same number because

${\displaystyle \lim _{m\to \infty }(S_{2m+1}-S_{2m})=\lim _{m\to \infty }a_{2m+1}=0.}$
Call the limit L, then the monotone convergence theorem also tells us extra information that

${\displaystyle S_{2m}\leq L\leq S_{2m+1}}$
for any m. This means the partial sums of an alternating series also "alternates" above and below the final limit. More precisely, when there is an odd (even) number of terms, i.e. the last term is a plus (minus) term, then the partial sum is above (below) the final limit.

This understanding leads immediately to an error bound of partial sums, shown below.


£#h5#£Proof of the alternating series estimation theorem£#/h5#£
We would like to show ${\displaystyle \left|S_{k}-L\right|\leq a_{k+1}\!}$ by splitting into two cases.

When k = 2m+1, i.e. odd, then

${\displaystyle \left|S_{2m+1}-L\right|=S_{2m+1}-L\leq S_{2m+1}-S_{2m+2}=a_{(2m+1)+1}}$
When k = 2m, i.e. even, then

${\displaystyle \left|S_{2m}-L\right|=L-S_{2m}\leq S_{2m+1}-S_{2m}=a_{2m+1}}$
as desired.

Both cases rely essentially on the last inequality derived in the previous proof.

For an alternative proof using Cauchy's convergence test, see Alternating series.

For a generalization, see Dirichlet's test.


£#h5#£Examples£#/h5#£
£#h5#£A typical example£#/h5#£
The alternating harmonic series

meets both conditions for the alternating series test and converges.
£#h5#£An example to show monotonicity is needed£#/h5#£
All of the conditions in the test, namely convergence to zero and monotonicity, should be met in order for the conclusion to be true. For example, take the series

${\displaystyle {\frac {1}{{\sqrt {2}}-1}}-{\frac {1}{{\sqrt {2}}+1}}+{\frac {1}{{\sqrt {3}}-1}}-{\frac {1}{{\sqrt {3}}+1}}+\cdots }$
The signs are alternating and the terms tend to zero. However, monotonicity is not present and we cannot apply the test. Actually the series is divergent. Indeed, for the partial sum ${\displaystyle S_{2n}}$ we have ${\displaystyle S_{2n}={\frac {2}{1}}+{\frac {2}{2}}+{\frac {2}{3}}+\cdots +{\frac {2}{n-1}}}$ which is twice the partial sum of the harmonic series, which is divergent. Hence the original series is divergent.


£#h5#£The test is only sufficient, not necessary£#/h5#£
Leibniz test's monotonicity is not a necessary condition, thus the test itself is only sufficient, but not necessary. (The second part of the test is well known necessary condition of convergence for all series.) Examples of nonmonotonic series that converge are ${\displaystyle \sum _{n=2}^{\infty }{\dfrac {(-1)^{n}}{n+(-1)^{n}}}}$ and ${\displaystyle \sum _{n=1}^{\infty }(-1)^{n}{\dfrac {\cos ^{2}n}{n^{2}}}.}$


£#h5#£See also£#/h5#£ £#ul#££#li#£Alternating series£#/li#£ £#li#£Dirichlet's test£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
^ In practice, the first few terms may increase. What is important is that ${\displaystyle b_{n}\geq b_{n+1}}$ for all ${\displaystyle n}$ after some point, because the first finite amount of terms would not change a series' convergence/divergence.

£#h5#£References£#/h5#£ £#ul#££#li#£Konrad Knopp (1956) Infinite Sequences and Series, § 3.4, Dover Publications ISBN 0-486-60153-6£#/li#£ £#li#£Konrad Knopp (1990) Theory and Application of Infinite Series, § 15, Dover Publications ISBN 0-486-66165-2£#/li#£ £#li#£James Stewart, Daniel Clegg, Saleem Watson (2016) Single Variable Calculus: Early Transcendentals (Instructor's Edition) 9E, Cengage ISBN 978-0-357-02228-9£#/li#£ £#li#£E. T. Whittaker & G. N. Watson (1963) A Course in Modern Analysis, 4th edition, §2.3, Cambridge University Press ISBN 0-521-58807-3£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Leibniz Criterion". MathWorld.£#/li#£ £#li#£Jeff Cruzan. "Alternating series"£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Arfken, G. "Alternating Series." §5.3 in Mathematical Methods for Physicists, 3rd ed. Orlando, FL: Academic Press, pp. 293-294, 1985.£#/li#££#li#£Bromwich, T. J. I'A. and MacRobert, T. M. "Alternating Series." §19 in An Introduction to the Theory of Infinite Series, 3rd ed. New York: Chelsea, pp. 55-57, 1991.£#/li#££#li#£Gardner, M. The Sixth Book of Mathematical Games from Scientific American. Chicago, IL: University of Chicago Press, p. 170, 1984.£#/li#££#li#£Hoffman, P. The Man Who Loved Only Numbers: The Story of Paul Erdős and the Search for Mathematical Truth. New York: Hyperion, p. 218, 1998.£#/li#££#li#£Pinsky, M. A. "Averaging an Alternating Series." Math. Mag. 51, 235-237, 1978.£#/li#££#li#£Shallit, J. and Davidson, J. L. "Continued Fractions for Some Alternating Series." Monatshefte Math. 111, 119-126, 1991.£#/li#££#li#£Sloane, N. J. A. Sequence A114884 in "The On-Line Encyclopedia of Integer Sequences."£#/li#££#li#£ Arfken, G. "Alternating Series." §5.3 in Mathematical Methods for Physicists, 3rd ed. Orlando, FL: Academic Press, pp. 293-294, 1985. £#/li#££#li#£ Bromwich, T. J. I'A. and MacRobert, T. M. "Alternating Series." §19 in An Introduction to the Theory of Infinite Series, 3rd ed. New York: Chelsea, pp. 55-57, 1991. £#/li#££#li#£ Gardner, M. The Sixth Book of Mathematical Games from Scientific American. Chicago, IL: University of Chicago Press, p. 170, 1984. £#/li#££#li#£ Hoffman, P. The Man Who Loved Only Numbers: The Story of Paul Erdős and the Search for Mathematical Truth. New York: Hyperion, p. 218, 1998. £#/li#££#li#£ Pinsky, M. A. "Averaging an Alternating Series." Math. Mag. 51, 235-237, 1978. £#/li#££#li#£ Shallit, J. and Davidson, J. L. "Continued Fractions for Some Alternating Series." Monatshefte Math. 111, 119-126, 1991. £#/li#££#li#£ Sloane, N. J. A. Sequence A114884 in "The On-Line Encyclopedia of Integer Sequences." £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Series > General Series £#/li#££#li#£ Foundations of Mathematics > Mathematical Problems > Unsolved Problems £#/li#££#/ul#£




£#h3#£Alternating Series Test£#/h3#£

In mathematical analysis, the alternating series test is the method used to show that an alternating series is convergent when its terms (1) decrease in absolute value, and (2) approach zero in the limit. The test was used by Gottfried Leibniz and is sometimes known as Leibniz's test, Leibniz's rule, or the Leibniz criterion. The test is only sufficient, not necessary, so some convergent alternating series may fail the first part of the test.


£#h5#£Formal Statement£#/h5#£
£#h5#£Alternating series test£#/h5#£
A series of the form

${\displaystyle \sum _{n=0}^{\infty }(-1)^{n}a_{n}=a_{0}-a_{1}+a_{2}-a_{3}+\cdots \!}$
where either all an are positive or all an are negative, is called an alternating series.

The alternating series test guarantees that an alternating series converges if the following two conditions are met:

£#li#£ ${\displaystyle |a_{n}|}$ decreases monotonically[1], i.e., ${\displaystyle |a_{n+1}|\leq |a_{n}|}$ , and£#/li#£ £#li#£ ${\displaystyle \lim _{n\to \infty }a_{n}=0}$ £#/li#£

£#h5#£Alternating series estimation theorem£#/h5#£
Moreover, let L denote the sum of the series, then the partial sum

${\displaystyle S_{k}=\sum _{n=0}^{k}(-1)^{n}a_{n}\!}$
approximates L with error bounded by the next omitted term:

${\displaystyle \left|S_{k}-L\right\vert \leq \left|S_{k}-S_{k+1}\right\vert =a_{k+1}.\!}$

£#h5#£Proof£#/h5#£
Suppose we are given a series of the form ${\displaystyle \sum _{n=1}^{\infty }(-1)^{n-1}a_{n}\!}$ , where ${\displaystyle \lim _{n\rightarrow \infty }a_{n}=0}$ and ${\displaystyle a_{n}\geq a_{n+1}}$ for all natural numbers n. (The case ${\displaystyle \sum _{n=1}^{\infty }(-1)^{n}a_{n}\!}$ follows by taking the negative.)


£#h5#£Proof of the alternating series test£#/h5#£
We will prove that both the partial sums ${\displaystyle S_{2m+1}=\sum _{n=1}^{2m+1}(-1)^{n-1}a_{n}}$ with odd number of terms, and ${\displaystyle S_{2m}=\sum _{n=1}^{2m}(-1)^{n-1}a_{n}}$ with even number of terms, converge to the same number L. Thus the usual partial sum ${\displaystyle S_{k}=\sum _{n=1}^{k}(-1)^{n-1}a_{n}}$ also converges to L.

The odd partial sums decrease monotonically:

${\displaystyle S_{2(m+1)+1}=S_{2m+1}-a_{2m+2}+a_{2m+3}\leq S_{2m+1}}$
while the even partial sums increase monotonically:

${\displaystyle S_{2(m+1)}=S_{2m}+a_{2m+1}-a_{2m+2}\geq S_{2m}}$
both because an decreases monotonically with n.

Moreover, since an are positive, ${\displaystyle S_{2m+1}-S_{2m}=a_{2m+1}\geq 0}$ . Thus we can collect these facts to form the following suggestive inequality:

${\displaystyle a_{1}-a_{2}=S_{2}\leq S_{2m}\leq S_{2m+1}\leq S_{1}=a_{1}.}$
Now, note that a1 − a2 is a lower bound of the monotonically decreasing sequence S2m+1, the monotone convergence theorem then implies that this sequence converges as m approaches infinity. Similarly, the sequence of even partial sum converges too.

Finally, they must converge to the same number because

${\displaystyle \lim _{m\to \infty }(S_{2m+1}-S_{2m})=\lim _{m\to \infty }a_{2m+1}=0.}$
Call the limit L, then the monotone convergence theorem also tells us extra information that

${\displaystyle S_{2m}\leq L\leq S_{2m+1}}$
for any m. This means the partial sums of an alternating series also "alternates" above and below the final limit. More precisely, when there is an odd (even) number of terms, i.e. the last term is a plus (minus) term, then the partial sum is above (below) the final limit.

This understanding leads immediately to an error bound of partial sums, shown below.


£#h5#£Proof of the alternating series estimation theorem£#/h5#£
We would like to show ${\displaystyle \left|S_{k}-L\right|\leq a_{k+1}\!}$ by splitting into two cases.

When k = 2m+1, i.e. odd, then

${\displaystyle \left|S_{2m+1}-L\right|=S_{2m+1}-L\leq S_{2m+1}-S_{2m+2}=a_{(2m+1)+1}}$
When k = 2m, i.e. even, then

${\displaystyle \left|S_{2m}-L\right|=L-S_{2m}\leq S_{2m+1}-S_{2m}=a_{2m+1}}$
as desired.

Both cases rely essentially on the last inequality derived in the previous proof.

For an alternative proof using Cauchy's convergence test, see Alternating series.

For a generalization, see Dirichlet's test.


£#h5#£Examples£#/h5#£
£#h5#£A typical example£#/h5#£
The alternating harmonic series

meets both conditions for the alternating series test and converges.
£#h5#£An example to show monotonicity is needed£#/h5#£
All of the conditions in the test, namely convergence to zero and monotonicity, should be met in order for the conclusion to be true. For example, take the series

${\displaystyle {\frac {1}{{\sqrt {2}}-1}}-{\frac {1}{{\sqrt {2}}+1}}+{\frac {1}{{\sqrt {3}}-1}}-{\frac {1}{{\sqrt {3}}+1}}+\cdots }$
The signs are alternating and the terms tend to zero. However, monotonicity is not present and we cannot apply the test. Actually the series is divergent. Indeed, for the partial sum ${\displaystyle S_{2n}}$ we have ${\displaystyle S_{2n}={\frac {2}{1}}+{\frac {2}{2}}+{\frac {2}{3}}+\cdots +{\frac {2}{n-1}}}$ which is twice the partial sum of the harmonic series, which is divergent. Hence the original series is divergent.


£#h5#£The test is only sufficient, not necessary£#/h5#£
Leibniz test's monotonicity is not a necessary condition, thus the test itself is only sufficient, but not necessary. (The second part of the test is well known necessary condition of convergence for all series.) Examples of nonmonotonic series that converge are ${\displaystyle \sum _{n=2}^{\infty }{\dfrac {(-1)^{n}}{n+(-1)^{n}}}}$ and ${\displaystyle \sum _{n=1}^{\infty }(-1)^{n}{\dfrac {\cos ^{2}n}{n^{2}}}.}$


£#h5#£See also£#/h5#£ £#ul#££#li#£Alternating series£#/li#£ £#li#£Dirichlet's test£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
^ In practice, the first few terms may increase. What is important is that ${\displaystyle b_{n}\geq b_{n+1}}$ for all ${\displaystyle n}$ after some point, because the first finite amount of terms would not change a series' convergence/divergence.

£#h5#£References£#/h5#£ £#ul#££#li#£Konrad Knopp (1956) Infinite Sequences and Series, § 3.4, Dover Publications ISBN 0-486-60153-6£#/li#£ £#li#£Konrad Knopp (1990) Theory and Application of Infinite Series, § 15, Dover Publications ISBN 0-486-66165-2£#/li#£ £#li#£James Stewart, Daniel Clegg, Saleem Watson (2016) Single Variable Calculus: Early Transcendentals (Instructor's Edition) 9E, Cengage ISBN 978-0-357-02228-9£#/li#£ £#li#£E. T. Whittaker & G. N. Watson (1963) A Course in Modern Analysis, 4th edition, §2.3, Cambridge University Press ISBN 0-521-58807-3£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Leibniz Criterion". MathWorld.£#/li#£ £#li#£Jeff Cruzan. "Alternating series"£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Zwillinger, D. (Ed.). "Convergence Tests." §1.3.3 in CRC Standard Mathematical Tables and Formulae, 30th ed. Boca Raton, FL: CRC Press, p. 32, 1996.£#/li#££#li#£ Zwillinger, D. (Ed.). "Convergence Tests." §1.3.3 in CRC Standard Mathematical Tables and Formulae, 30th ed. Boca Raton, FL: CRC Press, p. 32, 1996. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Series > Convergence £#/li#££#/ul#£




£#h3#£Alternating Tensor£#/h3#£

In mathematics and theoretical physics, a tensor is antisymmetric on (or with respect to) an index subset if it alternates sign (+/−) when any two indices of the subset are interchanged. The index subset must generally either be all covariant or all contravariant.

For example,

holds when the tensor is antisymmetric with respect to its first three indices.
If a tensor changes sign under exchange of each pair of its indices, then the tensor is completely (or totally) antisymmetric. A completely antisymmetric covariant tensor field of order ${\displaystyle k}$ may be referred to as a differential ${\displaystyle k}$ -form, and a completely antisymmetric contravariant tensor field may be referred to as a ${\displaystyle k}$ -vector field.


£#h5#£Antisymmetric and symmetric tensors£#/h5#£
A tensor A that is antisymmetric on indices ${\displaystyle i}$ and ${\displaystyle j}$ has the property that the contraction with a tensor B that is symmetric on indices ${\displaystyle i}$ and ${\displaystyle j}$ is identically 0.

For a general tensor U with components ${\displaystyle U_{ijk\dots }}$ and a pair of indices ${\displaystyle i}$ and ${\displaystyle j,}$ U has symmetric and antisymmetric parts defined as:

Similar definitions can be given for other pairs of indices. As the term "part" suggests, a tensor is the sum of its symmetric part and antisymmetric part for a given pair of indices, as in


£#h5#£Notation£#/h5#£
A shorthand notation for anti-symmetrization is denoted by a pair of square brackets. For example, in arbitrary dimensions, for an order 2 covariant tensor M,

and for an order 3 covariant tensor T,
In any 2 and 3 dimensions, these can be written as

where ${\displaystyle \delta _{ab\dots }^{cd\dots }}$ is the generalized Kronecker delta, and we use the Einstein notation to summation over like indices.
More generally, irrespective of the number of dimensions, antisymmetrization over ${\displaystyle p}$ indices may be expressed as

In general, every tensor of rank 2 can be decomposed into a symmetric and anti-symmetric pair as:

This decomposition is not in general true for tensors of rank 3 or more, which have more complex symmetries.


£#h5#£Examples£#/h5#£
Totally antisymmetric tensors include:

£#ul#££#li#£Trivially, all scalars and vectors (tensors of order 0 and 1) are totally antisymmetric (as well as being totally symmetric).£#/li#£ £#li#£The electromagnetic tensor, ${\displaystyle F_{\mu \nu }}$ in electromagnetism.£#/li#£ £#li#£The Riemannian volume form on a pseudo-Riemannian manifold.£#/li#££#/ul#£
£#h5#£See also£#/h5#£ £#ul#££#li#£Antisymmetric matrix£#/li#£ £#li#£Exterior algebra – Algebraic construction used in multilinear algebra and geometry£#/li#£ £#li#£Levi-Civita symbol – Antisymmetric permutation object acting on tensors£#/li#£ £#li#£Ricci calculus – Tensor index notation for tensor-based calculations£#/li#£ £#li#£Symmetric tensor – Tensor invariant under permutations of vectors it acts on£#/li#£ £#li#£Symmetrization£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Penrose, Roger (2007). The Road to Reality. Vintage books. ISBN 0-679-77631-1.£#/li#£ £#li#£J.A. Wheeler; C. Misner; K.S. Thorne (1973). Gravitation. W.H. Freeman & Co. pp. 85–86, §3.5. ISBN 0-7167-0344-0.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Antisymmetric Tensor – mathworld.wolfram.com£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Differential Geometry > Tensor Analysis £#/li#££#/ul#£




£#h3#£Alternating Zeta Function£#/h3#£

In mathematics, in the area of analytic number theory, the Dirichlet eta function is defined by the following Dirichlet series, which converges for any complex number having real part > 0:

This Dirichlet series is the alternating sum corresponding to the Dirichlet series expansion of the Riemann zeta function, ζ(s) — and for this reason the Dirichlet eta function is also known as the alternating zeta function, also denoted ζ*(s). The following relation holds:

Both Dirichlet eta function and Riemann zeta function are special cases of Polylogarithm.

While the Dirichlet series expansion for the eta function is convergent only for any complex number s with real part > 0, it is Abel summable for any complex number. This serves to define the eta function as an entire function. (The above relation and the facts that the eta function is entire and ${\displaystyle \eta (1)\neq 0}$ together show the zeta function is meromorphic with a simple pole at s = 1, and possibly additional poles at the other zeros of the factor ${\displaystyle 1-2^{1-s}}$ , although in fact these hypothetical additional poles do not exist.)

Equivalently, we may begin by defining

which is also defined in the region of positive real part ( ${\displaystyle \Gamma (s)}$ represents the Gamma function). This gives the eta function as a Mellin transform.
Hardy gave a simple proof of the functional equation for the eta function, which is

From this, one immediately has the functional equation of the zeta function also, as well as another means to extend the definition of eta to the entire complex plane.


£#h5#£Zeros£#/h5#£
The zeros of the eta function include all the zeros of the zeta function: the negative even integers (real equidistant simple zeros); the zeros along the critical line, none of which are known to be multiple and over 40% of which have been proven to be simple, and the hypothetical zeros in the critical strip but not on the critical line, which if they do exist must occur at the vertices of rectangles symmetrical around the x-axis and the critical line and whose multiplicity is unknown. In addition, the factor ${\displaystyle 1-2^{1-s}}$ adds an infinite number of complex simple zeros, located at equidistant points on the line ${\displaystyle \Re (s)=1}$ , at ${\displaystyle s_{n}=1+2n\pi i/\ln(2)}$ where n is any nonzero integer.

Under the Riemann hypothesis, the zeros of the eta function would be located symmetrically with respect to the real axis on two parallel lines ${\displaystyle \Re (s)=1/2,\Re (s)=1}$ , and on the perpendicular half line formed by the negative real axis.


£#h5#£Landau's problem with ζ(s) = η(s)/0 and solutions£#/h5#£
In the equation η(s) = (1 − 21−s) ζ(s), "the pole of ζ(s) at s = 1 is cancelled by the zero of the other factor" (Titchmarsh, 1986, p. 17), and as a result η(1) is neither infinite nor zero (see § Particular values). However, in the equation

η must be zero at all the points ${\displaystyle s_{n}=1+n{\frac {2\pi }{\ln {2}}}i,n\neq 0,n\in \mathbb {Z} }$ , where the denominator is zero, if the Riemann zeta function is analytic and finite there. The problem of proving this without defining the zeta function first was signaled and left open by E. Landau in his 1909 treatise on number theory: "Whether the eta series is different from zero or not at the points ${\displaystyle s_{n}\neq 1}$ , i.e., whether these are poles of zeta or not, is not readily apparent here."
A first solution for Landau's problem was published almost 40 years later by D. V. Widder in his book The Laplace Transform. It uses the next prime 3 instead of 2 to define a Dirichlet series similar to the eta function, which we will call the ${\displaystyle \lambda }$ function, defined for ${\displaystyle \Re (s)>0}$ and with some zeros also on ${\displaystyle \Re (s)=1}$ , but not equal to those of eta.

An elementary direct and ${\displaystyle \zeta \,}$ -independent proof of the vanishing of the eta function at ${\displaystyle s_{n}\neq 1}$ was published by J. Sondow in 2003. It expresses the value of the eta function as the limit of special Riemann sums associated to an integral known to be zero, using a relation between the partial sums of the Dirichlet series defining the eta and zeta functions for ${\displaystyle \Re (s)>1}$ .

Assuming ${\displaystyle \eta (s_{n})=0}$ , for each point ${\displaystyle s_{n}\neq 1}$ where ${\displaystyle 2^{s_{n}}=2}$ , we can now define ${\displaystyle \zeta (s_{n})\,}$ by continuity as follows,

The apparent singularity of zeta at ${\displaystyle s_{n}\neq 1}$ is now removed, and the zeta function is proven to be analytic everywhere in ${\displaystyle \Re {s}>0}$ , except at ${\displaystyle s=1}$ where


£#h5#£Integral representations£#/h5#£
A number of integral formulas involving the eta function can be listed. The first one follows from a change of variable of the integral representation of the Gamma function (Abel, 1823), giving a Mellin transform which can be expressed in different ways as a double integral (Sondow, 2005). This is valid for ${\displaystyle \Re s>0.}$

The Cauchy–Schlömilch transformation (Amdeberhan, Moll et al., 2010) can be used to prove this other representation, valid for ${\displaystyle \Re s>-1}$ . Integration by parts of the first integral above in this section yields another derivation.

The next formula, due to Lindelöf (1905), is valid over the whole complex plane, when the principal value is taken for the logarithm implicit in the exponential.

This corresponds to a Jensen (1895) formula for the entire function ${\displaystyle (s-1)\,\zeta (s)}$ , valid over the whole complex plane and also proven by Lindelöf. "This formula, remarquable by its simplicity, can be proven easily with the help of Cauchy's theorem, so important for the summation of series" wrote Jensen (1895). Similarly by converting the integration paths to contour integrals one can obtain other formulas for the eta function, such as this generalisation (Milgram, 2013) valid for ${\displaystyle 0<c<1}$ and all ${\displaystyle s}$ : The zeros on the negative real axis are factored out cleanly by making ${\displaystyle c\to 0^{+}}$ (Milgram, 2013) to obtain a formula valid for ${\displaystyle \Re s<0}$ :
£#h5#£Numerical algorithms£#/h5#£
Most of the series acceleration techniques developed for alternating series can be profitably applied to the evaluation of the eta function. One particularly simple, yet reasonable method is to apply Euler's transformation of alternating series, to obtain

Note that the second, inside summation is a forward difference.


£#h5#£Borwein's method£#/h5#£
Peter Borwein used approximations involving Chebyshev polynomials to produce a method for efficient evaluation of the eta function. If

then where for ${\displaystyle \Re (s)\geq {\frac {1}{2}}}$ the error term γn is bounded by
The factor of ${\displaystyle 3+{\sqrt {8}}\approx 5.8}$ in the error bound indicates that the Borwein series converges quite rapidly as n increases.


£#h5#£Particular values£#/h5#£ £#ul#££#li#£η(0) = 1⁄2, the Abel sum of Grandi's series 1 − 1 + 1 − 1 + · · ·.£#/li#£ £#li#£η(−1) = 1⁄4, the Abel sum of 1 − 2 + 3 − 4 + · · ·.£#/li#£ £#li#£For k an integer > 1, if Bk is the k-th Bernoulli number then £#/li#££#/ul#£
Also:

£#ul#££#li#£ ${\displaystyle \eta (1)=\ln 2}$ , this is the alternating harmonic series£#/li#£ £#li#£ ${\displaystyle \eta (2)={\pi ^{2} \over 12}}$ OEIS: A072691£#/li#£ £#li#£ ${\displaystyle \eta (4)={{7\pi ^{4}} \over 720}\approx 0.94703283}$ £#/li#£ £#li#£ ${\displaystyle \eta (6)={{31\pi ^{6}} \over 30240}\approx 0.98555109}$ £#/li#£ £#li#£ ${\displaystyle \eta (8)={{127\pi ^{8}} \over 1209600}\approx 0.99623300}$ £#/li#£ £#li#£ ${\displaystyle \eta (10)={{73\pi ^{10}} \over 6842880}\approx 0.99903951}$ £#/li#£ £#li#£ ${\displaystyle \eta (12)={{1414477\pi ^{12}} \over {1307674368000}}\approx 0.99975769}$ £#/li#££#/ul#£
The general form for even positive integers is:

Taking the limit ${\displaystyle n\to \infty }$ , one obtains ${\displaystyle \eta (\infty )=1}$ .


£#h5#£Derivatives£#/h5#£
The derivative with respect to the parameter s is for ${\displaystyle s\neq 1}$


£#h5#£References£#/h5#£ £#ul#££#li#£Jensen, J. L. W. V. (1895). "Remarques relatives aux réponses de MM. Franel et Kluyver". L'Intermédiaire des Mathématiciens. II: 346].£#/li#£ £#li#£Lindelöf, Ernst (1905). Le calcul des résidus et ses applications à la théorie des fonctions. Gauthier-Villars. p. 103.£#/li#£ £#li#£Widder, David Vernon (1946). The Laplace Transform. Princeton University Press. p. 230.£#/li#£ £#li#£Landau, Edmund, Handbuch der Lehre von der Verteilung der Primzahlen, Erster Band, Berlin, 1909, p. 160. (Second edition by Chelsea, New York, 1953, p. 160, 933)£#/li#£ £#li#£Titchmarsh, E. C. (1986). The Theory of the Riemann Zeta Function, Second revised (Heath-Brown) edition. Oxford University Press.£#/li#£ £#li#£Conrey, J. B. (1989). "More than two fifths of the zeros of the Riemann zeta function are on the critical line". Journal für die Reine und Angewandte Mathematik. 1989 (399): 1–26. doi:10.1515/crll.1989.399.1. S2CID 115910600.£#/li#£ £#li#£Knopp, Konrad (1990) [1922]. Theory and Application of Infinite Series. Dover. ISBN 0-486-66165-2.£#/li#£ £#li#£Borwein, P., An Efficient Algorithm for the Riemann Zeta Function, Constructive experimental and nonlinear analysis, CMS Conference Proc. 27 (2000), 29–34.£#/li#£ £#li#£Sondow, Jonathan (2005). "Double integrals for Euler's constant and ln 4/π and an analog of Hadjicostas's formula". Amer. Math. Monthly. 112: 61–65. arXiv:math.CO/0211148. Amer. Math. Monthly 112 (2005) 61–65, formula 18.£#/li#£ £#li#£Sondow, Jonathan (2003). "Zeros of the Alternating Zeta Function on the Line R(s)=1". Amer. Math. Monthly. 110: 435–437. arXiv:math/0209393. Amer. Math. Monthly, 110 (2003) 435–437.£#/li#£ £#li#£Gourdon, Xavier; Sebah, Pascal (2003). "Numerical evaluation of the Riemann Zeta-function" (PDF).£#/li#£ £#li#£Amdeberhan, T.; Glasser, M. L.; Jones, M. C; Moll, V. H.; Posey, R.; Varela, D. (2010). "The Cauchy–Schlomilch Transformation". arXiv:1004.2445 [math.CA]. p. 12.£#/li#£ £#li#£Milgram, Michael S. (2012). "Integral and Series Representations of Riemann's Zeta Function, Dirichlet's Eta Function and a Medley of Related Results". Journal of Mathematics. 2013: 1–17. arXiv:1208.3429. doi:10.1155/2013/181724..£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Riemann Zeta Function £#/li#££#/ul#£




£#h3#£Ambient Isotopy£#/h3#£

In the mathematical subject of topology, an ambient isotopy, also called an h-isotopy, is a kind of continuous distortion of an ambient space, for example a manifold, taking a submanifold to another submanifold. For example in knot theory, one considers two knots the same if one can distort one knot into the other without breaking it. Such a distortion is an example of an ambient isotopy. More precisely, let ${\displaystyle N}$ and ${\displaystyle M}$ be manifolds and ${\displaystyle g}$ and ${\displaystyle h}$ be embeddings of ${\displaystyle N}$ in ${\displaystyle M}$ . A continuous map

${\displaystyle F:M\times [0,1]\rightarrow M}$
is defined to be an ambient isotopy taking ${\displaystyle g}$ to ${\displaystyle h}$ if ${\displaystyle F_{0}}$ is the identity map, each map ${\displaystyle F_{t}}$ is a homeomorphism from ${\displaystyle M}$ to itself, and ${\displaystyle F_{1}\circ g=h}$ . This implies that the orientation must be preserved by ambient isotopies. For example, two knots that are mirror images of each other are, in general, not equivalent.


£#h5#£See also£#/h5#£ £#ul#££#li#£Isotopy£#/li#£ £#li#£Regular homotopy£#/li#£ £#li#£Regular isotopy£#/li#££#/ul#£
£#h5#£References£#/h5#£ £#ul#££#li#£M. A. Armstrong, Basic Topology, Springer-Verlag, 1983£#/li#£ £#li#£Sasho Kalajdzievski, An Illustrated Introduction to Topology and Homotopy, CRC Press, 2010, Chapter 10: Isotopy and Homotopy£#/li#££#/ul#£




£#h5#£ References £#/h5#£ £#ul#££#li#£Hirsch, M. W. Differential Topology. New York: Springer-Verlag, 1988.£#/li#££#li#£Hoste, J.; Thistlethwaite, M.; and Weeks, J. "The First 1701936 Knots." Math. Intell. 20, 33-48, Fall 1998.£#/li#££#li#£ Hirsch, M. W. Differential Topology. New York: Springer-Verlag, 1988. £#/li#££#li#£ Hoste, J.; Thistlethwaite, M.; and Weeks, J. "The First Knots." Math. Intell. 20, 33-48, Fall 1998. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Topology > Manifolds £#/li#££#/ul#£




£#h3#£Ambiguity Function£#/h3#£

In pulsed radar and sonar signal processing, an ambiguity function is a two-dimensional function of propagation delay ${\displaystyle \tau }$ and Doppler frequency ${\displaystyle f}$ , ${\displaystyle \chi (\tau ,f)}$ . It represents the distortion of a returned pulse due to the receiver matched filter (commonly, but not exclusively, used in pulse compression radar) of the return from a moving target. The ambiguity function is defined by the properties of the pulse and of the filter, and not any particular target scenario.

Many definitions of the ambiguity function exist; some are restricted to narrowband signals and others are suitable to describe the delay and Doppler relationship of wideband signals. Often the definition of the ambiguity function is given as the magnitude squared of other definitions (Weiss). For a given complex baseband pulse ${\displaystyle s(t)}$ , the narrowband ambiguity function is given by

${\displaystyle \chi (\tau ,f)=\int _{-\infty }^{\infty }s(t)s^{*}(t-\tau )e^{i2\pi ft}\,dt}$
where ${\displaystyle ^{*}}$ denotes the complex conjugate and ${\displaystyle i}$ is the imaginary unit. Note that for zero Doppler shift ( ${\displaystyle f=0}$ ), this reduces to the autocorrelation of ${\displaystyle s(t)}$ . A more concise way of representing the ambiguity function consists of examining the one-dimensional zero-delay and zero-Doppler "cuts"; that is, ${\displaystyle \chi (0,f)}$ and ${\displaystyle \chi (\tau ,0)}$ , respectively. The matched filter output as a function of time (the signal one would observe in a radar system) is a Doppler cut, with the constant frequency given by the target's Doppler shift: ${\displaystyle \chi (\tau ,f_{D})}$ .


£#h5#£Background and motivation£#/h5#£
Pulse-Doppler radar equipment sends out a series of radio frequency pulses. Each pulse has a certain shape (waveform)—how long the pulse is, what its frequency is, whether the frequency changes during the pulse, and so on. If the waves reflect off a single object, the detector will see a signal which, in the simplest case, is a copy of the original pulse but delayed by a certain time ${\displaystyle \tau }$ —related to the object's distance—and shifted by a certain frequency ${\displaystyle f}$ —related to the object's velocity (Doppler shift). If the original emitted pulse waveform is ${\displaystyle s(t)}$ , then the detected signal (neglecting noise, attenuation, and distortion, and wideband corrections) will be:

${\displaystyle s_{\tau ,f}(t)\equiv s(t-\tau )e^{i2\pi ft}.}$
The detected signal will never be exactly equal to any ${\displaystyle s_{\tau ,f}}$ because of noise. Nevertheless, if the detected signal has a high correlation with ${\displaystyle s_{\tau ,f}}$ , for a certain delay and Doppler shift ${\displaystyle (\tau ,f)}$ , then that suggests that there is an object with ${\displaystyle (\tau ,f)}$ . Unfortunately, this procedure may yield false positives, i.e. wrong values ${\displaystyle (\tau ',f')}$ which are nevertheless highly correlated with the detected signal. In this sense, the detected signal may be ambiguous.

The ambiguity occurs specifically when there is a high correlation between ${\displaystyle s_{\tau ,f}}$ and ${\displaystyle s_{\tau ',f'}}$ for ${\displaystyle (\tau ,f)\neq (\tau ',f')}$ . This motivates the ambiguity function ${\displaystyle \chi }$ . The defining property of ${\displaystyle \chi }$ is that the correlation between ${\displaystyle s_{\tau ,f}}$ and ${\displaystyle s_{\tau ',f'}}$ is equal to ${\displaystyle \chi (\tau -\tau ',f-f')}$ .

Different pulse shapes (waveforms) ${\displaystyle s(t)}$ have different ambiguity functions, and the ambiguity function is relevant when choosing what pulse to use.

The function ${\displaystyle \chi }$ is complex-valued; the degree of "ambiguity" is related to its magnitude ${\displaystyle |\chi (\tau ,f)|^{2}}$ .


£#h5#£Relationship to time–frequency distributions£#/h5#£
The ambiguity function plays a key role in the field of time–frequency signal processing, as it is related to the Wigner–Ville distribution by a 2-dimensional Fourier transform. This relationship is fundamental to the formulation of other time–frequency distributions: the bilinear time–frequency distributions are obtained by a 2-dimensional filtering in the ambiguity domain (that is, the ambiguity function of the signal). This class of distribution may be better adapted to the signals considered.

Moreover, the ambiguity distribution can be seen as the short-time Fourier transform of a signal using the signal itself as the window function. This remark has been used to define an ambiguity distribution over the time-scale domain instead of the time-frequency domain.


£#h5#£Wideband ambiguity function£#/h5#£
The wideband ambiguity function of ${\displaystyle s\in L^{2}(R)}$ is:

${\displaystyle WB_{ss}(\tau ,\alpha )={\sqrt {|{\alpha }|}}\int _{-\infty }^{\infty }s(t)s^{*}(\alpha (t-\tau ))\,dt}$
where ${\displaystyle {\alpha }}$ is a time scale factor of the received signal relative to the transmitted signal given by:

${\displaystyle \alpha ={\frac {c+v}{c-v}}}$
for a target moving with constant radial velocity v. The reflection of the signal is represented with compression (or expansion) in time by the factor ${\displaystyle \alpha }$ , which is equivalent to a compression by the factor ${\displaystyle \alpha ^{-1}}$ in the frequency domain (with an amplitude scaling). When the wave speed in the medium is sufficiently faster than the target speed, as is common with radar, this compression in frequency is closely approximated by a shift in frequency Δf = fc*v/c (known as the doppler shift). For a narrow band signal, this approximation results in the narrowband ambiguity function given above, which can be computed efficiently by making use of the FFT algorithm.


£#h5#£Ideal ambiguity function£#/h5#£
An ambiguity function of interest is a 2-dimensional Dirac delta function or "thumbtack" function; that is, a function which is infinite at (0,0) and zero elsewhere.

${\displaystyle \chi (\tau ,f)=\delta (\tau )\delta (f)\,}$
An ambiguity function of this kind would be somewhat of a misnomer; it would have no ambiguities at all, and both the zero-delay and zero-Doppler cuts would be an impulse. This is not usually desirable (if a target has any Doppler shift from an unknown velocity it will disappear from the radar picture), but if Doppler processing is independently performed, knowledge of the precise Doppler frequency allows ranging without interference from any other targets which are not also moving at exactly the same velocity.

This type of ambiguity function is produced by ideal white noise (infinite in duration and infinite in bandwidth). However, this would require infinite power and is not physically realizable. There is no pulse ${\displaystyle s(t)}$ that will produce ${\displaystyle \delta (\tau )\delta (f)}$ from the definition of the ambiguity function. Approximations exist, however, and noise-like signals such as binary phase-shift keyed waveforms using maximal-length sequences are the best known performers in this regard.


£#h5#£Properties£#/h5#£
(1) Maximum value

${\displaystyle |\chi (\tau ,f)|^{2}\leq |\chi (0,0)|^{2}}$
(2) Symmetry about the origin

${\displaystyle \chi (\tau ,f)=\exp[j2\pi \tau f]\chi ^{*}(-\tau ,-f)\,}$
(3) Volume invariance

${\displaystyle \int _{-\infty }^{\infty }\int _{-\infty }^{\infty }|\chi (\tau ,f)|^{2}\,d\tau \,df=|\chi (0,0)|^{2}=E^{2}}$
(4) Modulation by a linear FM signal

${\displaystyle {\text{If }}s(t)\rightarrow |\chi (\tau ,f)|{\text{ then }}s(t)\exp[j\pi kt^{2}]{\rightarrow }|\chi (\tau ,f+k\tau )|\,}$
(5) Frequency energy spectrum

${\displaystyle S(f)S^{*}(f)=\int _{-\infty }^{\infty }\chi (\tau ,0)e^{-j2\pi \tau f}\,d\tau }$
(6) Upper bounds for ${\displaystyle p>2}$ and lower bounds for ${\displaystyle p<2}$ exist for the ${\displaystyle p^{th}}$ power integrals

${\displaystyle \int _{-\infty }^{\infty }\int _{-\infty }^{\infty }|\chi (\tau ,f)|^{p}\,d\tau \,df}$ .
These bounds are sharp and are achieved if and only if ${\displaystyle s(t)}$ is a Gaussian function.


£#h5#£Square pulse£#/h5#£
Consider a simple square pulse of duration ${\displaystyle \tau }$ and amplitude ${\displaystyle A}$ :

${\displaystyle A(u(t)-u(t-\tau ))\,}$
where ${\displaystyle u(t)}$ is the Heaviside step function. The matched filter output is given by the autocorrelation of the pulse, which is a triangular pulse of height ${\displaystyle \tau ^{2}A^{2}}$ and duration ${\displaystyle 2\tau }$ (the zero-Doppler cut). However, if the measured pulse has a frequency offset due to Doppler shift, the matched filter output is distorted into a sinc function. The greater the Doppler shift, the smaller the peak of the resulting sinc, and the more difficult it is to detect the target.

In general, the square pulse is not a desirable waveform from a pulse compression standpoint, because the autocorrelation function is too short in amplitude, making it difficult to detect targets in noise, and too wide in time, making it difficult to discern multiple overlapping targets.


£#h5#£LFM pulse£#/h5#£
A commonly used radar or sonar pulse is the linear frequency modulated (LFM) pulse (or "chirp"). It has the advantage of greater bandwidth while keeping the pulse duration short and envelope constant. A constant envelope LFM pulse has an ambiguity function similar to that of the square pulse, except that it is skewed in the delay-Doppler plane. Slight Doppler mismatches for the LFM pulse do not change the general shape of the pulse and reduce the amplitude very little, but they do appear to shift the pulse in time. Thus, an uncompensated Doppler shift changes the target's apparent range; this phenomenon is called range-Doppler coupling.


£#h5#£Multistatic ambiguity functions£#/h5#£
The ambiguity function can be extended to multistatic radars, which comprise multiple non-colocated transmitters and/or receivers (and can include bistatic radar as a special case).

For these types of radar, the simple linear relationship between time and range that exists in the monostatic case no longer applies, and is instead dependent on the specific geometry – i.e. the relative location of transmitter(s), receiver(s) and target. Therefore, the multistatic ambiguity function is mostly usefully defined as a function of two- or three-dimensional position and velocity vectors for a given multistatic geometry and transmitted waveform.

Just as the monostatic ambiguity function is naturally derived from the matched filter, the multistatic ambiguity function is derived from the corresponding optimal multistatic detector – i.e. that which maximizes the probability of detection given a fixed probability of false alarm through joint processing of the signals at all receivers. The nature of this detection algorithm depends on whether or not the target fluctuations observed by each bistatic pair within the multistatic system are mutually correlated. If so, the optimal detector performs phase coherent summation of received signals which can result in very high target location accuracy. If not, the optimal detector performs incoherent summation of received signals which gives diversity gain. Such systems are sometimes described as MIMO radars due to the information theoretic similarities to MIMO communication systems.


£#h5#£Ambiguity function plane£#/h5#£
An ambiguity function plane can be viewed as a combination of an infinite number of radial lines.

Each radial line can be viewed as the fractional Fourier transform of a stationary random process.


£#h5#£Example£#/h5#£
The Ambiguity function (AF) is the operators that are related to the WDF.

${\displaystyle A_{x}(\tau ,n)=\int _{-\infty }^{\infty }x(t+{\frac {\tau }{2}})x^{*}(t-{\frac {\tau }{2}})e^{-j2\pi tn}dt}$
(1)If ${\displaystyle x(t)=exp[-\alpha \pi {(t-t_{0})^{2}}+j2\pi f_{0}t]}$

${\displaystyle A_{x}(\tau ,n)}$
${\displaystyle =\int _{-\infty }^{\infty }e^{-\alpha \pi (t+\tau /2-t_{0})^{2}+j2\pi f_{0}(t+\tau /2)}+e^{-\alpha \pi (t-\tau /2-t_{0})^{2}-j2\pi f_{0}(t-\tau /2)}e^{-j2\pi tn}dt}$
${\displaystyle =\int _{-\infty }^{\infty }e^{-\alpha \pi [2(t-t_{0})^{2}+\tau ^{2}/2]+j2\pi f_{0}\tau }e^{-j2\pi tn}dt}$
${\displaystyle =\int _{-\infty }^{\infty }e^{-\alpha \pi [2t^{2}-\tau ^{2}/2]+j2\pi f_{0}\tau }e^{-j2\pi tn}e^{-j2\pi t_{0}n}dt}$
${\displaystyle A_{x}(\tau ,n)={\sqrt {\frac {1}{2\alpha }}}exp[-\pi ({\frac {\alpha \tau ^{2}}{2}}+{\frac {n^{2}}{2\alpha }})]exp[j2\pi (f_{0}\tau -t_{0}n)]}$


WDF and AF for the signal with only 1 term

(2) If ${\displaystyle x(t)=exp[-\alpha _{1}\pi (t-t_{1})^{2}+j2\pi f_{1}t]+exp[-\alpha _{2}\pi (t-t_{2})^{2}+j2\pi f_{2}t]}$

${\displaystyle A_{x}(\tau ,n)}$
${\displaystyle =\int _{-\infty }^{\infty }x_{1}(t+\tau /2)x_{1}^{*}(t-\tau /2)e^{-j2\pi tn}dt}$ +
${\displaystyle \int _{-\infty }^{\infty }x_{2}(t+\tau /2)x_{2}^{*}(t-\tau /2)e^{-j2\pi tn}dt}$ +
${\displaystyle \int _{-\infty }^{\infty }x_{1}(t+\tau /2)x_{2}^{*}(t-\tau /2)e^{-j2\pi tn}dt}$ +
${\displaystyle \int _{-\infty }^{\infty }x_{2}(t+\tau /2)x_{1}^{*}(t-\tau /2)e^{-j2\pi tn}dt}$
${\displaystyle A_{x}(\tau ,n)=A_{x1}(\tau ,n)+A_{x2}(\tau ,n)+A_{x1x2}(\tau ,n)+A_{x2x1}(\tau ,n)}$


${\displaystyle A_{x}(\tau ,n)={\sqrt {\frac {1}{2\alpha _{1}}}}exp[-\pi ({\frac {\alpha _{1}\tau ^{2}}{2}}+{\frac {n^{2}}{2\alpha _{1}}})]exp[j2\pi (f_{1}\tau -t_{1}n)]}$
${\displaystyle A_{x}(\tau ,n)={\sqrt {\frac {1}{2\alpha _{2}}}}exp[-\pi ({\frac {\alpha _{2}\tau ^{2}}{2}}+{\frac {n^{2}}{2\alpha _{1}}})]exp[j2\pi (f_{2}\tau -t_{2}n)]}$

When ${\displaystyle \alpha _{1}=\alpha _{2}}$

${\displaystyle A_{x1x2}(\tau ,n)={\sqrt {\frac {1}{2\alpha _{u}}}}exp[-\pi (\alpha _{u}{\frac {(\tau -t_{d})^{2}}{2}}+{\frac {(n-f_{d})^{2}}{2\alpha _{u}}})]exp[j2\pi (f_{u}\tau -t_{u}n+f_{d}t_{u})]}$
where

£#ul#££#li#£ ${\displaystyle t_{u}=(t_{1}+t_{2}/2)}$ ,£#/li#£ £#li#£ ${\displaystyle f_{u}=(f_{1}+f_{2})/2}$ ,£#/li#£ £#li#£ ${\displaystyle \alpha _{u}=(\alpha _{1}+\alpha _{2})/2}$ ,£#/li#£ £#li#£ ${\displaystyle t_{d}=t_{1}+t_{2}}$ ,£#/li#£ £#li#£ ${\displaystyle f_{d}=f_{1}-f_{2}}$ ,£#/li#£ £#li#£ ${\displaystyle \alpha _{d}=\alpha _{1}-\alpha _{2}}$
${\displaystyle A_{x2x1}(\tau ,n)=A_{x1x2}^{*}(-\tau ,-n)}$
£#/li#££#/ul#£
When ${\displaystyle \alpha _{1}}$ ≠ ${\displaystyle \alpha _{2}}$

${\displaystyle A_{x1x2}(\tau ,n)={\sqrt {\frac {1}{2\alpha _{u}}}}exp[-\pi {\frac {[(n-f_{d})+j(\alpha _{1}t_{1}+\alpha _{2}t_{2})-j\alpha _{d}\tau /2]^{2}}{2\alpha _{u}}}exp[-\pi (\alpha _{1}(t_{1}-{\frac {\tau }{2}})^{2})+\alpha _{2}(t_{2}-{\frac {\tau }{2}})^{2})]exp[j2\pi f_{u}\tau ]}$
£#ul#££#li#£
${\displaystyle A_{x2x1}(\tau ,n)=A_{x1x2}^{*}(-\tau ,-n)}$
£#/li#££#/ul#£
WDF and AF for the signal with 2 terms

For the ambiguity function:

£#ul#££#li#£The auto term is always near to the origin£#/li#£ £#li#£The auto term is always near to the origin£#/li#££#/ul#£
£#h5#£See also£#/h5#£ £#ul#££#li#£Matched filter£#/li#£ £#li#£Pulse compression£#/li#£ £#li#£Pulse-Doppler radar£#/li#£ £#li#£Digital signal processing£#/li#£ £#li#£Philip Woodward£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£Further reading£#/h5#£ £#ul#££#li#£Richards, Mark A. Fundamentals of Radar Signal Processing. McGraw–Hill Inc., 2005. ISBN 0-07-144474-2.£#/li#£ £#li#£Ipatov, Valery P. Spread Spectrum and CDMA. Wiley & Sons, 2005. ISBN 0-470-09178-9£#/li#£ £#li#£Chernyak V.S. Fundamentals of Multisite Radar Systems, CRC Press, 1998.£#/li#£ £#li#£Solomon W. Golomb, and Guang Gong. Signal design for good correlation: for wireless communication, cryptography, and radar. Cambridge University Press, 2005.£#/li#£ £#li#£M. Soltanalian. Signal Design for Active Sensing and Communications. Uppsala Dissertations from the Faculty of Science and Technology (printed by Elanders Sverige AB), 2014.£#/li#£ £#li#£Nadav Levanon, and Eli Mozeson. Radar signals. Wiley. com, 2004.£#/li#£ £#li#£Augusto Aubry, Antonio De Maio, Bo Jiang, and Shuzhong Zhang. "Ambiguity function shaping for cognitive radar via complex quartic optimization." IEEE Transactions on Signal Processing 61 (2013): 5603-5619.£#/li#£ £#li#£Mojtaba Soltanalian, and Petre Stoica. "Computational design of sequences with good correlation properties." IEEE Transactions on Signal Processing, 60.5 (2012): 2180-2193.£#/li#£ £#li#£G. Krötzsch, M. A. Gómez-Méndez, Transformada Discreta de Ambigüedad, Revista Mexicana de Física, Vol. 63, pp. 505–515 (2017). "Transformada Discreta de Ambigüedad".£#/li#£ £#li#£2 National Taiwan University, Time-Frequency Analysis and Wavelet Transform 2021, Professor of Jian-Jiun Ding, Department of Electrical Engineering£#/li#£ £#li#£3 National Taiwan University, Time-Frequency Analysis and Wavelet Transform 2021, Professor of Jian-Jiun Ding, Department of Electrical Engineering£#/li#£ £#li#£4 National Taiwan University, Time-Frequency Analysis and Wavelet Transform 2021, Professor of Jian-Jiun Ding, Department of Electrical Engineering£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Integral Transforms > General Integral Transforms £#/li#££#/ul#£




£#h3#£Ambrose-Kakutani Theorem£#/h3#£

In mathematics, ergodic flows occur in geometry, through the geodesic and horocycle flows of closed hyperbolic surfaces. Both of these examples have been understood in terms of the theory of unitary representations of locally compact groups: if Γ is the fundamental group of a closed surface, regarded as a discrete subgroup of the Möbius group G = PSL(2,R), then the geodesic and horocycle flow can be identified with the natural actions of the subgroups A of real positive diagonal matrices and N of lower unitriangular matrices on the unit tangent bundle G / Γ. The Ambrose-Kakutani theorem expresses every ergodic flow as the flow built from an invertible ergodic transformation on a measure space using a ceiling function. In the case of geodesic flow, the ergodic transformation can be understood in terms of symbolic dynamics; and in terms of the ergodic actions of Γ on the boundary S1 = G / AN and G / A = S1 × S1 \ diag S1. Ergodic flows also arise naturally as invariants in the classification of von Neumann algebras: the flow of weights for a factor of type III0 is an ergodic flow on a measure space.


£#h5#£Hedlund's theorem: ergodicity of geodesic and horocycle flows£#/h5#£
The method using representation theory relies on the following two results:

£#ul#££#li#£If G = SL(2,R) acts unitarily on a Hilbert space H and ξ is a unit vector fixed by the subgroup N of upper unitriangular matrices, then ξ is fixed by G.£#/li#£ £#li#£If G = SL(2,R) acts unitarily on a Hilbert space H and ξ is a unit vector fixed by the subgroup A of diagonal matrices of determinant 1, then ξ is fixed by G.£#/li#££#/ul#£
(1) As a topological space, the homogeneous space X = G / N can be identified with R2 \ {0} with the standard action of G as 2 × 2 matrices. The subgroup of N has two kinds of orbits: orbits parallel to the x-axis with y ≠ 0; and points on the x-axis. A continuous function on X that is constant on N-orbits must therefore be constant on the real axis with the origin removed. Thus the matrix coefficient ψ(x) = (xξ,ξ) satisfies ψ(g) = 1 for g in A · N. By unitarity, ||gξ − ξ||2 = 2 − ψ(g) − ψ(g–1) = 0, so that gξ = ξ for all g in B = A · N = N · A. Now let s be the matrix ${\displaystyle {\begin{pmatrix}0&1\\-1&0\end{pmatrix}}}$ . Then, as is easily verified, the double coset BsB is dense in G; this is a special case of the Bruhat decomposition. Since ξ is fixed by B, the matrix coefficient ψ(g) is constant on BsB. By density, ψ(g) = 1 for all g in G. The same argument as above shows that gξ = ξ for all g in G.

(2) Suppose that ξ is fixed by A. For the unitary 1-parameter group N ≅ R, let P[a,b] be the spectral subspace corresponding to the interval [a,b]. Let g(s) be the diagonal matrix with entries s and s−1 for |s| > 1. Then g(s)P[a,b]g(s)−1 = P[s2a, s2a]. As |s| tends to infinity the latter projections tend to 0 in the strong operator topology if 0< a < b or a < b < 0. Since g(s)ξ = ξ, it follows P[a,b]ξ = 0 in either case. By the spectral theorem, it follows that ξ is in the spectral subspace P({0}); in other words ξ is fixed by N. But then, by the first result, ξ must be fixed by G.

The classical theorems of Gustav Hedlund from the early 1930s assert the ergodicity of the geodesic and horocycle flows corresponding to compact Riemann surfaces of constant negative curvature. Hedlund's theorem can be re-interpreted in terms of unitary representations of G and its subgroups. Let Γ be a cocompact subgroup of PSL(2,R) = G / {±I} for which all non-scalar elements are hyperbolic. Let X = Γ \ G / K where K is the subgroup of rotations ${\displaystyle {\begin{pmatrix}\cos \theta &\sin \theta \\-\sin \theta &\cos \theta \end{pmatrix}}}$ . The unit tangent bundle is SX = Γ \ G, with the geodesic flow given by the right action of A and the horocycle flow by the right action of N. This action if ergodic if L∞(Γ \ G)A = C, i.e. the functions fixed by A are just the constant functions. Since Γ \ G is compact, this will be the case if L2(Γ \ G)A = C. Let H = L2(Γ \ G). Thus G acts unitarily on H on the right. Any non-zero ξ in H fixed by A must be fixed by G, by the second result above. But in this case, if f is a continuous function on G of compact support with ∫ f = 1, then ξ = ∫ f(g) gξ dg. The right hand side equals ξ ∗ f, a continuous function on G. Since ξ is right-invariant under G, it follows that ξ is constant, as required. Hence the geodesic flow is ergodic. Replacing A by N and using the first result above, the same argument shows that the horocycle flow is ergodic.


£#h5#£Ambrose−Kakutani–Krengel–Kubo theorem£#/h5#£
£#h5#£Induced flows£#/h5#£
Examples of flows induced from non-singular invertible transformations of measure spaces were defined by von Neumann (1932) in his operator-theoretic approach to classical mechanics and ergodic theory. Let T be a non-singular invertible transformation of (X,μ) giving rise to an automorphism τ of A = L∞(X). This gives rise to an invertible transformation T ⊗ id of the measure space (X × R,μ × m), where m is Lebesgue measure, and hence an automorphism τ ⊗ id of A ⊗ L∞(R). Translation Lt defines a flow on R preserving m and hence a flow λt on L∞(R). Let S = L1 with corresponding automorphism σ of L∞(R). Thus τ ⊗ σ gives an automorphism of A ⊗ L∞(R) which commutes with the flow id ⊗ λt. The induced measure space Y is defined by B = L∞(Y) = L∞(X × R)τ ⊗ σ, the functions fixed by the automorphism τ ⊗ σ. It admits the induced flow given by the restriction of id ⊗ λt to B. Since λt acts ergodically on L∞(R), it follows that the functions fixed by the flow can be identified with L∞(X)τ. In particular if the original transformation is ergodic, the flow that it induces is also ergodic.


£#h5#£Flows built under a ceiling function£#/h5#£
The induced action can also be described in terms of unitary operators and it is this approach which clarifies the generalisation to special flows, i.e. flows built under ceiling functions. Let R be the Fourier transform on L2(R,m), a unitary operator such that Rλ(t)R∗ = Vt where λ(t) is translation by t and Vt is multiplication by eitx. Thus Vt lies in L∞(R). In particular V1 = R S R∗. A ceiling function h is a function in A with h ≥ ε1 with ε > 0. Then eihx gives a unitary representation of R in A, continuous in the strong operator topology and hence a unitary element W of A ⊗ L∞(R), acting on L2(X,μ) ⊗ L2(R). In particular W commutes with I ⊗Vt. So W1 = (I ⊗ R∗) W (I ⊗ R) commutes with I ⊗ λ(t). The action T on L∞(X) induces a unitary U on L2(X) using the square root of the Radon−Nikodym derivative of μ ∘ T with respect to μ. The induced algebra B is defined as the subalgebra of A ⊗ L∞(R) commuting with T ⊗ S. The induced flow σt is given by σt (b) = (I ⊗ λ(t)) b (I ⊗ λ(−t)).

The special flow corresponding to the ceiling function h with base transformation T is defined on the algebra B(H) given by the elements in A ⊗ L∞(R) commuting with (T ⊗ I) W1. The induced flow corresponds to the ceiling function h ≡ 1, the constant function. Again W1, and hence (T ⊗ I) W1, commutes with I ⊗ λ(t). The special flow on B(H) is again given by σt (b) = (I ⊗ λ(t)) b (I ⊗ λ(−t)). The same reasoning as for induced actions shows that the functions fixed by the flow correspond to the functions in A fixed by σ, so that the special flow is ergodic if the original non-singular transformation T is ergodic.


£#h5#£Relation to Hopf decomposition£#/h5#£
If St is an ergodic flow on the measure space (X,μ) corresponding to a 1-parameter group of automorphisms σt of A = L∞(X,μ), then by the Hopf decomposition either every St with t ≠ 0 is dissipative or every St with t ≠ 0 is conservative. In the dissipative case, the ergodic flow must be transitive, so that A can be identified with L∞(R) under Lebesgue measure and R acting by translation.

To prove the result on the dissipative case, note that A = L∞(X,μ) is a maximal Abelian von Neumann algebra acting on the Hilbert space L2(X,μ). The probability measure μ can be replaced by an equivalent invariant measure λ and there is a projection p in A such that σt(p) < p for t > 0 and λ(p – σt(p)) = t. In this case σt(p) =E([t,∞)) where E is a projection-valued measure on R. These projections generate a von Neumann subalgebra B of A. By ergodicity σt(p) ${\displaystyle \uparrow }$ 1 as t tends to −∞. The Hilbert space L2(X,λ) can be identified with the completion of the subspace of f in A with λ(|f|2) < ∞. The subspace corresponding to B can be identified with L2(R) and B with L∞(R). Since λ is invariant under St, it is implemented by a unitary representation Ut. By the Stone–von Neumann theorem for the covariant system B, Ut, the Hilbert space H = L2(X,λ) admits a decomposition L2(R) ⊗ ${\displaystyle \ell ^{2}}$ where B and Ut act only on the first tensor factor. If there is an element a of A not in B, then it lies in the commutant of B ⊗ C, i.e. in B ⊗ B( ${\displaystyle \ell ^{2}}$ ). If can thus be realised as a matrix with entries in B. Multiplying by χ[r,s] in B, the entries of a can be taken to be in L∞(R) ∩ L1(R). For such functions f, as an elementary case of the ergodic theorem the average of σt(f) over [−R,R] tends in the weak operator topology to ∫ f(t) dt. Hence for appropriate χ[r,s] this will produce an element in A which lies in C ⊗ B( ${\displaystyle \ell ^{2}}$ ) and is not a multiple of 1 ⊗ I. But such an element commutes with Ut so is fixed by σt, contradicting ergodicity. Hence A = B = L∞(R).

When all the σt with t ≠ 0 are conservative, the flow is said to be properly ergodic. In this case it follows that for every non-zero p in A and t ≠ 0, p ≤ σt (p) ∨ σ2t (p) ∨ σ3t (p) ∨ ⋅⋅⋅ In particular ∨±t>0 σt (p) = 1 for p ≠ 0.


£#h5#£Theorem of Ambrose–Kakutani–Krengel–Kubo£#/h5#£
The theorem states that every ergodic flow is isomorphic to a special flow corresponding to a ceiling function with ergodic base transformation. If the flow leaves a probability measure invariant, the same is true of the base transformation.

For simplicity only the original result of Ambrose (1941) is considered, the case of an ergodic flow preserving a probability measure μ. Let A = L∞(X,μ) and let σt be the ergodic flow. Since the flow is conservative, for any projection p ≠ 0, 1 in A there is a T > 0 without σT(p) ≤ p, so that (1 − p) ∧ σT(p) ≠ 0. On the other hand, as r > 0 decreases to zero

${\displaystyle a_{r}={1 \over r}\int _{0}^{r}\sigma _{t}(p)\,dt\rightarrow p}$
in the strong operator topology or equivalently the weak operator topology (these topologies coincide on unitaries, hence involutions, hence projections). Indeed, it suffices to show that if ν is any finite measure on A, then ν(ar) tends to ν(p). This follows because f(t) = ν(σt(p)) is a continuous function of t so that the average of f over [0,r] tends to f(0) as r tends to 0.

Note that 0 ≤ ar ≤ 1. Now for fixed r > 0, following Ambrose (1941), set

${\displaystyle q_{0}(r)=\chi _{[0,1/4]}(a_{r}),\,\,\,q_{1}(r)=\chi _{[3/4,1]}(a_{r}).}$
Set r = N–1 for N large and fN = ar. Thus 0 ≤ fN ≤ 1 in L∞(X,μ) and fN tends to a characteristic function p in L1(X,μ). But then, if ε = 1/4, it follows that χ[0,ε](fN) tends to χ[0,ε](p) = 1 – p in L1(X). Using the splitting A = pA ⊕ (1 − p)A, this reduces to proving that if 0 ≤ hN ≤ 1 in L∞(Y,ν) and hN tends to 0 in L1(Y,ν), then χ[1−ε,1](hN) tends to 0 in L1(Y,ν). But this follows easily by Chebyshev's inequality: indeed (1−ε) χ[1−ε,1](hN) ≤ hN, so that ν(χ[1−ε,1](hN)) ≤ (1−ε)−1 ν(hN), which tends to 0 by assumption.

Thus by definition q0(r) ∧ q1(r) = 0. Moreover, for r = N−1 sufficiently small, q0(r) ∧ σT(q1(r)) > 0. The above reasoning shows that q0(r) and q1(r) tend to 1 − p and p as r = N−1 tends to 0. This implies that q0(r)σT(q1(r)) tends to (1 − p)σT(p) ≠ 0, so is non-zero for N sufficiently large. Fixing one such N and, with r = N−1, setting q0= q0(r) and q1= q1(r), it can therefore be assumed that

${\displaystyle q_{0}\wedge q_{1}=0,\,\,\,\,q_{0}\wedge \sigma _{T}(q_{1})>0.}$
The definition of q0 and q1 also implies that if δ < r/4 = (4N)−1, then

${\displaystyle \sigma _{t}(q_{0})\wedge q_{1}=0\,\,\,\mathrm {for} \,\,\,|t|\leq \delta .}$
In fact if s < t

${\displaystyle \|\sigma _{t}(a_{r})-\sigma _{s}(a_{r})\|_{\infty }=r^{-1}\left\|\int _{r+s}^{r+t}\sigma _{x}(p)\,dx-\int _{s}^{t}\sigma _{x}(p)\,dx\right\|_{\infty }\leq {2|t-s| \over r}.}$
Take s = 0, so that t > 0 and suppose that e = σt(q0) ∧ q1 > 0. So e = σt(f) with f ≤ q0. Then σt(ar)e = σt(arf) ≤ 1/4 e and are ≥ 3/4 e, so that

${\displaystyle a_{r}-\sigma _{t}(a_{r})\geq a_{r}e-\sigma _{t}(a_{r})e\geq {3 \over 4}e-{1 \over 4}e={1 \over 2}e.}$
Hence ||ar − σt(ar)||∞ ≥ 1/2. On the other hand ||ar − σt(ar)||∞ is bounded above by 2t/r, so that t ≥ r/4. Hence σt(q0) ∧ q1 = 0 if |t| ≤ δ.

The elements ar depend continuously in operator norm on r on (0,1]; from the above σt(ar) is norm continuous in t. Let B0 the closure in the operator norm of the unital *-algebra generated by the σt(ar)'s. It is commutative and separable so, by the Gelfand–Naimark theorem, can be identified with C(Z) where Z is its spectrum, a compact metric space. By definition B0 is a subalgebra of A and its closure B in the weak or strong operator topology can be identified with L∞(Z,μ) where μ is also used for the restriction of μ to B. The subalgebra B is invariant under the flow σt, which is therefore ergodic. The analysis of this action on B0 and B yields all the tools necessary for constructing the ergodic transformation T and ceiling function h. This will first be carried out for B (so that A will temporarily be assumed to coincide with B) and then later extended to A.

The projections q0 and q1 correspond to characteristic functions of open sets. X0 and X1 The assumption of proper ergodicity implies that the union of either of these open sets under translates by σt as t runs over the positive or negative reals is conull (i.e. the complement has measure zero). Replacing X by their intersection, an open set, it can be assumed that these unions exhaust the whole space (which will now be locally compact instead of compact). Since the flow is recurrent any orbit of σt passes through both sets infinitely many times as t tends to either +∞ or −∞. Between a spell first in X0 and then in X1 f must assume the value 1/2 and then 3/4. The last time f equals 1/2 to the first time it equals 3/4 must involve a change in t of at least δ/4 by the Lipschitz continuity condition. Hence each orbit must intersect the set Ω of x for which f(x) = 1/2, f(σt(x)) > 1/2 for 0 < t ≤ δ/4 infinitely often. The definition implies that different insections with an orbit are separated by a distance of at least δ/4, so Ω intersects each orbit only countably many times and the intersections occur at indefinitely large negative and positive times. Thus each orbit is broken up into countably many half-open intervals [rn(x),rn+1(x)) of length at least δ/4 with rn(x) tending to ±∞ as n tends to ±∞. This partitioning can be normalised so that r0(x) ≤ 0 and r1(x) > 0. In particular if x lies in Ω, then t0 = 0. The function rn(x) is called the nth return time to Ω.

The cross-section Ω is a Borel set because on each compact set {σt(x)} with t in [N−1,δ/4] with N > 4/δ, the function g(t) = f(σt(x)) has an infimum greater than 1/2 + M−1 for a sufficiently large integer M. Hence Ω can be written as a countable intersection of sets, each of which is a countable unions of closed sets; so Ω is therefore a Borel set. This implies in particular that the functions rn are Borel functions on X. Given y in Ω, the invertible Borel transformation T is defined on Ω by S(y) = σt(y) where t = r1(y), the first return time to Ω. The functions rn(y) restrict to Borel functions on Ω and satisfy the cocycle relation:

${\displaystyle r_{m+n}=r_{m}+\tau ^{m}(r_{n}),}$
where τ is the automorphism induced by T. The hitting number Nt(x) for the flow St on X is defined as the integer N such that t lies in [rN(x),rN+1(x)). It is an integer-valued Borel function on R × X satisfying the cocycle identity

${\displaystyle N_{s+t}=N_{s}+\sigma _{s}(N_{t}).}$
The function h = r1 is a strictly positive Borel function on Ω so formally the flow can be reconstructed from the transformation T using h a ceiling function. The missing T-invariant measure class on Ω will be recovered using the second cocycle Nt. Indeed, the discrete measure on Z defines a measure class on the product Z × X and the flow St on the second factor extends to a flow on the product given by

${\displaystyle \rho _{t}(m,x)=(m+N_{t}(x),S_{t}(x)).}$
Likewise the base transformation T induces a transformation R on R × Ω defined by

${\displaystyle R(s,y)=(s-h(y),T(y)).}$
These transformations are related by an invertible Borel isomorphism Φ from R × Ω onto Z × X defined by

${\displaystyle \Phi (t,y)=(N_{t}(y),S_{t}(y)).}$
Its inverse Ψ from Z × X onto R × Ω is defined by

${\displaystyle \Psi (m,x)=(-r_{-m}(y),S_{r_{-m}(y)}y).}$
Under these maps the flow Rt is carried onto translation by t on the first factor of R × Ω and, in the other direction, the invertible R is carried onto translation by -1 on Z × X. It suffices to check that the measure class on Z × X carries over onto the same measure class as some produce measure m × ν on R × Ω, where m is Lebesgue measure and ν is a probability measure on Ω with measure class invariant under T. The measure class on Z × X is invariant under R, so defines a measure class on R × Ω, invariant under translation on the first factor. On the other hand, the only measure class on R invariant under translation is Lebesgue measure, so the measure class on R × Ω is equivalent to that of m × ν for some probability measure on Ω. By construction ν is quasi-invariant under T. Unravelling this construction, it follows that the original flow is isomorphic to the flow built under the ceiling function h for the base transformation T on (Ω,ν).

The above reasoning was made with the assumption that B = A. In general A is replaced by a norm closed separable unital *-subalgebra A0 containing B0, invariant under σt and such that σt(f) is a norm continuous function of t for any f in A0. To construct A0, first take a generating set for the von Neumann algebra A formed of countably many projections invariant under σt with t rational. Replace each of this countable set of projections by averages over intervals [0,N−1] with respect to σt. The norm closed unital *-algebra that these generate yields A0. By definition it contains B0 = C(Y). By the Gelfand-Naimark theorem A0 has the form C(X). The construction with ar above applies equally well here: indeed since B0 is a subalgebra of A0, Y is a continuous quotient of X, so a function such as ar is equally well a function on X. The construction therefore carries over mutatis mutandis to A, through the quotient map.

In summary there exists a measure space (Y,λ) and an ergodic action of Z × R on M = L∞(Y,λ) given by commuting actions τn and σt such that there is a τ-invariant subalgebra of M isomorphic to ${\displaystyle \ell ^{\infty }}$ (Z) and a σ-invariant subalgebra of M isomorphic to L∞(R). The original ergodic flow is given by the restriction of σ to Mτ and the corresponding base transformation given by the restriction of τ to Mσ.

Given a flow, it is possible to describe how two different single base transformations that can be used to construct the flow are related. be transformed back into an action of Z on Y, i.e. into an invertible transformation TY on Y. Set-theoretically TY (x) is defined to be Tm(x) where m ≥ 1 is the smallest integer such that Tm(x) lies in X. It is straightforward to see that applying the same process to the inverse of T yields the inverse of TY. The construction can be described measure theoretically as follows. Let e = χY in B = L∞(X,ν) with ν(e) ≠ 0. Then e is an orthogonal sum of projections en defined as follows:

${\displaystyle e_{1}=e\tau ^{-1}(e),\,\,e_{2}=e(1-\tau ^{-1}(e))\tau ^{-2}(e),\,\,e_{3}=e(1-\tau ^{-1}(e))(1-\tau ^{-2}(e))\tau ^{-3}(e),...}$
Then if f lies in en B, the corresponding automorphism is τe(f) = τn(f).

With these definitions two ergodic transformations τ1, τ2 of B1 and B2 arise from the same flow provided there are non-zero projections e1 and e2 in B1 and B2 such that the systems (τ1)e1, e1B1 and (τ2)e2, e2B2 are isomorphic.


£#h5#£See also£#/h5#£ £#ul#££#li#£Anosov flow£#/li#£ £#li#£Axiom A£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£von Neumann, John (1932), "Zur Operatorenmethode In Der Klassischen Mechanik", Annals of Mathematics (in German), 33 (3): 587–642, doi:10.2307/1968537, JSTOR 1968537£#/li#£ £#li#£Morse, Marston (1966), Lectures on Symbolic Dynamics, 1937–1938, Mimeographed Notes by Rufus Oldenburger, Institute for Advanced Study£#/li#£ £#li#£Hopf, Eberhard (1939), "Statistik der geodätischen Linien in Mannigfaltigkeiten negativer Krümmung", Leipzig Ber. Verhandl. Sächs. Akad. Wiss., 91: 261–304£#/li#£ £#li#£Ambrose, Warren (1941), "Representation of ergodic flows", Ann. of Math., 42: 723–739, JSTOR 1969259£#/li#£ £#li#£Ambrose, Warren; Kakutani, Shizuo (1942), "Structure and continuity of measurable flows", Duke Math. J., 9: 25–42, doi:10.1215/s0012-7094-42-00904-9£#/li#£ £#li#£Rohlin, V. A. (1966), "Selected topics from the metric theory of dynamical systems", Ten papers on functional analysis and measure theory, American Mathematical Society Translations. Series 2, vol. 49, American Mathematical Society, pp. 171–240£#/li#£ £#li#£Fomin, Sergei V.; Gelfand, I. M. (1952), "Geodesic flows on manifolds of constant negative curvature", Uspekhi Mat. Nauk, 7 (1): 118–137£#/li#£ £#li#£Mautner, F. I. (1957), "Geodesic flows on symmetric Riemann spaces", Ann. Math., 65 (3): 416–431, doi:10.2307/1970054, JSTOR 1970054£#/li#£ £#li#£Riesz, Frigyes; Sz.-Nagy, Béla (1955), Functional analysis, translated by Leo F. Boron, Frederick Ungar£#/li#£ £#li#£Moore, C. C. (1966), "Ergodicity of flows on homogeneous spaces", Amer. J. Math., 88 (1): 154–178, doi:10.2307/2373052, JSTOR 2373052£#/li#£ £#li#£Mackey, George W. (1966), "Ergodic theory and virtual groups", Math. Ann., 166: 187–207, doi:10.1007/BF01361167£#/li#£ £#li#£Mackey, George W. (1978), "Ergodic theory", Unitary group representations in physics, probability, and number theory, Mathematics Lecture Note Series, vol. 55, Benjamin/Cummings Publishing Co, pp. 133–142, ISBN 0805367020£#/li#£ £#li#£Mackey, George W. (1990), "Von Neumann and the Early Days of Ergodic Theory", in Glimm, J.; Impagliazzo, J.; Singer, I. (eds.), The Legacy of John von Neumann, Proceedings of Symposia in Pure Mathematics, vol. 50, American Mathematical Society, pp. 34~47, ISBN 9780821814871£#/li#£ £#li#£Krengel, Ulrich (1968), "Darstellungssätze für Strömungen und Halbströmungen I", Math. Annalen (in German), 176 (3): 181–190, doi:10.1007/bf02052824, S2CID 124603266£#/li#£ £#li#£Kubo, Izumi (1969), "Quasi-flows", Nagoya Math. J., 35: 1–30, doi:10.1017/s002776300001299x£#/li#£ £#li#£Howe, Roger E.; Moore, Calvin C. (1979), "Asymptotic properties of unitary representations", J. Funct. Anal., 32: 72–96, doi:10.1016/0022-1236(79)90078-8£#/li#£ £#li#£Cornfeld, I. P.; Fomin, S. V.; Sinaĭ, Ya. G. (1982), Ergodic theory, Grundlehren der Mathematischen Wissenschaften, vol. 245, translated by A. B. Sosinskiĭ, Springer-Verlag, ISBN 0-387-90580-4£#/li#£ £#li#£Zimmer, Robert J. (1984), Ergodic theory and semisimple groups, Monographs in Mathematics, vol. 81, Birkhäuser, ISBN 3-7643-3184-4£#/li#£ £#li#£Bedford, Tim; Keane, Michael; Series, Caroline, eds. (1991), Ergodic theory, symbolic dynamics and hyperbolic spaces, Oxford University Press, ISBN 019853390X£#/li#£ £#li#£Adams, Scot (2008), "Decay to zero of matrix coefficients at adjoint infinity", Group representations, ergodic theory, and mathematical physics: a tribute to George W. Mackey, Contemp. Math., vol. 449, Amer. Math. Soc., pp. 43–50£#/li#£ £#li#£Moore, C. C. (2008), "Virtual groups 45 years later", Group representations, ergodic theory, and mathematical physics: a tribute to George W. Mackey, Contemp. Math., vol. 449, Amer. Math. Soc., pp. 267~300£#/li#£ £#li#£Pedersen, Gert K. (1979), C∗-algebras and their automorphism groups, London Mathematical Society Monographs, vol. 14, Academic Press, ISBN 0-12-549450-5£#/li#£ £#li#£Varadarajan, V. S. (1985), Geometry of quantum theory (Second ed.), Springer-Verlag, ISBN 0-387-96124-0£#/li#£ £#li#£Takesaki, M. (2003), Theory of operator algebras, II, Encyclopaedia of Mathematical Sciences, vol. 125, Springer-Verlag, ISBN 3-540-42914-X£#/li#£ £#li#£Takesaki, M. (2003a), Theory of operator algebras, III, Encyclopaedia of Mathematical Sciences, vol. 127, Springer-Verlag, ISBN 3-540-42913-1£#/li#£ £#li#£Morris, Dave Witte (2005), Ratner's theorems on unipotent flows, Chicago Lectures in Mathematics, University of Chicago Press, arXiv:math/0310402, Bibcode:2003math.....10402W, ISBN 0-226-53983-0£#/li#£ £#li#£Nadkarni, M. G. (2013), Basic ergodic theory, Texts and Readings in Mathematics, vol. 6 (Third ed.), Hindustan Book Agency, ISBN 978-93-80250-43-4£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Dynamical Systems £#/li#££#/ul#£




£#h3#£Amenable£#/h3#£

Amenable may refer to:

£#ul#££#li#£Amenable group£#/li#£ £#li#£Amenable species£#/li#£ £#li#£Amenable number£#/li#£ £#li#£Amenable set£#/li#££#/ul#£
£#h5#£See also£#/h5#£ £#ul#££#li#£Agreeableness£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Helemskii, A. Ya. The Homology of Banach and Topological Algebras. Dordrecht, Netherlands: Kluwer, 1989.£#/li#££#li#£Helemskii, A. Ya. "The Homology in Algebra of Analysis." In Handbook of Algebra, Vol. 2. Amsterdam, Netherlands: Elsevier, 1997.£#/li#££#li#£Johnson, B. E. Cohomology in Banach Algebras. Providence, RI: Amer. Math. Soc., 1972.£#/li#££#li#£ Helemskii, A. Ya. The Homology of Banach and Topological Algebras. Dordrecht, Netherlands: Kluwer, 1989. £#/li#££#li#£ Helemskii, A. Ya. "The Homology in Algebra of Analysis." In Handbook of Algebra, Vol. 2. Amsterdam, Netherlands: Elsevier, 1997. £#/li#££#li#£ Johnson, B. E. Cohomology in Banach Algebras. Providence, RI: Amer. Math. Soc., 1972. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Functional Analysis £#/li#££#/ul#£




£#h3#£Analysis£#/h3#£

Analysis is the process of breaking a complex topic or substance into smaller parts in order to gain a better understanding of it. The technique has been applied in the study of mathematics and logic since before Aristotle (384–322 B.C.), though analysis as a formal concept is a relatively recent development.

The word comes from the Ancient Greek ἀνάλυσις (analysis, "a breaking-up" or "an untying;" from ana- "up, throughout" and lysis "a loosening"). From it also comes the word's plural, analyses.

As a formal concept, the method has variously been ascribed to Alhazen, René Descartes (Discourse on the Method), and Galileo Galilei. It has also been ascribed to Isaac Newton, in the form of a practical method of physical discovery (which he did not name).

The converse of analysis is synthesis: putting the pieces back together again in new or different whole.


£#h5#£Applications£#/h5#£
£#h5#£Science£#/h5#£
The field of chemistry uses analysis in three ways: to identify the components of a particular chemical compound (qualitative analysis), to identify the proportions of components in a mixture (quantitative analysis), and to break down chemical processes and examine chemical reactions between elements of matter. For an example of its use, analysis of the concentration of elements is important in managing a nuclear reactor, so nuclear scientists will analyze neutron activation to develop discrete measurements within vast samples. A matrix can have a considerable effect on the way a chemical analysis is conducted and the quality of its results. Analysis can be done manually or with a device.

Types of Analysis:

A) Qualitative Analysis: It is concerned with which components are in a given sample or compound.

Example: Precipitation reaction

B) Quantitative Analysis: It is to determine the quantity of individual component present in a given sample or compound.

Example: To find concentration by uv-spectrophotometer.


£#h5#£Isotopes£#/h5#£
Chemists can use isotope analysis to assist analysts with issues in anthropology, archeology, food chemistry, forensics, geology, and a host of other questions of physical science. Analysts can discern the origins of natural and man-made isotopes in the study of environmental radioactivity.


£#h5#£Business£#/h5#£ £#ul#££#li#£Financial statement analysis – the analysis of the accounts and the economic prospects of a firm£#/li#£ £#li#£Fundamental analysis – a stock valuation method that uses financial analysis£#/li#£ £#li#£Gap analysis – involves the comparison of actual performance with potential or desired performance of an organization£#/li#£ £#li#£Business analysis – involves identifying the needs and determining the solutions to business problems£#/li#£ £#li#£Price analysis – involves the breakdown of a price to a unit figure£#/li#£ £#li#£Market analysis – consists of suppliers and customers, and price is determined by the interaction of supply and demand£#/li#£ £#li#£Technical analysis – the study of price action in securities markets in order to forecast future prices£#/li#£ £#li#£Opportunity analysis – consists of customers trends within the industry, customer demand and experience determine purchasing behavior£#/li#££#/ul#£
£#h5#£Computer science£#/h5#£ £#ul#££#li#£Requirements analysis – encompasses those tasks that go into determining the needs or conditions to meet for a new or altered product, taking account of the possibly conflicting requirements of the various stakeholders, such as beneficiaries or users.£#/li#£ £#li#£Competitive analysis (online algorithm) – shows how online algorithms perform and demonstrates the power of randomization in algorithms£#/li#£ £#li#£Lexical analysis – the process of processing an input sequence of characters and producing as output a sequence of symbols£#/li#£ £#li#£Object-oriented analysis and design – à la Booch£#/li#£ £#li#£Program analysis (computer science) – the process of automatically analysing the behavior of computer programs£#/li#£ £#li#£Semantic analysis (computer science) – a pass by a compiler that adds semantical information to the parse tree and performs certain checks£#/li#£ £#li#£Static code analysis – the analysis of computer software that is performed without actually executing programs built from that£#/li#£ £#li#£Structured systems analysis and design methodology – à la Yourdon£#/li#£ £#li#£Syntax analysis – a process in compilers that recognizes the structure of programming languages, also known as parsing£#/li#£ £#li#£Worst-case execution time – determines the longest time that a piece of software can take to run£#/li#££#/ul#£
£#h5#£Economics£#/h5#£ £#ul#££#li#£Agroecosystem analysis£#/li#£ £#li#£Input–output model if applied to a region, is called Regional Impact Multiplier System£#/li#££#/ul#£
£#h5#£Engineering£#/h5#£
Analysts in the field of engineering look at requirements, structures, mechanisms, systems and dimensions. Electrical engineers analyse systems in electronics. Life cycles and system failures are broken down and studied by engineers. It is also looking at different factors incorporated within the design.


£#h5#£Intelligence£#/h5#£
The field of intelligence employs analysts to break down and understand a wide array of questions. Intelligence agencies may use heuristics, inductive and deductive reasoning, social network analysis, dynamic network analysis, link analysis, and brainstorming to sort through problems they face. Military intelligence may explore issues through the use of game theory, Red Teaming, and wargaming. Signals intelligence applies cryptanalysis and frequency analysis to break codes and ciphers. Business intelligence applies theories of competitive intelligence analysis and competitor analysis to resolve questions in the marketplace. Law enforcement intelligence applies a number of theories in crime analysis.


£#h5#£Linguistics£#/h5#£
Linguistics explores individual languages and language in general. It breaks language down and analyses its component parts: theory, sounds and their meaning, utterance usage, word origins, the history of words, the meaning of words and word combinations, sentence construction, basic construction beyond the sentence level, stylistics, and conversation. It examines the above using statistics and modeling, and semantics. It analyses language in context of anthropology, biology, evolution, geography, history, neurology, psychology, and sociology. It also takes the applied approach, looking at individual language development and clinical issues.


£#h5#£Literature£#/h5#£
Literary criticism is the analysis of literature. The focus can be as diverse as the analysis of Homer or Freud. While not all literary-critical methods are primarily analytical in nature, the main approach to the teaching of literature in the west since the mid-twentieth century, literary formal analysis or close reading, is. This method, rooted in the academic movement labelled The New Criticism, approaches texts – chiefly short poems such as sonnets, which by virtue of their small size and significant complexity lend themselves well to this type of analysis – as units of discourse that can be understood in themselves, without reference to biographical or historical frameworks. This method of analysis breaks up the text linguistically in a study of prosody (the formal analysis of meter) and phonic effects such as alliteration and rhyme, and cognitively in examination of the interplay of syntactic structures, figurative language, and other elements of the poem that work to produce its larger effects.


£#h5#£Mathematics£#/h5#£
Modern mathematical analysis is the study of infinite processes. It is the branch of mathematics that includes calculus. It can be applied in the study of classical concepts of mathematics, such as real numbers, complex variables, trigonometric functions, and algorithms, or of non-classical concepts like constructivism, harmonics, infinity, and vectors.

Florian Cajori explains in A History of Mathematics (1893) the difference between modern and ancient mathematical analysis, as distinct from logical analysis, as follows:

The terms synthesis and analysis are used in mathematics in a more special sense than in logic. In ancient mathematics they had a different meaning from what they now have. The oldest definition of mathematical analysis as opposed to synthesis is that given in [appended to] Euclid, XIII. 5, which in all probability was framed by Eudoxus: "Analysis is the obtaining of the thing sought by assuming it and so reasoning up to an admitted truth; synthesis is the obtaining of the thing sought by reasoning up to the inference and proof of it."

The analytic method is not conclusive, unless all operations involved in it are known to be reversible. To remove all doubt, the Greeks, as a rule, added to the analytic process a synthetic one, consisting of a reversion of all operations occurring in the analysis. Thus the aim of analysis was to aid in the discovery of synthetic proofs or solutions.

James Gow uses a similar argument as Cajori, with the following clarification, in his A Short History of Greek Mathematics (1884):

The synthetic proof proceeds by shewing that the proposed new truth involves certain admitted truths. An analytic proof begins by an assumption, upon which a synthetic reasoning is founded. The Greeks distinguished theoretic from problematic analysis. A theoretic analysis is of the following kind. To prove that A is B, assume first that A is B. If so, then, since B is C and C is D and D is E, therefore A is E. If this be known a falsity, A is not B. But if this be a known truth and all the intermediate propositions be convertible, then the reverse process, A is E, E is D, D is C, C is B, therefore A is B, constitutes a synthetic proof of the original theorem. Problematic analysis is applied in all cases where it is proposed to construct a figure which is assumed to satisfy a given condition. The problem is then converted into some theorem which is involved in the condition and which is proved synthetically, and the steps of this synthetic proof taken backwards are a synthetic solution of the problem.


£#h5#£Music£#/h5#£ £#ul#££#li#£Musical analysis – a process attempting to answer the question "How does this music work?" £#ul#££#li#£Musical Analysis is a study of how the composers use the notes together to compose music. Those studying music will find differences with each composer's musical analysis, which differs depending on the culture and history of music studied. An analysis of music is meant to simplify the music for you.£#/li#££#/ul#££#/li#£ £#li#£Schenkerian analysis £#ul#££#li#£Schenkerian analysis is a collection of music analysis that focuses on the production of the graphic representation. This includes both analytical procedure as well as the notational style. Simply put, it analyzes tonal music which includes all chords and tones within a composition.£#/li#££#/ul#££#/li#££#/ul#£
£#h5#£Philosophy£#/h5#£ £#ul#££#li#£Philosophical analysis – a general term for the techniques used by philosophers £#ul#££#li#£Philosophical analysis refers to the clarification and composition of words put together and the entailed meaning behind them. Philosophical analysis dives deeper into the meaning of words and seeks to clarify that meaning by contrasting the various definitions. It is the study of reality, justification of claims, and the analysis of various concepts. Branches of philosophy include logic, justification, metaphysics, values and ethics. If questions can be answered empirically, meaning it can be answered by using the senses, then it is not considered philosophical. Non-philosophical questions also include events that happened in the past, or questions science or mathematics can answer.£#/li#££#/ul#££#/li#£ £#li#£Analysis is the name of a prominent journal in philosophy.£#/li#££#/ul#£
£#h5#£Psychotherapy£#/h5#£ £#ul#££#li#£Psychoanalysis – seeks to elucidate connections among unconscious components of patients' mental processes£#/li#£ £#li#£Transactional analysis £#ul#££#li#£Transactional analysis is used by therapists to try to gain a better understanding of the unconscious. It focuses on understanding and intervening human behavior.£#/li#££#/ul#££#/li#££#/ul#£
£#h5#£Policy£#/h5#£ £#ul#££#li#£Policy analysis – The use of statistical data to predict the effects of policy decisions made by governments and agencies £#ul#££#li#£Policy analysis includes a systematic process to find the most efficient and effective option to address the current situation.£#/li#££#/ul#££#/li#£ £#li#£Qualitative analysis – The use of anecdotal evidence to predict the effects of policy decisions or, more generally, influence policy decisions£#/li#££#/ul#£
£#h5#£Signal processing£#/h5#£ £#ul#££#li#£Finite element analysis – a computer simulation technique used in engineering analysis£#/li#£ £#li#£Independent component analysis£#/li#£ £#li#£Link quality analysis – the analysis of signal quality£#/li#£ £#li#£Path quality analysis£#/li#£ £#li#£Fourier analysis£#/li#££#/ul#£
£#h5#£Statistics£#/h5#£
In statistics, the term analysis may refer to any method used for data analysis. Among the many such methods, some are:

£#ul#££#li#£Analysis of variance (ANOVA) – a collection of statistical models and their associated procedures which compare means by splitting the overall observed variance into different parts£#/li#£ £#li#£Boolean analysis – a method to find deterministic dependencies between variables in a sample, mostly used in exploratory data analysis£#/li#£ £#li#£Cluster analysis – techniques for finding groups (called clusters), based on some measure of proximity or similarity£#/li#£ £#li#£Factor analysis – a method to construct models describing a data set of observed variables in terms of a smaller set of unobserved variables (called factors)£#/li#£ £#li#£Meta-analysis – combines the results of several studies that address a set of related research hypotheses£#/li#£ £#li#£Multivariate analysis – analysis of data involving several variables, such as by factor analysis, regression analysis, or principal component analysis£#/li#£ £#li#£Principal component analysis – transformation of a sample of correlated variables into uncorrelated variables (called principal components), mostly used in exploratory data analysis£#/li#£ £#li#£Regression analysis – techniques for analysing the relationships between several predictive variables and one or more outcomes in the data£#/li#£ £#li#£Scale analysis (statistics) – methods to analyse survey data by scoring responses on a numeric scale£#/li#£ £#li#£Sensitivity analysis – the study of how the variation in the output of a model depends on variations in the inputs£#/li#£ £#li#£Sequential analysis – evaluation of sampled data as it is collected, until the criterion of a stopping rule is met£#/li#£ £#li#£Spatial analysis – the study of entities using geometric or geographic properties£#/li#£ £#li#£Time-series analysis – methods that attempt to understand a sequence of data points spaced apart at uniform time intervals£#/li#££#/ul#£
£#h5#£Other£#/h5#£ £#ul#££#li#£Aura analysis – a technique in which supporters of the method claim that the body's aura, or energy field is analysed£#/li#£ £#li#£Bowling analysis – Analysis of the performance of cricket players£#/li#£ £#li#£Lithic analysis – the analysis of stone tools using basic scientific techniques £#ul#££#li#£Lithic analysis is most often used by archeologists in determining which types of tools were used at a given time period pertaining to current artifacts discovered.£#/li#££#/ul#££#/li#£ £#li#£Protocol analysis – a means for extracting persons' thoughts while they are performing a task£#/li#££#/ul#£
£#h5#£See also£#/h5#£ £#ul#££#li#£Formal analysis£#/li#£ £#li#£Metabolism in biology£#/li#£ £#li#£Methodology£#/li#£ £#li#£Scientific method£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Analysis at the Indiana Philosophy Ontology Project£#/li#£ £#li#£"Analysis" entry in the Stanford Encyclopedia of Philosophy£#/li#£ £#li#£Analysis at PhilPapers£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Bottazzini, U. The "Higher Calculus": A History of Real and Complex Analysis from Euler to Weierstrass. New York: Springer-Verlag, 1986.£#/li#££#li#£Bressoud, D. M. A Radical Approach to Real Analysis. Washington, DC: Math. Assoc. Amer., 1994.£#/li#££#li#£Derbyshire, J. Prime Obsession: Bernhard Riemann and the Greatest Unsolved Problem in Mathematics. New York: Penguin, 2004.£#/li#££#li#£Ehrlich, P. Real Numbers, Generalization of the Reals, & Theories of Continua. Norwell, MA: Kluwer, 1994.£#/li#££#li#£Enderton, H. B. A Mathematical Introduction to Logic. New York: Academic Press, 1972.£#/li#££#li#£Hairer, E. and Wanner, G. Analysis by Its History. New York: Springer-Verlag, 1996.£#/li#££#li#£Royden, H. L. Real Analysis, 3rd ed. New York: Macmillan, 1988.£#/li#££#li#£Weisstein, E. W. "Books about Analysis." http://www.ericweisstein.com/encyclopedias/books/Analysis.html.£#/li#££#li#£Wheeden, R. L. and Zygmund, A. Measure and Integral: An Introduction to Real Analysis. New York: Dekker, 1977.£#/li#££#li#£Whittaker, E. T. and Watson, G. N. A Course in Modern Analysis, 4th ed. Cambridge, England: Cambridge University Press, 1990.£#/li#££#li#£ Bottazzini, U. The "Higher Calculus": A History of Real and Complex Analysis from Euler to Weierstrass. New York: Springer-Verlag, 1986. £#/li#££#li#£ Bressoud, D. M. A Radical Approach to Real Analysis. Washington, DC: Math. Assoc. Amer., 1994. £#/li#££#li#£ Derbyshire, J. Prime Obsession: Bernhard Riemann and the Greatest Unsolved Problem in Mathematics. New York: Penguin, 2004. £#/li#££#li#£ Ehrlich, P. Real Numbers, Generalization of the Reals, & Theories of Continua. Norwell, MA: Kluwer, 1994. £#/li#££#li#£ Enderton, H. B. A Mathematical Introduction to Logic. New York: Academic Press, 1972. £#/li#££#li#£ Hairer, E. and Wanner, G. Analysis by Its History. New York: Springer-Verlag, 1996. £#/li#££#li#£ Royden, H. L. Real Analysis, 3rd ed. New York: Macmillan, 1988. £#/li#££#li#£ Weisstein, E. W. "Books about Analysis." http://www.ericweisstein.com/encyclopedias/books/Analysis.html. £#/li#££#li#£ Wheeden, R. L. and Zygmund, A. Measure and Integral: An Introduction to Real Analysis. New York: Dekker, 1977. £#/li#££#li#£ Whittaker, E. T. and Watson, G. N. A Course in Modern Analysis, 4th ed. Cambridge, England: Cambridge University Press, 1990. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > General Analysis £#/li#££#li#£ Foundations of Mathematics > Logic > General Logic £#/li#££#li#£ History and Terminology > Disciplinary Terminology > Medical Terminology £#/li#££#/ul#£




£#h3#£Analytic Continuation£#/h3#£

In complex analysis, a branch of mathematics, analytic continuation is a technique to extend the domain of definition of a given analytic function. Analytic continuation often succeeds in defining further values of a function, for example in a new region where an infinite series representation in terms of which it is initially defined becomes divergent.

The step-wise continuation technique may, however, come up against difficulties. These may have an essentially topological nature, leading to inconsistencies (defining more than one value). They may alternatively have to do with the presence of singularities. The case of several complex variables is rather different, since singularities then need not be isolated points, and its investigation was a major reason for the development of sheaf cohomology.


£#h5#£Initial discussion£#/h5#£
Suppose f is an analytic function defined on a non-empty open subset U of the complex plane ${\displaystyle \mathbb {C} }$ . If V is a larger open subset of ${\displaystyle \mathbb {C} }$ , containing U, and F is an analytic function defined on V such that

${\displaystyle F(z)=f(z)\qquad \forall z\in U,}$
then F is called an analytic continuation of f. In other words, the restriction of F to U is the function f we started with.

Analytic continuations are unique in the following sense: if V is the connected domain of two analytic functions F1 and F2 such that U is contained in V and for all z in U

${\displaystyle F_{1}(z)=F_{2}(z)=f(z),}$
then

${\displaystyle F_{1}=F_{2}}$
on all of V. This is because F1 − F2 is an analytic function which vanishes on the open, connected domain U of f and hence must vanish on its entire domain. This follows directly from the identity theorem for holomorphic functions.


£#h5#£Applications£#/h5#£
A common way to define functions in complex analysis proceeds by first specifying the function on a small domain only, and then extending it by analytic continuation.

In practice, this continuation is often done by first establishing some functional equation on the small domain and then using this equation to extend the domain. Examples are the Riemann zeta function and the gamma function.

The concept of a universal cover was first developed to define a natural domain for the analytic continuation of an analytic function. The idea of finding the maximal analytic continuation of a function in turn led to the development of the idea of Riemann surfaces.


£#h5#£Worked example£#/h5#£
Begin with a particular analytic function ${\displaystyle f}$ . In this case, it is given by a power series centered at ${\displaystyle z=1}$ :

${\displaystyle f(z)=\sum _{k=0}^{\infty }(-1)^{k}(z-1)^{k}.}$
By the Cauchy–Hadamard theorem, its radius of convergence is 1. That is, ${\displaystyle f}$ is defined and analytic on the open set ${\displaystyle U=\{|z-1|<1\}}$ which has boundary ${\displaystyle \partial U=\{|z-1|=1\}}$ . Indeed, the series diverges at ${\displaystyle z=0\in \partial U}$ .

Pretend we don't know that ${\displaystyle f(z)=1/z}$ , and focus on recentering the power series at a different point ${\displaystyle a\in U}$ :

${\displaystyle f(z)=\sum _{k=0}^{\infty }a_{k}(z-a)^{k}.}$
We'll calculate the ${\displaystyle a_{k}}$ 's and determine whether this new power series converges in an open set ${\displaystyle V}$ which is not contained in ${\displaystyle U}$ . If so, we will have analytically continued ${\displaystyle f}$ to the region ${\displaystyle U\cup V}$ which is strictly larger than ${\displaystyle U}$ .

The distance from ${\displaystyle a}$ to ${\displaystyle \partial U}$ is ${\displaystyle \rho =1-|a-1|>0}$ . Take ${\displaystyle 0<r<\rho }$ ; let ${\displaystyle D}$ be the disk of radius ${\displaystyle r}$ around ${\displaystyle a}$ ; and let ${\displaystyle \partial D}$ be its boundary. Then ${\displaystyle D\cup \partial D\subset U}$ . Using Cauchy's differentiation formula to calculate the new coefficients,

${\displaystyle {\begin{aligned}a_{k}&={\frac {f^{(k)}(a)}{k!}}\\&={\frac {1}{2\pi i}}\int _{\partial D}{\frac {f(\zeta )d\zeta }{(\zeta -a)^{k+1}}}\\&={\frac {1}{2\pi i}}\int _{\partial D}{\frac {\sum _{n=0}^{\infty }(-1)^{n}(\zeta -1)^{n}d\zeta }{(\zeta -a)^{k+1}}}\\&={\frac {1}{2\pi i}}\sum _{n=0}^{\infty }(-1)^{n}\int _{\partial D}{\frac {(\zeta -1)^{n}d\zeta }{(\zeta -a)^{k+1}}}\\&={\frac {1}{2\pi i}}\sum _{n=0}^{\infty }(-1)^{n}\int _{0}^{2\pi }{\frac {(a+re^{i\theta }-1)^{n}rie^{i\theta }d\theta }{(re^{i\theta })^{k+1}}}\\&={\frac {1}{2\pi }}\sum _{n=0}^{\infty }(-1)^{n}\int _{0}^{2\pi }{\frac {(a-1+re^{i\theta })^{n}d\theta }{(re^{i\theta })^{k}}}\\&={\frac {1}{2\pi }}\sum _{n=0}^{\infty }(-1)^{n}\int _{0}^{2\pi }{\frac {\sum _{m=0}^{n}{\binom {n}{m}}(a-1)^{n-m}(re^{i\theta })^{m}d\theta }{(re^{i\theta })^{k}}}\\&={\frac {1}{2\pi }}\sum _{n=0}^{\infty }(-1)^{n}\int _{0}^{2\pi }{\binom {n}{k}}(a-1)^{n-k}d\theta \\&=\sum _{n=0}^{\infty }(-1)^{n}{\binom {n}{k}}(a-1)^{n-k}\\&=(-1)^{k}a^{-k-1}\end{aligned}}}$
That is,

${\displaystyle f(z)=\sum _{k=0}^{\infty }a_{k}(z-a)^{k}=\sum _{k=0}^{\infty }(-1)^{k}a^{-k-1}(z-a)^{k}={\frac {1}{a}}\sum _{k=0}^{\infty }\left(1-{\frac {z}{a}}\right)^{k},}$
which has radius of convergence ${\displaystyle |a|}$ , and ${\displaystyle V=\{|z-a|<|a|\}.}$ If we choose ${\displaystyle a\in U}$ with ${\displaystyle |a|>1}$ , then ${\displaystyle V}$ is not a subset of ${\displaystyle U}$ and is actually larger in area than ${\displaystyle U}$ . The plot shows the result for ${\displaystyle a={\tfrac {1}{2}}(3+i).}$

We can continue the process: select ${\displaystyle b\in U\cup V}$ , recenter the power series at ${\displaystyle b}$ , and determine where the new power series converges. If the region contains points not in ${\displaystyle U\cup V}$ , then we will have analytically continued ${\displaystyle f}$ even farther. This particular ${\displaystyle f}$ can be analytically continued to the punctured complex plane ${\displaystyle \mathbb {C} \setminus \{0\}.}$


£#h5#£Formal definition of a germ£#/h5#£
The power series defined below is generalized by the idea of a germ. The general theory of analytic continuation and its generalizations is known as sheaf theory. Let

${\displaystyle f(z)=\sum _{k=0}^{\infty }\alpha _{k}(z-z_{0})^{k}}$
be a power series converging in the disk Dr(z0), r > 0, defined by

${\displaystyle D_{r}(z_{0})=\{z\in \mathbb {C} :|z-z_{0}|<r\}}$ .
Note that without loss of generality, here and below, we will always assume that a maximal such r was chosen, even if that r is ∞. Also note that it would be equivalent to begin with an analytic function defined on some small open set. We say that the vector

${\displaystyle g=(z_{0},\alpha _{0},\alpha _{1},\alpha _{2},\ldots )}$
is a germ of f. The base g0 of g is z0, the stem of g is (α0, α1, α2, ...) and the top g1 of g is α0. The top of g is the value of f at z0.

Any vector g = (z0, α0, α1, ...) is a germ if it represents a power series of an analytic function around z0 with some radius of convergence r > 0. Therefore, we can safely speak of the set of germs ${\displaystyle {\mathcal {G}}}$ .


£#h5#£The topology of the set of germs£#/h5#£
Let g and h be germs. If ${\displaystyle |h_{0}-g_{0}|<r}$ where r is the radius of convergence of g and if the power series defined by g and h specify identical functions on the intersection of the two domains, then we say that h is generated by (or compatible with) g, and we write g ≥ h. This compatibility condition is neither transitive, symmetric nor antisymmetric. If we extend the relation by transitivity, we obtain a symmetric relation, which is therefore also an equivalence relation on germs (but not an ordering). This extension by transitivity is one definition of analytic continuation. The equivalence relation will be denoted ${\displaystyle \cong }$ .

We can define a topology on ${\displaystyle {\mathcal {G}}}$ . Let r > 0, and let

${\displaystyle U_{r}(g)=\{h\in {\mathcal {G}}:g\geq h,|g_{0}-h_{0}|<r\}.}$
The sets Ur(g), for all r > 0 and ${\displaystyle g\in {\mathcal {G}}}$ define a basis of open sets for the topology on ${\displaystyle {\mathcal {G}}}$ .

A connected component of ${\displaystyle {\mathcal {G}}}$ (i.e., an equivalence class) is called a sheaf. We also note that the map defined by ${\displaystyle \phi _{g}(h)=h_{0}:U_{r}(g)\to \mathbb {C} ,}$ where r is the radius of convergence of g, is a chart. The set of such charts forms an atlas for ${\displaystyle {\mathcal {G}}}$ , hence ${\displaystyle {\mathcal {G}}}$ is a Riemann surface. ${\displaystyle {\mathcal {G}}}$ is sometimes called the universal analytic function.


£#h5#£Examples of analytic continuation£#/h5#£
${\displaystyle L(z)=\sum _{k=1}^{\infty }{\frac {(-1)^{k+1}}{k}}(z-1)^{k}}$
is a power series corresponding to the natural logarithm near z = 1. This power series can be turned into a germ

${\displaystyle g=\left(1,0,1,-{\frac {1}{2}},{\frac {1}{3}},-{\frac {1}{4}},{\frac {1}{5}},-{\frac {1}{6}},\ldots \right)}$
This germ has a radius of convergence of 1, and so there is a sheaf S corresponding to it. This is the sheaf of the logarithm function.

The uniqueness theorem for analytic functions also extends to sheaves of analytic functions: if the sheaf of an analytic function contains the zero germ (i.e., the sheaf is uniformly zero in some neighborhood) then the entire sheaf is zero. Armed with this result, we can see that if we take any germ g of the sheaf S of the logarithm function, as described above, and turn it into a power series f(z) then this function will have the property that exp(f(z)) = z. If we had decided to use a version of the inverse function theorem for analytic functions, we could construct a wide variety of inverses for the exponential map, but we would discover that they are all represented by some germ in S. In that sense, S is the "one true inverse" of the exponential map.

In older literature, sheaves of analytic functions were called multi-valued functions. See sheaf for the general concept.


£#h5#£Natural boundary£#/h5#£
Suppose that a power series has radius of convergence r and defines an analytic function f inside that disc. Consider points on the circle of convergence. A point for which there is a neighbourhood on which f has an analytic extension is regular, otherwise singular. The circle is a natural boundary if all its points are singular.

More generally, we may apply the definition to any open connected domain on which f is analytic, and classify the points of the boundary of the domain as regular or singular: the domain boundary is then a natural boundary if all points are singular, in which case the domain is a domain of holomorphy.


£#h5#£Example I: A function with a natural boundary at zero (the prime zeta function)£#/h5#£
For ${\displaystyle \Re (s)>1}$ we define the so-called prime zeta function, ${\displaystyle P(s)}$ , to be

${\displaystyle P(s):=\sum _{p\ {\text{ prime}}}p^{-s}.}$
This function is analogous to the summatory form of the Riemann zeta function when ${\displaystyle \Re (s)>1}$ in so much as it is the same summatory function as ${\displaystyle \zeta (s)}$ , except with indices restricted only to the prime numbers instead of taking the sum over all positive natural numbers. The prime zeta function has an analytic continuation to all complex s such that ${\displaystyle 0<\Re (s)<1}$ , a fact which follows from the expression of ${\displaystyle P(s)}$ by the logarithms of the Riemann zeta function as

${\displaystyle P(s)=\sum _{n\geq 1}\mu (n){\frac {\log \zeta (ns)}{n}}.}$
Since ${\displaystyle \zeta (s)}$ has a simple, non-removable pole at ${\displaystyle s:=1}$ , it can then be seen that ${\displaystyle P(s)}$ has a simple pole at ${\displaystyle s:={\tfrac {1}{k}},\forall k\in \mathbb {Z} ^{+}}$ . Since the set of points

${\displaystyle \operatorname {Sing} _{P}:=\left\{k^{-1}:k\in \mathbb {Z} ^{+}\right\}=\left\{1,{\frac {1}{2}},{\frac {1}{3}},{\frac {1}{4}},\ldots \right\}}$
has accumulation point 0 (the limit of the sequence as ${\displaystyle k\mapsto \infty }$ ), we can see that zero forms a natural boundary for ${\displaystyle P(s)}$ . This implies that ${\displaystyle P(s)}$ has no analytic continuation for s left of (or at) zero, i.e., there is no continuation possible for ${\displaystyle P(s)}$ when ${\displaystyle 0\geq \Re (s)}$ . As a remark, this fact can be problematic if we are performing a complex contour integral over an interval whose real parts are symmetric about zero, say ${\displaystyle I_{F}\subseteq \mathbb {C} \ {\text{such that}}\ \Re (s)\in (-C,C),\forall s\in I_{F}}$ for some ${\displaystyle C>0}$ , where the integrand is a function with denominator that depends on ${\displaystyle P(s)}$ in an essential way.


£#h5#£Example II: A typical lacunary series (natural boundary as subsets of the unit circle)£#/h5#£
For integers ${\displaystyle c\geq 2}$ , we define the lacunary series of order c by the power series expansion

${\displaystyle {\mathcal {L}}_{c}(z):=\sum _{n\geq 1}z^{c^{n}},|z|<1.}$
Clearly, since ${\displaystyle c^{n+1}=c\cdot c^{n}}$ there is a functional equation for ${\displaystyle {\mathcal {L}}_{c}(z)}$ for any z satisfying ${\displaystyle |z|<1}$ given by ${\displaystyle {\mathcal {L}}_{c}(z)=z^{c}+{\mathcal {L}}_{c}(z^{c})}$ . It is also not difficult to see that for any integer ${\displaystyle m\geq 1}$ , we have another functional equation for ${\displaystyle {\mathcal {L}}_{c}(z)}$ given by

${\displaystyle {\mathcal {L}}_{c}(z)=\sum _{i=0}^{m-1}z^{c^{i}}+{\mathcal {L}}_{c}(z^{c^{m}}),\forall |z|<1.}$
For any positive natural numbers c, the lacunary series function diverges at ${\displaystyle z=1}$ . We consider the question of analytic continuation of ${\displaystyle {\mathcal {L}}_{c}(z)}$ to other complex z such that ${\displaystyle |z|>1.}$ As we shall see, for any ${\displaystyle n\geq 1}$ , the function ${\displaystyle {\mathcal {L}}_{c}(z)}$ diverges at the ${\displaystyle c^{n}}$ -th roots of unity. Hence, since the set formed by all such roots is dense on the boundary of the unit circle, there is no analytic continuation of ${\displaystyle {\mathcal {L}}_{c}(z)}$ to complex z whose modulus exceeds one.

The proof of this fact is generalized from a standard argument for the case where ${\displaystyle c:=2.}$ Namely, for integers ${\displaystyle n\geq 1}$ , let

${\displaystyle {\mathcal {R}}_{c,n}:=\left\{z\in \mathbb {D} \cup \partial {\mathbb {D} }:z^{c^{n}}=1\right\},}$
where ${\displaystyle \mathbb {D} }$ denotes the open unit disk in the complex plane and ${\displaystyle |{\mathcal {R}}_{c,n}|=c^{n}}$ , i.e., there are ${\displaystyle c^{n}}$ distinct complex numbers z that lie on or inside the unit circle such that ${\displaystyle z^{c^{n}}=1}$ . Now the key part of the proof is to use the functional equation for ${\displaystyle {\mathcal {L}}_{c}(z)}$ when ${\displaystyle |z|<1}$ to show that

${\displaystyle \forall z\in {\mathcal {R}}_{c,n},\qquad {\mathcal {L}}_{c}(z)=\sum _{i=0}^{c^{n}-1}z^{c^{i}}+{\mathcal {L}}_{c}(z^{c^{n}})=\sum _{i=0}^{c^{n}-1}z^{c^{i}}+{\mathcal {L}}_{c}(1)=+\infty .}$
Thus for any arc on the boundary of the unit circle, there are an infinite number of points z within this arc such that ${\displaystyle {\mathcal {L}}_{c}(z)=\infty }$ . This condition is equivalent to saying that the circle ${\displaystyle C_{1}:=\{z:|z|=1\}}$ forms a natural boundary for the function ${\displaystyle {\mathcal {L}}_{c}(z)}$ for any fixed choice of ${\displaystyle c\in \mathbb {Z} \quad c>1.}$ Hence, there is no analytic continuation for these functions beyond the interior of the unit circle.


£#h5#£Monodromy theorem£#/h5#£
The monodromy theorem gives a sufficient condition for the existence of a direct analytic continuation (i.e., an extension of an analytic function to an analytic function on a bigger set).

Suppose ${\displaystyle D\subset \mathbb {C} }$ is an open set and f an analytic function on D. If G is a simply connected domain containing D, such that f has an analytic continuation along every path in G, starting from some fixed point a in D, then f has a direct analytic continuation to G.

In the above language this means that if G is a simply connected domain, and S is a sheaf whose set of base points contains G, then there exists an analytic function f on G whose germs belong to S.


£#h5#£Hadamard's gap theorem£#/h5#£
For a power series

${\displaystyle f(z)=\sum _{k=0}^{\infty }a_{k}z^{n_{k}}}$
with

${\displaystyle \liminf _{k\to \infty }{\frac {n_{k+1}}{n_{k}}}>1}$
the circle of convergence is a natural boundary. Such a power series is called lacunary. This theorem has been substantially generalized by Eugen Fabry (see Fabry's gap theorem) and George Pólya.


£#h5#£Pólya's theorem£#/h5#£
Let

${\displaystyle f(z)=\sum _{k=0}^{\infty }\alpha _{k}(z-z_{0})^{k}}$
be a power series, then there exist εk ∈ {−1, 1} such that

${\displaystyle f(z)=\sum _{k=0}^{\infty }\varepsilon _{k}\alpha _{k}(z-z_{0})^{k}}$
has the convergence disc of f around z0 as a natural boundary.

The proof of this theorem makes use of Hadamard's gap theorem.


£#h5#£A useful theorem: A sufficient condition for analytic continuation to the non-positive integers£#/h5#£
In most cases, if an analytic continuation of a complex function exists, it is given by an integral formula. The next theorem, provided its hypotheses are met, provides a sufficient condition under which we can continue an analytic function from its convergent points along the positive reals to arbitrary ${\displaystyle s\in \mathbb {C} }$ (with the exception of at finitely-many poles). Moreover, the formula gives an explicit representation for the values of the continuation to the non-positive integers expressed exactly by higher order (integer) derivatives of the original function evaluated at zero.


£#h5#£Hypotheses of the theorem£#/h5#£
We require that a function ${\displaystyle F:\mathbb {R} ^{+}\to \mathbb {C} }$ satisfies the following conditions in order to apply the theorem on continuation of this function stated below:

£#ul#££#li#£(T-1). The function must have continuous derivatives of all orders, i.e., ${\displaystyle F\in {\mathcal {C}}^{\infty }(\mathbb {R} ^{+})}$ . In other words, for any integers ${\displaystyle j\geq 1}$ , the integral-order ${\displaystyle j^{th}}$ derivative ${\displaystyle F^{(j)}(x)={\frac {d^{(j)}}{dx^{(j)}}}[F(x)]}$ must exist, be continuous on ${\displaystyle \mathbb {R} ^{+}}$ , and itself be differentiable, so that all higher order derivatives of F are smooth functions of x on the positive real numbers;£#/li#£ £#li#£(T-2). We require that the function F is rapidly decreasing in that for all ${\displaystyle n\in \mathbb {Z} ^{+}}$ we obtain the limiting behavior that ${\displaystyle t^{n}F(t)\to 0}$ as t becomes unbounded, tending to infinity;£#/li#£ £#li#£(T-3). The (reciprocal gamma-scaled) Mellin transform of F exists for all complex s such that ${\displaystyle \Re (s)>0}$ with the exception of ${\displaystyle s\in \{\zeta _{1}(F),\zeta _{2}(F),\ldots ,\zeta _{k}(F)\}}$ (or for all s with positive real parts except possibly at a finite number of exceptional poles):£#/li#££#/ul#£
${\displaystyle {\widetilde {\mathcal {M}}}[F](s):={\frac {1}{\Gamma (s)}}\int _{0}^{\infty }t^{s}F(t){\frac {dt}{t}},\qquad \left|{\widetilde {\mathcal {M}}}[F](s)\right|\in (-\infty ,+\infty ),\forall s\in \{z\in \mathbb {C} :\Re (z)>0\}\setminus \{\zeta _{1}(F),\ldots ,\zeta _{k}(F)\}.}$

£#h5#£The conclusion of the theorem£#/h5#£
Let F be any function defined on the positive reals that satisfies all of the conditions (T1)-(T3) above. Then the integral representation of the scaled Mellin transform of F at s, denoted by ${\displaystyle {\widetilde {\mathcal {M}}}[F](s)}$ , has an meromorphic continuation to the complex plane ${\displaystyle \mathbb {C} \setminus \{\zeta _{1}(F),\ldots ,\zeta _{k}(F)\}}$ . Moreover, we have that for any non-negative ${\displaystyle n\in \mathbb {Z} }$ , the continuation of F at the point ${\displaystyle s:=-n}$ is given explicitly by the formula

${\displaystyle {\widetilde {\mathcal {M}}}[F](-n)=(-1)^{n}\times F^{(n)}(0)\equiv (-1)^{n}\times {\frac {\partial ^{n}}{{\partial x}^{n}}}\left[F(x)\right]|_{x=0}.}$

£#h5#£Examples£#/h5#£
£#h5#£Example I: The connection of the Riemann zeta function to the Bernoulli numbers£#/h5#£
We can apply the theorem to the function

${\displaystyle F_{\zeta }(x):={\frac {x}{e^{x}-1}}=\sum _{n\geq 0}B_{n}{\frac {x^{n}}{n!}},}$
which corresponds to the exponential generating function of the Bernoulli numbers, ${\displaystyle B_{n}}$ . For ${\displaystyle \Re (s)>1}$ , we can express ${\displaystyle \zeta (s)={\widetilde {\mathcal {M}}}[F_{\zeta }](s)}$ , since we can compute that the next integral formula for the reciprocal powers of the integers ${\displaystyle n\geq 1}$ holds for s in this range:

${\displaystyle {\frac {1}{n^{s}}}={\frac {1}{\Gamma (s)}}\int _{0}^{+\infty }t^{s-1}e^{-nt}dt,\Re (s)>1.}$
Now since the integrand of the last equation is a uniformly continuous function of t for each positive integer n, we have an integral representation for ${\displaystyle \zeta (s)}$ whenever ${\displaystyle \Re (s)>1}$ given by

${\displaystyle \zeta (s)=\sum _{n\geq 1}n^{-s}={\frac {1}{\Gamma (s)}}\int _{0}^{+\infty }\left(\sum _{n\geq 1}e^{-nt}\right)t^{s-1}dt={\frac {1}{\Gamma (s)}}\int _{0}^{\infty }t^{s-1}{\frac {F_{\zeta }(t)}{t}}dt.}$
When we perform integration by parts to the Mellin transform integral for this ${\displaystyle F_{\zeta }(x)}$ , we also obtain the relation that

${\displaystyle \zeta (s)={\frac {1}{(s-1)}}{\widetilde {\mathcal {M}}}[F_{\zeta }](s-1).}$
Moreover, since ${\displaystyle e^{t}\gg t^{n}}$ for any fixed integer polynomial power of t, we meet the hypothesis of the theorem which requires that ${\displaystyle \lim _{t\to +\infty }t^{n}\cdot F_{\zeta }(t),\forall n\in \mathbb {Z} ^{+}}$ . The standard application of Taylor's theorem to the ordinary generating function of the Bernoulli numbers shows that ${\displaystyle F_{\zeta }^{(n)}(0)={\frac {B_{n}}{n!}}\times n!=B_{n}}$ . In particular, by the observation made above to shift ${\displaystyle s\mapsto s-1}$ , and these remarks, we can compute the values of the so-called trivial zeros of the Riemann zeta function (for ${\displaystyle \zeta (-2n)}$ ) and the rational-valued negative odd integer order constants, ${\displaystyle \zeta (-(2n+1)),n\geq 0}$ , according to the formula

${\displaystyle \zeta (-n)=-{\frac {1}{n+1}}{\widetilde {\mathcal {M}}}[F_{\zeta }](-n-1)={\frac {(-1)^{n}}{n+1}}F_{\zeta }^{(n+1)}(0)={\begin{cases}-{\frac {1}{2}},&n=0;\\\infty ,&n=1;\\-{\frac {B_{n+1}}{n+1}},&n\geq 2.\end{cases}}}$

£#h5#£Example II: An interpretation of F as the summatory function for some arithmetic sequence£#/h5#£
Suppose that F is a smooth, sufficiently decreasing function on the positive reals satisfying the additional condition that

${\displaystyle \Delta [F](x-1)=F(x)-F(x-1)=:f(x),\forall x\in \mathbb {Z} ^{+}.}$
In application to number theoretic contexts, we consider such F to be the summatory function of the arithmetic function f,

${\displaystyle F(x):={\sum _{n\geq x}}^{\prime }f(n)}$
where we take ${\displaystyle F(x)=0,\forall 0<x<1}$ and the prime-notation on the previous sum corresponds to the standard conventions used to state Perron's theorem:

${\displaystyle F_{f}(x):={\sum _{n\leq x}}^{\prime }f(n)={\begin{cases}\sum _{n\leq [x]}f(n),&x\in \mathbb {R} ^{+}\setminus \mathbb {Z} ;\\\sum _{n\leq x}f(n)-{\frac {f(x)}{2}},&x\in \mathbb {R} ^{+}\cap \mathbb {Z} .\end{cases}}}$
We are interested in the analytic continuation of the DGF of f, or equivalently of the Dirichlet series over f at s,

${\displaystyle D_{f}(s):=\sum _{n\geq 1}{\frac {f(n)}{n^{s}}}.}$
Typically, we have a particular value of the abscissa of convergence, ${\displaystyle \sigma _{0,f}>0}$ , defined such that ${\displaystyle D_{f}(s)}$ is absolutely convergent for all complex s satisfying ${\displaystyle \Re (s)>\sigma _{0,f}}$ , and where ${\displaystyle D_{f}(s)}$ is assumed to have a pole at ${\displaystyle s:=\pm \sigma _{0,f}}$ and so that the initial Dirichlet series for ${\displaystyle D_{f}(s)}$ diverges for all s such that ${\displaystyle \Re (s)\leq \sigma _{0,f}}$ . It is known that there is a relationship between the Mellin transform of the summatory function of any f to the continuation of its DGF at ${\displaystyle s\mapsto -s}$ of the form:

${\displaystyle D_{f}(s)={\mathcal {M}}[F](-s)=\int _{1}^{\infty }{\frac {F_{f}(s)}{x^{s+1}}}dx}$
That is to say that, provided ${\displaystyle D_{f}(s)}$ has a continuation to the complex plane left of the origin, we can express the summatory function of any f by the inverse Mellin transform of the DGF of f continued to s with real parts less than zero as:

${\displaystyle F_{f}(x)={\mathcal {M}}^{-1}\left[{\mathcal {M}}[F_{f}](-s)\right](x)={\mathcal {M}}^{-1}[D_{f}(-s)](x).}$
We can form the DGF, or Dirichlet generating function, of any prescribed f given our smooth target function F by performing summation by parts as

${\displaystyle {\begin{aligned}D_{f}(s)&={\frac {1}{\Gamma (s)}}\int _{0}^{+\infty }\left(\sum _{n\geq 1}(F(n)-F(n-1))e^{-nt}\right)t^{s}dt\\&={\frac {1}{\Gamma (s)}}\int _{0}^{\infty }\lim _{N\to \infty }\left[F(N)e^{-Nt}+\sum _{k=0}^{N-1}F(k)e^{-kt}\left(1-e^{-t}\right)\right]dt\\&={\frac {1}{\Gamma (s)}}\int _{0}^{\infty }t^{s-1}(1-e^{-t})\int _{0}^{\infty }F(r/t)e^{-r}drdt\\&={\frac {1}{\Gamma (s)}}\int _{0}^{\infty }t^{s-1}\left(1-e^{-t}\right){\widetilde {F}}\left({\frac {1}{t}}\right)dt\\&={\frac {1}{\Gamma (s)}}\int _{0}^{\infty }{\frac {\left(1-e^{-1/u}\right)}{u^{s}(1-u)}}F\left({\frac {u}{1-u}}\right)du,\end{aligned}}}$
where ${\displaystyle {\hat {F}}(x)\equiv {\mathcal {L}}[F](x)}$ is the Laplace-Borel transform of F, which if

${\displaystyle F(z):=\sum _{n\geq 0}{\frac {f_{n}}{n!}}z^{n}}$
corresponds to the exponential generating function of some sequence enumerated by ${\displaystyle f_{n}/n!=F^{(n)}(0)/n!}$ (as prescribed by the Taylor series expansion of F about zero), then

${\displaystyle {\widetilde {F}}(z)=\sum _{n\geq 0}f_{n}z^{n}}$
is its ordinary generating function form over the sequence whose coefficients are enumerated by ${\displaystyle [z^{n}]{\widetilde {F}}(z)\equiv f_{n}=F^{(n)}(0)}$ .

So it follows that if we write

${\displaystyle G_{F}(x):={\frac {x}{1-x}}F\left({\frac {x}{1-x}}\right)=\sum _{n\geq 0}\left(\sum _{k=0}^{n}{\binom {n}{k}}[z^{k}]F(z)\right)x^{n+1},}$
alternately interpreted as a signed variant of the binomial transform of F, then we can express the DGF as the following Mellin transform at ${\displaystyle -s}$ :

${\displaystyle {\begin{aligned}D_{f}(s)&={\mathcal {M}}[G_{F}](-s){\mathcal {M}}\left[1-e^{-1/x}\right](-s)\\&={\frac {{\mathcal {M}}[G_{F}](-s)}{s-1}}\left(1-\Gamma (s)\right)\end{aligned}}}$
Finally, since the gamma function has a meromorphic continuation to ${\displaystyle \mathbb {C} \setminus \mathbb {N} }$ , for all ${\displaystyle s\in \mathbb {C} \setminus \{0,1,2,\ldots \},}$ we have an analytic continuation of the DGF for f at -s of the form

${\displaystyle D_{f}(-s)=-{\frac {1-\Gamma (-s)}{s+1}}{\mathcal {M}}[G_{F}](s),}$
where a formula for ${\displaystyle D_{f}(-n)}$ for non-negative integers n is given according to the formula in the theorem as

${\displaystyle D_{f}(-n)=(-1)^{n}{\frac {d^{n}}{{dx}^{n}}}\left[\left(1-e^{-1/x}\right){\frac {x}{1-x}}F\left({\frac {x}{1-x}}\right)\right]{\Biggr |}_{x=0}.}$
Moreover, provided that the arithmetic function f satisfies ${\displaystyle f(1)\neq 1}$ so that its Dirichlet inverse function exists, the DGF of ${\displaystyle f^{-1}}$ is continued to any ${\displaystyle s\in \mathbb {C} \cap \{z:\Re (z)\in (-\infty ,-\sigma _{0,f})\cup (\sigma _{0,f},+\infty )\}}$ , that is any complex s excluding s in a f-defined, or application dependent f-specific, so-called critical strip between the vertical lines ${\displaystyle z=\pm \sigma _{0,f}}$ , and the value of this inverse function DGF when ${\displaystyle \Re (s)<-\sigma _{0,f}}$ is given by

${\displaystyle D_{f^{-1}}(-s)={\begin{cases}0,&n\in \mathbb {N} ;\\-{\frac {s+1}{1-\Gamma (-s)}}{\mathcal {M}}[G_{F}^{-1}](s),&{\text{otherwise.}}\end{cases}}}$
To continue the DGF of the Dirichlet inverse function to s inside this f-defined critical strip, we must require some knowledge of a functional equation for the DGF, ${\displaystyle D_{f}(s)}$ , that allows us to relate the s such that the Dirichlet series that defines this function initially is absolutely convergent to the values of s inside this strip—in essence, a formula providing that ${\displaystyle D_{f}(s)=\xi _{f}(s)\times D_{f}(\sigma _{0,f}-s)}$ is necessary to define the DGF in this strip.


£#h5#£See also£#/h5#£ £#ul#££#li#£Mittag-Leffler star£#/li#£ £#li#£Holomorphic functional calculus£#/li#£ £#li#£Numerical analytic continuation£#/li#££#/ul#£
£#h5#£References£#/h5#£ £#ul#££#li#£Lars Ahlfors (1979). Complex Analysis (3 ed.). McGraw-Hill. pp. 172, 284.£#/li#£ £#li#£Ludwig Bieberbach (1955). Analytische Fortsetzung. Springer-Verlag.£#/li#£ £#li#£P. Dienes (1957). The Taylor series: an introduction to the theory of functions of a complex variable. New York: Dover Publications, Inc.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Analytic continuation", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#£ £#li#£Analytic Continuation at MathPages£#/li#£ £#li#£Weisstein, Eric W. "Analytic Continuation". MathWorld.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Arfken, G. Mathematical Methods for Physicists, 3rd ed. Orlando, FL: Academic Press, pp. 378-380, 1985.£#/li#££#li#£Davis, P. J. and Pollak, H. "On the Analytic Continuation of Mapping Functions." Trans. Amer. Math. Soc. 87, 198-225, 1958.£#/li#££#li#£Flanigan, F. J. Complex Variables: Harmonic and Analytic Functions. New York: Dover, 1983.£#/li#££#li#£Havil, J. "Analytic Continuation." §16.3 in Gamma: Exploring Euler's Constant. Princeton, NJ: Princeton University Press, pp. 91-193, 2003.£#/li#££#li#£Knopp, K. "Analytic Continuation and Complete Definition of Analytic Functions." Ch. 8 in Theory of Functions Parts I and II, Two Volumes Bound as One, Part I. New York: Dover, pp. 83-111, 1996.£#/li#££#li#£Krantz, S. G. "Uniqueness of Analytic Continuation" and "Analytic Continuation." §3.2.3 and Ch. 10 in Handbook of Complex Variables. Boston, MA: Birkhäuser, pp. 38-39 and 123-141, 1999.£#/li#££#li#£Levinson, N. and Raymond, R. Complex Variables. New York: McGraw-Hill, pp. 398-402, 1970.£#/li#££#li#£Morse, P. M. and Feshbach, H. Methods of Theoretical Physics, Part I. New York: McGraw-Hill, pp. 389-390 and 392-398, 1953.£#/li#££#li#£Needham, T. "Analytic Continuation." §5.XI in Visual Complex Analysis. New York: Clarendon Press, pp. 247-257, 2000.£#/li#££#li#£Rudin, W. Real and Complex Analysis. New York: McGraw-Hill, pp. 319-327, 1987.£#/li#££#li#£Whittaker, E. T. and Watson, G. N. "The Process of Continuation." §5.5 in A Course in Modern Analysis, 4th ed. Cambridge, England: Cambridge University Press, pp. 96-98, 1990.£#/li#££#li#£ Arfken, G. Mathematical Methods for Physicists, 3rd ed. Orlando, FL: Academic Press, pp. 378-380, 1985. £#/li#££#li#£ Davis, P. J. and Pollak, H. "On the Analytic Continuation of Mapping Functions." Trans. Amer. Math. Soc. 87, 198-225, 1958. £#/li#££#li#£ Flanigan, F. J. Complex Variables: Harmonic and Analytic Functions. New York: Dover, 1983. £#/li#££#li#£ Havil, J. "Analytic Continuation." §16.3 in Gamma: Exploring Euler's Constant. Princeton, NJ: Princeton University Press, pp. 91-193, 2003. £#/li#££#li#£ Knopp, K. "Analytic Continuation and Complete Definition of Analytic Functions." Ch. 8 in Theory of Functions Parts I and II, Two Volumes Bound as One, Part I. New York: Dover, pp. 83-111, 1996. £#/li#££#li#£ Krantz, S. G. "Uniqueness of Analytic Continuation" and "Analytic Continuation." §3.2.3 and Ch. 10 in Handbook of Complex Variables. Boston, MA: Birkhäuser, pp. 38-39 and 123-141, 1999. £#/li#££#li#£ Levinson, N. and Raymond, R. Complex Variables. New York: McGraw-Hill, pp. 398-402, 1970. £#/li#££#li#£ Morse, P. M. and Feshbach, H. Methods of Theoretical Physics, Part I. New York: McGraw-Hill, pp. 389-390 and 392-398, 1953. £#/li#££#li#£ Needham, T. "Analytic Continuation." §5.XI in Visual Complex Analysis. New York: Clarendon Press, pp. 247-257, 2000. £#/li#££#li#£ Rudin, W. Real and Complex Analysis. New York: McGraw-Hill, pp. 319-327, 1987. £#/li#££#li#£ Whittaker, E. T. and Watson, G. N. "The Process of Continuation." §5.5 in A Course in Modern Analysis, 4th ed. Cambridge, England: Cambridge University Press, pp. 96-98, 1990. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Complex Analysis > Analytic Continuation £#/li#££#li#£ Interactive Entries > Animated GIFs £#/li#££#/ul#£




£#h3#£Analytic Function£#/h3#£

In mathematics, an analytic function is a function that is locally given by a convergent power series. There exist both real analytic functions and complex analytic functions. Functions of each type are infinitely differentiable, but complex analytic functions exhibit properties that do not generally hold for real analytic functions. A function is analytic if and only if its Taylor series about x0 converges to the function in some neighborhood for every x0 in its domain.


£#h5#£Definitions£#/h5#£
Formally, a function ${\displaystyle f}$ is real analytic on an open set ${\displaystyle D}$ in the real line if for any ${\displaystyle x_{0}\in D}$ one can write

${\displaystyle f(x)=\sum _{n=0}^{\infty }a_{n}\left(x-x_{0}\right)^{n}=a_{0}+a_{1}(x-x_{0})+a_{2}(x-x_{0})^{2}+a_{3}(x-x_{0})^{3}+\cdots }$
in which the coefficients ${\displaystyle a_{0},a_{1},\dots }$ are real numbers and the series is convergent to ${\displaystyle f(x)}$ for ${\displaystyle x}$ in a neighborhood of ${\displaystyle x_{0}}$ .

Alternatively, a real analytic function is an infinitely differentiable function such that the Taylor series at any point ${\displaystyle x_{0}}$ in its domain

${\displaystyle T(x)=\sum _{n=0}^{\infty }{\frac {f^{(n)}(x_{0})}{n!}}(x-x_{0})^{n}}$
converges to ${\displaystyle f(x)}$ for ${\displaystyle x}$ in a neighborhood of ${\displaystyle x_{0}}$ pointwise. The set of all real analytic functions on a given set ${\displaystyle D}$ is often denoted by ${\displaystyle {\mathcal {C}}^{\,\omega }(D)}$ .

A function ${\displaystyle f}$ defined on some subset of the real line is said to be real analytic at a point ${\displaystyle x}$ if there is a neighborhood ${\displaystyle D}$ of ${\displaystyle x}$ on which ${\displaystyle f}$ is real analytic.

The definition of a complex analytic function is obtained by replacing, in the definitions above, "real" with "complex" and "real line" with "complex plane". A function is complex analytic if and only if it is holomorphic i.e. it is complex differentiable. For this reason the terms "holomorphic" and "analytic" are often used interchangeably for such functions.


£#h5#£Examples£#/h5#£
Typical examples of analytic functions are

£#ul#££#li#£All elementary functions: £#ul#££#li#£All polynomials: if a polynomial has degree n, any terms of degree larger than n in its Taylor series expansion must immediately vanish to 0, and so this series will be trivially convergent. Furthermore, every polynomial is its own Maclaurin series.£#/li#£ £#li#£The exponential function is analytic. Any Taylor series for this function converges not only for x close enough to x0 (as in the definition) but for all values of x (real or complex).£#/li#£ £#li#£The trigonometric functions, logarithm, and the power functions are analytic on any open set of their domain.£#/li#££#/ul#££#/li#£ £#li#£Most special functions (at least in some range of the complex plane): £#ul#££#li#£hypergeometric functions£#/li#£ £#li#£Bessel functions£#/li#£ £#li#£gamma functions£#/li#££#/ul#££#/li#££#/ul#£
Typical examples of functions that are not analytic are

£#ul#££#li#£The absolute value function when defined on the set of real numbers or complex numbers is not everywhere analytic because it is not differentiable at 0. Piecewise defined functions (functions given by different formulae in different regions) are typically not analytic where the pieces meet.£#/li#£ £#li#£The complex conjugate function z → z* is not complex analytic, although its restriction to the real line is the identity function and therefore real analytic, and it is real analytic as a function from ${\displaystyle \mathbb {R} ^{2}}$ to ${\displaystyle \mathbb {R} ^{2}}$ .£#/li#£ £#li#£Other non-analytic smooth functions, and in particular any smooth function ${\displaystyle f}$ with compact support, i.e. ${\displaystyle f\in {\mathcal {C}}_{0}^{\infty }(\mathbb {R} ^{n})}$ , cannot be analytic on ${\displaystyle \mathbb {R} ^{n}}$ .£#/li#££#/ul#£
£#h5#£Alternative characterizations£#/h5#£
The following conditions are equivalent:

£#li#£ ${\displaystyle f}$ is real analytic on an open set ${\displaystyle D}$ .£#/li#£ £#li#£There is a complex analytic extension of ${\displaystyle f}$ to an open set ${\displaystyle G\subset \mathbb {C} }$ which contains ${\displaystyle D}$ .£#/li#£ £#li#£ ${\displaystyle f}$ is smooth and for every compact set ${\displaystyle K\subset D}$ there exists a constant ${\displaystyle C}$ such that for every ${\displaystyle x\in K}$ and every non-negative integer ${\displaystyle k}$ the following bound holds £#/li#£
Complex analytic functions are exactly equivalent to holomorphic functions, and are thus much more easily characterized.

For the case of an analytic function with several variables (see below), the real analyticity can be characterized using the Fourier–Bros–Iagolnitzer transform.

In the multivariable case, real analytic functions satisfy a direct generalization of the third characterization. Let ${\displaystyle U\subset \mathbb {R} ^{n}}$ be an open set, and let ${\displaystyle f:U\to \mathbb {R} }$ .

Then ${\displaystyle f}$ is real analytic on ${\displaystyle U}$ if and only if ${\displaystyle f\in C^{\infty }(U)}$ and for every compact ${\displaystyle K\subseteq U}$ there exists a constant ${\displaystyle C}$ such that for every multi-index ${\displaystyle \alpha \in \mathbb {Z} _{\geq 0}^{n}}$ the following bound holds

${\displaystyle \sup _{x\in K}\left|{\frac {\partial ^{\alpha }f}{\partial x^{\alpha }}}(x)\right|\leq C^{|\alpha |+1}\alpha !}$

£#h5#£Properties of analytic functions£#/h5#£ £#ul#££#li#£The sums, products, and compositions of analytic functions are analytic.£#/li#£ £#li#£The reciprocal of an analytic function that is nowhere zero is analytic, as is the inverse of an invertible analytic function whose derivative is nowhere zero. (See also the Lagrange inversion theorem.)£#/li#£ £#li#£Any analytic function is smooth, that is, infinitely differentiable. The converse is not true for real functions; in fact, in a certain sense, the real analytic functions are sparse compared to all real infinitely differentiable functions. For the complex numbers, the converse does hold, and in fact any function differentiable once on an open set is analytic on that set (see "analyticity and differentiability" below).£#/li#£ £#li#£For any open set ${\displaystyle \Omega \subseteq \mathbb {C} }$ , the set A(Ω) of all analytic functions ${\displaystyle u\ :\ \Omega \to \mathbb {C} }$ is a Fréchet space with respect to the uniform convergence on compact sets. The fact that uniform limits on compact sets of analytic functions are analytic is an easy consequence of Morera's theorem. The set ${\displaystyle \scriptstyle A_{\infty }(\Omega )}$ of all bounded analytic functions with the supremum norm is a Banach space.£#/li#££#/ul#£
A polynomial cannot be zero at too many points unless it is the zero polynomial (more precisely, the number of zeros is at most the degree of the polynomial). A similar but weaker statement holds for analytic functions. If the set of zeros of an analytic function ƒ has an accumulation point inside its domain, then ƒ is zero everywhere on the connected component containing the accumulation point. In other words, if (rn) is a sequence of distinct numbers such that ƒ(rn) = 0 for all n and this sequence converges to a point r in the domain of D, then ƒ is identically zero on the connected component of D containing r. This is known as the identity theorem.

Also, if all the derivatives of an analytic function at a point are zero, the function is constant on the corresponding connected component.

These statements imply that while analytic functions do have more degrees of freedom than polynomials, they are still quite rigid.


£#h5#£Analyticity and differentiability£#/h5#£
As noted above, any analytic function (real or complex) is infinitely differentiable (also known as smooth, or ${\displaystyle {\mathcal {C}}^{\infty }}$ ). (Note that this differentiability is in the sense of real variables; compare complex derivatives below.) There exist smooth real functions that are not analytic: see non-analytic smooth function. In fact there are many such functions.

The situation is quite different when one considers complex analytic functions and complex derivatives. It can be proved that any complex function differentiable (in the complex sense) in an open set is analytic. Consequently, in complex analysis, the term analytic function is synonymous with holomorphic function.


£#h5#£Real versus complex analytic functions£#/h5#£
Real and complex analytic functions have important differences (one could notice that even from their different relationship with differentiability). Analyticity of complex functions is a more restrictive property, as it has more restrictive necessary conditions and complex analytic functions have more structure than their real-line counterparts.

According to Liouville's theorem, any bounded complex analytic function defined on the whole complex plane is constant. The corresponding statement for real analytic functions, with the complex plane replaced by the real line, is clearly false; this is illustrated by

${\displaystyle f(x)={\frac {1}{x^{2}+1}}.}$
Also, if a complex analytic function is defined in an open ball around a point x0, its power series expansion at x0 is convergent in the whole open ball (holomorphic functions are analytic). This statement for real analytic functions (with open ball meaning an open interval of the real line rather than an open disk of the complex plane) is not true in general; the function of the example above gives an example for x0 = 0 and a ball of radius exceeding 1, since the power series 1 − x2 + x4 − x6... diverges for |x| ≥ 1.

Any real analytic function on some open set on the real line can be extended to a complex analytic function on some open set of the complex plane. However, not every real analytic function defined on the whole real line can be extended to a complex function defined on the whole complex plane. The function ƒ(x) defined in the paragraph above is a counterexample, as it is not defined for x = ±i. This explains why the Taylor series of ƒ(x) diverges for |x| > 1, i.e., the radius of convergence is 1 because the complexified function has a pole at distance 1 from the evaluation point 0 and no further poles within the open disc of radius 1 around the evaluation point.


£#h5#£Analytic functions of several variables£#/h5#£
One can define analytic functions in several variables by means of power series in those variables (see power series). Analytic functions of several variables have some of the same properties as analytic functions of one variable. However, especially for complex analytic functions, new and interesting phenomena show up in 2 or more complex dimensions:

£#ul#££#li#£Zero sets of complex analytic functions in more than one variable are never discrete. This can be proved by Hartogs's extension theorem.£#/li#£ £#li#£Domains of holomorphy for single-valued functions consist of arbitrary (connected) open sets. In several complex variables, however, only some connected open sets are domains of holomorphy. The characterization of domains of holomorphy leads to the notion of pseudoconvexity.£#/li#££#/ul#£
£#h5#£See also£#/h5#£ £#ul#££#li#£Cauchy–Riemann equations£#/li#£ £#li#£Holomorphic function£#/li#£ £#li#£Paley–Wiener theorem£#/li#£ £#li#£Quasi-analytic function£#/li#£ £#li#£Infinite compositions of analytic functions£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Conway, John B. (1978). Functions of One Complex Variable I. Graduate Texts in Mathematics 11 (2nd ed.). Springer-Verlag. ISBN 978-0-387-90328-6.£#/li#£ £#li#£Krantz, Steven; Parks, Harold R. (2002). A Primer of Real Analytic Functions (2nd ed.). Birkhäuser. ISBN 0-8176-4264-1.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Analytic function", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#£ £#li#£Weisstein, Eric W. "Analytic Function". MathWorld.£#/li#£ £#li#£Solver for all zeros of a complex analytic function that lie within a rectangular region by Ivan B. Ivanov£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Knopp, K. "Analytic Continuation and Complete Definition of Analytic Functions." Ch. 8 in Theory of Functions Parts I and II, Two Volumes Bound as One, Part I. New York: Dover, pp. 83-111, 1996.£#/li#££#li#£Krantz, S. G. "Alternative Terminology for Holomorphic Functions." §1.3.6 in Handbook of Complex Variables. Boston, MA: Birkhäuser, p. 16, 1999.£#/li#££#li#£Morse, P. M. and Feshbach, H. "Analytic Functions." §4.2 in Methods of Theoretical Physics, Part I. New York: McGraw-Hill, pp. 356-374, 1953.£#/li#££#li#£Whittaker, E. T. and Watson, G. N. A Course in Modern Analysis, 4th ed. Cambridge, England: Cambridge University Press, 1990.£#/li#££#li#£ Knopp, K. "Analytic Continuation and Complete Definition of Analytic Functions." Ch. 8 in Theory of Functions Parts I and II, Two Volumes Bound as One, Part I. New York: Dover, pp. 83-111, 1996. £#/li#££#li#£ Krantz, S. G. "Alternative Terminology for Holomorphic Functions." §1.3.6 in Handbook of Complex Variables. Boston, MA: Birkhäuser, p. 16, 1999. £#/li#££#li#£ Morse, P. M. and Feshbach, H. "Analytic Functions." §4.2 in Methods of Theoretical Physics, Part I. New York: McGraw-Hill, pp. 356-374, 1953. £#/li#££#li#£ Whittaker, E. T. and Watson, G. N. A Course in Modern Analysis, 4th ed. Cambridge, England: Cambridge University Press, 1990. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Complex Analysis > General Complex Analysis £#/li#££#/ul#£




£#h3#£Analytic Torsion£#/h3#£

In mathematics, Reidemeister torsion (or R-torsion, or Reidemeister–Franz torsion) is a topological invariant of manifolds introduced by Kurt Reidemeister (Reidemeister 1935) for 3-manifolds and generalized to higher dimensions by Wolfgang Franz (1935) and Georges de Rham (1936). Analytic torsion (or Ray–Singer torsion) is an invariant of Riemannian manifolds defined by Daniel B. Ray and Isadore M. Singer (1971, 1973a, 1973b) as an analytic analogue of Reidemeister torsion. Jeff Cheeger (1977, 1979) and Werner Müller (1978) proved Ray and Singer's conjecture that Reidemeister torsion and analytic torsion are the same for compact Riemannian manifolds.

Reidemeister torsion was the first invariant in algebraic topology that could distinguish between closed manifolds which are homotopy equivalent but not homeomorphic, and can thus be seen as the birth of geometric topology as a distinct field. It can be used to classify lens spaces.

Reidemeister torsion is closely related to Whitehead torsion; see (Milnor 1966). It has also given some important motivation to arithmetic topology; see (Mazur). For more recent work on torsion see the books (Turaev 2002) and (Nicolaescu 2002, 2003).


£#h5#£Definition of analytic torsion£#/h5#£
If M is a Riemannian manifold and E a vector bundle over M, then there is a Laplacian operator acting on the i-forms with values in E. If the eigenvalues on i-forms are λj then the zeta function ζi is defined to be

${\displaystyle \zeta _{i}(s)=\sum _{\lambda _{j}>0}\lambda _{j}^{-s}}$
for s large, and this is extended to all complex s by analytic continuation. The zeta regularized determinant of the Laplacian acting on i-forms is

${\displaystyle \Delta _{i}=\exp(-\zeta _{i}^{\prime }(0))}$
which is formally the product of the positive eigenvalues of the laplacian acting on i-forms. The analytic torsion T(M,E) is defined to be

${\displaystyle T(M,E)=\exp \left(\sum _{i}(-1)^{i}i\zeta _{i}^{\prime }(0)/2\right)=\prod _{i}\Delta _{i}^{-(-1)^{i}i/2}.}$

£#h5#£Definition of Reidemeister torsion£#/h5#£
Let ${\displaystyle X}$ be a finite connected CW-complex with fundamental group ${\displaystyle \pi :=\pi _{1}(X)}$ and universal cover ${\displaystyle {\tilde {X}}}$ , and let ${\displaystyle U}$ be an orthogonal finite-dimensional ${\displaystyle \pi }$ -representation. Suppose that

${\displaystyle H_{n}^{\pi }(X;U):=H_{n}(U\otimes _{\mathbf {Z} [\pi ]}C_{*}({\tilde {X}}))=0}$
for all n. If we fix a cellular basis for ${\displaystyle C_{*}({\tilde {X}})}$ and an orthogonal ${\displaystyle \mathbf {R} }$ -basis for ${\displaystyle U}$ , then ${\displaystyle D_{*}:=U\otimes _{\mathbf {Z} [\pi ]}C_{*}({\tilde {X}})}$ is a contractible finite based free ${\displaystyle \mathbf {R} }$ -chain complex. Let ${\displaystyle \gamma _{*}:D_{*}\to D_{*+1}}$ be any chain contraction of D*, i.e. ${\displaystyle d_{n+1}\circ \gamma _{n}+\gamma _{n-1}\circ d_{n}=id_{D_{n}}}$ for all ${\displaystyle n}$ . We obtain an isomorphism ${\displaystyle (d_{*}+\gamma _{*})_{\text{odd}}:D_{\text{odd}}\to D_{\text{even}}}$ with ${\displaystyle D_{\text{odd}}:=\oplus _{n\,odd}\,D_{n}}$ , ${\displaystyle D_{\text{even}}:=\oplus _{n\,{\text{even}}}\,D_{n}}$ . We define the Reidemeister torsion

${\displaystyle \rho (X;U):=|\det(A)|^{-1}\in \mathbf {R} ^{>0}}$
where A is the matrix of ${\displaystyle (d_{*}+\gamma _{*})_{\text{odd}}}$ with respect to the given bases. The Reidemeister torsion ${\displaystyle \rho (X;U)}$ is independent of the choice of the cellular basis for ${\displaystyle C_{*}({\tilde {X}})}$ , the orthogonal basis for ${\displaystyle U}$ and the chain contraction ${\displaystyle \gamma _{*}}$ .

Let ${\displaystyle M}$ be a compact smooth manifold, and let ${\displaystyle \rho \colon \pi (M)\rightarrow GL(E)}$ be a unimodular representation. ${\displaystyle M}$ has a smooth triangulation. For any choice of a volume ${\displaystyle \mu \in \det H_{*}(M)}$ , we get an invariant ${\displaystyle \tau _{M}(\rho :\mu )\in \mathbf {R} ^{+}}$ . Then we call the positive real number ${\displaystyle \tau _{M}(\rho :\mu )}$ the Reidemeister torsion of the manifold ${\displaystyle M}$ with respect to ${\displaystyle \rho }$ and ${\displaystyle \mu }$ .


£#h5#£A short history of Reidemeister torsion£#/h5#£
Reidemeister torsion was first used to combinatorially classify 3-dimensional lens spaces in (Reidemeister 1935) by Reidemeister, and in higher-dimensional spaces by Franz. The classification includes examples of homotopy equivalent 3-dimensional manifolds which are not homeomorphic — at the time (1935) the classification was only up to PL homeomorphism, but later E.J. Brody (1960) showed that this was in fact a classification up to homeomorphism.

J. H. C. Whitehead defined the "torsion" of a homotopy equivalence between finite complexes. This is a direct generalization of the Reidemeister, Franz, and de Rham concept; but is a more delicate invariant. Whitehead torsion provides a key tool for the study of combinatorial or differentiable manifolds with nontrivial fundamental group and is closely related to the concept of "simple homotopy type", see (Milnor 1966)

In 1960 Milnor discovered the duality relation of torsion invariants of manifolds and show that the (twisted) Alexander polynomial of knots is the Reidemeister torsion of its knot complement in ${\displaystyle S^{3}}$ . (Milnor 1962) For each q the Poincaré duality ${\displaystyle P_{o}}$ induces

${\displaystyle P_{o}\colon \operatorname {det} (H_{q}(M)){\overset {\sim }{\,\longrightarrow \,}}(\operatorname {det} (H_{n-q}(M)))^{-1}}$
and then we obtain

${\displaystyle \Delta (t)=\pm t^{n}\Delta (1/t).}$
The representation of the fundamental group of knot complement plays a central role in them. It gives the relation between knot theory and torsion invariants.


£#h5#£Cheeger–Müller theorem£#/h5#£
Let ${\displaystyle (M,g)}$ be an orientable compact Riemann manifold of dimension n and ${\displaystyle \rho \colon \pi (M)\rightarrow \mathop {GL} (E)}$ a representation of the fundamental group of ${\displaystyle M}$ on a real vector space of dimension N. Then we can define the de Rham complex

${\displaystyle \Lambda ^{0}{\stackrel {d_{0}}{\longrightarrow }}\Lambda ^{1}{\stackrel {d_{1}}{\longrightarrow }}\cdots {\stackrel {d_{n-1}}{\longrightarrow }}\Lambda ^{n}}$
and the formal adjoint ${\displaystyle d_{p}}$ and ${\displaystyle \delta _{p}}$ due to the flatness of ${\displaystyle E_{q}}$ . As usual, we also obtain the Hodge Laplacian on p-forms

${\displaystyle \Delta _{p}=\delta _{p+1}d_{p}+d_{p-1}\delta _{p}.}$
Assuming that ${\displaystyle \partial M=0}$ , the Laplacian is then a symmetric positive semi-positive elliptic operator with pure point spectrum

${\displaystyle 0\leq \lambda _{0}\leq \lambda _{1}\leq \cdots \rightarrow \infty .}$
As before, we can therefore define a zeta function associated with the Laplacian ${\displaystyle \Delta _{q}}$ on ${\displaystyle \Lambda ^{q}(E)}$ by

${\displaystyle \zeta _{q}(s;\rho )=\sum _{\lambda _{j}>0}\lambda _{j}^{-s}={\frac {1}{\Gamma (s)}}\int _{0}^{\infty }t^{s-1}{\text{Tr}}(e^{-t\Delta _{q}}-P_{q})dt,\ \ \ {\text{Re}}(s)>{\frac {n}{2}}}$
where ${\displaystyle P}$ is the projection of ${\displaystyle L^{2}\Lambda (E)}$ onto the kernel space ${\displaystyle {\mathcal {H}}^{q}(E)}$ of the Laplacian ${\displaystyle \Delta _{q}}$ . It was moreover shown by (Seeley 1967) that ${\displaystyle \zeta _{q}(s;\rho )}$ extends to a meromorphic function of ${\displaystyle s\in \mathbf {C} }$ which is holomorphic at ${\displaystyle s=0}$ .

As in the case of an orthogonal representation, we define the analytic torsion ${\displaystyle T_{M}(\rho ;E)}$ by

${\displaystyle T_{M}(\rho ;E)=\exp {\biggl (}{\frac {1}{2}}\sum _{q=0}^{n}(-l)^{q}q{\frac {d}{ds}}\zeta _{q}(s;\rho ){\biggl |}_{s=0}{\biggr )}.}$
In 1971 D.B. Ray and I.M. Singer conjectured that ${\displaystyle T_{M}(\rho ;E)=\tau _{M}(\rho ;\mu )}$ for any unitary representation ${\displaystyle \rho }$ . This Ray–Singer conjecture was eventually proved, independently, by Cheeger (1977, 1979) and Müller (1978). Both approaches focus on the logarithm of torsions and their traces. This is easier for odd-dimensional manifolds than in the even-dimensional case, which involves additional technical difficulties. This Cheeger–Müller theorem (that the two notions of torsion are equivalent), along with Atiyah–Patodi–Singer theorem, later provided the basis for Chern–Simons perturbation theory.

A proof of the Cheeger-Müller theorem for arbitrary representations was later given by J. M. Bismut and Weiping Zhang. Their proof uses the Witten deformation.


£#h5#£References£#/h5#£ £#ul#££#li#£Bismut, J. -M.; Zhang, W. (1994-03-01), "Milnor and ray-singer metrics on the equivariant determinant of a flat vector bundle", Geometric & Functional Analysis GAFA, 4 (2): 136–212, doi:10.1007/BF01895837, ISSN 1420-8970, S2CID 121327250£#/li#£ £#li#£Brody, E. J. (1960), "The topological classification of the lens spaces", Annals of Mathematics, 2, 71 (1): 163–184, doi:10.2307/1969884, JSTOR 1969884, MR 0116336£#/li#£ £#li#£Cheeger, Jeff (1977), "Analytic Torsion and Reidemeister Torsion", Proceedings of the National Academy of Sciences of the United States of America, 74 (7): 2651–2654, Bibcode:1977PNAS...74.2651C, doi:10.1073/pnas.74.7.2651, MR 0451312, PMC 431228, PMID 16592411£#/li#£ £#li#£Cheeger, Jeff (1979), "Analytic torsion and the heat equation", Annals of Mathematics, 2, 109 (2): 259–322, doi:10.2307/1971113, JSTOR 1971113, MR 0528965£#/li#£ £#li#£Franz, Wolfgang (1935), "Ueber die Torsion einer Ueberdeckung", Journal für die reine und angewandte Mathematik, 173: 245–254£#/li#£ £#li#£Milnor, John (1962), "A duality theorem for Reidemeister torsion", Annals of Mathematics, 76 (1): 137–138, doi:10.2307/1970268, JSTOR 1970268£#/li#£ £#li#£Milnor, John (1966), "Whitehead torsion", Bulletin of the American Mathematical Society, 72 (3): 358–426, doi:10.1090/S0002-9904-1966-11484-2, MR 0196736£#/li#£ £#li#£Mishchenko, Aleksandr S. (2001) [1994], "Reidemeister torsion", Encyclopedia of Mathematics, EMS Press£#/li#£ £#li#£Müller, Werner (1978), "Analytic torsion and R-torsion of Riemannian manifolds", Advances in Mathematics, 28 (3): 233–305, doi:10.1016/0001-8708(78)90116-0, MR 0498252£#/li#£ £#li#£Nicolaescu, Liviu I. (2002), Notes on the Reidemeister torsion (PDF) Online book£#/li#£ £#li#£Nicolaescu, Liviu I. (2003), The Reidemeister torsion of 3-manifolds, de Gruyter Studies in Mathematics, vol. 30, Berlin: Walter de Gruyter & Co., pp. xiv+249, doi:10.1515/9783110198102, ISBN 3-11-017383-2, MR 1968575£#/li#£ £#li#£Ray, Daniel B.; Singer, Isadore M. (1973a), "Analytic torsion for complex manifolds.", Annals of Mathematics, 2, 98 (1): 154–177, doi:10.2307/1970909, JSTOR 1970909, MR 0383463£#/li#£ £#li#£Ray, Daniel B.; Singer, Isadore M. (1973b), "Analytic torsion.", Partial differential equations, Proc. Sympos. Pure Math., vol. XXIII, Providence, R.I.: Amer. Math. Soc., pp. 167–181, MR 0339293£#/li#£ £#li#£Ray, Daniel B.; Singer, Isadore M. (1971), "R-torsion and the Laplacian on Riemannian manifolds.", Advances in Mathematics, 7 (2): 145–210, doi:10.1016/0001-8708(71)90045-4, MR 0295381£#/li#£ £#li#£Reidemeister, Kurt (1935), "Homotopieringe und Linsenräume", Abh. Math. Sem. Univ. Hamburg, 11: 102–109, doi:10.1007/BF02940717, S2CID 124078064£#/li#£ £#li#£de Rham, Georges (1936), "Sur les nouveaux invariants topologiques de M. Reidemeister", Recueil Mathématique (Matematicheskii Sbornik), Nouvelle Série, 1 (5): 737–742, Zbl 0016.04501£#/li#£ £#li#£Turaev, Vladimir (2002), Torsions of 3-dimensional manifolds, Progress in Mathematics, vol. 208, Basel: Birkhäuser Verlag, pp. x+196, doi:10.1007/978-3-0348-7999-6, ISBN 3-7643-6911-6, MR 1958479£#/li#£ £#li#£Mazur, Barry. "Remarks on the Alexander polynomial" (PDF).£#/li#£ £#li#£Seeley, R. T. (1967), "Complex powers of an elliptic operator", in Calderón, Alberto P. (ed.), Singular Integrals (Proc. Sympos. Pure Math., Chicago, Ill., 1966), Proceedings of Symposia in Pure Mathematics, vol. 10, Providence, R.I.: Amer. Math. Soc., pp. 288–307, ISBN 978-0-8218-1410-9, MR 0237943£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Ray, D. B. and Singer, I. M. "R-Torsion and the Laplacian on Riemannian Manifolds." Adv. Math. 7, 145-210, 1971.£#/li#££#li#£Ray, D. B. and Singer, I. M. "Analytic Torsion for Complex Manifolds." Ann. Math., Second Series, 98, 154-177, 1973.£#/li#££#li#£ Ray, D. B. and Singer, I. M. "-Torsion and the Laplacian on Riemannian Manifolds." Adv. Math. 7, 145-210, 1971. £#/li#££#li#£ Ray, D. B. and Singer, I. M. "Analytic Torsion for Complex Manifolds." Ann. Math., Second Series, 98, 154-177, 1973. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Topology > Manifolds £#/li#££#li#£ Algebra > Homological Algebra £#/li#££#li#£ Topology > Algebraic Topology £#/li#££#li#£ Calculus and Analysis > Differential Geometry > General Differential Geometry £#/li#££#/ul#£




£#h3#£Andrew's Sine£#/h3#£

£#h5#£ References £#/h5#£ £#ul#££#li#£Press, W. H.; Flannery, B. P.; Teukolsky, S. A.; and Vetterling, W. T. Numerical Recipes in FORTRAN: The Art of Scientific Computing, 2nd ed. Cambridge, England: Cambridge University Press, p. 697, 1992.£#/li#££#li#£ Press, W. H.; Flannery, B. P.; Teukolsky, S. A.; and Vetterling, W. T. Numerical Recipes in FORTRAN: The Art of Scientific Computing, 2nd ed. Cambridge, England: Cambridge University Press, p. 697, 1992. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Miscellaneous Special Functions £#/li#££#/ul#£




£#h3#££#/h3#£




£#h3#£Andrews-Schur Identity£#/h3#£

£#h5#£ References £#/h5#£ £#ul#££#li#£Andrews, G. E. "A Polynomial Identity which Implies the Rogers-Ramanujan Identities." Scripta Math. 28, 297-305, 1970.£#/li#££#li#£Paule, P. "Short and Easy Computer Proofs of the Rogers-Ramanujan Identities and of Identities of Similar Type." Electronic J. Combinatorics 1, No. 1, R10, 1-9, 1994. http://www.combinatorics.org/Volume_1/Abstracts/v1i1r10.html.£#/li#££#li#£ Andrews, G. E. "A Polynomial Identity which Implies the Rogers-Ramanujan Identities." Scripta Math. 28, 297-305, 1970. £#/li#££#li#£ Paule, P. "Short and Easy Computer Proofs of the Rogers-Ramanujan Identities and of Identities of Similar Type." Electronic J. Combinatorics 1, No. 1, R10, 1-9, 1994. http://www.combinatorics.org/Volume_1/Abstracts/v1i1r10.html. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > q-Series £#/li#££#/ul#£




£#h3#£Anger Differential Equation£#/h3#£

In mathematics, the Anger function, introduced by C. T. Anger (1855), is a function defined as

${\displaystyle \mathbf {J} _{\nu }(z)={\frac {1}{\pi }}\int _{0}^{\pi }\cos(\nu \theta -z\sin \theta )\,d\theta }$
and is closely related to Bessel functions.

The Weber function (also known as Lommel–Weber function), introduced by H. F. Weber (1879), is a closely related function defined by

${\displaystyle \mathbf {E} _{\nu }(z)={\frac {1}{\pi }}\int _{0}^{\pi }\sin(\nu \theta -z\sin \theta )\,d\theta }$
and is closely related to Bessel functions of the second kind.


£#h5#£Relation between Weber and Anger functions£#/h5#£
The Anger and Weber functions are related by

${\displaystyle {\begin{aligned}\sin(\pi \nu )\mathbf {J} _{\nu }(z)&=\cos(\pi \nu )\mathbf {E} _{\nu }(z)-\mathbf {E} _{-\nu }(z),\\-\sin(\pi \nu )\mathbf {E} _{\nu }(z)&=\cos(\pi \nu )\mathbf {J} _{\nu }(z)-\mathbf {J} _{-\nu }(z),\end{aligned}}}$
so in particular if ν is not an integer they can be expressed as linear combinations of each other. If ν is an integer then Anger functions Jν are the same as Bessel functions Jν, and Weber functions can be expressed as finite linear combinations of Struve functions.


£#h5#£Power series expansion£#/h5#£
The Anger function has the power series expansion

${\displaystyle \mathbf {J} _{\nu }(z)=\cos {\frac {\pi \nu }{2}}\sum _{k=0}^{\infty }{\frac {(-1)^{k}z^{2k}}{4^{k}\Gamma \left(k+{\frac {\nu }{2}}+1\right)\Gamma \left(k-{\frac {\nu }{2}}+1\right)}}+\sin {\frac {\pi \nu }{2}}\sum _{k=0}^{\infty }{\frac {(-1)^{k}z^{2k+1}}{2^{2k+1}\Gamma \left(k+{\frac {\nu }{2}}+{\frac {3}{2}}\right)\Gamma \left(k-{\frac {\nu }{2}}+{\frac {3}{2}}\right)}}.}$
While the Weber function has the power series expansion

${\displaystyle \mathbf {E} _{\nu }(z)=\sin {\frac {\pi \nu }{2}}\sum _{k=0}^{\infty }{\frac {(-1)^{k}z^{2k}}{4^{k}\Gamma \left(k+{\frac {\nu }{2}}+1\right)\Gamma \left(k-{\frac {\nu }{2}}+1\right)}}-\cos {\frac {\pi \nu }{2}}\sum _{k=0}^{\infty }{\frac {(-1)^{k}z^{2k+1}}{2^{2k+1}\Gamma \left(k+{\frac {\nu }{2}}+{\frac {3}{2}}\right)\Gamma \left(k-{\frac {\nu }{2}}+{\frac {3}{2}}\right)}}.}$

£#h5#£Differential equations£#/h5#£
The Anger and Weber functions are solutions of inhomogeneous forms of Bessel's equation

${\displaystyle z^{2}y^{\prime \prime }+zy^{\prime }+(z^{2}-\nu ^{2})y=0.}$
More precisely, the Anger functions satisfy the equation

${\displaystyle z^{2}y^{\prime \prime }+zy^{\prime }+(z^{2}-\nu ^{2})y={\frac {(z-\nu )\sin(\pi \nu )}{\pi }},}$
and the Weber functions satisfy the equation

${\displaystyle z^{2}y^{\prime \prime }+zy^{\prime }+(z^{2}-\nu ^{2})y=-{\frac {z+\nu +(z-\nu )\cos(\pi \nu )}{\pi }}.}$

£#h5#£Recurrence relations£#/h5#£
The Anger function satisfies this inhomogeneous form of recurrence relation

${\displaystyle z\mathbf {J} _{\nu -1}(z)+z\mathbf {J} _{\nu +1}(z)=2\nu \mathbf {J} _{\nu }(z)-{\frac {2\sin \pi \nu }{\pi }}.}$
While the Weber function satisfies this inhomogeneous form of recurrence relation

${\displaystyle z\mathbf {E} _{\nu -1}(z)+z\mathbf {E} _{\nu +1}(z)=2\nu \mathbf {E} _{\nu }(z)-{\frac {2(1-\cos \pi \nu )}{\pi }}.}$

£#h5#£Delay differential equations£#/h5#£
The Anger and Weber functions satisfy these homogeneous forms of delay differential equations

${\displaystyle \mathbf {J} _{\nu -1}(z)-\mathbf {J} _{\nu +1}(z)=2{\dfrac {\partial }{\partial z}}\mathbf {J} _{\nu }(z),}$
${\displaystyle \mathbf {E} _{\nu -1}(z)-\mathbf {E} _{\nu +1}(z)=2{\dfrac {\partial }{\partial z}}\mathbf {E} _{\nu }(z).}$
The Anger and Weber functions also satisfy these inhomogeneous forms of delay differential equations

${\displaystyle z{\dfrac {\partial }{\partial z}}\mathbf {J} _{\nu }(z)\pm \nu \mathbf {J} _{\nu }(z)=\pm z\mathbf {J} _{\nu \mp 1}(z)\pm {\frac {\sin \pi \nu }{\pi }},}$
${\displaystyle z{\dfrac {\partial }{\partial z}}\mathbf {E} _{\nu }(z)\pm \nu \mathbf {E} _{\nu }(z)=\pm z\mathbf {E} _{\nu \mp 1}(z)\pm {\frac {1-\cos \pi \nu }{\pi }}.}$

£#h5#£References£#/h5#£ £#ul#££#li#£Abramowitz, Milton; Stegun, Irene Ann, eds. (1983) [June 1964]. "Chapter 12". Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables. Applied Mathematics Series. Vol. 55 (Ninth reprint with additional corrections of tenth original printing with corrections (December 1972); first ed.). Washington D.C.; New York: United States Department of Commerce, National Bureau of Standards; Dover Publications. p. 498. ISBN 978-0-486-61272-0. LCCN 64-60036. MR 0167642. LCCN 65-12253.£#/li#£ £#li#£C.T. Anger, Neueste Schr. d. Naturf. d. Ges. i. Danzig, 5 (1855) pp. 1–29£#/li#£ £#li#£Prudnikov, A.P. (2001) [1994], "Anger function", Encyclopedia of Mathematics, EMS Press£#/li#£ £#li#£Prudnikov, A.P. (2001) [1994], "Weber function", Encyclopedia of Mathematics, EMS Press£#/li#£ £#li#£G.N. Watson, "A treatise on the theory of Bessel functions", 1–2, Cambridge Univ. Press (1952)£#/li#£ £#li#£H.F. Weber, Zurich Vierteljahresschrift, 24 (1879) pp. 33–76£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Abramowitz, M. and Stegun, I. A. (Eds.). "Anger and Weber Functions." §12.3 in Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing. New York: Dover, pp. 498-499, 1972.£#/li#££#li#£Gradshteyn, I. S. and Ryzhik, I. M. Tables of Integrals, Series, and Products, 6th ed. San Diego, CA: Academic Press, p. 989, 2000.£#/li#££#li#£Zwillinger, D. Handbook of Differential Equations, 3rd ed. Boston, MA: Academic Press, p. 121, 1997.£#/li#££#li#£ Abramowitz, M. and Stegun, I. A. (Eds.). "Anger and Weber Functions." §12.3 in Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing. New York: Dover, pp. 498-499, 1972. £#/li#££#li#£ Gradshteyn, I. S. and Ryzhik, I. M. Tables of Integrals, Series, and Products, 6th ed. San Diego, CA: Academic Press, p. 989, 2000. £#/li#££#li#£ Zwillinger, D. Handbook of Differential Equations, 3rd ed. Boston, MA: Academic Press, p. 121, 1997. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Differential Equations > Ordinary Differential Equations £#/li#££#/ul#£




£#h3#£Anger Function£#/h3#£

In mathematics, the Anger function, introduced by C. T. Anger (1855), is a function defined as

${\displaystyle \mathbf {J} _{\nu }(z)={\frac {1}{\pi }}\int _{0}^{\pi }\cos(\nu \theta -z\sin \theta )\,d\theta }$
and is closely related to Bessel functions.

The Weber function (also known as Lommel–Weber function), introduced by H. F. Weber (1879), is a closely related function defined by

${\displaystyle \mathbf {E} _{\nu }(z)={\frac {1}{\pi }}\int _{0}^{\pi }\sin(\nu \theta -z\sin \theta )\,d\theta }$
and is closely related to Bessel functions of the second kind.


£#h5#£Relation between Weber and Anger functions£#/h5#£
The Anger and Weber functions are related by

${\displaystyle {\begin{aligned}\sin(\pi \nu )\mathbf {J} _{\nu }(z)&=\cos(\pi \nu )\mathbf {E} _{\nu }(z)-\mathbf {E} _{-\nu }(z),\\-\sin(\pi \nu )\mathbf {E} _{\nu }(z)&=\cos(\pi \nu )\mathbf {J} _{\nu }(z)-\mathbf {J} _{-\nu }(z),\end{aligned}}}$
so in particular if ν is not an integer they can be expressed as linear combinations of each other. If ν is an integer then Anger functions Jν are the same as Bessel functions Jν, and Weber functions can be expressed as finite linear combinations of Struve functions.


£#h5#£Power series expansion£#/h5#£
The Anger function has the power series expansion

${\displaystyle \mathbf {J} _{\nu }(z)=\cos {\frac {\pi \nu }{2}}\sum _{k=0}^{\infty }{\frac {(-1)^{k}z^{2k}}{4^{k}\Gamma \left(k+{\frac {\nu }{2}}+1\right)\Gamma \left(k-{\frac {\nu }{2}}+1\right)}}+\sin {\frac {\pi \nu }{2}}\sum _{k=0}^{\infty }{\frac {(-1)^{k}z^{2k+1}}{2^{2k+1}\Gamma \left(k+{\frac {\nu }{2}}+{\frac {3}{2}}\right)\Gamma \left(k-{\frac {\nu }{2}}+{\frac {3}{2}}\right)}}.}$
While the Weber function has the power series expansion

${\displaystyle \mathbf {E} _{\nu }(z)=\sin {\frac {\pi \nu }{2}}\sum _{k=0}^{\infty }{\frac {(-1)^{k}z^{2k}}{4^{k}\Gamma \left(k+{\frac {\nu }{2}}+1\right)\Gamma \left(k-{\frac {\nu }{2}}+1\right)}}-\cos {\frac {\pi \nu }{2}}\sum _{k=0}^{\infty }{\frac {(-1)^{k}z^{2k+1}}{2^{2k+1}\Gamma \left(k+{\frac {\nu }{2}}+{\frac {3}{2}}\right)\Gamma \left(k-{\frac {\nu }{2}}+{\frac {3}{2}}\right)}}.}$

£#h5#£Differential equations£#/h5#£
The Anger and Weber functions are solutions of inhomogeneous forms of Bessel's equation

${\displaystyle z^{2}y^{\prime \prime }+zy^{\prime }+(z^{2}-\nu ^{2})y=0.}$
More precisely, the Anger functions satisfy the equation

${\displaystyle z^{2}y^{\prime \prime }+zy^{\prime }+(z^{2}-\nu ^{2})y={\frac {(z-\nu )\sin(\pi \nu )}{\pi }},}$
and the Weber functions satisfy the equation

${\displaystyle z^{2}y^{\prime \prime }+zy^{\prime }+(z^{2}-\nu ^{2})y=-{\frac {z+\nu +(z-\nu )\cos(\pi \nu )}{\pi }}.}$

£#h5#£Recurrence relations£#/h5#£
The Anger function satisfies this inhomogeneous form of recurrence relation

${\displaystyle z\mathbf {J} _{\nu -1}(z)+z\mathbf {J} _{\nu +1}(z)=2\nu \mathbf {J} _{\nu }(z)-{\frac {2\sin \pi \nu }{\pi }}.}$
While the Weber function satisfies this inhomogeneous form of recurrence relation

${\displaystyle z\mathbf {E} _{\nu -1}(z)+z\mathbf {E} _{\nu +1}(z)=2\nu \mathbf {E} _{\nu }(z)-{\frac {2(1-\cos \pi \nu )}{\pi }}.}$

£#h5#£Delay differential equations£#/h5#£
The Anger and Weber functions satisfy these homogeneous forms of delay differential equations

${\displaystyle \mathbf {J} _{\nu -1}(z)-\mathbf {J} _{\nu +1}(z)=2{\dfrac {\partial }{\partial z}}\mathbf {J} _{\nu }(z),}$
${\displaystyle \mathbf {E} _{\nu -1}(z)-\mathbf {E} _{\nu +1}(z)=2{\dfrac {\partial }{\partial z}}\mathbf {E} _{\nu }(z).}$
The Anger and Weber functions also satisfy these inhomogeneous forms of delay differential equations

${\displaystyle z{\dfrac {\partial }{\partial z}}\mathbf {J} _{\nu }(z)\pm \nu \mathbf {J} _{\nu }(z)=\pm z\mathbf {J} _{\nu \mp 1}(z)\pm {\frac {\sin \pi \nu }{\pi }},}$
${\displaystyle z{\dfrac {\partial }{\partial z}}\mathbf {E} _{\nu }(z)\pm \nu \mathbf {E} _{\nu }(z)=\pm z\mathbf {E} _{\nu \mp 1}(z)\pm {\frac {1-\cos \pi \nu }{\pi }}.}$

£#h5#£References£#/h5#£ £#ul#££#li#£Abramowitz, Milton; Stegun, Irene Ann, eds. (1983) [June 1964]. "Chapter 12". Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables. Applied Mathematics Series. Vol. 55 (Ninth reprint with additional corrections of tenth original printing with corrections (December 1972); first ed.). Washington D.C.; New York: United States Department of Commerce, National Bureau of Standards; Dover Publications. p. 498. ISBN 978-0-486-61272-0. LCCN 64-60036. MR 0167642. LCCN 65-12253.£#/li#£ £#li#£C.T. Anger, Neueste Schr. d. Naturf. d. Ges. i. Danzig, 5 (1855) pp. 1–29£#/li#£ £#li#£Prudnikov, A.P. (2001) [1994], "Anger function", Encyclopedia of Mathematics, EMS Press£#/li#£ £#li#£Prudnikov, A.P. (2001) [1994], "Weber function", Encyclopedia of Mathematics, EMS Press£#/li#£ £#li#£G.N. Watson, "A treatise on the theory of Bessel functions", 1–2, Cambridge Univ. Press (1952)£#/li#£ £#li#£H.F. Weber, Zurich Vierteljahresschrift, 24 (1879) pp. 33–76£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Abramowitz, M. and Stegun, I. A. (Eds.). "Anger and Weber Functions." §12.3 in Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing. New York: Dover, pp. 498-499, 1972.£#/li#££#li#£Prudnikov, A. P.; Marichev, O. I.; and Brychkov, Yu. A. "The Anger Function J_nu(x) and Weber Function E_nu(x)." §1.5 in Integrals and Series, Vol. 3: More Special Functions. Newark, NJ: Gordon and Breach, p. 28, 1990.£#/li#££#li#£Watson, G. N. A Treatise on the Theory of Bessel Functions, 2nd ed. Cambridge, England: Cambridge University Press, 1966.£#/li#££#li#£ Abramowitz, M. and Stegun, I. A. (Eds.). "Anger and Weber Functions." §12.3 in Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing. New York: Dover, pp. 498-499, 1972. £#/li#££#li#£ Prudnikov, A. P.; Marichev, O. I.; and Brychkov, Yu. A. "The Anger Function and Weber Function ." §1.5 in Integrals and Series, Vol. 3: More Special Functions. Newark, NJ: Gordon and Breach, p. 28, 1990. £#/li#££#li#£ Watson, G. N. A Treatise on the Theory of Bessel Functions, 2nd ed. Cambridge, England: Cambridge University Press, 1966. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Bessel Functions £#/li#££#li#£ Calculus and Analysis > Complex Analysis > Entire Functions £#/li#££#li#£ Calculus and Analysis > Calculus > Integrals > Definite Integrals £#/li#££#/ul#£




£#h3#£Angle-Preserving Transformation£#/h3#£

In mathematics, a conformal map is a function that locally preserves angles, but not necessarily lengths.

More formally, let ${\displaystyle U}$ and ${\displaystyle V}$ be open subsets of ${\displaystyle \mathbb {R} ^{n}}$ . A function ${\displaystyle f:U\to V}$ is called conformal (or angle-preserving) at a point ${\displaystyle u_{0}\in U}$ if it preserves angles between directed curves through ${\displaystyle u_{0}}$ , as well as preserving orientation. Conformal maps preserve both angles and the shapes of infinitesimally small figures, but not necessarily their size or curvature.

The conformal property may be described in terms of the Jacobian derivative matrix of a coordinate transformation. The transformation is conformal whenever the Jacobian at each point is a positive scalar times a rotation matrix (orthogonal with determinant one). Some authors define conformality to include orientation-reversing mappings whose Jacobians can be written as any scalar times any orthogonal matrix.

For mappings in two dimensions, the (orientation-preserving) conformal mappings are precisely the locally invertible complex analytic functions. In three and higher dimensions, Liouville's theorem sharply limits the conformal mappings to a few types.

The notion of conformality generalizes in a natural way to maps between Riemannian or semi-Riemannian manifolds.


£#h5#£Conformal maps in two dimensions£#/h5#£
If ${\displaystyle U}$ is an open subset of the complex plane ${\displaystyle \mathbb {C} }$ , then a function ${\displaystyle f:U\to \mathbb {C} }$ is conformal if and only if it is holomorphic and its derivative is everywhere non-zero on ${\displaystyle U}$ . If ${\displaystyle f}$ is antiholomorphic (conjugate to a holomorphic function), it preserves angles but reverses their orientation.

In the literature, there is another definition of conformal: a mapping ${\displaystyle f}$ which is one-to-one and holomorphic on an open set in the plane. The open mapping theorem forces the inverse function (defined on the image of ${\displaystyle f}$ ) to be holomorphic. Thus, under this definition, a map is conformal if and only if it is biholomorphic. The two definitions for conformal maps are not equivalent. Being one-to-one and holomorphic implies having a non-zero derivative. However, the exponential function is a holomorphic function with a nonzero derivative, but is not one-to-one since it is periodic.

The Riemann mapping theorem, one of the profound results of complex analysis, states that any non-empty open simply connected proper subset of ${\displaystyle \mathbb {C} }$ admits a bijective conformal map to the open unit disk in ${\displaystyle \mathbb {C} }$ .


£#h5#£Global conformal maps on the Riemann sphere£#/h5#£
A map of the Riemann sphere onto itself is conformal if and only if it is a Möbius transformation.

The complex conjugate of a Möbius transformation preserves angles, but reverses the orientation. For example, circle inversions.


£#h5#£Conformal maps in three or more dimensions£#/h5#£
£#h5#£Riemannian geometry£#/h5#£
In Riemannian geometry, two Riemannian metrics ${\displaystyle g}$ and ${\displaystyle h}$ on a smooth manifold ${\displaystyle M}$ are called conformally equivalent if ${\displaystyle g=uh}$ for some positive function ${\displaystyle u}$ on ${\displaystyle M}$ . The function ${\displaystyle u}$ is called the conformal factor.

A diffeomorphism between two Riemannian manifolds is called a conformal map if the pulled back metric is conformally equivalent to the original one. For example, stereographic projection of a sphere onto the plane augmented with a point at infinity is a conformal map.

One can also define a conformal structure on a smooth manifold, as a class of conformally equivalent Riemannian metrics.


£#h5#£Euclidean space£#/h5#£
A classical theorem of Joseph Liouville shows that there are much fewer conformal maps in higher dimensions than in two dimensions. Any conformal map from an open subset of Euclidean space into the same Euclidean space of dimension three or greater can be composed from three types of transformations: a homothety, an isometry, and a special conformal transformation.


£#h5#£Applications£#/h5#£
£#h5#£Cartography£#/h5#£
In cartography, several named map projections, including the Mercator projection and the stereographic projection are conformal. They are specially useful for use in marine navigation because of its unique property of representing any course of constant bearing as a straight segment. Such a course, known as a rhumb (or, mathematically, a loxodrome) is preferred in marine navigation because ships can sail in a constant compass direction.


£#h5#£Physics and engineering£#/h5#£
Conformal mappings are invaluable for solving problems in engineering and physics that can be expressed in terms of functions of a complex variable yet exhibit inconvenient geometries. By choosing an appropriate mapping, the analyst can transform the inconvenient geometry into a much more convenient one. For example, one may wish to calculate the electric field, ${\displaystyle E(z)}$ , arising from a point charge located near the corner of two conducting planes separated by a certain angle (where ${\displaystyle z}$ is the complex coordinate of a point in 2-space). This problem per se is quite clumsy to solve in closed form. However, by employing a very simple conformal mapping, the inconvenient angle is mapped to one of precisely ${\displaystyle \pi }$ radians, meaning that the corner of two planes is transformed to a straight line. In this new domain, the problem (that of calculating the electric field impressed by a point charge located near a conducting wall) is quite easy to solve. The solution is obtained in this domain, ${\displaystyle E(w)}$ , and then mapped back to the original domain by noting that ${\displaystyle w}$ was obtained as a function (viz., the composition of ${\displaystyle E}$ and ${\displaystyle w}$ ) of ${\displaystyle z}$ , whence ${\displaystyle E(w)}$ can be viewed as ${\displaystyle E(w(z))}$ , which is a function of ${\displaystyle z}$ , the original coordinate basis. Note that this application is not a contradiction to the fact that conformal mappings preserve angles, they do so only for points in the interior of their domain, and not at the boundary. Another example is the application of conformal mapping technique for solving the boundary value problem of liquid sloshing in tanks.

If a function is harmonic (that is, it satisfies Laplace's equation ${\displaystyle \nabla ^{2}f=0}$ ) over a plane domain (which is two-dimensional), and is transformed via a conformal map to another plane domain, the transformation is also harmonic. For this reason, any function which is defined by a potential can be transformed by a conformal map and still remain governed by a potential. Examples in physics of equations defined by a potential include the electromagnetic field, the gravitational field, and, in fluid dynamics, potential flow, which is an approximation to fluid flow assuming constant density, zero viscosity, and irrotational flow. One example of a fluid dynamic application of a conformal map is the Joukowsky transform.

Conformal maps are also valuable in solving nonlinear partial differential equations in some specific geometries. Such analytic solutions provide a useful check on the accuracy of numerical simulations of the governing equation. For example, in the case of very viscous free-surface flow around a semi-infinite wall, the domain can be mapped to a half-plane in which the solution is one-dimensional and straightforward to calculate.

For discrete systems, Noury and Yang presented a way to convert discrete systems root locus into continuous root locus through a well-know conformal mapping in geometry (aka inversion mapping).


£#h5#£Maxwell's equations£#/h5#£
A large group of conformal maps for relating solutions of Maxwell's equations was identified by Ebenezer Cunningham (1908) and Harry Bateman (1910). Their training at Cambridge University had given them facility with the method of image charges and associated methods of images for spheres and inversion. As recounted by Andrew Warwick (2003) Masters of Theory:

Each four-dimensional solution could be inverted in a four-dimensional hyper-sphere of pseudo-radius ${\displaystyle K}$ in order to produce a new solution.
Warwick highlights this "new theorem of relativity" as a Cambridge response to Einstein, and as founded on exercises using the method of inversion, such as found in James Hopwood Jeans textbook Mathematical Theory of Electricity and Magnetism.


£#h5#£General relativity£#/h5#£
In general relativity, conformal maps are the simplest and thus most common type of causal transformations. Physically, these describe different universes in which all the same events and interactions are still (causally) possible, but a new additional force is necessary to effect this (that is, replication of all the same trajectories would necessitate departures from geodesic motion because the metric tensor is different). It is often used to try to make models amenable to extension beyond curvature singularities, for example to permit description of the universe even before the Big Bang.


£#h5#£See also£#/h5#£ £#ul#££#li#£Biholomorphic map£#/li#£ £#li#£Carathéodory's theorem – A conformal map extends continuously to the boundary£#/li#£ £#li#£Penrose diagram£#/li#£ £#li#£Schwarz–Christoffel mapping – a conformal transformation of the upper half-plane onto the interior of a simple polygon£#/li#£ £#li#£Special linear group – transformations that preserve volume (as opposed to angles) and orientation£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£Further reading£#/h5#£ £#ul#££#li#£Ahlfors, Lars V. (1973), Conformal invariants: topics in geometric function theory, New York: McGraw–Hill Book Co., MR 0357743£#/li#£ £#li#£Constantin Carathéodory (1932) Conformal Representation, Cambridge Tracts in Mathematics and Physics£#/li#£ £#li#£Chanson, H. (2009), Applied Hydrodynamics: An Introduction to Ideal and Real Fluid Flows, CRC Press, Taylor & Francis Group, Leiden, The Netherlands, 478 pages, ISBN 978-0-415-49271-3£#/li#£ £#li#£Churchill, Ruel V. (1974), Complex Variables and Applications, New York: McGraw–Hill Book Co., ISBN 978-0-07-010855-4£#/li#£ £#li#£E.P. Dolzhenko (2001) [1994], "Conformal mapping", Encyclopedia of Mathematics, EMS Press£#/li#£ £#li#£Rudin, Walter (1987), Real and complex analysis (3rd ed.), New York: McGraw–Hill Book Co., ISBN 978-0-07-054234-1, MR 0924157£#/li#£ £#li#£Weisstein, Eric W. "Conformal Mapping". MathWorld.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Interactive visualizations of many conformal maps£#/li#£ £#li#£Conformal Maps by Michael Trott, Wolfram Demonstrations Project.£#/li#£ £#li#£Conformal Mapping images of current flow in different geometries without and with magnetic field by Gerhard Brunthaler.£#/li#£ £#li#£Conformal Transformation: from Circle to Square.£#/li#£ £#li#£Online Conformal Map Grapher.£#/li#£ £#li#£Joukowski Transform Interactive WebApp£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Complex Analysis > Conformal Mapping £#/li#££#/ul#£




£#h3#£Angle Bracket£#/h3#£

A bracket is either of two tall fore- or back-facing punctuation marks commonly used to isolate a segment of text or data from its surroundings. Typically deployed in symmetric pairs, an individual bracket may be identified as a left or right bracket or, alternatively, an opening bracket or closing bracket, respectively, depending on the directionality of the context.

Specific forms of the mark include rounded brackets (also called parentheses), square bracket, curly brackets (also called braces), and angle brackets (also called chevrons), as well as various less common pairs of symbols.

As well as signifying the overall class of punctuation, the word bracket is commonly used to refer to a specific form of bracket, which varies from region to region. In most English-speaking countries, an unqualified 'bracket' refers to the round bracket; in the United States, the square bracket.

Various forms of brackets are used in mathematics, with specific mathematical meanings, often for denoting specific mathematical functions and subformulas.


£#h5#£History£#/h5#£
Chevrons ⟨ ⟩ were the earliest type of bracket to appear in written English. Desiderius Erasmus Roterodamus coined the term lunula to refer to the rounded parentheses ( ) recalling the shape of the crescent moon (Latin: luna).

Most typewriters only had the left and right parenthesis (and quotation marks). Square brackets appeared with some teleprinters.

Braces (curly brackets) first became part of a character set with the 8-bit code of the IBM 7030 Stretch.

In 1961, ASCII contained parenthesis, square, and curly brackets, and also less-than and greater-than signs that could be used as angle brackets.


£#h5#£Typography£#/h5#£
In English, typographers mostly prefer not to set brackets in italics, even when the enclosed text is italic. However, in other languages like German, if brackets enclose text in italics, they are usually also set in italics.


£#h5#£Parentheses £#/h5#£
Parentheses (singular, parenthesis ) are also called "brackets" (UK, Ireland, Canada, West Indies, New Zealand, South Africa and Australia), "parens" , "round brackets", "circle brackets" or "smooth brackets".


£#h5#£Uses of ( )£#/h5#£
Parentheses contain adjunctive material that serves to clarify (in the manner of a gloss) or is aside from the main point. A milder effect may be obtained by using a pair of commas as the delimiter, though if the sentence contains commas for other purposes, visual confusion may result. That issue is fixed by using a pair of dashes instead, to bracket the parenthetical.

In American usage, parentheses are usually considered separate from other brackets, and calling them "brackets" is unusual.

Parentheses may be used in formal writing to add supplementary information, such as "Senator John McCain (R - Arizona) spoke at length". They can also indicate shorthand for "either singular or plural" for nouns, e.g. "the claim(s)". It can also be used for gender neutral language, especially in languages with grammatical gender, e.g. "(s)he agreed with his/her physician" (the slash in the second instance, as one alternative is replacing the other, not adding to it).

Parenthetical phrases have been used extensively in informal writing and stream of consciousness literature. Examples include the southern American author William Faulkner (see Absalom, Absalom! and the Quentin section of The Sound and the Fury) as well as poet E. E. Cummings.

Parentheses have historically been used where the dash is currently used in alternatives, such as "parenthesis)(parentheses". Examples of this usage can be seen in editions of Fowler's.

Parentheses may be nested (generally with one set (such as this) inside another set). This is not commonly used in formal writing (though sometimes other brackets [especially square brackets] will be used for one or more inner set of parentheses [in other words, secondary {or even tertiary} phrases can be found within the main parenthetical sentence]).

Any punctuation inside parentheses or other brackets is independent of the rest of the text: "Mrs. Pennyfarthing (What? Yes, that was her name!) was my landlady." In this use, the explanatory text in the parentheses is a parenthesis. Parenthesized text is usually short and within a single sentence. Where several sentences of supplemental material are used in parentheses the final full stop would be within the parentheses, or simply omitted. Again, the parenthesis implies that the meaning and flow of the text is supplemental to the rest of the text and the whole would be unchanged were the parenthesized sentences removed.

In more formal usage, "parenthesis" may refer to the entire bracketed text, not just to the punctuation marks used (so all the text in this set of round brackets may be said to be "a parenthesis", "a parenthetical", or "a parenthetical phrase").

In linguistics, parentheses are used for indistinguishable or unidentified utterances. They are also seen for silent articulation (mouthing), where the expected phonetic transcription is derived from lip-reading, and with periods to indicate silent pauses, for example (…) or (2 sec).

In Mathematica and the Wolfram language, parentheses are used to indicate grouping for example with pure anonymous functions.


£#h5#£Enumerations£#/h5#£
An unpaired right parenthesis is often used as part of a label in an ordered list:

a) educational testing,
b) technical writing and diagrams,
c) market research, and
d) elections.


£#h5#£Accounting£#/h5#£
Traditionally in accounting, contra amounts are placed in parentheses. A debit balance account in a series of credit balances will have parenthesis and vice versa.


£#h5#£Parentheses in mathematics£#/h5#£
Parentheses are used in mathematical notation to indicate grouping, often inducing a different order of operations. For example: in the usual order of algebraic operations, 4 × 3 + 2 equals 14, since the multiplication is done before the addition. However, 4 × (3 + 2) equals 20, because the parentheses override normal precedence, causing the addition to be done first. Some authors follow the convention in mathematical equations that, when parentheses have one level of nesting, the inner pair are parentheses and the outer pair are square brackets. Example:

${\displaystyle [4\times (3+2)]^{2}=400.}$
A related convention is that when parentheses have two levels of nesting, curly brackets (braces) are the outermost pair. Following this convention, when more than three levels of nesting are needed, often a cycle of parentheses, square brackets, and curly brackets will continue. This helps to distinguish between one such level and the next.

Various notations, like the vinculum, have a similar effect in specifying order of operations, or otherwise grouping several characters together for a common purpose.

Parentheses are also used to set apart the arguments in mathematical functions. For example, f(x) is the function f applied to the variable x. In coordinate systems parentheses are used to denote a set of coordinates; so in the Cartesian coordinate system (4, 7) may represent the point located at 4 on the x-axis and 7 on the y-axis.

Parentheses may be used exclusively or in combination with square brackets to represent intervals.

Parentheses may be used to represent a binomial coefficient, and also matrices.


£#h5#£Parentheses in programming languages£#/h5#£
Parentheses are included in the syntaxes of many programming languages. Typically needed to denote an argument; to tell the compiler what data type the Method/Function needs to look for first in order to initialise. In some cases, such as in LISP, parentheses are a fundamental construct of the language. They are also often used for scoping functions and operators and for arrays. In syntax diagrams they are used for grouping, such as in extended Backus–Naur form.


£#h5#£Taxonomy£#/h5#£
If it is desired to include the subgenus when giving the scientific name of an animal species or subspecies, the subgenus's name is provided in parentheses between the genus name and the specific epithet. For instance, Polyphylla (Xerasiobia) alba is a way to cite the species Polyphylla alba while also mentioning that it's in the subgenus Xerasiobia. There is also a convention of citing a subgenus by enclosing it in parentheses after its genus, e.g., Polyphylla (Xerasiobia) is a way to refer to the subgenus Xerasiobia within the genus Polyphylla. Parentheses are similarly used to cite a subgenus with the name of a prokaryotic species, although the International Code of Nomenclature of Prokaryotes (ICNP) requires the use of the abbreviation "subgen." as well, e.g., Acetobacter (subgen. Gluconoacetobacter) liquefaciens.

In some contexts, it is typical to cite the author's name alongside the taxon. In these contexts, parentheses mean that the author placed that species in a different genus from the one in that combination. The International Code of Zoological Nomenclature gives the example of Hymenolepis diminuta (Rudolphi, 1819) to indicate that Karl Rudolphi did not consider this species to be in the genus Hymenolepis when he first described the species. The author citation in zoology also allows the possibility of citing whoever transferred the species to the new genus, as in, Methiolopsis geniculata (Stål, 1878) Rehn, 1957. Parentheses are similarly used for new combinations of prokaryotes as well; the ICNP provides the example: Microbacterium oxydans (Chatelain and Second 1966) Schumann et al. 1999 to indicate that Chatelain and Second first described the species in a different genus, namely Brevibacterium, but in 1999 Schumann and colleagues transferred it to its present genus. Author citations in botany also use parentheses in this way where the author (or abbreviation thereof) of the basionym is in parentheses followed by the author (or abbreviation thereof) of whoever created that particular combination; the International Code of Nomenclature for algae, fungi, and plants provides the example Helianthemum aegyptiacum (L.) Mill. to indicate that Carl Linnaeus first described this species in a different genus, in this case Cistus, but then Philip Miller transferred it to the genus Helianthemum.


£#h5#£Chemistry and physics£#/h5#£
Parentheses are used in chemistry to denote a repeated substructure within a molecule, e.g. HC(CH3)3 (isobutane) or, similarly, to indicate the stoichiometry of ionic compounds with such substructures: e.g. Ca(NO3)2 (calcium nitrate).

They can be used in various fields as notation to indicate the amount of uncertainty in a numerical quantity. For example:

1234.56789(11)
is equivalent to:

1234.56789 ± 0.00011
e.g. the value of the Boltzmann constant could be quoted as 1.38064852(79)×10−23 J⋅K−1 .


£#h5#£Square brackets £#/h5#£
Square brackets [ and ] are also called simply "brackets" (US), as well as "crotchets", "closed brackets", or "hard brackets".

Tournament brackets, the diagrammatic representation of the series of games played during a sports tournament usually leading to a single winner, are so named for their resemblance to brackets or braces.


£#h5#£Uses of [ ]£#/h5#£
Square brackets are often used to insert explanatory material or to mark where a [word or] passage was omitted from an original material by someone other than the original author, or to mark modifications in quotations. In transcribed interviews, sounds, responses and reactions that are not words but that can be described are set off in square brackets — "... [laughs] ...".

When quoted material is in any way altered, the alterations are enclosed in square brackets within the quotation to show that the quotation is not exactly as given, or to add an annotation. For example: The Plaintiff asserted his cause is just, stating,

[m]y causes is [sic] just.

In the original quoted sentence, the word "my" was capitalized: it has been modified in the quotation given and the change signalled with brackets. Similarly, where the quotation contained a grammatical error (is/are), the quoting author signalled that the error was in the original with "[sic]" (Latin for 'thus').

A bracketed ellipsis, [...], is often used to indicate omitted material: "I'd like to thank [several unimportant people] for their tolerance [...]" Bracketed comments inserted into a quote indicate where the original has been modified for clarity: "I appreciate it [the honor], but I must refuse", and "the future of psionics [see definition] is in doubt". Or one can quote the original statement "I hate to do laundry" with a (sometimes grammatical) modification inserted: He "hate[s] to do laundry".

Additionally, a small letter can be replaced by a capital one, when the beginning of the original printed text is being quoted in another piece of text or when the original text has been omitted for succinctness— for example, when referring to a verbose original: "To the extent that policymakers and elite opinion in general have made use of economic analysis at all, they have, as the saying goes, done so the way a drunkard uses a lamppost: for support, not illumination", can be quoted succinctly as: "[P]olicymakers [...] have made use of economic analysis [...] the way a drunkard uses a lamppost: for support, not illumination." When nested parentheses are needed, brackets are sometimes used as a substitute for the inner pair of parentheses within the outer pair. When deeper levels of nesting are needed, convention is to alternate between parentheses and brackets at each level.

Alternatively, empty square brackets can also indicate omitted material, usually single letter only. The original, "Reading is also a process and it also changes you." can be rewritten in a quote as: It has been suggested that reading can "also change[] you".

In translated works, brackets are used to signify the same word or phrase in the original language to avoid ambiguity. For example: He is trained in the way of the open hand [karate].

Style and usage guides originating in the news industry of the twentieth century, such as the AP Stylebook, recommend against the use of square brackets because "They cannot be transmitted over news wires." However, this guidance has little relevance outside of the technological constraints of the industry and era.

In linguistics, phonetic transcriptions are generally enclosed within square brackets, often using the International Phonetic Alphabet#Brackets and transcription delimiters, whereas phonemic transcriptions typically use paired slashes. Pipes (| |) are often used to indicate a morphophonemic rather than phonemic representation. Other conventions are double slashes (// //), double pipes (|| ||) and curly brackets ({ }).

In lexicography, square brackets usually surround the section of a dictionary entry which contains the etymology of the word the entry defines.


£#h5#£Proofreading£#/h5#£
Brackets (called move-left symbols or move right symbols) are added to the sides of text in proofreading to indicate changes in indentation:

Square brackets are used to denote parts of the text that need to be checked when preparing drafts prior to finalizing a document.


£#h5#£Law£#/h5#£
Square brackets are used in some countries in the citation of law reports to identify parallel citations to non-official reporters. For example:

Chronicle Pub. Co. v Superior Court (1998) 54 Cal.2d 548, [7 Cal.Rptr. 109]

In some other countries (such as England and Wales), square brackets are used to indicate that the year is part of the citation and parentheses are used to indicate the year the judgment was given. For example:

National Coal Board v England [1954] AC 403

This case is in the 1954 volume of the Appeal Cases reports, although the decision may have been given in 1953 or earlier. Compare with:

(1954) 98 Sol Jo 176

This citation reports a decision from 1954, in volume 98 of the Solicitors Journal which may be published in 1955 or later.

They often denote points that have not yet been agreed to in legal drafts and the year in which a report was made for certain case law decisions.


£#h5#£Square brackets in mathematics£#/h5#£
Brackets are used in mathematics in a variety of notations, including standard notations for commutators, the floor function, the Lie bracket, equivalence classes, the Iverson bracket, and matrices.

Square brackets may be used exclusively or in combination with parentheses to represent intervals. [0,5] For example, represents the set of real numbers from 0 to 5 inclusive. Both parentheses and brackets are used to denote a half-open interval; [5, 12) would be the set of all real numbers between 5 and 12, including 5 but not 12. The numbers may come as close as they like to 12, including 11.999 and so forth, but 12.0 is not included. In some European countries, the notation [5, 12[ is also used. The endpoint adjoining the square bracket is known as closed, whereas the endpoint adjoining the parenthesis is known as open.

In group theory and ring theory, brackets denote the commutator. In group theory, the commutator [g, h] is commonly defined as g −1 h −1 g h . In ring theory, the commutator [a, b] is defined as a b − b a .


£#h5#£Chemistry£#/h5#£
Square brackets can also be used in chemistry to represent the concentration of a chemical substance in solution and to denote charge a Lewis structure of an ion (particularly distributed charge in a complex ion), repeating chemical units (particularly in polymers) and transition state structures, among other uses.


£#h5#£Square brackets in programming languages£#/h5#£
Brackets are used in many computer programming languages, primarily for array indexing. But they are also used to denote general tuples, sets and other structures, just as in mathematics. There may be several other uses as well, depending on the language at hand. In syntax diagrams they are used for optional portions, such as in extended Backus–Naur form.


£#h5#£Curly brackets £#/h5#£
Curly brackets { and } are also known as "curly braces" or simply "braces" (UK and US), "definite brackets", "swirly brackets", "birdie brackets", "French brackets", "Scottish brackets", "squirrelly brackets", "gullwings", "seagulls", "squiggly brackets", "twirly brackets", "Tuborg brackets" (DK), "accolades" (NL), "pointy brackets", "fancy brackets", "M Braces", "moustache brackets", "squiggly parentheses", or "flower brackets" (India).


£#h5#£Uses of { }£#/h5#£
Curly brackets are rarely used in prose and have no widely accepted use in formal writing, but may be used to mark words or sentences that should be taken as a group, to avoid confusion when other types of brackets are already in use, or for a special purpose specific to the publication (such as in a dictionary). More commonly, they are used to indicate a group of lines that should be taken together, such as in when referring to several lines of poetry that should be repeated.

As an extension to the International Phonetic Alphabet (IPA), braces are used for prosodic notation.


£#h5#£Music£#/h5#£
In music, they are known as "accolades" or "braces", and connect two or more lines (staves) of music that are played simultaneously.


£#h5#£Curly brackets in programming languages£#/h5#£
In many programming languages, curly brackets enclose groups of statements and create a local scope. Such languages (C, C#, C++ and many others) are therefore called curly bracket languages. They are also used to define structures and enumerated type in these languages.

In syntax diagrams they are used for repetition, such as in extended Backus–Naur form.

In the Z formal specification language, braces define a set.


£#h5#£Curly brackets in mathematics£#/h5#£
In mathematics they delimit sets and are often also used to denote the Poisson bracket between two quantities.

In ring theory, braces denote the anticommutator where {a, b} is defined as a b + b a .


£#h5#£Angle brackets £#/h5#£
"Angle brackets" ⟨ and ⟩ are also called "chevrons", "pointy brackets", "triangular brackets", "diamond brackets", "tuples", "guillemets", "left and right carets", "broken brackets", or "brokets".

The ASCII less-than and greater-than characters <> are often used for angle brackets. In most cases only those characters are accepted by computer programs, the Unicode angle brackets are not recognized (for instance in HTML tags). The characters for "single" guillemets ‹› are also often used, and sometimes normal guillemets «» when nested angle brackets are needed.


£#h5#£Shape£#/h5#£
Angle brackets are larger than less-than and greater-than signs, which in turn are larger than guillemets.


£#h5#£Uses of ⟨ ⟩£#/h5#£
Angle brackets are infrequently used to denote words that are thought instead of spoken, such as:

⟨ What an unusual flower! ⟩
In textual criticism, and hence in many editions of pre-modern works, chevrons denote sections of the text which are illegible or otherwise lost; the editor will often insert their own reconstruction where possible within them.

In comic books, chevrons are often used to mark dialogue that has been translated notionally from another language; in other words, if a character is speaking another language, instead of writing in the other language and providing a translation, one writes the translated text within chevrons. Since no foreign language is actually written, this is only notionally translated.

In linguistics, angle brackets identify graphemes (e.g., letters of an alphabet) or orthography, as in "The English word /kæt/ is spelled ⟨cat⟩."

In epigraphy, they may be used for mechanical transliterations of a text into the Latin script.

In East Asian punctuation, angle brackets are used as quotation marks. Chevron-like symbols are part of standard Chinese, Japanese and Korean punctuation, where they generally enclose the titles of books: ︿ and ﹀ or ︽ and ︾ for traditional vertical printing, and 〈 and 〉 or 《 and 》 for horizontal printing.


£#h5#£Angle brackets in mathematics£#/h5#£
Angle brackets (or 'chevrons') are used in group theory to write group presentations, and to denote the subgroup generated by a collection of elements. In set theory, chevrons or parentheses are used to denote ordered pairs and other tuples, whereas curly brackets are used for unordered sets.


£#h5#£Physics and mechanics£#/h5#£
In physical sciences and statistical mechanics, angle brackets are used to denote an average (expected value) over time or over another continuous parameter. For example:

${\displaystyle \left\langle V(t)^{2}\right\rangle =\lim _{T\to \infty }{\frac {1}{T}}\int _{-{\frac {T}{2}}}^{\frac {T}{2}}V(t)^{2}\,{\rm {d}}t.}$
In mathematical physics, especially quantum mechanics, it is common to write the inner product between elements as ⟨a|b⟩, as a short version of ⟨a|·|b⟩, or ⟨a|Ô|b⟩, where Ô is an operator. This is known as Dirac notation or bra–ket notation, to note vectors from the dual spaces of the Bra ⟨A| and the Ket |B⟩. But there are other notations used.

In continuum mechanics, chevrons may be used as Macaulay brackets.


£#h5#£Angle brackets in programming languages£#/h5#£
In C++ chevrons (actually less-than and greater-than) are used to surround arguments to templates.

In the Z formal specification language chevrons define a sequence.

In HTML, chevrons (actually 'greater than' and 'less than' symbols) are used to bracket meta text. For example <b> denotes that the following text should be displayed as bold. Pairs of meta text tags are required – much as brackets themselves are usually in pairs. The end of the bold text segment would be indicated by </b>. This use is sometimes extended as an informal mechanism for communicating mood or tone in digital formats such as messaging, for example adding "<sighs>" at the end of a sentence.


£#h5#£Other brackets£#/h5#£
£#h5#£Lenticular brackets【】£#/h5#£
Some East Asian languages use lenticular brackets 【 】, a combination of square brackets and round brackets called 方頭括號 (fāngtóu kuòhào) in Chinese and すみ付き (sumitsuki) in Japanese. They are used in titles and headings in both Chinese and Japanese. In Japanese, they are most frequently seen in dictionaries for quoting Chinese characters and Sino-Japanese loanwords.


£#h5#£Floor ⌊ ⌋ and ceiling ⌈ ⌉ corner brackets£#/h5#£
The floor corner brackets ⌊ and ⌋, the ceiling corner brackets ⌈ and ⌉ (U+2308, U+2309) are used to denote the integer floor and ceiling functions.


£#h5#£Quine corners ⌜⌝ and half brackets ⸤ ⸥ or ⸢ ⸣£#/h5#£
The Quine corners ⌜ and ⌝ have at least two uses in mathematical logic: either as quasi-quotation, a generalization of quotation marks, or to denote the Gödel number of the enclosed expression.

Half brackets are used in English to mark added text, such as in translations: "Bill saw ⸤her⸥".

In editions of papyrological texts, half brackets, ⸤ and ⸥ or ⸢ and ⸣, enclose text which is lacking in the papyrus due to damage, but can be restored by virtue of another source, such as an ancient quotation of the text transmitted by the papyrus. For example, Callimachus Iambus 1.2 reads: ἐκ τῶν ὅκου βοῦν κολλύ⸤βου π⸥ιπρήσκουσιν. A hole in the papyrus has obliterated βου π, but these letters are supplied by an ancient commentary on the poem. Second intermittent sources can be between ⸢ and ⸣. Quine corners are sometimes used instead of half brackets.


£#h5#£Double brackets ⟦ ⟧£#/h5#£
Double brackets (or white square brackets or Scott brackets), ⟦ ⟧, are used to indicate the semantic evaluation function in formal semantics for natural language and denotational semantics for programming languages. The brackets stand for a function that maps a linguistic expression to its "denotation" or semantic value. In mathematics, double brackets may also be used to denote intervals of integers or, less often, the floor function. In papyrology, following the Leiden Conventions, they are used to enclose text that has been deleted in antiquity.


£#h5#£Brackets with quills ⁅ ⁆£#/h5#£
Known as "spike parentheses" (Swedish: piggparenteser), ⁅ and ⁆ are used in Swedish bilingual dictionaries to enclose supplemental constructions.


£#h5#£Unicode£#/h5#£
Representations of various kinds of brackets in Unicode and HTML are given below.

The angle brackets or chevrons at U+27E8 and U+27E9 are for mathematical use and Western languages, whereas U+3008 and U+3009 are for East Asian languages. The chevrons at U+2329 and U+232A are deprecated in favour of the U+3008 and U+3009 East Asian angle brackets. Unicode discourages their use for mathematics and in Western texts, because they are canonically equivalent to the CJK code points U+300x and thus likely to render as double-width symbols. The less-than and greater-than symbols are often used as replacements for chevrons.


£#h5#£See also£#/h5#£ £#ul#££#li#£Bracket (mathematics)£#/li#£ £#li#£International variation in quotation marks£#/li#£ £#li#£Emoticon£#/li#£ £#li#£Japanese typographic symbols£#/li#£ £#li#£Order of operations£#/li#£ £#li#£Triple parentheses£#/li#£ £#li#£Arrowhead (combining and standalone characters similar to angle brackets or less-than and greater-than characters)£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£Bibliography£#/h5#£ £#ul#££#li#£Lennard, John (1991). But I Digress: The Exploitation of Parentheses in English Printed Verse. Oxford: Clarendon Press. ISBN 0-19-811247-5.£#/li#£ £#li#£Turnbull; et al. (1964). The Graphics of Communication. New York: Holt. States that what are depicted as brackets above are called braces and braces are called brackets. This was the terminology in US printing prior to computers.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£ Media related to Brackets at Wikimedia Commons£#/li#£ £#li#£ The dictionary definition of bracket at Wiktionary£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Bringhurst, R. The Elements of Typographic Style, 2nd ed. Point Roberts, WA: Hartley and Marks, p. 271, 1997.£#/li#££#li#£ Bringhurst, R. The Elements of Typographic Style, 2nd ed. Point Roberts, WA: Hartley and Marks, p. 271, 1997. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Differential Forms £#/li#££#/ul#£




£#h3#£Angular Acceleration£#/h3#£

In physics, angular acceleration refers to the time rate of change of angular velocity. As there are two types of angular velocity, namely spin angular velocity and orbital angular velocity, there are naturally also two types of angular acceleration, called spin angular acceleration and orbital angular acceleration respectively. Spin angular acceleration refers to the angular acceleration of a rigid body about its centre of rotation, and orbital angular acceleration refers to the angular acceleration of a point particle about a fixed origin.

Angular acceleration is measured in units of angle per unit time squared (which in SI units is radians per second squared), and is usually represented by the symbol alpha (α). In two dimensions, angular acceleration is a pseudoscalar whose sign is taken to be positive if the angular speed increases counterclockwise or decreases clockwise, and is taken to be negative if the angular speed increases clockwise or decreases counterclockwise. In three dimensions, angular acceleration is a pseudovector.

For rigid bodies, angular acceleration must be caused by a net external torque. However, this is not so for non-rigid bodies: For example, a figure skater can speed up her rotation (thereby obtaining an angular acceleration) simply by contracting her arms and legs inwards, which involves no external torque.


£#h5#£Orbital angular acceleration of a point particle£#/h5#£
£#h5#£Particle in two dimensions£#/h5#£
In two dimensions, the orbital angular acceleration is the rate at which the two-dimensional orbital angular velocity of the particle about the origin changes. The instantaneous angular velocity ω at any point in time is given by

${\displaystyle \omega ={\frac {v_{\perp }}{r}},}$
where ${\displaystyle r}$ is the distance from the origin and ${\displaystyle v_{\perp }}$ is the cross-radial component of the instantaneous velocity (i.e. the component perpendicular to the position vector), which by convention is positive for counter-clockwise motion and negative for clockwise motion.

Therefore, the instantaneous angular acceleration α of the particle is given by

${\displaystyle \alpha ={\frac {d}{dt}}\left({\frac {v_{\perp }}{r}}\right).}$
Expanding the right-hand-side using the product rule from differential calculus, this becomes

${\displaystyle \alpha ={\frac {1}{r}}{\frac {dv_{\perp }}{dt}}-{\frac {v_{\perp }}{r^{2}}}{\frac {dr}{dt}}.}$
In the special case where the particle undergoes circular motion about the origin, ${\displaystyle {\frac {dv_{\perp }}{dt}}}$ becomes just the tangential acceleration ${\displaystyle a_{\perp }}$ , and ${\displaystyle {\frac {dr}{dt}}}$ vanishes (since the distance from the origin stays constant), so the above equation simplifies to

${\displaystyle \alpha ={\frac {a_{\perp }}{r}}.}$
In two dimensions, angular acceleration is a number with plus or minus sign indicating orientation, but not pointing in a direction. The sign is conventionally taken to be positive if the angular speed increases in the counter-clockwise direction or decreases in the clockwise direction, and the sign is taken negative if the angular speed increases in the clockwise direction or decreases in the counter-clockwise direction. Angular acceleration then may be termed a pseudoscalar, a numerical quantity which changes sign under a parity inversion, such as inverting one axis or switching the two axes.


£#h5#£Particle in three dimensions£#/h5#£
In three dimensions, the orbital angular acceleration is the rate at which three-dimensional orbital angular velocity vector changes with time. The instantaneous angular velocity vector ${\displaystyle {\boldsymbol {\omega }}}$ at any point in time is given by

${\displaystyle {\boldsymbol {\omega }}={\frac {\mathbf {r} \times \mathbf {v} }{r^{2}}},}$
where ${\displaystyle \mathbf {r} }$ is the particle's position vector, ${\displaystyle r}$ its distance from the origin, and ${\displaystyle \mathbf {v} }$ its velocity vector.

Therefore, the orbital angular acceleration is the vector ${\displaystyle {\boldsymbol {\alpha }}}$ defined by

${\displaystyle {\boldsymbol {\alpha }}={\frac {d}{dt}}\left({\frac {\mathbf {r} \times \mathbf {v} }{r^{2}}}\right).}$
Expanding this derivative using the product rule for cross-products and the ordinary quotient rule, one gets:

${\displaystyle {\begin{aligned}{\boldsymbol {\alpha }}&={\frac {1}{r^{2}}}\left(\mathbf {r} \times {\frac {d\mathbf {v} }{dt}}+{\frac {d\mathbf {r} }{dt}}\times \mathbf {v} \right)-{\frac {2}{r^{3}}}{\frac {dr}{dt}}\left(\mathbf {r} \times \mathbf {v} \right)\\\\&={\frac {1}{r^{2}}}\left(\mathbf {r} \times \mathbf {a} +\mathbf {v} \times \mathbf {v} \right)-{\frac {2}{r^{3}}}{\frac {dr}{dt}}\left(\mathbf {r} \times \mathbf {v} \right)\\\\&={\frac {\mathbf {r} \times \mathbf {a} }{r^{2}}}-{\frac {2}{r^{3}}}{\frac {dr}{dt}}\left(\mathbf {r} \times \mathbf {v} \right).\end{aligned}}}$
Since ${\displaystyle \mathbf {r} \times \mathbf {v} }$ is just ${\displaystyle r^{2}{\boldsymbol {\omega }}}$ , the second term may be rewritten as ${\displaystyle -{\frac {2}{r}}{\frac {dr}{dt}}{\boldsymbol {\omega }}}$ . In the case where the distance ${\displaystyle r}$ of the particle from the origin does not change with time (which includes circular motion as a subcase), the second term vanishes and the above formula simplifies to

${\displaystyle {\boldsymbol {\alpha }}={\frac {\mathbf {r} \times \mathbf {a} }{r^{2}}}.}$
From the above equation, one can recover the cross-radial acceleration in this special case as:

${\displaystyle \mathbf {a} _{\perp }={\boldsymbol {\alpha }}\times \mathbf {r} .}$
Unlike in two dimensions, the angular acceleration in three dimensions need not be associated with a change in the angular speed ${\displaystyle \omega =|{\boldsymbol {\omega }}|}$ : If the particle's position vector "twists" in space, changing its instantaneous plane of angular displacement, the change in the direction of the angular velocity ${\displaystyle {\boldsymbol {\omega }}}$ will still produce a nonzero angular acceleration. This cannot not happen if the position vector is restricted to a fixed plane, in which case ${\displaystyle {\boldsymbol {\omega }}}$ has a fixed direction perpendicular to the plane.

The angular acceleration vector is more properly called a pseudovector: It has three components which transform under rotations in the same way as the Cartesian coordinates of a point do, but which do not transform like Cartesian coordinates under reflections.


£#h5#£Relation to torque£#/h5#£
The net torque on a point particle is defined to be the pseudovector

${\displaystyle {\boldsymbol {\tau }}=\mathbf {r} \times \mathbf {F} ,}$
where ${\displaystyle \mathbf {F} }$ is the net force on the particle.

Torque is the rotational analogue of force: it induces change in the rotational state of a system, just as force induces change in the translational state of a system. As force on a particle is connected to acceleration by the equation ${\displaystyle \mathbf {F} =m\mathbf {a} }$ , one may write a similar equation connecting torque on a particle to angular acceleration, though this relation is necessarily more complicated.

First, substituting ${\displaystyle \mathbf {F} =m\mathbf {a} }$ into the above equation for torque, one gets

${\displaystyle {\boldsymbol {\tau }}=m\left(\mathbf {r} \times \mathbf {a} \right)=mr^{2}\left({\frac {\mathbf {r} \times \mathbf {a} }{r^{2}}}\right).}$
From the previous section:

${\displaystyle {\boldsymbol {\alpha }}={\frac {\mathbf {r} \times \mathbf {a} }{r^{2}}}-{\frac {2}{r}}{\frac {dr}{dt}}{\boldsymbol {\omega }},}$
where ${\displaystyle {\boldsymbol {\alpha }}}$ is orbital angular acceleration and ${\displaystyle {\boldsymbol {\omega }}}$ is orbital angular velocity. Therefore:

${\displaystyle {\boldsymbol {\tau }}=mr^{2}\left({\boldsymbol {\alpha }}+{\frac {2}{r}}{\frac {dr}{dt}}{\boldsymbol {\omega }}\right)=mr^{2}{\boldsymbol {\alpha }}+2mr{\frac {dr}{dt}}{\boldsymbol {\omega }}.}$
In the special case of constant distance ${\displaystyle r}$ of the particle from the origin ( ${\displaystyle {\tfrac {dr}{dt}}=0}$ ), the second term in the above equation vanishes and the above equation simplifies to

${\displaystyle {\boldsymbol {\tau }}=mr^{2}{\boldsymbol {\alpha }},}$
which can be interpreted as a "rotational analogue" to ${\displaystyle \mathbf {F} =m\mathbf {a} }$ , where the quantity ${\displaystyle mr^{2}}$ (known as the moment of inertia of the particle) plays the role of the mass ${\displaystyle m}$ . However, unlike ${\displaystyle \mathbf {F} =m\mathbf {a} }$ , this equation does not apply to an arbitrary trajectory, only to a trajectory contained within a spherical shell about the origin.


£#h5#£See also£#/h5#£ £#ul#££#li#£Torque£#/li#£ £#li#£Angular momentum£#/li#£ £#li#£Angular speed£#/li#£ £#li#£Angular velocity£#/li#££#/ul#£
£#h5#£References£#/h5#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Differential Geometry > General Differential Geometry £#/li#££#/ul#£




£#h3#£Angular Distance£#/h3#£

Angular distance ${\displaystyle \theta }$ (also known as angular separation, apparent distance, or apparent separation) is the angle between the two sightlines, or between two point objects as viewed from an observer.

Angular distance appears in mathematics (in particular geometry and trigonometry) and all natural sciences (e.g. astronomy and geophysics). In the classical mechanics of rotating objects, it appears alongside angular velocity, angular acceleration, angular momentum, moment of inertia and torque.


£#h5#£Use£#/h5#£
The term angular distance (or separation) is technically synonymous with angle itself, but is meant to suggest the linear distance between objects (for instance, a couple of stars observed from Earth).


£#h5#£Measurement£#/h5#£
Since the angular distance (or separation) is conceptually identical to an angle, it is measured in the same units, such as degrees or radians, using instruments such as goniometers or optical instruments specially designed to point in well-defined directions and record the corresponding angles (such as telescopes).


£#h5#£Equation£#/h5#£
£#h5#£General case£#/h5#£
To derive the equation that describes the angular separation of two points located on the surface of a sphere as seen from the center of the sphere, we use the example of two astronomical objects ${\displaystyle A}$ and ${\displaystyle B}$ observed from the Earth. The objects ${\displaystyle A}$ and ${\displaystyle B}$ are defined by their celestial coordinates, namely their right ascensions (RA), ${\displaystyle (\alpha _{A},\alpha _{B})\in [0,2\pi ]}$ ; and declinations (dec), ${\displaystyle (\delta _{A},\delta _{B})\in [-\pi /2,\pi /2]}$ . Let ${\displaystyle O}$ indicate the observer on Earth, assumed to be located at the center of the celestial sphere. The dot product of the vectors ${\displaystyle \mathbf {OA} }$ and ${\displaystyle \mathbf {OB} }$ is equal to:

${\displaystyle \mathbf {OA} \cdot \mathbf {OB} =R^{2}\cos \theta }$
which is equivalent to:

${\displaystyle \mathbf {n_{A}} .\mathbf {n_{B}} =\cos \theta }$
In the ${\displaystyle (x,y,z)}$ frame, the two unitary vectors are decomposed into:

${\displaystyle \mathbf {n_{A}} ={\begin{pmatrix}\cos \delta _{A}\cos \alpha _{A}\\\cos \delta _{A}\sin \alpha _{A}\\\sin \delta _{A}\end{pmatrix}}\mathrm {\qquad and\qquad } \mathbf {n_{B}} ={\begin{pmatrix}\cos \delta _{B}\cos \alpha _{B}\\\cos \delta _{B}\sin \alpha _{B}\\\sin \delta _{B}\end{pmatrix}}.}$
Therefore,

${\displaystyle \mathbf {n_{A}} \mathbf {n_{B}} =\cos \delta _{A}\cos \alpha _{A}\cos \delta _{B}\cos \alpha _{B}+\cos \delta _{A}\sin \alpha _{A}\cos \delta _{B}\sin \alpha _{B}+\sin \delta _{A}\sin \delta _{B}\equiv \cos \theta }$
then:

${\displaystyle \theta =\cos ^{-1}\left[\sin \delta _{A}\sin \delta _{B}+\cos \delta _{A}\cos \delta _{B}\cos(\alpha _{A}-\alpha _{B})\right]}$

£#h5#£Small angular distance approximation£#/h5#£
The above expression is valid for any position of A and B on the sphere. In astronomy, it often happens that the considered objects are really close in the sky: stars in a telescope field of view, binary stars, the satellites of the giant planets of the solar system, etc. In the case where ${\displaystyle \theta \ll 1}$ radian, implying ${\displaystyle \alpha _{A}-\alpha _{B}\ll 1}$ and ${\displaystyle \delta _{A}-\delta _{B}\ll 1}$ , we can develop the above expression and simplify it. In the small-angle approximation, at second order, the above expression becomes:

${\displaystyle \cos \theta \approx 1-{\frac {\theta ^{2}}{2}}\approx \sin \delta _{A}\sin \delta _{B}+\cos \delta _{A}\cos \delta _{B}\left[1-{\frac {(\alpha _{A}-\alpha _{B})^{2}}{2}}\right]}$
meaning

${\displaystyle 1-{\frac {\theta ^{2}}{2}}\approx \cos(\delta _{A}-\delta _{B})-\cos \delta _{A}\cos \delta _{B}{\frac {(\alpha _{A}-\alpha _{B})^{2}}{2}}}$
hence

${\displaystyle 1-{\frac {\theta ^{2}}{2}}\approx 1-{\frac {(\delta _{A}-\delta _{B})^{2}}{2}}-\cos \delta _{A}\cos \delta _{B}{\frac {(\alpha _{A}-\alpha _{B})^{2}}{2}}}$ .
Given that ${\displaystyle \delta _{A}-\delta _{B}\ll 1}$ and ${\displaystyle \alpha _{A}-\alpha _{B}\ll 1}$ , at a second-order development it turns that ${\displaystyle \cos \delta _{A}\cos \delta _{B}{\frac {(\alpha _{A}-\alpha _{B})^{2}}{2}}\approx \cos ^{2}\delta _{A}{\frac {(\alpha _{A}-\alpha _{B})^{2}}{2}}}$ , so that

${\displaystyle \theta \approx {\sqrt {\left[(\alpha _{A}-\alpha _{B})\cos \delta _{A}\right]^{2}+(\delta _{A}-\delta _{B})^{2}}}}$

£#h5#£Small angular distance: planar approximation£#/h5#£
If we consider a detector imaging a small sky field (dimension much less than one radian) with the ${\displaystyle y}$ -axis pointing up, parallel to the meridian of right ascension ${\displaystyle \alpha }$ , and the ${\displaystyle x}$ -axis along the parallel of declination ${\displaystyle \delta }$ , the angular separation can be written as:

${\displaystyle \theta \approx {\sqrt {\delta x^{2}+\delta y^{2}}}}$
where ${\displaystyle \delta x=(\alpha _{A}-\alpha _{B})\cos \delta _{A}}$ and ${\displaystyle \delta y=\delta _{A}-\delta _{B}}$ .

Note that the ${\displaystyle y}$ -axis is equal to the declination, whereas the ${\displaystyle x}$ -axis is the right ascension modulated by ${\displaystyle \cos \delta _{A}}$ because the section of a sphere of radius ${\displaystyle R}$ at declination (latitude) ${\displaystyle \delta }$ is ${\displaystyle R'=R\cos \delta _{A}}$ (see Figure).


£#h5#£See also£#/h5#£ £#ul#££#li#£Milliradian£#/li#£ £#li#£Gradian£#/li#£ £#li#£Hour angle£#/li#£ £#li#£Central angle£#/li#£ £#li#£Angle of rotation£#/li#£ £#li#£Angular diameter£#/li#£ £#li#£Angular displacement£#/li#£ £#li#£Great-circle distance£#/li#£ £#li#£Cosine similarity § Angular distance and similarity£#/li#££#/ul#£
£#h5#£References£#/h5#£ £#ul#££#li#£CASTOR, author(s) unknown. "The Spherical Trigonometry vs. Vector Analysis".£#/li#£ £#li#£Weisstein, Eric W. "Angular Distance". MathWorld.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Differential Geometry > General Differential Geometry £#/li#££#/ul#£




£#h3#£Angular Velocity£#/h3#£

In physics, angular velocity or rotational velocity (ω or Ω), also known as angular frequency vector, is a pseudovector representation of how fast the angular position or orientation of an object changes with time (i.e. how quickly an object rotates or revolves relative to a point or axis). The magnitude of the pseudovector represents the angular speed, the rate at which the object rotates or revolves, and its direction is normal to the instantaneous plane of rotation or angular displacement. The orientation of angular velocity is conventionally specified by the right-hand rule.

There are two types of angular velocity.

£#ul#££#li#£Orbital angular velocity refers to how fast a point object revolves about a fixed origin, i.e. the time rate of change of its angular position relative to the origin.£#/li#£ £#li#£Spin angular velocity refers to how fast a rigid body rotates with respect to its center of rotation and is independent of the choice of origin, in contrast to orbital angular velocity.£#/li#££#/ul#£
In general, angular velocity has dimension of angle per unit time (angle replacing distance from linear velocity with time in common). The SI unit of angular velocity is radians per second, with the radian being a dimensionless quantity, thus the SI units of angular velocity may be listed as s−1. Angular velocity is usually represented by the symbol omega (ω, sometimes Ω). By convention, positive angular velocity indicates counter-clockwise rotation, while negative is clockwise.

For example, a geostationary satellite completes one orbit per day above the equator, or 360 degrees per 24 hours, and has angular velocity ω = (360°)/(24 h) = 15°/h, or (2π rad)/(24 h) ≈ 0.26 rad/h. If angle is measured in radians, the linear velocity is the radius times the angular velocity, ${\displaystyle v=r\omega }$ . With orbital radius 42,000 km from the earth's center, the satellite's speed through space is thus v = 42,000 km × 0.26/h ≈ 11,000 km/h. The angular velocity is positive since the satellite travels eastward with the Earth's rotation (counter-clockwise from above the north pole.)


£#h5#£Orbital angular velocity of a point particle£#/h5#£
£#h5#£Particle in two dimensions£#/h5#£
In the simplest case of circular motion at radius ${\displaystyle r}$ , with position given by the angular displacement ${\displaystyle \phi (t)}$ from the x-axis, the orbital angular velocity is the rate of change of angle with respect to time: ${\textstyle \omega ={\frac {d\phi }{dt}}}$ . If ${\displaystyle \phi }$ is measured in radians, the arc-length from the positive x-axis around the circle to the particle is ${\displaystyle \ell =r\phi }$ , and the linear velocity is ${\textstyle v(t)={\frac {d\ell }{dt}}=r\omega (t)}$ , so that ${\textstyle \omega ={\frac {v}{r}}}$ .

In the general case of a particle moving in the plane, the orbital angular velocity is the rate at which the position vector relative to a chosen origin "sweeps out" angle. The diagram shows the position vector ${\displaystyle \mathbf {r} }$ from the origin ${\displaystyle O}$ to a particle ${\displaystyle P}$ , with its polar coordinates ${\displaystyle (r,\phi )}$ . (All variables are functions of time ${\displaystyle t}$ .) The particle has linear velocity splitting as ${\displaystyle \mathbf {v} =\mathbf {v} _{\|}+\mathbf {v} _{\perp }}$ , with the radial component ${\displaystyle \mathbf {v} _{\|}}$ parallel to the radius, and the cross-radial (or tangential) component ${\displaystyle \mathbf {v} _{\perp }}$ perpendicular to the radius. When there is no radial component, the particle moves around the origin in a circle; but when there is no cross-radial component, it moves in a straight line from the origin. Since radial motion leaves the angle unchanged, only the cross-radial component of linear velocity contributes to angular velocity.

The angular velocity ω is the rate of change of angular position with respect to time, which can be computed from the cross-radial velocity as:

${\displaystyle \omega ={\frac {d\phi }{dt}}={\frac {v_{\perp }}{r}}.}$
Here the cross-radial speed ${\displaystyle v_{\perp }}$ is the signed magnitude of ${\displaystyle \mathbf {v} _{\perp }}$ , positive for counter-clockwise motion, negative for clockwise. Taking polar coordinates for the linear velocity ${\displaystyle \mathbf {v} }$ gives magnitude ${\displaystyle v}$ (linear speed) and angle ${\displaystyle \theta }$ relative to the radius vector; in these terms, ${\displaystyle v_{\perp }=v\sin(\theta )}$ , so that

${\displaystyle \omega ={\frac {v\sin(\theta )}{r}}.}$
These formulas may be derived doing ${\displaystyle \mathbf {r} =(r\cos(\varphi ),r\sin(\varphi ))}$ , being ${\displaystyle r}$ a function of the distance to the origin with respect to time, and ${\displaystyle \varphi }$ a function of the angle between the vector and the x axis. Then ${\textstyle {\frac {d\mathbf {r} }{dt}}=({\dot {r}}\cos(\varphi )-r{\dot {\varphi }}\sin(\varphi ),{\dot {r}}\sin(\varphi )+r{\dot {\varphi }}\cos(\varphi ))}$ . Which is equal to ${\displaystyle {\dot {r}}(\cos(\varphi ),\sin(\varphi ))+r{\dot {\varphi }}(-\sin(\varphi ),\cos(\varphi ))={\dot {r}}{\hat {r}}+r{\dot {\varphi }}{\hat {\varphi }}}$ . (See Unit vector in cylindrical coordinates). Knowing ${\textstyle {\frac {d\mathbf {r} }{dt}}=\mathbf {v} }$ , we conclude that the radial component of the velocity is given by ${\displaystyle {\dot {r}}}$ , because ${\displaystyle {\hat {r}}}$ is a radial unit vector; and the perpendicular component is given by ${\displaystyle r{\dot {\varphi }}}$ because ${\displaystyle {\hat {\varphi }}}$ is a perpendicular unit vector.

In two dimensions, angular velocity is a number with plus or minus sign indicating orientation, but not pointing in a direction. The sign is conventionally taken to be positive if the radius vector turns counter-clockwise, and negative if clockwise. Angular velocity then may be termed a pseudoscalar, a numerical quantity which changes sign under a parity inversion, such as inverting one axis or switching the two axes.


£#h5#£Particle in three dimensions£#/h5#£
In three-dimensional space, we again have the position vector r of a moving particle. Here, orbital angular velocity is a pseudovector whose magnitude is the rate at which r sweeps out angle, and whose direction is perpendicular to the instantaneous plane in which r sweeps out angle (i.e. the plane spanned by r and v). However, as there are two directions perpendicular to any plane, an additional condition is necessary to uniquely specify the direction of the angular velocity; conventionally, the right-hand rule is used.

Let the pseudovector ${\displaystyle \mathbf {u} }$ be the unit vector perpendicular to the plane spanned by r and v, so that the right-hand rule is satisfied (i.e. the instantaneous direction of angular displacement is counter-clockwise looking from the top of ${\displaystyle \mathbf {u} }$ ). Taking polar coordinates ${\displaystyle (r,\phi )}$ in this plane, as in the two-dimensional case above, one may define the orbital angular velocity vector as:

${\displaystyle {\boldsymbol {\omega }}=\omega \mathbf {u} ={\frac {d\phi }{dt}}\mathbf {u} ={\frac {v\sin(\theta )}{r}}\mathbf {u} ,}$
where θ is the angle between r and v. In terms of the cross product, this is:

${\displaystyle {\boldsymbol {\omega }}={\frac {\mathbf {r} \times \mathbf {v} }{r^{2}}}.}$
From the above equation, one can recover the tangential velocity as:

${\displaystyle \mathbf {v} _{\perp }={\boldsymbol {\omega }}\times \mathbf {r} }$

£#h5#£Addition of angular velocity vectors£#/h5#£
If a point rotates with orbital angular velocity ${\displaystyle \omega _{1}}$ about its center of rotation in a coordinate frame ${\displaystyle F_{1}}$ which itself rotates with a spin angular velocity ${\displaystyle \omega _{2}}$ with respect to an external frame ${\displaystyle F_{2}}$ , we can define ${\displaystyle \omega _{1}+\omega _{2}}$ to be the composite orbital angular velocity vector of the point about its center of rotation with respect to ${\displaystyle F_{2}}$ . This operation coincides with usual addition of vectors, and it gives angular velocity the algebraic structure of a true vector, rather than just a pseudo-vector.

The only non-obvious property of the above addition is commutativity. This can be proven from the fact that the velocity tensor W (see below) is skew-symmetric, so that ${\displaystyle R=e^{W\cdot dt}}$ is a rotation matrix which can be expanded as ${\displaystyle R=I+W\cdot dt+{\tfrac {1}{2}}(W\cdot dt)^{2}+\cdots }$ . The composition of rotations is not commutative, but ${\displaystyle (I+W_{1}\cdot dt)(I+W_{2}\cdot dt)=(I+W_{2}\cdot dt)(I+W_{1}\cdot dt)}$ is commutative to first order, and therefore ${\displaystyle \omega _{1}+\omega _{2}=\omega _{2}+\omega _{1}}$ .

Notice that this also defines the subtraction as the addition of a negative vector.


£#h5#£Spin angular velocity of a rigid body or reference frame£#/h5#£
Given a rotating frame of three unit coordinate vectors, all the three must have the same angular speed at each instant. In such a frame, each vector may be considered as a moving particle with constant scalar radius.

The rotating frame appears in the context of rigid bodies, and special tools have been developed for it: the spin angular velocity may be described as a vector or equivalently as a tensor.

Consistent with the general definition, the spin angular velocity of a frame is defined as the orbital angular velocity of any of the three vectors (same for all) with respect to its own center of rotation. The addition of angular velocity vectors for frames is also defined by the usual vector addition (composition of linear movements), and can be useful to decompose the rotation as in a gimbal. All components of the vector can be calculated as derivatives of the parameters defining the moving frames (Euler angles or rotation matrices). As in the general case, addition is commutative: ${\displaystyle \omega _{1}+\omega _{2}=\omega _{2}+\omega _{1}}$ .

By Euler's rotation theorem, any rotating frame possesses an instantaneous axis of rotation, which is the direction of the angular velocity vector, and the magnitude of the angular velocity is consistent with the two-dimensional case.

If we choose a reference point ${\displaystyle {\boldsymbol {R}}}$ fixed in the rigid body, the velocity ${\displaystyle {\dot {\boldsymbol {r}}}}$ of any point in the body is given by

${\displaystyle {\dot {\boldsymbol {r}}}={\dot {\boldsymbol {R}}}+{\boldsymbol {\omega }}\times ({\boldsymbol {r}}-{\boldsymbol {R}})}$

£#h5#£Components from the basis vectors of a body-fixed frame£#/h5#£
Consider a rigid body rotating about a fixed point O. Construct a reference frame in the body consisting of an orthonormal set of vectors ${\displaystyle \mathbf {e} _{1},\mathbf {e} _{2},\mathbf {e} _{3}}$ fixed to the body and with their common origin at O. The angular velocity vector of both frame and body about O is then

${\displaystyle {\boldsymbol {\omega }}=\left({\dot {\mathbf {e} }}_{1}\cdot \mathbf {e} _{2}\right)\mathbf {e} _{3}+\left({\dot {\mathbf {e} }}_{2}\cdot \mathbf {e} _{3}\right)\mathbf {e} _{1}+\left({\dot {\mathbf {e} }}_{3}\cdot \mathbf {e} _{1}\right)\mathbf {e} _{2},}$
where ${\displaystyle {\dot {\mathbf {e} }}_{i}={\frac {d\mathbf {e} _{i}}{dt}}}$ is the time rate of change of the frame vector ${\displaystyle \mathbf {e} _{i},i=1,2,3,}$ due to the rotation.

Note that this formula is incompatible with the expression

${\displaystyle {\boldsymbol {\omega }}={\frac {\mathbf {r} \times \mathbf {v} }{r^{2}}}.}$
as that formula defines only the angular velocity of a single point about O, while the formula in this section applies to a frame or rigid body. In the case of a rigid body a single ${\displaystyle {\boldsymbol {\omega }}}$ has to account for the motion of all particles in the body.


£#h5#£Components from Euler angles£#/h5#£
The components of the spin angular velocity pseudovector were first calculated by Leonhard Euler using his Euler angles and the use of an intermediate frame:

£#ul#££#li#£One axis of the reference frame (the precession axis)£#/li#£ £#li#£The line of nodes of the moving frame with respect to the reference frame (nutation axis)£#/li#£ £#li#£One axis of the moving frame (the intrinsic rotation axis)£#/li#££#/ul#£
Euler proved that the projections of the angular velocity pseudovector on each of these three axes is the derivative of its associated angle (which is equivalent to decomposing the instantaneous rotation into three instantaneous Euler rotations). Therefore:

${\displaystyle {\boldsymbol {\omega }}={\dot {\alpha }}\mathbf {u} _{1}+{\dot {\beta }}\mathbf {u} _{2}+{\dot {\gamma }}\mathbf {u} _{3}}$
This basis is not orthonormal and it is difficult to use, but now the velocity vector can be changed to the fixed frame or to the moving frame with just a change of bases. For example, changing to the mobile frame:

${\displaystyle {\boldsymbol {\omega }}=({\dot {\alpha }}\sin \beta \sin \gamma +{\dot {\beta }}\cos \gamma ){\hat {\mathbf {i} }}+({\dot {\alpha }}\sin \beta \cos \gamma -{\dot {\beta }}\sin \gamma ){\hat {\mathbf {j} }}+({\dot {\alpha }}\cos \beta +{\dot {\gamma }}){\hat {\mathbf {k} }}}$
where ${\displaystyle {\hat {\mathbf {i} }},{\hat {\mathbf {j} }},{\hat {\mathbf {k} }}}$ are unit vectors for the frame fixed in the moving body. This example has been made using the Z-X-Z convention for Euler angles.


£#h5#£Tensor £#/h5#£
The angular velocity vector ${\displaystyle {\boldsymbol {\omega }}=(\omega _{x},\omega _{y},\omega _{z})}$ defined above may be equivalently expressed as an angular velocity tensor, the matrix (or linear mapping) W = W(t) defined by:

${\displaystyle W={\begin{pmatrix}0&-\omega _{z}&\omega _{y}\\\omega _{z}&0&-\omega _{x}\\-\omega _{y}&\omega _{x}&0\\\end{pmatrix}}}$
This is an infinitesimal rotation matrix. The linear mapping W acts as ${\displaystyle ({\boldsymbol {\omega }}\times )}$ :

${\displaystyle {\boldsymbol {\omega }}\times \mathbf {r} =W\cdot \mathbf {r} .}$

£#h5#£Calculation from the orientation matrix£#/h5#£
A vector ${\displaystyle \mathbf {r} }$ undergoing uniform circular motion around a fixed axis satisfies:

${\displaystyle {\frac {d\mathbf {r} }{dt}}={\boldsymbol {\omega }}\times \mathbf {r} =W\cdot \mathbf {r} }$
Given the orientation matrix A(t) of a frame, whose columns are the moving orthonormal coordinate vectors ${\displaystyle \mathbf {e} _{1},\mathbf {e} _{2},\mathbf {e} _{3}}$ , we can obtain its angular velocity tensor W(t) as follows. Angular velocity must be the same for the three vectors ${\displaystyle \mathbf {r} =\mathbf {e} _{i}}$ , so arranging the three vector equations into columns of a matrix, we have:

${\displaystyle {\frac {dA}{dt}}=W\cdot A.}$
(This holds even if A(t) does not rotate uniformly.) Therefore the angular velocity tensor is:

${\displaystyle W={\frac {dA}{dt}}\cdot A^{-1}={\frac {dA}{dt}}\cdot A^{\mathrm {T} },}$
since the inverse of the orthogonal matrix ${\displaystyle A}$ is its transpose ${\displaystyle A^{\mathrm {T} }}$ .


£#h5#£Properties£#/h5#£
In general, the angular velocity in an n-dimensional space is the time derivative of the angular displacement tensor, which is a second rank skew-symmetric tensor.

This tensor W will have n(n−1)/2 independent components, which is the dimension of the Lie algebra of the Lie group of rotations of an n-dimensional inner product space.


£#h5#£Duality with respect to the velocity vector£#/h5#£
In three dimensions, angular velocity can be represented by a pseudovector because second rank tensors are dual to pseudovectors in three dimensions. Since the angular velocity tensor W = W(t) is a skew-symmetric matrix:

${\displaystyle W={\begin{pmatrix}0&-\omega _{z}&\omega _{y}\\\omega _{z}&0&-\omega _{x}\\-\omega _{y}&\omega _{x}&0\\\end{pmatrix}},}$
its Hodge dual is a vector, which is precisely the previous angular velocity vector ${\displaystyle {\boldsymbol {\omega }}=[\omega _{x},\omega _{y},\omega _{z}]}$ .


£#h5#£Exponential of W£#/h5#£
If we know an initial frame A(0) and we are given a constant angular velocity tensor W, we can obtain A(t) for any given t. Recall the matrix differential equation:

${\displaystyle {\frac {dA}{dt}}=W\cdot A.}$
This equation can be integrated to give:

${\displaystyle A(t)=e^{Wt}A(0),}$
which shows a connection with the Lie group of rotations.


£#h5#£W is skew-symmetric£#/h5#£
We prove that angular velocity tensor is skew symmetric, i.e. ${\displaystyle W={\frac {dA(t)}{dt}}\cdot A^{\text{T}}}$ satisfies ${\displaystyle W^{\text{T}}=-W}$ .

A rotation matrix A is orthogonal, inverse to its transpose, so we have ${\displaystyle I=A\cdot A^{\text{T}}}$ . For ${\displaystyle A=A(t)}$ a frame matrix, taking the time derivative of the equation gives:

${\displaystyle 0={\frac {dA}{dt}}A^{\text{T}}+A{\frac {dA^{\text{T}}}{dt}}}$
Applying the formula ${\displaystyle (AB)^{\text{T}}=B^{\text{T}}A^{\text{T}}}$ ,

${\displaystyle 0={\frac {dA}{dt}}A^{\text{T}}+\left({\frac {dA}{dt}}A^{\text{T}}\right)^{\text{T}}=W+W^{\text{T}}}$
Thus, W is the negative of its transpose, which implies it is skew symmetric.


£#h5#£Coordinate-free description£#/h5#£
At any instant ${\displaystyle t}$ , the angular velocity tensor represents a linear map between the position vector ${\displaystyle \mathbf {r} (t)}$ and the velocity vectors ${\displaystyle \mathbf {v} (t)}$ of a point on a rigid body rotating around the origin:

${\displaystyle \mathbf {v} =W\mathbf {r} .}$
The relation between this linear map and the angular velocity pseudovector ${\displaystyle {\boldsymbol {\omega }}}$ is the following.

Because W is the derivative of an orthogonal transformation, the bilinear form

${\displaystyle B(\mathbf {r} ,\mathbf {s} )=(W\mathbf {r} )\cdot \mathbf {s} }$
is skew-symmetric. Thus we can apply the fact of exterior algebra that there is a unique linear form ${\displaystyle L}$ on ${\displaystyle \Lambda ^{2}V}$ that

${\displaystyle L(\mathbf {r} \wedge \mathbf {s} )=B(\mathbf {r} ,\mathbf {s} )}$
where ${\displaystyle \mathbf {r} \wedge \mathbf {s} \in \Lambda ^{2}V}$ is the exterior product of ${\displaystyle \mathbf {r} }$ and ${\displaystyle \mathbf {s} }$ .

Taking the sharp L♯ of L we get

${\displaystyle (W\mathbf {r} )\cdot \mathbf {s} =L^{\sharp }\cdot (\mathbf {r} \wedge \mathbf {s} )}$
Introducing ${\displaystyle {\boldsymbol {\omega }}:={\star }(L^{\sharp })}$ , as the Hodge dual of L♯, and applying the definition of the Hodge dual twice supposing that the preferred unit 3-vector is ${\displaystyle \star 1}$

${\displaystyle (W\mathbf {r} )\cdot \mathbf {s} ={\star }({\star }(L^{\sharp })\wedge \mathbf {r} \wedge \mathbf {s} )={\star }({\boldsymbol {\omega }}\wedge \mathbf {r} \wedge \mathbf {s} )={\star }({\boldsymbol {\omega }}\wedge \mathbf {r} )\cdot \mathbf {s} =({\boldsymbol {\omega }}\times \mathbf {r} )\cdot \mathbf {s} ,}$
where

${\displaystyle {\boldsymbol {\omega }}\times \mathbf {r} :={\star }({\boldsymbol {\omega }}\wedge \mathbf {r} )}$
by definition.

Because ${\displaystyle \mathbf {s} }$ is an arbitrary vector, from nondegeneracy of scalar product follows

${\displaystyle W\mathbf {r} ={\boldsymbol {\omega }}\times \mathbf {r} }$

£#h5#£Angular velocity as a vector field£#/h5#£
Since the spin angular velocity tensor of a rigid body (in its rest frame) is a linear transformation that maps positions to velocities (within the rigid body), it can be regarded as a constant vector field. In particular, the spin angular velocity is a Killing vector field belonging to an element of the Lie algebra SO(3) of the 3-dimensional rotation group SO(3).

Also, it can be shown that the spin angular velocity vector field is exactly half of the curl of the linear velocity vector field v(r) of the rigid body. In symbols,

${\displaystyle {\boldsymbol {\omega }}={\frac {1}{2}}\nabla \times \mathbf {v} }$

£#h5#£Rigid body considerations£#/h5#£
The same equations for the angular speed can be obtained reasoning over a rotating rigid body. Here is not assumed that the rigid body rotates around the origin. Instead, it can be supposed rotating around an arbitrary point that is moving with a linear velocity V(t) in each instant.

To obtain the equations, it is convenient to imagine a rigid body attached to the frames and consider a coordinate system that is fixed with respect to the rigid body. Then we will study the coordinate transformations between this coordinate and the fixed "laboratory" system.

As shown in the figure on the right, the lab system's origin is at point O, the rigid body system origin is at O′ and the vector from O to O′ is R. A particle (i) in the rigid body is located at point P and the vector position of this particle is Ri in the lab frame, and at position ri in the body frame. It is seen that the position of the particle can be written:

${\displaystyle \mathbf {R} _{i}=\mathbf {R} +\mathbf {r} _{i}}$
The defining characteristic of a rigid body is that the distance between any two points in a rigid body is unchanging in time. This means that the length of the vector ${\displaystyle \mathbf {r} _{i}}$ is unchanging. By Euler's rotation theorem, we may replace the vector ${\displaystyle \mathbf {r} _{i}}$ with ${\displaystyle {\mathcal {R}}\mathbf {r} _{io}}$ where ${\displaystyle {\mathcal {R}}}$ is a 3×3 rotation matrix and ${\displaystyle \mathbf {r} _{io}}$ is the position of the particle at some fixed point in time, say t = 0. This replacement is useful, because now it is only the rotation matrix ${\displaystyle {\mathcal {R}}}$ that is changing in time and not the reference vector ${\displaystyle \mathbf {r} _{io}}$ , as the rigid body rotates about point O′. Also, since the three columns of the rotation matrix represent the three versors of a reference frame rotating together with the rigid body, any rotation about any axis becomes now visible, while the vector ${\displaystyle \mathbf {r} _{i}}$ would not rotate if the rotation axis were parallel to it, and hence it would only describe a rotation about an axis perpendicular to it (i.e., it would not see the component of the angular velocity pseudovector parallel to it, and would only allow the computation of the component perpendicular to it). The position of the particle is now written as:

${\displaystyle \mathbf {R} _{i}=\mathbf {R} +{\mathcal {R}}\mathbf {r} _{io}}$
Taking the time derivative yields the velocity of the particle:

${\displaystyle \mathbf {V} _{i}=\mathbf {V} +{\frac {d{\mathcal {R}}}{dt}}\mathbf {r} _{io}}$
where Vi is the velocity of the particle (in the lab frame) and V is the velocity of O′ (the origin of the rigid body frame). Since ${\displaystyle {\mathcal {R}}}$ is a rotation matrix its inverse is its transpose. So we substitute ${\displaystyle {\mathcal {I}}={\mathcal {R}}^{\text{T}}{\mathcal {R}}}$ :

${\displaystyle \mathbf {V} _{i}=\mathbf {V} +{\frac {d{\mathcal {R}}}{dt}}{\mathcal {I}}\mathbf {r} _{io}}$
${\displaystyle \mathbf {V} _{i}=\mathbf {V} +{\frac {d{\mathcal {R}}}{dt}}{\mathcal {R}}^{\text{T}}{\mathcal {R}}\mathbf {r} _{io}}$
${\displaystyle \mathbf {V} _{i}=\mathbf {V} +{\frac {d{\mathcal {R}}}{dt}}{\mathcal {R}}^{\text{T}}\mathbf {r} _{i}}$
or

${\displaystyle \mathbf {V} _{i}=\mathbf {V} +W\mathbf {r} _{i}}$
where ${\displaystyle W={\frac {d{\mathcal {R}}}{dt}}{\mathcal {R}}^{\text{T}}}$ is the previous angular velocity tensor.

It can be proved that this is a skew symmetric matrix, so we can take its dual to get a 3 dimensional pseudovector that is precisely the previous angular velocity vector ${\displaystyle {\boldsymbol {\omega }}}$ :

${\displaystyle {\boldsymbol {\omega }}=[\omega _{x},\omega _{y},\omega _{z}]}$
Substituting ω for W into the above velocity expression, and replacing matrix multiplication by an equivalent cross product:

${\displaystyle \mathbf {V} _{i}=\mathbf {V} +{\boldsymbol {\omega }}\times \mathbf {r} _{i}}$
It can be seen that the velocity of a point in a rigid body can be divided into two terms – the velocity of a reference point fixed in the rigid body plus the cross product term involving the orbital angular velocity of the particle with respect to the reference point. This angular velocity is what physicists call the "spin angular velocity" of the rigid body, as opposed to the orbital angular velocity of the reference point O′ about the origin O.


£#h5#£Consistency£#/h5#£
We have supposed that the rigid body rotates around an arbitrary point. We should prove that the spin angular velocity previously defined is independent of the choice of origin, which means that the spin angular velocity is an intrinsic property of the spinning rigid body. (Note the marked contrast of this with the orbital angular velocity of a point particle, which certainly does depend on the choice of origin.)

See the graph to the right: The origin of lab frame is O, while O1 and O2 are two fixed points on the rigid body, whose velocity is ${\displaystyle \mathbf {v} _{1}}$ and ${\displaystyle \mathbf {v} _{2}}$ respectively. Suppose the angular velocity with respect to O1 and O2 is ${\displaystyle {\boldsymbol {\omega }}_{1}}$ and ${\displaystyle {\boldsymbol {\omega }}_{2}}$ respectively. Since point P and O2 have only one velocity,

${\displaystyle \mathbf {v} _{1}+{\boldsymbol {\omega }}_{1}\times \mathbf {r} _{1}=\mathbf {v} _{2}+{\boldsymbol {\omega }}_{2}\times \mathbf {r} _{2}}$
${\displaystyle \mathbf {v} _{2}=\mathbf {v} _{1}+{\boldsymbol {\omega }}_{1}\times \mathbf {r} =\mathbf {v} _{1}+{\boldsymbol {\omega }}_{1}\times (\mathbf {r} _{1}-\mathbf {r} _{2})}$
The above two yields that

${\displaystyle ({\boldsymbol {\omega }}_{2}-{\boldsymbol {\omega }}_{1})\times \mathbf {r} _{2}=0}$
Since the point P (and thus ${\displaystyle \mathbf {r} _{2}}$ ) is arbitrary, it follows that

${\displaystyle {\boldsymbol {\omega }}_{1}={\boldsymbol {\omega }}_{2}}$
If the reference point is the instantaneous axis of rotation the expression of the velocity of a point in the rigid body will have just the angular velocity term. This is because the velocity of the instantaneous axis of rotation is zero. An example of the instantaneous axis of rotation is the hinge of a door. Another example is the point of contact of a purely rolling spherical (or, more generally, convex) rigid body.


£#h5#£See also£#/h5#£ £#ul#££#li#£Angular acceleration£#/li#£ £#li#£Angular frequency£#/li#£ £#li#£Angular momentum£#/li#£ £#li#£Areal velocity£#/li#£ £#li#£Isometry£#/li#£ £#li#£Orthogonal group£#/li#£ £#li#£Rigid body dynamics£#/li#£ £#li#£Vorticity£#/li#££#/ul#£
£#h5#£References£#/h5#£ £#ul#££#li#£ Symon, Keith (1971). Mechanics. Addison-Wesley, Reading, MA. ISBN 978-0-201-07392-8.£#/li#£ £#li#£ Landau, L.D.; Lifshitz, E.M. (1997). Mechanics. Butterworth-Heinemann. ISBN 978-0-7506-2896-9.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£A college text-book of physics By Arthur Lalanne Kimball (Angular Velocity of a particle)£#/li#£ £#li#£Pickering, Steve (2009). "ω Speed of Rotation [Angular Velocity]". Sixty Symbols. Brady Haran for the University of Nottingham.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Differential Geometry > General Differential Geometry £#/li#££#/ul#£




£#h3#£Anosov Automorphism£#/h3#£

In mathematics, more particularly in the fields of dynamical systems and geometric topology, an Anosov map on a manifold M is a certain type of mapping, from M to itself, with rather clearly marked local directions of "expansion" and "contraction". Anosov systems are a special case of Axiom A systems.

Anosov diffeomorphisms were introduced by Dmitri Victorovich Anosov, who proved that their behaviour was in an appropriate sense generic (when they exist at all).


£#h5#£Overview£#/h5#£
Three closely related definitions must be distinguished:

£#ul#££#li#£If a differentiable map f on M has a hyperbolic structure on the tangent bundle, then it is called an Anosov map. Examples include the Bernoulli map, and Arnold's cat map.£#/li#£ £#li#£If the map is a diffeomorphism, then it is called an Anosov diffeomorphism.£#/li#£ £#li#£If a flow on a manifold splits the tangent bundle into three invariant subbundles, with one subbundle that is exponentially contracting, and one that is exponentially expanding, and a third, non-expanding, non-contracting one-dimensional sub-bundle (spanned by the flow direction), then the flow is called an Anosov flow.£#/li#££#/ul#£
A classical example of Anosov diffeomorphism is the Arnold's cat map.

Anosov proved that Anosov diffeomorphisms are structurally stable and form an open subset of mappings (flows) with the C1 topology.

Not every manifold admits an Anosov diffeomorphism; for example, there are no such diffeomorphisms on the sphere . The simplest examples of compact manifolds admitting them are the tori: they admit the so-called linear Anosov diffeomorphisms, which are isomorphisms having no eigenvalue of modulus 1. It was proved that any other Anosov diffeomorphism on a torus is topologically conjugate to one of this kind.

The problem of classifying manifolds that admit Anosov diffeomorphisms turned out to be very difficult, and still as of 2012 has no answer. The only known examples are infranil manifolds, and it is conjectured that they are the only ones.

A sufficient condition for transitivity is that all points are nonwandering: ${\displaystyle \Omega (f)=M}$ .

Also, it is unknown if every ${\displaystyle C^{1}}$ volume-preserving Anosov diffeomorphism is ergodic. Anosov proved it under a ${\displaystyle C^{2}}$ assumption. It is also true for ${\displaystyle C^{1+\alpha }}$ volume-preserving Anosov diffeomorphisms.

For ${\displaystyle C^{2}}$ transitive Anosov diffeomorphism ${\displaystyle f\colon M\to M}$ there exists a unique SRB measure (the acronym stands for Sinai, Ruelle and Bowen) ${\displaystyle \mu _{f}}$ supported on ${\displaystyle M}$ such that its basin ${\displaystyle B(\mu _{f})}$ is of full volume, where

${\displaystyle B(\mu _{f})=\left\{x\in M:{\frac {1}{n}}\sum _{k=0}^{n-1}\delta _{f^{k}x}\to \mu _{f}\right\}.}$

£#h5#£Anosov flow on (tangent bundles of) Riemann surfaces£#/h5#£
As an example, this section develops the case of the Anosov flow on the tangent bundle of a Riemann surface of negative curvature. This flow can be understood in terms of the flow on the tangent bundle of the Poincaré half-plane model of hyperbolic geometry. Riemann surfaces of negative curvature may be defined as Fuchsian models, that is, as the quotients of the upper half-plane and a Fuchsian group. For the following, let H be the upper half-plane; let Γ be a Fuchsian group; let M = H/Γ be a Riemann surface of negative curvature as the quotient of "M" by the action of the group Γ, and let ${\displaystyle T^{1}M}$ be the tangent bundle of unit-length vectors on the manifold M, and let ${\displaystyle T^{1}H}$ be the tangent bundle of unit-length vectors on H. Note that a bundle of unit-length vectors on a surface is the principal bundle of a complex line bundle.


£#h5#£Lie vector fields£#/h5#£
One starts by noting that ${\displaystyle T^{1}H}$ is isomorphic to the Lie group PSL(2,R). This group is the group of orientation-preserving isometries of the upper half-plane. The Lie algebra of PSL(2,R) is sl(2,R), and is represented by the matrices

${\displaystyle J={\begin{pmatrix}1/2&0\\0&-1/2\\\end{pmatrix}}\qquad X={\begin{pmatrix}0&1\\0&0\\\end{pmatrix}}\qquad Y={\begin{pmatrix}0&0\\1&0\end{pmatrix}}}$
which have the algebra

${\displaystyle [J,X]=X\qquad [J,Y]=-Y\qquad [X,Y]=2J}$
The exponential maps

${\displaystyle g_{t}=\exp(tJ)={\begin{pmatrix}e^{t/2}&0\\0&e^{-t/2}\\\end{pmatrix}}\qquad h_{t}^{*}=\exp(tX)={\begin{pmatrix}1&t\\0&1\\\end{pmatrix}}\qquad h_{t}=\exp(tY)={\begin{pmatrix}1&0\\t&1\\\end{pmatrix}}}$
define right-invariant flows on the manifold of ${\displaystyle T^{1}H=\operatorname {PSL} (2,\mathbb {R} )}$ , and likewise on ${\displaystyle T^{1}M}$ . Defining ${\displaystyle P=T^{1}H}$ and ${\displaystyle Q=T^{1}M}$ , these flows define vector fields on P and Q, whose vectors lie in TP and TQ. These are just the standard, ordinary Lie vector fields on the manifold of a Lie group, and the presentation above is a standard exposition of a Lie vector field.


£#h5#£Anosov flow£#/h5#£
The connection to the Anosov flow comes from the realization that ${\displaystyle g_{t}}$ is the geodesic flow on P and Q. Lie vector fields being (by definition) left invariant under the action of a group element, one has that these fields are left invariant under the specific elements ${\displaystyle g_{t}}$ of the geodesic flow. In other words, the spaces TP and TQ are split into three one-dimensional spaces, or subbundles, each of which are invariant under the geodesic flow. The final step is to notice that vector fields in one subbundle expand (and expand exponentially), those in another are unchanged, and those in a third shrink (and do so exponentially).

More precisely, the tangent bundle TQ may be written as the direct sum

${\displaystyle TQ=E^{+}\oplus E^{0}\oplus E^{-}}$
or, at a point ${\displaystyle g\cdot e=q\in Q}$ , the direct sum

${\displaystyle T_{q}Q=E_{q}^{+}\oplus E_{q}^{0}\oplus E_{q}^{-}}$
corresponding to the Lie algebra generators Y, J and X, respectively, carried, by the left action of group element g, from the origin e to the point q. That is, one has ${\displaystyle E_{e}^{+}=Y,E_{e}^{0}=J}$ and ${\displaystyle E_{e}^{-}=X}$ . These spaces are each subbundles, and are preserved (are invariant) under the action of the geodesic flow; that is, under the action of group elements ${\displaystyle g=g_{t}}$ .

To compare the lengths of vectors in ${\displaystyle T_{q}Q}$ at different points q, one needs a metric. Any inner product at ${\displaystyle T_{e}P=sl(2,\mathbb {R} )}$ extends to a left-invariant Riemannian metric on P, and thus to a Riemannian metric on Q. The length of a vector ${\displaystyle v\in E_{q}^{+}}$ expands exponentially as exp(t) under the action of ${\displaystyle g_{t}}$ . The length of a vector ${\displaystyle v\in E_{q}^{-}}$ shrinks exponentially as exp(-t) under the action of ${\displaystyle g_{t}}$ . Vectors in ${\displaystyle E_{q}^{0}}$ are unchanged. This may be seen by examining how the group elements commute. The geodesic flow is invariant,

${\displaystyle g_{s}g_{t}=g_{t}g_{s}=g_{s+t}}$
but the other two shrink and expand:

${\displaystyle g_{s}h_{t}^{*}=h_{t\exp(-s)}^{*}g_{s}}$
and

${\displaystyle g_{s}h_{t}=h_{t\exp(s)}g_{s}}$
where we recall that a tangent vector in ${\displaystyle E_{q}^{+}}$ is given by the derivative, with respect to t, of the curve ${\displaystyle h_{t}}$ , the setting ${\displaystyle t=0}$ .


£#h5#£Geometric interpretation of the Anosov flow£#/h5#£
When acting on the point ${\displaystyle z=i}$ of the upper half-plane, ${\displaystyle g_{t}}$ corresponds to a geodesic on the upper half plane, passing through the point ${\displaystyle z=i}$ . The action is the standard Möbius transformation action of SL(2,R) on the upper half-plane, so that

${\displaystyle g_{t}\cdot i={\begin{pmatrix}\exp(t/2)&0\\0&\exp(-t/2)\end{pmatrix}}\cdot i=i\exp(t)}$
A general geodesic is given by

${\displaystyle {\begin{pmatrix}a&b\\c&d\end{pmatrix}}\cdot i\exp(t)={\frac {ai\exp(t)+b}{ci\exp(t)+d}}}$
with a, b, c and d real, with ${\displaystyle ad-bc=1}$ . The curves ${\displaystyle h_{t}^{*}}$ and ${\displaystyle h_{t}}$ are called horocycles. Horocycles correspond to the motion of the normal vectors of a horosphere on the upper half-plane.


£#h5#£See also£#/h5#£ £#ul#££#li#£Ergodic flow£#/li#£ £#li#£Morse–Smale system£#/li#£ £#li#£Pseudo-Anosov map£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£"Y-system,U-system, C-system", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#£ £#li#£Anthony Manning, Dynamics of geodesic and horocycle flows on surfaces of constant negative curvature, (1991), appearing as Chapter 3 in Ergodic Theory, Symbolic Dynamics and Hyperbolic Spaces, Tim Bedford, Michael Keane and Caroline Series, Eds. Oxford University Press, Oxford (1991). ISBN 0-19-853390-X (Provides an expository introduction to the Anosov flow on SL(2,R).)£#/li#£ £#li#£This article incorporates material from Anosov diffeomorphism on PlanetMath, which is licensed under the Creative Commons Attribution/Share-Alike License.£#/li#£ £#li#£Toshikazu Sunada, Magnetic flows on a Riemann surface, Proc. KAIST Math. Workshop (1993), 93–108.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Dynamical Systems £#/li#££#/ul#£




£#h3#£Anosov Diffeomorphism£#/h3#£

In mathematics, more particularly in the fields of dynamical systems and geometric topology, an Anosov map on a manifold M is a certain type of mapping, from M to itself, with rather clearly marked local directions of "expansion" and "contraction". Anosov systems are a special case of Axiom A systems.

Anosov diffeomorphisms were introduced by Dmitri Victorovich Anosov, who proved that their behaviour was in an appropriate sense generic (when they exist at all).


£#h5#£Overview£#/h5#£
Three closely related definitions must be distinguished:

£#ul#££#li#£If a differentiable map f on M has a hyperbolic structure on the tangent bundle, then it is called an Anosov map. Examples include the Bernoulli map, and Arnold's cat map.£#/li#£ £#li#£If the map is a diffeomorphism, then it is called an Anosov diffeomorphism.£#/li#£ £#li#£If a flow on a manifold splits the tangent bundle into three invariant subbundles, with one subbundle that is exponentially contracting, and one that is exponentially expanding, and a third, non-expanding, non-contracting one-dimensional sub-bundle (spanned by the flow direction), then the flow is called an Anosov flow.£#/li#££#/ul#£
A classical example of Anosov diffeomorphism is the Arnold's cat map.

Anosov proved that Anosov diffeomorphisms are structurally stable and form an open subset of mappings (flows) with the C1 topology.

Not every manifold admits an Anosov diffeomorphism; for example, there are no such diffeomorphisms on the sphere . The simplest examples of compact manifolds admitting them are the tori: they admit the so-called linear Anosov diffeomorphisms, which are isomorphisms having no eigenvalue of modulus 1. It was proved that any other Anosov diffeomorphism on a torus is topologically conjugate to one of this kind.

The problem of classifying manifolds that admit Anosov diffeomorphisms turned out to be very difficult, and still as of 2012 has no answer. The only known examples are infranil manifolds, and it is conjectured that they are the only ones.

A sufficient condition for transitivity is that all points are nonwandering: ${\displaystyle \Omega (f)=M}$ .

Also, it is unknown if every ${\displaystyle C^{1}}$ volume-preserving Anosov diffeomorphism is ergodic. Anosov proved it under a ${\displaystyle C^{2}}$ assumption. It is also true for ${\displaystyle C^{1+\alpha }}$ volume-preserving Anosov diffeomorphisms.

For ${\displaystyle C^{2}}$ transitive Anosov diffeomorphism ${\displaystyle f\colon M\to M}$ there exists a unique SRB measure (the acronym stands for Sinai, Ruelle and Bowen) ${\displaystyle \mu _{f}}$ supported on ${\displaystyle M}$ such that its basin ${\displaystyle B(\mu _{f})}$ is of full volume, where

${\displaystyle B(\mu _{f})=\left\{x\in M:{\frac {1}{n}}\sum _{k=0}^{n-1}\delta _{f^{k}x}\to \mu _{f}\right\}.}$

£#h5#£Anosov flow on (tangent bundles of) Riemann surfaces£#/h5#£
As an example, this section develops the case of the Anosov flow on the tangent bundle of a Riemann surface of negative curvature. This flow can be understood in terms of the flow on the tangent bundle of the Poincaré half-plane model of hyperbolic geometry. Riemann surfaces of negative curvature may be defined as Fuchsian models, that is, as the quotients of the upper half-plane and a Fuchsian group. For the following, let H be the upper half-plane; let Γ be a Fuchsian group; let M = H/Γ be a Riemann surface of negative curvature as the quotient of "M" by the action of the group Γ, and let ${\displaystyle T^{1}M}$ be the tangent bundle of unit-length vectors on the manifold M, and let ${\displaystyle T^{1}H}$ be the tangent bundle of unit-length vectors on H. Note that a bundle of unit-length vectors on a surface is the principal bundle of a complex line bundle.


£#h5#£Lie vector fields£#/h5#£
One starts by noting that ${\displaystyle T^{1}H}$ is isomorphic to the Lie group PSL(2,R). This group is the group of orientation-preserving isometries of the upper half-plane. The Lie algebra of PSL(2,R) is sl(2,R), and is represented by the matrices

${\displaystyle J={\begin{pmatrix}1/2&0\\0&-1/2\\\end{pmatrix}}\qquad X={\begin{pmatrix}0&1\\0&0\\\end{pmatrix}}\qquad Y={\begin{pmatrix}0&0\\1&0\end{pmatrix}}}$
which have the algebra

${\displaystyle [J,X]=X\qquad [J,Y]=-Y\qquad [X,Y]=2J}$
The exponential maps

${\displaystyle g_{t}=\exp(tJ)={\begin{pmatrix}e^{t/2}&0\\0&e^{-t/2}\\\end{pmatrix}}\qquad h_{t}^{*}=\exp(tX)={\begin{pmatrix}1&t\\0&1\\\end{pmatrix}}\qquad h_{t}=\exp(tY)={\begin{pmatrix}1&0\\t&1\\\end{pmatrix}}}$
define right-invariant flows on the manifold of ${\displaystyle T^{1}H=\operatorname {PSL} (2,\mathbb {R} )}$ , and likewise on ${\displaystyle T^{1}M}$ . Defining ${\displaystyle P=T^{1}H}$ and ${\displaystyle Q=T^{1}M}$ , these flows define vector fields on P and Q, whose vectors lie in TP and TQ. These are just the standard, ordinary Lie vector fields on the manifold of a Lie group, and the presentation above is a standard exposition of a Lie vector field.


£#h5#£Anosov flow£#/h5#£
The connection to the Anosov flow comes from the realization that ${\displaystyle g_{t}}$ is the geodesic flow on P and Q. Lie vector fields being (by definition) left invariant under the action of a group element, one has that these fields are left invariant under the specific elements ${\displaystyle g_{t}}$ of the geodesic flow. In other words, the spaces TP and TQ are split into three one-dimensional spaces, or subbundles, each of which are invariant under the geodesic flow. The final step is to notice that vector fields in one subbundle expand (and expand exponentially), those in another are unchanged, and those in a third shrink (and do so exponentially).

More precisely, the tangent bundle TQ may be written as the direct sum

${\displaystyle TQ=E^{+}\oplus E^{0}\oplus E^{-}}$
or, at a point ${\displaystyle g\cdot e=q\in Q}$ , the direct sum

${\displaystyle T_{q}Q=E_{q}^{+}\oplus E_{q}^{0}\oplus E_{q}^{-}}$
corresponding to the Lie algebra generators Y, J and X, respectively, carried, by the left action of group element g, from the origin e to the point q. That is, one has ${\displaystyle E_{e}^{+}=Y,E_{e}^{0}=J}$ and ${\displaystyle E_{e}^{-}=X}$ . These spaces are each subbundles, and are preserved (are invariant) under the action of the geodesic flow; that is, under the action of group elements ${\displaystyle g=g_{t}}$ .

To compare the lengths of vectors in ${\displaystyle T_{q}Q}$ at different points q, one needs a metric. Any inner product at ${\displaystyle T_{e}P=sl(2,\mathbb {R} )}$ extends to a left-invariant Riemannian metric on P, and thus to a Riemannian metric on Q. The length of a vector ${\displaystyle v\in E_{q}^{+}}$ expands exponentially as exp(t) under the action of ${\displaystyle g_{t}}$ . The length of a vector ${\displaystyle v\in E_{q}^{-}}$ shrinks exponentially as exp(-t) under the action of ${\displaystyle g_{t}}$ . Vectors in ${\displaystyle E_{q}^{0}}$ are unchanged. This may be seen by examining how the group elements commute. The geodesic flow is invariant,

${\displaystyle g_{s}g_{t}=g_{t}g_{s}=g_{s+t}}$
but the other two shrink and expand:

${\displaystyle g_{s}h_{t}^{*}=h_{t\exp(-s)}^{*}g_{s}}$
and

${\displaystyle g_{s}h_{t}=h_{t\exp(s)}g_{s}}$
where we recall that a tangent vector in ${\displaystyle E_{q}^{+}}$ is given by the derivative, with respect to t, of the curve ${\displaystyle h_{t}}$ , the setting ${\displaystyle t=0}$ .


£#h5#£Geometric interpretation of the Anosov flow£#/h5#£
When acting on the point ${\displaystyle z=i}$ of the upper half-plane, ${\displaystyle g_{t}}$ corresponds to a geodesic on the upper half plane, passing through the point ${\displaystyle z=i}$ . The action is the standard Möbius transformation action of SL(2,R) on the upper half-plane, so that

${\displaystyle g_{t}\cdot i={\begin{pmatrix}\exp(t/2)&0\\0&\exp(-t/2)\end{pmatrix}}\cdot i=i\exp(t)}$
A general geodesic is given by

${\displaystyle {\begin{pmatrix}a&b\\c&d\end{pmatrix}}\cdot i\exp(t)={\frac {ai\exp(t)+b}{ci\exp(t)+d}}}$
with a, b, c and d real, with ${\displaystyle ad-bc=1}$ . The curves ${\displaystyle h_{t}^{*}}$ and ${\displaystyle h_{t}}$ are called horocycles. Horocycles correspond to the motion of the normal vectors of a horosphere on the upper half-plane.


£#h5#£See also£#/h5#£ £#ul#££#li#£Ergodic flow£#/li#£ £#li#£Morse–Smale system£#/li#£ £#li#£Pseudo-Anosov map£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£"Y-system,U-system, C-system", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#£ £#li#£Anthony Manning, Dynamics of geodesic and horocycle flows on surfaces of constant negative curvature, (1991), appearing as Chapter 3 in Ergodic Theory, Symbolic Dynamics and Hyperbolic Spaces, Tim Bedford, Michael Keane and Caroline Series, Eds. Oxford University Press, Oxford (1991). ISBN 0-19-853390-X (Provides an expository introduction to the Anosov flow on SL(2,R).)£#/li#£ £#li#£This article incorporates material from Anosov diffeomorphism on PlanetMath, which is licensed under the Creative Commons Attribution/Share-Alike License.£#/li#£ £#li#£Toshikazu Sunada, Magnetic flows on a Riemann surface, Proc. KAIST Math. Workshop (1993), 93–108.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Anosov, D. V. "Geodesic Flow on Closed Riemannian Manifolds of Negative Curvature." Trudy Mat. Inst. Steklov 90, 1-209, 1970.£#/li#££#li#£Smale, S. "Differentiable Dynamical Systems." Bull. Amer. Math. Soc. 73, 747-817, 1967.£#/li#££#li#£ Anosov, D. V. "Geodesic Flow on Closed Riemannian Manifolds of Negative Curvature." Trudy Mat. Inst. Steklov 90, 1-209, 1970. £#/li#££#li#£ Smale, S. "Differentiable Dynamical Systems." Bull. Amer. Math. Soc. 73, 747-817, 1967. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Dynamical Systems £#/li#££#/ul#£




£#h3#£Anosov Flow£#/h3#£

In mathematics, more particularly in the fields of dynamical systems and geometric topology, an Anosov map on a manifold M is a certain type of mapping, from M to itself, with rather clearly marked local directions of "expansion" and "contraction". Anosov systems are a special case of Axiom A systems.

Anosov diffeomorphisms were introduced by Dmitri Victorovich Anosov, who proved that their behaviour was in an appropriate sense generic (when they exist at all).


£#h5#£Overview£#/h5#£
Three closely related definitions must be distinguished:

£#ul#££#li#£If a differentiable map f on M has a hyperbolic structure on the tangent bundle, then it is called an Anosov map. Examples include the Bernoulli map, and Arnold's cat map.£#/li#£ £#li#£If the map is a diffeomorphism, then it is called an Anosov diffeomorphism.£#/li#£ £#li#£If a flow on a manifold splits the tangent bundle into three invariant subbundles, with one subbundle that is exponentially contracting, and one that is exponentially expanding, and a third, non-expanding, non-contracting one-dimensional sub-bundle (spanned by the flow direction), then the flow is called an Anosov flow.£#/li#££#/ul#£
A classical example of Anosov diffeomorphism is the Arnold's cat map.

Anosov proved that Anosov diffeomorphisms are structurally stable and form an open subset of mappings (flows) with the C1 topology.

Not every manifold admits an Anosov diffeomorphism; for example, there are no such diffeomorphisms on the sphere . The simplest examples of compact manifolds admitting them are the tori: they admit the so-called linear Anosov diffeomorphisms, which are isomorphisms having no eigenvalue of modulus 1. It was proved that any other Anosov diffeomorphism on a torus is topologically conjugate to one of this kind.

The problem of classifying manifolds that admit Anosov diffeomorphisms turned out to be very difficult, and still as of 2012 has no answer. The only known examples are infranil manifolds, and it is conjectured that they are the only ones.

A sufficient condition for transitivity is that all points are nonwandering: ${\displaystyle \Omega (f)=M}$ .

Also, it is unknown if every ${\displaystyle C^{1}}$ volume-preserving Anosov diffeomorphism is ergodic. Anosov proved it under a ${\displaystyle C^{2}}$ assumption. It is also true for ${\displaystyle C^{1+\alpha }}$ volume-preserving Anosov diffeomorphisms.

For ${\displaystyle C^{2}}$ transitive Anosov diffeomorphism ${\displaystyle f\colon M\to M}$ there exists a unique SRB measure (the acronym stands for Sinai, Ruelle and Bowen) ${\displaystyle \mu _{f}}$ supported on ${\displaystyle M}$ such that its basin ${\displaystyle B(\mu _{f})}$ is of full volume, where

${\displaystyle B(\mu _{f})=\left\{x\in M:{\frac {1}{n}}\sum _{k=0}^{n-1}\delta _{f^{k}x}\to \mu _{f}\right\}.}$

£#h5#£Anosov flow on (tangent bundles of) Riemann surfaces£#/h5#£
As an example, this section develops the case of the Anosov flow on the tangent bundle of a Riemann surface of negative curvature. This flow can be understood in terms of the flow on the tangent bundle of the Poincaré half-plane model of hyperbolic geometry. Riemann surfaces of negative curvature may be defined as Fuchsian models, that is, as the quotients of the upper half-plane and a Fuchsian group. For the following, let H be the upper half-plane; let Γ be a Fuchsian group; let M = H/Γ be a Riemann surface of negative curvature as the quotient of "M" by the action of the group Γ, and let ${\displaystyle T^{1}M}$ be the tangent bundle of unit-length vectors on the manifold M, and let ${\displaystyle T^{1}H}$ be the tangent bundle of unit-length vectors on H. Note that a bundle of unit-length vectors on a surface is the principal bundle of a complex line bundle.


£#h5#£Lie vector fields£#/h5#£
One starts by noting that ${\displaystyle T^{1}H}$ is isomorphic to the Lie group PSL(2,R). This group is the group of orientation-preserving isometries of the upper half-plane. The Lie algebra of PSL(2,R) is sl(2,R), and is represented by the matrices

${\displaystyle J={\begin{pmatrix}1/2&0\\0&-1/2\\\end{pmatrix}}\qquad X={\begin{pmatrix}0&1\\0&0\\\end{pmatrix}}\qquad Y={\begin{pmatrix}0&0\\1&0\end{pmatrix}}}$
which have the algebra

${\displaystyle [J,X]=X\qquad [J,Y]=-Y\qquad [X,Y]=2J}$
The exponential maps

${\displaystyle g_{t}=\exp(tJ)={\begin{pmatrix}e^{t/2}&0\\0&e^{-t/2}\\\end{pmatrix}}\qquad h_{t}^{*}=\exp(tX)={\begin{pmatrix}1&t\\0&1\\\end{pmatrix}}\qquad h_{t}=\exp(tY)={\begin{pmatrix}1&0\\t&1\\\end{pmatrix}}}$
define right-invariant flows on the manifold of ${\displaystyle T^{1}H=\operatorname {PSL} (2,\mathbb {R} )}$ , and likewise on ${\displaystyle T^{1}M}$ . Defining ${\displaystyle P=T^{1}H}$ and ${\displaystyle Q=T^{1}M}$ , these flows define vector fields on P and Q, whose vectors lie in TP and TQ. These are just the standard, ordinary Lie vector fields on the manifold of a Lie group, and the presentation above is a standard exposition of a Lie vector field.


£#h5#£Anosov flow£#/h5#£
The connection to the Anosov flow comes from the realization that ${\displaystyle g_{t}}$ is the geodesic flow on P and Q. Lie vector fields being (by definition) left invariant under the action of a group element, one has that these fields are left invariant under the specific elements ${\displaystyle g_{t}}$ of the geodesic flow. In other words, the spaces TP and TQ are split into three one-dimensional spaces, or subbundles, each of which are invariant under the geodesic flow. The final step is to notice that vector fields in one subbundle expand (and expand exponentially), those in another are unchanged, and those in a third shrink (and do so exponentially).

More precisely, the tangent bundle TQ may be written as the direct sum

${\displaystyle TQ=E^{+}\oplus E^{0}\oplus E^{-}}$
or, at a point ${\displaystyle g\cdot e=q\in Q}$ , the direct sum

${\displaystyle T_{q}Q=E_{q}^{+}\oplus E_{q}^{0}\oplus E_{q}^{-}}$
corresponding to the Lie algebra generators Y, J and X, respectively, carried, by the left action of group element g, from the origin e to the point q. That is, one has ${\displaystyle E_{e}^{+}=Y,E_{e}^{0}=J}$ and ${\displaystyle E_{e}^{-}=X}$ . These spaces are each subbundles, and are preserved (are invariant) under the action of the geodesic flow; that is, under the action of group elements ${\displaystyle g=g_{t}}$ .

To compare the lengths of vectors in ${\displaystyle T_{q}Q}$ at different points q, one needs a metric. Any inner product at ${\displaystyle T_{e}P=sl(2,\mathbb {R} )}$ extends to a left-invariant Riemannian metric on P, and thus to a Riemannian metric on Q. The length of a vector ${\displaystyle v\in E_{q}^{+}}$ expands exponentially as exp(t) under the action of ${\displaystyle g_{t}}$ . The length of a vector ${\displaystyle v\in E_{q}^{-}}$ shrinks exponentially as exp(-t) under the action of ${\displaystyle g_{t}}$ . Vectors in ${\displaystyle E_{q}^{0}}$ are unchanged. This may be seen by examining how the group elements commute. The geodesic flow is invariant,

${\displaystyle g_{s}g_{t}=g_{t}g_{s}=g_{s+t}}$
but the other two shrink and expand:

${\displaystyle g_{s}h_{t}^{*}=h_{t\exp(-s)}^{*}g_{s}}$
and

${\displaystyle g_{s}h_{t}=h_{t\exp(s)}g_{s}}$
where we recall that a tangent vector in ${\displaystyle E_{q}^{+}}$ is given by the derivative, with respect to t, of the curve ${\displaystyle h_{t}}$ , the setting ${\displaystyle t=0}$ .


£#h5#£Geometric interpretation of the Anosov flow£#/h5#£
When acting on the point ${\displaystyle z=i}$ of the upper half-plane, ${\displaystyle g_{t}}$ corresponds to a geodesic on the upper half plane, passing through the point ${\displaystyle z=i}$ . The action is the standard Möbius transformation action of SL(2,R) on the upper half-plane, so that

${\displaystyle g_{t}\cdot i={\begin{pmatrix}\exp(t/2)&0\\0&\exp(-t/2)\end{pmatrix}}\cdot i=i\exp(t)}$
A general geodesic is given by

${\displaystyle {\begin{pmatrix}a&b\\c&d\end{pmatrix}}\cdot i\exp(t)={\frac {ai\exp(t)+b}{ci\exp(t)+d}}}$
with a, b, c and d real, with ${\displaystyle ad-bc=1}$ . The curves ${\displaystyle h_{t}^{*}}$ and ${\displaystyle h_{t}}$ are called horocycles. Horocycles correspond to the motion of the normal vectors of a horosphere on the upper half-plane.


£#h5#£See also£#/h5#£ £#ul#££#li#£Ergodic flow£#/li#£ £#li#£Morse–Smale system£#/li#£ £#li#£Pseudo-Anosov map£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£"Y-system,U-system, C-system", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#£ £#li#£Anthony Manning, Dynamics of geodesic and horocycle flows on surfaces of constant negative curvature, (1991), appearing as Chapter 3 in Ergodic Theory, Symbolic Dynamics and Hyperbolic Spaces, Tim Bedford, Michael Keane and Caroline Series, Eds. Oxford University Press, Oxford (1991). ISBN 0-19-853390-X (Provides an expository introduction to the Anosov flow on SL(2,R).)£#/li#£ £#li#£This article incorporates material from Anosov diffeomorphism on PlanetMath, which is licensed under the Creative Commons Attribution/Share-Alike License.£#/li#£ £#li#£Toshikazu Sunada, Magnetic flows on a Riemann surface, Proc. KAIST Math. Workshop (1993), 93–108.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Anosov, D. "Roughness of Geodesic Flows on Compact Riemannian Manifolds of Negative Curvature." Dokl. Akad. Nauk SSSR 145, 707-709, 1962. English translation in Soviet Math. Dokl. 3, 1068-1069, 1962.£#/li#££#li#£Anosov, D. "Ergodic Properties of Geodesic Flows on Closed Riemannian Manifolds of Negative Curvature." Dokl. Akad. Nauk SSSR 151, 1250-1252, 1963. English translated in Soviet Math. Dokl. 4, 1153-1156, 1963.£#/li#££#li#£ Anosov, D. "Roughness of Geodesic Flows on Compact Riemannian Manifolds of Negative Curvature." Dokl. Akad. Nauk SSSR 145, 707-709, 1962. English translation in Soviet Math. Dokl. 3, 1068-1069, 1962. £#/li#££#li#£ Anosov, D. "Ergodic Properties of Geodesic Flows on Closed Riemannian Manifolds of Negative Curvature." Dokl. Akad. Nauk SSSR 151, 1250-1252, 1963. English translated in Soviet Math. Dokl. 4, 1153-1156, 1963. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Dynamical Systems £#/li#££#li#£ Topology > Bundles £#/li#££#/ul#£




£#h3#£Anosov Map£#/h3#£

In mathematics, more particularly in the fields of dynamical systems and geometric topology, an Anosov map on a manifold M is a certain type of mapping, from M to itself, with rather clearly marked local directions of "expansion" and "contraction". Anosov systems are a special case of Axiom A systems.

Anosov diffeomorphisms were introduced by Dmitri Victorovich Anosov, who proved that their behaviour was in an appropriate sense generic (when they exist at all).


£#h5#£Overview£#/h5#£
Three closely related definitions must be distinguished:

£#ul#££#li#£If a differentiable map f on M has a hyperbolic structure on the tangent bundle, then it is called an Anosov map. Examples include the Bernoulli map, and Arnold's cat map.£#/li#£ £#li#£If the map is a diffeomorphism, then it is called an Anosov diffeomorphism.£#/li#£ £#li#£If a flow on a manifold splits the tangent bundle into three invariant subbundles, with one subbundle that is exponentially contracting, and one that is exponentially expanding, and a third, non-expanding, non-contracting one-dimensional sub-bundle (spanned by the flow direction), then the flow is called an Anosov flow.£#/li#££#/ul#£
A classical example of Anosov diffeomorphism is the Arnold's cat map.

Anosov proved that Anosov diffeomorphisms are structurally stable and form an open subset of mappings (flows) with the C1 topology.

Not every manifold admits an Anosov diffeomorphism; for example, there are no such diffeomorphisms on the sphere . The simplest examples of compact manifolds admitting them are the tori: they admit the so-called linear Anosov diffeomorphisms, which are isomorphisms having no eigenvalue of modulus 1. It was proved that any other Anosov diffeomorphism on a torus is topologically conjugate to one of this kind.

The problem of classifying manifolds that admit Anosov diffeomorphisms turned out to be very difficult, and still as of 2012 has no answer. The only known examples are infranil manifolds, and it is conjectured that they are the only ones.

A sufficient condition for transitivity is that all points are nonwandering: ${\displaystyle \Omega (f)=M}$ .

Also, it is unknown if every ${\displaystyle C^{1}}$ volume-preserving Anosov diffeomorphism is ergodic. Anosov proved it under a ${\displaystyle C^{2}}$ assumption. It is also true for ${\displaystyle C^{1+\alpha }}$ volume-preserving Anosov diffeomorphisms.

For ${\displaystyle C^{2}}$ transitive Anosov diffeomorphism ${\displaystyle f\colon M\to M}$ there exists a unique SRB measure (the acronym stands for Sinai, Ruelle and Bowen) ${\displaystyle \mu _{f}}$ supported on ${\displaystyle M}$ such that its basin ${\displaystyle B(\mu _{f})}$ is of full volume, where

${\displaystyle B(\mu _{f})=\left\{x\in M:{\frac {1}{n}}\sum _{k=0}^{n-1}\delta _{f^{k}x}\to \mu _{f}\right\}.}$

£#h5#£Anosov flow on (tangent bundles of) Riemann surfaces£#/h5#£
As an example, this section develops the case of the Anosov flow on the tangent bundle of a Riemann surface of negative curvature. This flow can be understood in terms of the flow on the tangent bundle of the Poincaré half-plane model of hyperbolic geometry. Riemann surfaces of negative curvature may be defined as Fuchsian models, that is, as the quotients of the upper half-plane and a Fuchsian group. For the following, let H be the upper half-plane; let Γ be a Fuchsian group; let M = H/Γ be a Riemann surface of negative curvature as the quotient of "M" by the action of the group Γ, and let ${\displaystyle T^{1}M}$ be the tangent bundle of unit-length vectors on the manifold M, and let ${\displaystyle T^{1}H}$ be the tangent bundle of unit-length vectors on H. Note that a bundle of unit-length vectors on a surface is the principal bundle of a complex line bundle.


£#h5#£Lie vector fields£#/h5#£
One starts by noting that ${\displaystyle T^{1}H}$ is isomorphic to the Lie group PSL(2,R). This group is the group of orientation-preserving isometries of the upper half-plane. The Lie algebra of PSL(2,R) is sl(2,R), and is represented by the matrices

${\displaystyle J={\begin{pmatrix}1/2&0\\0&-1/2\\\end{pmatrix}}\qquad X={\begin{pmatrix}0&1\\0&0\\\end{pmatrix}}\qquad Y={\begin{pmatrix}0&0\\1&0\end{pmatrix}}}$
which have the algebra

${\displaystyle [J,X]=X\qquad [J,Y]=-Y\qquad [X,Y]=2J}$
The exponential maps

${\displaystyle g_{t}=\exp(tJ)={\begin{pmatrix}e^{t/2}&0\\0&e^{-t/2}\\\end{pmatrix}}\qquad h_{t}^{*}=\exp(tX)={\begin{pmatrix}1&t\\0&1\\\end{pmatrix}}\qquad h_{t}=\exp(tY)={\begin{pmatrix}1&0\\t&1\\\end{pmatrix}}}$
define right-invariant flows on the manifold of ${\displaystyle T^{1}H=\operatorname {PSL} (2,\mathbb {R} )}$ , and likewise on ${\displaystyle T^{1}M}$ . Defining ${\displaystyle P=T^{1}H}$ and ${\displaystyle Q=T^{1}M}$ , these flows define vector fields on P and Q, whose vectors lie in TP and TQ. These are just the standard, ordinary Lie vector fields on the manifold of a Lie group, and the presentation above is a standard exposition of a Lie vector field.


£#h5#£Anosov flow£#/h5#£
The connection to the Anosov flow comes from the realization that ${\displaystyle g_{t}}$ is the geodesic flow on P and Q. Lie vector fields being (by definition) left invariant under the action of a group element, one has that these fields are left invariant under the specific elements ${\displaystyle g_{t}}$ of the geodesic flow. In other words, the spaces TP and TQ are split into three one-dimensional spaces, or subbundles, each of which are invariant under the geodesic flow. The final step is to notice that vector fields in one subbundle expand (and expand exponentially), those in another are unchanged, and those in a third shrink (and do so exponentially).

More precisely, the tangent bundle TQ may be written as the direct sum

${\displaystyle TQ=E^{+}\oplus E^{0}\oplus E^{-}}$
or, at a point ${\displaystyle g\cdot e=q\in Q}$ , the direct sum

${\displaystyle T_{q}Q=E_{q}^{+}\oplus E_{q}^{0}\oplus E_{q}^{-}}$
corresponding to the Lie algebra generators Y, J and X, respectively, carried, by the left action of group element g, from the origin e to the point q. That is, one has ${\displaystyle E_{e}^{+}=Y,E_{e}^{0}=J}$ and ${\displaystyle E_{e}^{-}=X}$ . These spaces are each subbundles, and are preserved (are invariant) under the action of the geodesic flow; that is, under the action of group elements ${\displaystyle g=g_{t}}$ .

To compare the lengths of vectors in ${\displaystyle T_{q}Q}$ at different points q, one needs a metric. Any inner product at ${\displaystyle T_{e}P=sl(2,\mathbb {R} )}$ extends to a left-invariant Riemannian metric on P, and thus to a Riemannian metric on Q. The length of a vector ${\displaystyle v\in E_{q}^{+}}$ expands exponentially as exp(t) under the action of ${\displaystyle g_{t}}$ . The length of a vector ${\displaystyle v\in E_{q}^{-}}$ shrinks exponentially as exp(-t) under the action of ${\displaystyle g_{t}}$ . Vectors in ${\displaystyle E_{q}^{0}}$ are unchanged. This may be seen by examining how the group elements commute. The geodesic flow is invariant,

${\displaystyle g_{s}g_{t}=g_{t}g_{s}=g_{s+t}}$
but the other two shrink and expand:

${\displaystyle g_{s}h_{t}^{*}=h_{t\exp(-s)}^{*}g_{s}}$
and

${\displaystyle g_{s}h_{t}=h_{t\exp(s)}g_{s}}$
where we recall that a tangent vector in ${\displaystyle E_{q}^{+}}$ is given by the derivative, with respect to t, of the curve ${\displaystyle h_{t}}$ , the setting ${\displaystyle t=0}$ .


£#h5#£Geometric interpretation of the Anosov flow£#/h5#£
When acting on the point ${\displaystyle z=i}$ of the upper half-plane, ${\displaystyle g_{t}}$ corresponds to a geodesic on the upper half plane, passing through the point ${\displaystyle z=i}$ . The action is the standard Möbius transformation action of SL(2,R) on the upper half-plane, so that

${\displaystyle g_{t}\cdot i={\begin{pmatrix}\exp(t/2)&0\\0&\exp(-t/2)\end{pmatrix}}\cdot i=i\exp(t)}$
A general geodesic is given by

${\displaystyle {\begin{pmatrix}a&b\\c&d\end{pmatrix}}\cdot i\exp(t)={\frac {ai\exp(t)+b}{ci\exp(t)+d}}}$
with a, b, c and d real, with ${\displaystyle ad-bc=1}$ . The curves ${\displaystyle h_{t}^{*}}$ and ${\displaystyle h_{t}}$ are called horocycles. Horocycles correspond to the motion of the normal vectors of a horosphere on the upper half-plane.


£#h5#£See also£#/h5#£ £#ul#££#li#£Ergodic flow£#/li#£ £#li#£Morse–Smale system£#/li#£ £#li#£Pseudo-Anosov map£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£"Y-system,U-system, C-system", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#£ £#li#£Anthony Manning, Dynamics of geodesic and horocycle flows on surfaces of constant negative curvature, (1991), appearing as Chapter 3 in Ergodic Theory, Symbolic Dynamics and Hyperbolic Spaces, Tim Bedford, Michael Keane and Caroline Series, Eds. Oxford University Press, Oxford (1991). ISBN 0-19-853390-X (Provides an expository introduction to the Anosov flow on SL(2,R).)£#/li#£ £#li#£This article incorporates material from Anosov diffeomorphism on PlanetMath, which is licensed under the Creative Commons Attribution/Share-Alike License.£#/li#£ £#li#£Toshikazu Sunada, Magnetic flows on a Riemann surface, Proc. KAIST Math. Workshop (1993), 93–108.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Anosov, D. "Roughness of Geodesic Flows on Compact Riemannian Manifolds of Negative Curvature." Dokl. Akad. Nauk SSSR 145, 707-709, 1962. English translation in Soviet Math. Dokl. 3, 1068-1069, 1962.£#/li#££#li#£Anosov, D. "Ergodic Properties of Geodesic Flows on Closed Riemannian Manifolds of Negative Curvature." Dokl. Akad. Nauk SSSR 151, 1250-1252, 1963. English translated in Soviet Math. Dokl. 4, 1153-1156, 1963.£#/li#££#li#£Lichtenberg, A. J. and Lieberman, M. A. Regular and Chaotic Dynamics, 2nd ed. New York: Springer-Verlag, pp. 305-307, 1992.£#/li#££#li#£Sondow, J. "Fixed Points of Anosov Maps of Certain Manifolds." Proc. Amer. Math. Soc. 61, 381-384, 1976.£#/li#££#li#£ Anosov, D. "Roughness of Geodesic Flows on Compact Riemannian Manifolds of Negative Curvature." Dokl. Akad. Nauk SSSR 145, 707-709, 1962. English translation in Soviet Math. Dokl. 3, 1068-1069, 1962. £#/li#££#li#£ Anosov, D. "Ergodic Properties of Geodesic Flows on Closed Riemannian Manifolds of Negative Curvature." Dokl. Akad. Nauk SSSR 151, 1250-1252, 1963. English translated in Soviet Math. Dokl. 4, 1153-1156, 1963. £#/li#££#li#£ Lichtenberg, A. J. and Lieberman, M. A. Regular and Chaotic Dynamics, 2nd ed. New York: Springer-Verlag, pp. 305-307, 1992. £#/li#££#li#£ Sondow, J. "Fixed Points of Anosov Maps of Certain Manifolds." Proc. Amer. Math. Soc. 61, 381-384, 1976. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Dynamical Systems £#/li#££#/ul#£




£#h3#£Anti-Analytic Function£#/h3#£

In mathematics, antiholomorphic functions (also called antianalytic functions) are a family of functions closely related to but distinct from holomorphic functions.

A function of the complex variable z defined on an open set in the complex plane is said to be antiholomorphic if its derivative with respect to z exists in the neighbourhood of each and every point in that set, where z is the complex conjugate.

A definition of antiholomorphic function follows:

"[a] function ${\displaystyle f(z)=u+iv}$ of one or more complex variables ${\displaystyle z=\left(z_{1},\dots ,z_{n}\right)\in \mathbb {C} ^{n}}$ [is said to be anti-holomorphic if (and only if) it] is the complex conjugate of a holomorphic function ${\displaystyle {\overline {f\left(z\right)}}=u-iv}$ ."

One can show that if f(z) is a holomorphic function on an open set D, then f(z) is an antiholomorphic function on D, where D is the reflection against the x-axis of D, or in other words, D is the set of complex conjugates of elements of D. Moreover, any antiholomorphic function can be obtained in this manner from a holomorphic function. This implies that a function is antiholomorphic if and only if it can be expanded in a power series in z in a neighborhood of each point in its domain. Also, a function f(z) is antiholomorphic on an open set D if and only if the function f(z) is holomorphic on D.

If a function is both holomorphic and antiholomorphic, then it is constant on any connected component of its domain.


£#h5#£References£#/h5#£



£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Complex Analysis > General Complex Analysis £#/li#££#/ul#£




£#h3#£Anticlastic£#/h3#£

Classicism, in the arts, refers generally to a high regard for a classical period, classical antiquity in the Western tradition, as setting standards for taste which the classicists seek to emulate. In its purest form, classicism is an aesthetic attitude dependent on principles based in the culture, art and literature of ancient Greece and Rome, with the emphasis on form, simplicity, proportion, clarity of structure, perfection, restrained emotion, as well as explicit appeal to the intellect. The art of classicism typically seeks to be formal and restrained: of the Discobolus Sir Kenneth Clark observed, "if we object to his restraint and compression we are simply objecting to the classicism of classic art. A violent emphasis or a sudden acceleration of rhythmic movement would have destroyed those qualities of balance and completeness through which it retained until the present century its position of authority in the restricted repertoire of visual images." Classicism, as Clark noted, implies a canon of widely accepted ideal forms, whether in the Western canon that he was examining in The Nude (1956), or the literary Chinese classics or Chinese art, where the revival of classic styles is also a recurring feature.

Classicism is a force which is often present in post-medieval European and European influenced traditions; however, some periods felt themselves more connected to the classical ideals than others, particularly the Age of Enlightenment, when Neoclassicism was an important movement in the visual arts.


£#h5#£General term£#/h5#£
Classicism is a specific genre of philosophy, expressing itself in literature, architecture, art, and music, which has Ancient Greek and Roman sources and an emphasis on society. It was particularly expressed in the Neoclassicism of the Age of Enlightenment.

Classicism is a recurrent tendency in the Late Antique period, and had a major revival in Carolingian and Ottonian art. There was another, more durable revival in the Italian renaissance when the fall of Byzantium and rising trade with the Islamic cultures brought a flood of knowledge about, and from, the antiquity of Europe. Until that time, the identification with antiquity had been seen as a continuous history of Christendom from the conversion of Roman Emperor Constantine I. Renaissance classicism introduced a host of elements into European culture, including the application of mathematics and empiricism into art, humanism, literary and depictive realism, and formalism. Importantly it also introduced Polytheism, or "paganism", and the juxtaposition of ancient and modern.

The classicism of the Renaissance led to, and gave way to, a different sense of what was "classical" in the 16th and 17th centuries. In this period, classicism took on more overtly structural overtones of orderliness, predictability, the use of geometry and grids, the importance of rigorous discipline and pedagogy, as well as the formation of schools of art and music. The court of Louis XIV was seen as the center of this form of classicism, with its references to the gods of Olympus as a symbolic prop for absolutism, its adherence to axiomatic and deductive reasoning, and its love of order and predictability.

This period sought the revival of classical art forms, including Greek drama and music. Opera, in its modern European form, had its roots in attempts to recreate the combination of singing and dancing with theatre thought to be the Greek norm. Examples of this appeal to classicism included Dante, Petrarch, and Shakespeare in poetry and theatre. Tudor drama, in particular, modeled itself after classical ideals and divided works into Tragedy and Comedy. Studying Ancient Greek became regarded as essential for a well-rounded education in the liberal arts.

The Renaissance also explicitly returned to architectural models and techniques associated with Greek and Roman antiquity, including the golden rectangle as a key proportion for buildings, the classical orders of columns, as well as a host of ornament and detail associated with Greek and Roman architecture. They also began reviving plastic arts such as bronze casting for sculpture, and used the classical naturalism as the foundation of drawing, painting and sculpture.

The Age of Enlightenment identified itself with a vision of antiquity which, while continuous with the classicism of the previous century, was shaken by the physics of Sir Isaac Newton, the improvements in machinery and measurement, and a sense of liberation which they saw as being present in the Greek civilization, particularly in its struggles against the Persian Empire. The ornate, organic, and complexly integrated forms of the baroque were to give way to a series of movements that regarded themselves expressly as "classical" or "neo-classical", or would rapidly be labelled as such. For example, the painting of Jacques-Louis David was seen as an attempt to return to formal balance, clarity, manliness, and vigor in art.

The 19th century saw the classical age as being the precursor of academicism, including such movements as uniformitarianism in the sciences, and the creation of rigorous categories in artistic fields. Various movements of the Romantic period saw themselves as classical revolts against a prevailing trend of emotionalism and irregularity, for example the Pre-Raphaelites. By this point, classicism was old enough that previous classical movements received revivals; for example, the Renaissance was seen as a means to combine the organic medieval with the orderly classical. The 19th century continued or extended many classical programs in the sciences, most notably the Newtonian program to account for the movement of energy between bodies by means of exchange of mechanical and thermal energy.

The 20th century saw a number of changes in the arts and sciences. Classicism was used both by those who rejected, or saw as temporary, transfigurations in the political, scientific, and social world and by those who embraced the changes as a means to overthrow the perceived weight of the 19th century. Thus, both pre-20th century disciplines were labelled "classical" and modern movements in art which saw themselves as aligned with light, space, sparseness of texture, and formal coherence.

In the present day philosophy classicism is used as a term particularly in relation to Apollonian over Dionysian impulses in society and art; that is a preference for rationality, or at least rationally guided catharsis, over emotionalism.


£#h5#£In the theatre£#/h5#£
Classicism in the theatre was developed by 17th century French playwrights from what they judged to be the rules of Greek classical theatre, including the "Classical unities" of time, place and action, found in the Poetics of Aristotle.

£#ul#££#li#£Unity of time referred to the need for the entire action of the play to take place in a fictional 24-hour period£#/li#£ £#li#£Unity of place meant that the action should unfold in a single location£#/li#£ £#li#£Unity of action meant that the play should be constructed around a single 'plot-line', such as a tragic love affair or a conflict between honour and duty.£#/li#££#/ul#£
Examples of classicist playwrights are Pierre Corneille, Jean Racine and Molière. In the period of Romanticism, Shakespeare, who conformed to none of the classical rules, became the focus of French argument over them, in which the Romantics eventually triumphed; Victor Hugo was among the first French playwrights to break these conventions.

The influence of these French rules on playwrights in other nations is debatable. In the English theatre, Restoration playwrights such as William Wycherly and William Congreve would have been familiar with them. William Shakespeare and his contemporaries did not follow this Classicist philosophy, in particular since they were not French and also because they wrote several decades prior to their establishment. Those of Shakespeare's plays that seem to display the unities, such as The Tempest, probably indicate a familiarity with actual models from classical antiquity.


£#h5#£In architecture£#/h5#£
Classicism in architecture developed during the Italian Renaissance, notably in the writings and designs of Leon Battista Alberti and the work of Filippo Brunelleschi. It places emphasis on symmetry, proportion, geometry and the regularity of parts as they are demonstrated in the architecture of Classical antiquity and, in particular, the architecture of Ancient Rome, of which many examples remained.

Orderly arrangements of columns, pilasters and lintels, as well as the use of semicircular arches, hemispherical domes, niches and aedicules replaced the more complex proportional systems and irregular profiles of medieval buildings. This style quickly spread to other Italian cities and then to France, Germany, England, Russia and elsewhere.

In the 16th century, Sebastiano Serlio helped codify the classical orders and Andrea Palladio's legacy evolved into the long tradition of Palladian architecture. Building off of these influences, the 17th-century architects Inigo Jones and Christopher Wren firmly established classicism in England.

For the development of classicism from the mid-18th-century onwards, see Neoclassical architecture.


£#h5#£In the fine arts£#/h5#£ £#ul#££#li#£For Greek art of the 5th century B.C.E., see Classical art in ancient Greece and the Severe style£#/li#££#/ul#£
Italian Renaissance painting and sculpture are marked by their renewal of classical forms, motifs and subjects. In the 15th century Leon Battista Alberti was important in theorizing many of the ideas for painting that came to a fully realized product with Raphael's School of Athens during the High Renaissance. The themes continued largely unbroken into the 17th century, when artists such as Nicolas Poussin and Charles Le Brun represented of the more rigid classicism. Like Italian classicizing ideas in the 15th and 16th centuries, it spread through Europe in the mid to late 17th century.

Later classicism in painting and sculpture from the mid-18th and 19th centuries is generally referred to as Neoclassicism.


£#h5#£Political philosophy£#/h5#£
Classicism in political philosophy dates back to the ancient Greeks. Western political philosophy is often attributed to the great Greek philosopher Plato. Although political theory of this time starts with Plato, it quickly becomes complex when Plato's pupil, Aristotle, formulates his own ideas. "The political theories of both philosophers are closely tied to their ethical theories, and their interest is in questions concerning constitutions or forms of government."

However, Plato and Aristotle are not the seedbed but simply the seeds that grew from a seedbed of political predecessors who had debated this topic for centuries before their time. For example, Herodotus sketched out a debate between Theseus, a king of the time, and Creon's messenger. The debate simply shows proponents of democracy, monarchy, and oligarchy and how they all feel about these forms of government. Herodotus' sketch is just one of the beginning seedbeds for which Plato and Aristotle grew their own political theories.

Another Greek philosopher who was pivotal in the development of Classical political philosophy was Socrates. Although he was not a theory-builder, he often stimulated fellow citizens with paradoxes that challenged them to reflect on their own beliefs. Socrates thought "the values that ought to determine how individuals live their lives should also shape the political life of the community." he believed the people of Athens involved wealth and money too much into the politics of their city. He judged the citizens for the way they amassed wealth and power over simple things like projects for their community.

Just like Plato and Aristotle, Socrates did not come up with these ideas alone. Socrates ideals stem back from Protagoras and other 'sophists'. These 'teachers of political arts' were the first to think and act as Socrates did. Where the two diverge is in the way they practiced their ideals. Protagoras' ideals were loved by Athens. Whereas Socrates challenged and pushed the citizens and he was not as loved.

In the end, ancient Greece is to be credited with the foundation of Classical political philosophy.


£#h5#£See also£#/h5#£ £#ul#££#li#£Classical tradition£#/li#£ £#li#£Quarrel of the Ancients and the Moderns£#/li#£ £#li#£Weimar Classicism£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£Further reading£#/h5#£ £#ul#££#li#£Kallendorf, Craig (2007). A Companion to the Classical Tradition. Blackwell Publishing. ISBN 9781405122948. Retrieved 2012-05-06. Essays by various authors on topics related to historical periods, places, and themes. Limited preview online.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Renaissance & Classicism from encyclopedia£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Differential Geometry > General Differential Geometry £#/li#££#/ul#£




£#h3#£Anticommutative£#/h3#£

In mathematics, anticommutativity is a specific property of some non-commutative mathematical operations. Swapping the position of two arguments of an antisymmetric operation yields a result which is the inverse of the result with unswapped arguments. The notion inverse refers to a group structure on the operation's codomain, possibly with another operation. Subtraction is an anticommutative operation because −(a − b) = b − a; for example, 2 − 10 = −(10 − 2) = −8. Another prominent example of an anticommutative operation is the Lie bracket.

In mathematical physics, where symmetry is of central importance, these operations are mostly called antisymmetric operations, and are extended in an associative setting to cover more than two arguments.


£#h5#£Definition£#/h5#£
If ${\displaystyle A,B}$ are two abelian groups, a bilinear map ${\displaystyle f:A^{2}\to B}$ is anticommutative if for all ${\displaystyle x,y\in A}$ we have

${\displaystyle f(x,y)=-f(y,x).}$
More generally, a multilinear map ${\displaystyle g:A^{n}\to B}$ is anticommutative if for all ${\displaystyle x_{1},\dots x_{n}\in A}$ we have

${\displaystyle g(x_{1},x_{2},\dots x_{n})={\text{sgn}}(\sigma )g(x_{\sigma (1)},x_{\sigma (2)},\dots x_{\sigma (n)})}$
where ${\displaystyle {\text{sgn}}(\sigma )}$ is the sign of the permutation ${\displaystyle \sigma }$ .


£#h5#£Properties£#/h5#£
If the abelian group ${\displaystyle B}$ has no 2-torsion, implying that if ${\displaystyle x=-x}$ then ${\displaystyle x=0}$ , then any anticommutative bilinear map ${\displaystyle f:A^{2}\to B}$ satisfies

${\displaystyle f(x,x)=0.}$
More generally, by transposing two elements, any anticommutative multilinear map ${\displaystyle g:A^{n}\to B}$ satisfies

${\displaystyle g(x_{1},x_{2},\dots x_{n})=0}$
if any of the ${\displaystyle x_{i}}$ are equal; such a map is said to be alternating. Conversely, using multilinearity, any alternating map is anticommutative. In the binary case this works as follows: if ${\displaystyle f:A^{2}\to B}$ is alternating then by bilinearity we have

${\displaystyle f(x+y,x+y)=f(x,x)+f(x,y)+f(y,x)+f(y,y)=f(x,y)+f(y,x)=0}$
and the proof in the multilinear case is the same but in only two of the inputs.


£#h5#£Examples£#/h5#£
Examples of anticommutative binary operations include:

£#ul#££#li#£Cross product£#/li#£ £#li#£Lie bracket of a Lie algebra£#/li#£ £#li#£Lie bracket of a Lie ring£#/li#£ £#li#£Subtraction£#/li#££#/ul#£
£#h5#£See also£#/h5#£ £#ul#££#li#£Commutativity£#/li#£ £#li#£Commutator£#/li#£ £#li#£Exterior algebra£#/li#£ £#li#£Graded-commutative ring£#/li#£ £#li#£Operation (mathematics)£#/li#£ £#li#£Symmetry in mathematics£#/li#£ £#li#£Particle statistics (for anticommutativity in physics).£#/li#££#/ul#£
£#h5#£References£#/h5#£ £#ul#££#li#£Bourbaki, Nicolas (1989), "Chapter III. Tensor algebras, exterior algebras, symmetric algebras", Algebra. Chapters 1–3, Elements of Mathematics (2nd printing ed.), Berlin-Heidelberg-New York City: Springer-Verlag, ISBN 3-540-64243-9, MR 0979982, Zbl 0904.00001.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Gainov, A.T. (2001) [1994], "Anti-commutative algebra", Encyclopedia of Mathematics, EMS Press. Which references the Original Russian work£#/li#£ £#li#£Weisstein, Eric W. "Anticommutative". MathWorld.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Operator Theory £#/li#££#/ul#£




£#h3#£Anticommutator£#/h3#£

In mathematics, the commutator gives an indication of the extent to which a certain binary operation fails to be commutative. There are different definitions used in group theory and ring theory.


£#h5#£Group theory£#/h5#£
The commutator of two elements, g and h, of a group G, is the element

[g, h] = g−1h−1gh.
This element is equal to the group's identity if and only if g and h commute (from the definition gh = hg [g, h], being [g, h] equal to the identity if and only if gh = hg).

The set of all commutators of a group is not in general closed under the group operation, but the subgroup of G generated by all commutators is closed and is called the derived group or the commutator subgroup of G. Commutators are used to define nilpotent and solvable groups and the largest abelian quotient group.

The definition of the commutator above is used throughout this article, but many other group theorists define the commutator as

[g, h] = ghg−1h−1.

£#h5#£Identities (group theory)£#/h5#£
Commutator identities are an important tool in group theory. The expression ax denotes the conjugate of a by x, defined as x−1ax.

£#li#£ ${\displaystyle x^{y}=x[x,y].}$ £#/li#£ £#li#£ ${\displaystyle [y,x]=[x,y]^{-1}.}$ £#/li#£ £#li#£ ${\displaystyle [x,zy]=[x,y]\cdot [x,z]^{y}}$ and ${\displaystyle [xz,y]=[x,y]^{z}\cdot [z,y].}$ £#/li#£ £#li#£ ${\displaystyle \left[x,y^{-1}\right]=[y,x]^{y^{-1}}}$ and ${\displaystyle \left[x^{-1},y\right]=[y,x]^{x^{-1}}.}$ £#/li#£ £#li#£ ${\displaystyle \left[\left[x,y^{-1}\right],z\right]^{y}\cdot \left[\left[y,z^{-1}\right],x\right]^{z}\cdot \left[\left[z,x^{-1}\right],y\right]^{x}=1}$ and ${\displaystyle \left[\left[x,y\right],z^{x}\right]\cdot \left[[z,x],y^{z}\right]\cdot \left[[y,z],x^{y}\right]=1.}$ £#/li#£
Identity (5) is also known as the Hall–Witt identity, after Philip Hall and Ernst Witt. It is a group-theoretic analogue of the Jacobi identity for the ring-theoretic commutator (see next section).

N.B., the above definition of the conjugate of a by x is used by some group theorists. Many other group theorists define the conjugate of a by x as xax−1. This is often written ${\displaystyle {}^{x}a}$ . Similar identities hold for these conventions.

Many identities are used that are true modulo certain subgroups. These can be particularly useful in the study of solvable groups and nilpotent groups. For instance, in any group, second powers behave well:

${\displaystyle (xy)^{2}=x^{2}y^{2}[y,x][[y,x],y].}$
If the derived subgroup is central, then

${\displaystyle (xy)^{n}=x^{n}y^{n}[y,x]^{\binom {n}{2}}.}$

£#h5#£Ring theory£#/h5#£
The commutator of two elements a and b of a ring (including any associative algebra) is defined by

${\displaystyle [a,b]=ab-ba.}$
It is zero if and only if a and b commute. In linear algebra, if two endomorphisms of a space are represented by commuting matrices in terms of one basis, then they are so represented in terms of every basis. By using the commutator as a Lie bracket, every associative algebra can be turned into a Lie algebra.

The anticommutator of two elements a and b of a ring or an associative algebra is defined by

${\displaystyle \{a,b\}=ab+ba.}$
Sometimes ${\displaystyle [a,b]_{+}}$ is used to denote anticommutator, while ${\displaystyle [a,b]_{-}}$ is then used for commutator. The anticommutator is used less often, but can be used to define Clifford algebras and Jordan algebras, and in the derivation of the Dirac equation in particle physics.

The commutator of two operators acting on a Hilbert space is a central concept in quantum mechanics, since it quantifies how well the two observables described by these operators can be measured simultaneously. The uncertainty principle is ultimately a theorem about such commutators, by virtue of the Robertson–Schrödinger relation. In phase space, equivalent commutators of function star-products are called Moyal brackets, and are completely isomorphic to the Hilbert space commutator structures mentioned.


£#h5#£Identities (ring theory)£#/h5#£
The commutator has the following properties:


£#h5#£Lie-algebra identities£#/h5#£
£#li#£ ${\displaystyle [A+B,C]=[A,C]+[B,C]}$ £#/li#£ £#li#£ ${\displaystyle [A,A]=0}$ £#/li#£ £#li#£ ${\displaystyle [A,B]=-[B,A]}$ £#/li#£ £#li#£ ${\displaystyle [A,[B,C]]+[B,[C,A]]+[C,[A,B]]=0}$ £#/li#£
Relation (3) is called anticommutativity, while (4) is the Jacobi identity.


£#h5#£Additional identities£#/h5#£
£#li#£ ${\displaystyle [A,BC]=[A,B]C+B[A,C]}$ £#/li#£ £#li#£ ${\displaystyle [A,BCD]=[A,B]CD+B[A,C]D+BC[A,D]}$ £#/li#£ £#li#£ ${\displaystyle [A,BCDE]=[A,B]CDE+B[A,C]DE+BC[A,D]E+BCD[A,E]}$ £#/li#£ £#li#£ ${\displaystyle [AB,C]=A[B,C]+[A,C]B}$ £#/li#£ £#li#£ ${\displaystyle [ABC,D]=AB[C,D]+A[B,D]C+[A,D]BC}$ £#/li#£ £#li#£ ${\displaystyle [ABCD,E]=ABC[D,E]+AB[C,E]D+A[B,E]CD+[A,E]BCD}$ £#/li#£ £#li#£ ${\displaystyle [A,B+C]=[A,B]+[A,C]}$ £#/li#£ £#li#£ ${\displaystyle [A+B,C+D]=[A,C]+[A,D]+[B,C]+[B,D]}$ £#/li#£ £#li#£ ${\displaystyle [AB,CD]=A[B,C]D+[A,C]BD+CA[B,D]+C[A,D]B}$ £#/li#£ £#li#£ ${\displaystyle [[A,C],[B,D]]=[[[A,B],C],D]+[[[B,C],D],A]+[[[C,D],A],B]+[[[D,A],B],C]}$ £#/li#£
If A is a fixed element of a ring R, identity (1) can be interpreted as a Leibniz rule for the map ${\displaystyle \operatorname {ad} _{A}:R\rightarrow R}$ given by ${\displaystyle \operatorname {ad} _{A}(B)=[A,B]}$ . In other words, the map adA defines a derivation on the ring R. Identities (2), (3) represent Leibniz rules for more than two factors, and are valid for any derivation. Identities (4)–(6) can also be interpreted as Leibniz rules. Identities (7), (8) express Z-bilinearity.

Some of the above identities can be extended to the anticommutator using the above ± subscript notation. For example:

£#li#£ ${\displaystyle [AB,C]_{\pm }=A[B,C]_{-}+[A,C]_{\pm }B}$ £#/li#£ £#li#£ ${\displaystyle [AB,CD]_{\pm }=A[B,C]_{-}D+AC[B,D]_{-}+[A,C]_{-}DB+C[A,D]_{\pm }B}$ £#/li#£ £#li#£ ${\displaystyle [[A,B],[C,D]]=[[[B,C]_{+},A]_{+},D]-[[[B,D]_{+},A]_{+},C]+[[[A,D]_{+},B]_{+},C]-[[[A,C]_{+},B]_{+},D]}$ £#/li#£ £#li#£ ${\displaystyle \left[A,[B,C]_{\pm }\right]+\left[B,[C,A]_{\pm }\right]+\left[C,[A,B]_{\pm }\right]=0}$ £#/li#£ £#li#£ ${\displaystyle [A,BC]_{\pm }=[A,B]_{-}C+B[A,C]_{\pm }}$ £#/li#£ £#li#£ ${\displaystyle [A,BC]=[A,B]_{\pm }C\mp B[A,C]_{\pm }}$ £#/li#£

£#h5#£Exponential identities£#/h5#£
Consider a ring or algebra in which the exponential ${\displaystyle e^{A}=\exp(A)=1+A+{\tfrac {1}{2!}}A^{2}+\cdots }$ can be meaningfully defined, such as a Banach algebra or a ring of formal power series.

In such a ring, Hadamard's lemma applied to nested commutators gives: ${\textstyle e^{A}Be^{-A}\ =\ B+[A,B]+{\frac {1}{2!}}[A,[A,B]]+{\frac {1}{3!}}[A,[A,[A,B]]]+\cdots \ =\ e^{\operatorname {ad} _{A}}(B).}$ (For the last expression, see Adjoint derivation below.) This formula underlies the Baker–Campbell–Hausdorff expansion of log(exp(A) exp(B)).

A similar expansion expresses the group commutator of expressions ${\displaystyle e^{A}}$ (analogous to elements of a Lie group) in terms of a series of nested commutators (Lie brackets),


£#h5#£Graded rings and algebras£#/h5#£
When dealing with graded algebras, the commutator is usually replaced by the graded commutator, defined in homogeneous components as

${\displaystyle [\omega ,\eta ]_{gr}:=\omega \eta -(-1)^{\deg \omega \deg \eta }\eta \omega .}$

£#h5#£Adjoint derivation£#/h5#£
Especially if one deals with multiple commutators in a ring R, another notation turns out to be useful. For an element ${\displaystyle x\in R}$ , we define the adjoint mapping ${\displaystyle \mathrm {ad} _{x}:R\to R}$ by:

${\displaystyle \operatorname {ad} _{x}(y)=[x,y]=xy-yx.}$
This mapping is a derivation on the ring R:

${\displaystyle \mathrm {ad} _{x}\!(yz)\ =\ \mathrm {ad} _{x}\!(y)\,z\,+\,y\,\mathrm {ad} _{x}\!(z).}$
By the Jacobi identity, it is also a derivation over the commutation operation:

${\displaystyle \mathrm {ad} _{x}[y,z]\ =\ [\mathrm {ad} _{x}\!(y),z]\,+\,[y,\mathrm {ad} _{x}\!(z)].}$
Composing such mappings, we get for example ${\displaystyle \operatorname {ad} _{x}\operatorname {ad} _{y}(z)=[x,[y,z]\,]}$ and

We may consider ${\displaystyle \mathrm {ad} }$ itself as a mapping, ${\displaystyle \mathrm {ad} :R\to \mathrm {End} (R)}$ , where ${\displaystyle \mathrm {End} (R)}$ is the ring of mappings from R to itself with composition as the multiplication operation. Then ${\displaystyle \mathrm {ad} }$ is a Lie algebra homomorphism, preserving the commutator:
${\displaystyle \operatorname {ad} _{[x,y]}=\left[\operatorname {ad} _{x},\operatorname {ad} _{y}\right].}$
By contrast, it is not always a ring homomorphism: usually ${\displaystyle \operatorname {ad} _{xy}\,\neq \,\operatorname {ad} _{x}\operatorname {ad} _{y}}$ .


£#h5#£General Leibniz rule£#/h5#£
The general Leibniz rule, expanding repeated derivatives of a product, can be written abstractly using the adjoint representation:

${\displaystyle x^{n}y=\sum _{k=0}^{n}{\binom {n}{k}}\operatorname {ad} _{x}^{k}\!(y)\,x^{n-k}.}$
Replacing x by the differentiation operator ${\displaystyle \partial }$ , and y by the multiplication operator ${\displaystyle m_{f}:g\mapsto fg}$ , we get ${\displaystyle \operatorname {ad} (\partial )(m_{f})=m_{\partial (f)}}$ , and applying both sides to a function g, the identity becomes the usual Leibniz rule for the n-th derivative ${\displaystyle \partial ^{n}\!(fg)}$ .


£#h5#£See also£#/h5#£ £#ul#££#li#£Anticommutativity£#/li#£ £#li#£Associator£#/li#£ £#li#£Baker–Campbell–Hausdorff formula£#/li#£ £#li#£Canonical commutation relation£#/li#£ £#li#£Centralizer a.k.a. commutant£#/li#£ £#li#£Derivation (abstract algebra)£#/li#£ £#li#£Moyal bracket£#/li#£ £#li#£Pincherle derivative£#/li#£ £#li#£Poisson bracket£#/li#£ £#li#£Ternary commutator£#/li#£ £#li#£Three subgroups lemma£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Fraleigh, John B. (1976), A First Course In Abstract Algebra (2nd ed.), Reading: Addison-Wesley, ISBN 0-201-01984-1£#/li#£ £#li#£Griffiths, David J. (2004), Introduction to Quantum Mechanics (2nd ed.), Prentice Hall, ISBN 0-13-805326-X£#/li#£ £#li#£Herstein, I. N. (1975), Topics In Algebra (2nd ed.), Wiley, ISBN 0471010901£#/li#£ £#li#£Liboff, Richard L. (2003), Introductory Quantum Mechanics (4th ed.), Addison-Wesley, ISBN 0-8053-8714-5£#/li#£ £#li#£McKay, Susan (2000), Finite p-groups, Queen Mary Maths Notes, vol. 18, University of London, ISBN 978-0-902480-17-9, MR 1802994£#/li#£ £#li#£McMahon, D. (2008), Quantum Field Theory, McGraw Hill, ISBN 978-0-07-154382-8£#/li#££#/ul#£
£#h5#£Further reading£#/h5#£ £#ul#££#li#£McKenzie, R.; Snow, J. (2005), "Congruence modular varieties: commutator theory", in Kudryavtsev, V. B.; Rosenberg, I. G. (eds.), Structural Theory of Automata, Semigroups, and Universal Algebra, NATO Science Series II, vol. 207, Springer, pp. 273–329, doi:10.1007/1-4020-3817-8_11, ISBN 9781402038174£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Commutator", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Operator Theory £#/li#££#/ul#£




£#h3#£Anticonformal Mapping£#/h3#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Complex Analysis > Conformal Mapping £#/li#££#/ul#£




£#h3#£Antiderivative£#/h3#£

In calculus, an antiderivative, inverse derivative, primitive function, primitive integral or indefinite integral of a function f is a differentiable function F whose derivative is equal to the original function f. This can be stated symbolically as F' = f. The process of solving for antiderivatives is called antidifferentiation (or indefinite integration), and its opposite operation is called differentiation, which is the process of finding a derivative. Antiderivatives are often denoted by capital Roman letters such as F and G.

Antiderivatives are related to definite integrals through the second fundamental theorem of calculus: the definite integral of a function over a closed interval where the function is Riemann integrable is equal to the difference between the values of an antiderivative evaluated at the endpoints of the interval.

In physics, antiderivatives arise in the context of rectilinear motion (e.g., in explaining the relationship between position, velocity and acceleration). The discrete equivalent of the notion of antiderivative is antidifference.


£#h5#£Examples£#/h5#£
The function ${\displaystyle F(x)={\tfrac {x^{3}}{3}}}$ is an antiderivative of ${\displaystyle f(x)=x^{2}}$ , since the derivative of ${\displaystyle {\tfrac {x^{3}}{3}}}$ is ${\displaystyle x^{2}}$ , and since the derivative of a constant is zero, ${\displaystyle x^{2}}$ will have an infinite number of antiderivatives, such as ${\displaystyle {\tfrac {x^{3}}{3}},{\tfrac {x^{3}}{3}}+1,{\tfrac {x^{3}}{3}}-2}$ , etc. Thus, all the antiderivatives of ${\displaystyle x^{2}}$ can be obtained by changing the value of c in ${\displaystyle F(x)={\tfrac {x^{3}}{3}}+c}$ , where c is an arbitrary constant known as the constant of integration. Essentially, the graphs of antiderivatives of a given function are vertical translations of each other, with each graph's vertical location depending upon the value c.

More generally, the power function ${\displaystyle f(x)=x^{n}}$ has antiderivative ${\displaystyle F(x)={\tfrac {x^{n+1}}{n+1}}+c}$ if n ≠ −1, and ${\displaystyle F(x)=\ln |x|+c}$ if n = −1.

In physics, the integration of acceleration yields velocity plus a constant. The constant is the initial velocity term that would be lost upon taking the derivative of velocity, because the derivative of a constant term is zero. This same pattern applies to further integrations and derivatives of motion (position, velocity, acceleration, and so on).


£#h5#£Uses and properties£#/h5#£
Antiderivatives can be used to compute definite integrals, using the fundamental theorem of calculus: if F is an antiderivative of the integrable function f over the interval ${\displaystyle [a,b]}$ , then:

${\displaystyle \int _{a}^{b}f(x)\,\mathrm {d} x=F(b)-F(a).}$
Because of this, each of the infinitely many antiderivatives of a given function f may be called the "indefinite integral" of f and written using the integral symbol with no bounds:

${\displaystyle \int f(x)\,\mathrm {d} x.}$
If F is an antiderivative of f, and the function f is defined on some interval, then every other antiderivative G of f differs from F by a constant: there exists a number c such that ${\displaystyle G(x)=F(x)+c}$ for all x. c is called the constant of integration. If the domain of F is a disjoint union of two or more (open) intervals, then a different constant of integration may be chosen for each of the intervals. For instance

${\displaystyle F(x)={\begin{cases}-{\frac {1}{x}}+c_{1}\quad x<0\\-{\frac {1}{x}}+c_{2}\quad x>0\end{cases}}}$
is the most general antiderivative of ${\displaystyle f(x)=1/x^{2}}$ on its natural domain ${\displaystyle (-\infty ,0)\cup (0,\infty ).}$

Every continuous function f has an antiderivative, and one antiderivative F is given by the definite integral of f with variable upper boundary:

${\displaystyle F(x)=\int _{0}^{x}f(t)\,\mathrm {d} t.}$
Varying the lower boundary produces other antiderivatives (but not necessarily all possible antiderivatives). This is another formulation of the fundamental theorem of calculus.

There are many functions whose antiderivatives, even though they exist, cannot be expressed in terms of elementary functions (like polynomials, exponential functions, logarithms, trigonometric functions, inverse trigonometric functions and their combinations). Examples of these are

${\displaystyle \int e^{-x^{2}}\,\mathrm {d} x,\qquad \int \sin x^{2}\,\mathrm {d} x,\qquad \int {\frac {\sin x}{x}}\,\mathrm {d} x,\qquad \int {\frac {1}{\ln x}}\,\mathrm {d} x,\qquad \int x^{x}\,\mathrm {d} x.}$
From left to right, the functions are the error function, the Fresnel function, the sine integral, the logarithmic integral function and Sophomore's dream. For a more detailed discussion, see also Differential Galois theory.


£#h5#£Techniques of integration£#/h5#£
Finding antiderivatives of elementary functions is often considerably harder than finding their derivatives (indeed, there is no pre-defined method for computing indefinite integrals). For some elementary functions, it is impossible to find an antiderivative in terms of other elementary functions. To learn more, see elementary functions and nonelementary integral.

There exist many properties and techniques for finding antiderivatives. These include, among others:

£#ul#££#li#£The linearity of integration (which breaks complicated integrals into simpler ones)£#/li#£ £#li#£Integration by substitution, often combined with trigonometric identities or the natural logarithm £#ul#££#li#£The inverse chain rule method (a special case of integration by substitution)£#/li#££#/ul#££#/li#£ £#li#£Integration by parts (to integrate products of functions)£#/li#£ £#li#£Inverse function integration (a formula that expresses the antiderivative of the inverse f−1 of an invertible and continuous function f, in terms of the antiderivative of f and of f−1).£#/li#£ £#li#£The method of partial fractions in integration (which allows us to integrate all rational functions—fractions of two polynomials)£#/li#£ £#li#£The Risch algorithm£#/li#£ £#li#£Additional techniques for multiple integrations (see for instance double integrals, polar coordinates, the Jacobian and the Stokes' theorem)£#/li#£ £#li#£Numerical integration (a technique for approximating a definite integral when no elementary antiderivative exists, as in the case of exp(−x2))£#/li#£ £#li#£Algebraic manipulation of integrand (so that other integration techniques, such as integration by substitution, may be used)£#/li#£ £#li#£Cauchy formula for repeated integration (to calculate the n-times antiderivative of a function) £#/li#££#/ul#£
Computer algebra systems can be used to automate some or all of the work involved in the symbolic techniques above, which is particularly useful when the algebraic manipulations involved are very complex or lengthy. Integrals which have already been derived can be looked up in a table of integrals.


£#h5#£Of non-continuous functions£#/h5#£
Non-continuous functions can have antiderivatives. While there are still open questions in this area, it is known that:

£#ul#££#li#£Some highly pathological functions with large sets of discontinuities may nevertheless have antiderivatives.£#/li#£ £#li#£In some cases, the antiderivatives of such pathological functions may be found by Riemann integration, while in other cases these functions are not Riemann integrable.£#/li#££#/ul#£
Assuming that the domains of the functions are open intervals:

£#ul#££#li#£A necessary, but not sufficient, condition for a function f to have an antiderivative is that f have the intermediate value property. That is, if [a, b] is a subinterval of the domain of f and y is any real number between f(a) and f(b), then there exists a c between a and b such that f(c) = y. This is a consequence of Darboux's theorem.£#/li#£ £#li#£The set of discontinuities of f must be a meagre set. This set must also be an F-sigma set (since the set of discontinuities of any function must be of this type). Moreover, for any meagre F-sigma set, one can construct some function f having an antiderivative, which has the given set as its set of discontinuities.£#/li#£ £#li#£If f has an antiderivative, is bounded on closed finite subintervals of the domain and has a set of discontinuities of Lebesgue measure 0, then an antiderivative may be found by integration in the sense of Lebesgue. In fact, using more powerful integrals like the Henstock–Kurzweil integral, every function for which an antiderivative exists is integrable, and its general integral coincides with its antiderivative.£#/li#£ £#li#£If f has an antiderivative F on a closed interval ${\displaystyle [a,b]}$ , then for any choice of partition ${\displaystyle a=x_{0}<x_{1}<x_{2}<\dots <x_{n}=b,}$ if one chooses sample points ${\displaystyle x_{i}^{*}\in [x_{i-1},x_{i}]}$ as specified by the mean value theorem, then the corresponding Riemann sum telescopes to the value ${\displaystyle F(b)-F(a)}$ .£#/li#££#/ul#£
${\displaystyle {\begin{aligned}\sum _{i=1}^{n}f(x_{i}^{*})(x_{i}-x_{i-1})&=\sum _{i=1}^{n}[F(x_{i})-F(x_{i-1})]\\&=F(x_{n})-F(x_{0})=F(b)-F(a)\end{aligned}}}$
However if f is unbounded, or if f is bounded but the set of discontinuities of f has positive Lebesgue measure, a different choice of sample points ${\displaystyle x_{i}^{*}}$ may give a significantly different value for the Riemann sum, no matter how fine the partition. See Example 4 below.

£#h5#£Some examples£#/h5#£
£#h5#£See also£#/h5#£ £#ul#££#li#£Antiderivative (complex analysis)£#/li#£ £#li#£Formal antiderivative£#/li#£ £#li#£Jackson integral£#/li#£ £#li#£Lists of integrals£#/li#£ £#li#£Symbolic integration£#/li#£ £#li#£Area£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£
£#h5#£Further reading£#/h5#£ £#ul#££#li#£Introduction to Classical Real Analysis, by Karl R. Stromberg; Wadsworth, 1981 (see also)£#/li#£ £#li#£Historical Essay On Continuity Of Derivatives by Dave L. Renfro£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Wolfram Integrator — Free online symbolic integration with Mathematica£#/li#£ £#li#£Mathematical Assistant on Web — symbolic computations online. Allows users to integrate in small steps (with hints for next step (integration by parts, substitution, partial fractions, application of formulas and others), powered by Maxima£#/li#£ £#li#£Function Calculator from WIMS£#/li#£ £#li#£Integral at HyperPhysics£#/li#£ £#li#£Antiderivatives and indefinite integrals at the Khan Academy£#/li#£ £#li#£Integral calculator at Symbolab£#/li#£ £#li#£The Antiderivative at MIT£#/li#£ £#li#£Introduction to Integrals at SparkNotes£#/li#£ £#li#£Antiderivatives at Harvy Mudd College£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Calculus > Integrals > Indefinite Integrals £#/li#££#/ul#£




£#h3#£Antidifferentiation£#/h3#£

In calculus, an antiderivative, inverse derivative, primitive function, primitive integral or indefinite integral of a function f is a differentiable function F whose derivative is equal to the original function f. This can be stated symbolically as F' = f. The process of solving for antiderivatives is called antidifferentiation (or indefinite integration), and its opposite operation is called differentiation, which is the process of finding a derivative. Antiderivatives are often denoted by capital Roman letters such as F and G.

Antiderivatives are related to definite integrals through the second fundamental theorem of calculus: the definite integral of a function over a closed interval where the function is Riemann integrable is equal to the difference between the values of an antiderivative evaluated at the endpoints of the interval.

In physics, antiderivatives arise in the context of rectilinear motion (e.g., in explaining the relationship between position, velocity and acceleration). The discrete equivalent of the notion of antiderivative is antidifference.


£#h5#£Examples£#/h5#£
The function ${\displaystyle F(x)={\tfrac {x^{3}}{3}}}$ is an antiderivative of ${\displaystyle f(x)=x^{2}}$ , since the derivative of ${\displaystyle {\tfrac {x^{3}}{3}}}$ is ${\displaystyle x^{2}}$ , and since the derivative of a constant is zero, ${\displaystyle x^{2}}$ will have an infinite number of antiderivatives, such as ${\displaystyle {\tfrac {x^{3}}{3}},{\tfrac {x^{3}}{3}}+1,{\tfrac {x^{3}}{3}}-2}$ , etc. Thus, all the antiderivatives of ${\displaystyle x^{2}}$ can be obtained by changing the value of c in ${\displaystyle F(x)={\tfrac {x^{3}}{3}}+c}$ , where c is an arbitrary constant known as the constant of integration. Essentially, the graphs of antiderivatives of a given function are vertical translations of each other, with each graph's vertical location depending upon the value c.

More generally, the power function ${\displaystyle f(x)=x^{n}}$ has antiderivative ${\displaystyle F(x)={\tfrac {x^{n+1}}{n+1}}+c}$ if n ≠ −1, and ${\displaystyle F(x)=\ln |x|+c}$ if n = −1.

In physics, the integration of acceleration yields velocity plus a constant. The constant is the initial velocity term that would be lost upon taking the derivative of velocity, because the derivative of a constant term is zero. This same pattern applies to further integrations and derivatives of motion (position, velocity, acceleration, and so on).


£#h5#£Uses and properties£#/h5#£
Antiderivatives can be used to compute definite integrals, using the fundamental theorem of calculus: if F is an antiderivative of the integrable function f over the interval ${\displaystyle [a,b]}$ , then:

${\displaystyle \int _{a}^{b}f(x)\,\mathrm {d} x=F(b)-F(a).}$
Because of this, each of the infinitely many antiderivatives of a given function f may be called the "indefinite integral" of f and written using the integral symbol with no bounds:

${\displaystyle \int f(x)\,\mathrm {d} x.}$
If F is an antiderivative of f, and the function f is defined on some interval, then every other antiderivative G of f differs from F by a constant: there exists a number c such that ${\displaystyle G(x)=F(x)+c}$ for all x. c is called the constant of integration. If the domain of F is a disjoint union of two or more (open) intervals, then a different constant of integration may be chosen for each of the intervals. For instance

${\displaystyle F(x)={\begin{cases}-{\frac {1}{x}}+c_{1}\quad x<0\\-{\frac {1}{x}}+c_{2}\quad x>0\end{cases}}}$
is the most general antiderivative of ${\displaystyle f(x)=1/x^{2}}$ on its natural domain ${\displaystyle (-\infty ,0)\cup (0,\infty ).}$

Every continuous function f has an antiderivative, and one antiderivative F is given by the definite integral of f with variable upper boundary:

${\displaystyle F(x)=\int _{0}^{x}f(t)\,\mathrm {d} t.}$
Varying the lower boundary produces other antiderivatives (but not necessarily all possible antiderivatives). This is another formulation of the fundamental theorem of calculus.

There are many functions whose antiderivatives, even though they exist, cannot be expressed in terms of elementary functions (like polynomials, exponential functions, logarithms, trigonometric functions, inverse trigonometric functions and their combinations). Examples of these are

${\displaystyle \int e^{-x^{2}}\,\mathrm {d} x,\qquad \int \sin x^{2}\,\mathrm {d} x,\qquad \int {\frac {\sin x}{x}}\,\mathrm {d} x,\qquad \int {\frac {1}{\ln x}}\,\mathrm {d} x,\qquad \int x^{x}\,\mathrm {d} x.}$
From left to right, the functions are the error function, the Fresnel function, the sine integral, the logarithmic integral function and Sophomore's dream. For a more detailed discussion, see also Differential Galois theory.


£#h5#£Techniques of integration£#/h5#£
Finding antiderivatives of elementary functions is often considerably harder than finding their derivatives (indeed, there is no pre-defined method for computing indefinite integrals). For some elementary functions, it is impossible to find an antiderivative in terms of other elementary functions. To learn more, see elementary functions and nonelementary integral.

There exist many properties and techniques for finding antiderivatives. These include, among others:

£#ul#££#li#£The linearity of integration (which breaks complicated integrals into simpler ones)£#/li#£ £#li#£Integration by substitution, often combined with trigonometric identities or the natural logarithm £#ul#££#li#£The inverse chain rule method (a special case of integration by substitution)£#/li#££#/ul#££#/li#£ £#li#£Integration by parts (to integrate products of functions)£#/li#£ £#li#£Inverse function integration (a formula that expresses the antiderivative of the inverse f−1 of an invertible and continuous function f, in terms of the antiderivative of f and of f−1).£#/li#£ £#li#£The method of partial fractions in integration (which allows us to integrate all rational functions—fractions of two polynomials)£#/li#£ £#li#£The Risch algorithm£#/li#£ £#li#£Additional techniques for multiple integrations (see for instance double integrals, polar coordinates, the Jacobian and the Stokes' theorem)£#/li#£ £#li#£Numerical integration (a technique for approximating a definite integral when no elementary antiderivative exists, as in the case of exp(−x2))£#/li#£ £#li#£Algebraic manipulation of integrand (so that other integration techniques, such as integration by substitution, may be used)£#/li#£ £#li#£Cauchy formula for repeated integration (to calculate the n-times antiderivative of a function) £#/li#££#/ul#£
Computer algebra systems can be used to automate some or all of the work involved in the symbolic techniques above, which is particularly useful when the algebraic manipulations involved are very complex or lengthy. Integrals which have already been derived can be looked up in a table of integrals.


£#h5#£Of non-continuous functions£#/h5#£
Non-continuous functions can have antiderivatives. While there are still open questions in this area, it is known that:

£#ul#££#li#£Some highly pathological functions with large sets of discontinuities may nevertheless have antiderivatives.£#/li#£ £#li#£In some cases, the antiderivatives of such pathological functions may be found by Riemann integration, while in other cases these functions are not Riemann integrable.£#/li#££#/ul#£
Assuming that the domains of the functions are open intervals:

£#ul#££#li#£A necessary, but not sufficient, condition for a function f to have an antiderivative is that f have the intermediate value property. That is, if [a, b] is a subinterval of the domain of f and y is any real number between f(a) and f(b), then there exists a c between a and b such that f(c) = y. This is a consequence of Darboux's theorem.£#/li#£ £#li#£The set of discontinuities of f must be a meagre set. This set must also be an F-sigma set (since the set of discontinuities of any function must be of this type). Moreover, for any meagre F-sigma set, one can construct some function f having an antiderivative, which has the given set as its set of discontinuities.£#/li#£ £#li#£If f has an antiderivative, is bounded on closed finite subintervals of the domain and has a set of discontinuities of Lebesgue measure 0, then an antiderivative may be found by integration in the sense of Lebesgue. In fact, using more powerful integrals like the Henstock–Kurzweil integral, every function for which an antiderivative exists is integrable, and its general integral coincides with its antiderivative.£#/li#£ £#li#£If f has an antiderivative F on a closed interval ${\displaystyle [a,b]}$ , then for any choice of partition ${\displaystyle a=x_{0}<x_{1}<x_{2}<\dots <x_{n}=b,}$ if one chooses sample points ${\displaystyle x_{i}^{*}\in [x_{i-1},x_{i}]}$ as specified by the mean value theorem, then the corresponding Riemann sum telescopes to the value ${\displaystyle F(b)-F(a)}$ .£#/li#££#/ul#£
${\displaystyle {\begin{aligned}\sum _{i=1}^{n}f(x_{i}^{*})(x_{i}-x_{i-1})&=\sum _{i=1}^{n}[F(x_{i})-F(x_{i-1})]\\&=F(x_{n})-F(x_{0})=F(b)-F(a)\end{aligned}}}$
However if f is unbounded, or if f is bounded but the set of discontinuities of f has positive Lebesgue measure, a different choice of sample points ${\displaystyle x_{i}^{*}}$ may give a significantly different value for the Riemann sum, no matter how fine the partition. See Example 4 below.

£#h5#£Some examples£#/h5#£
£#h5#£See also£#/h5#£ £#ul#££#li#£Antiderivative (complex analysis)£#/li#£ £#li#£Formal antiderivative£#/li#£ £#li#£Jackson integral£#/li#£ £#li#£Lists of integrals£#/li#£ £#li#£Symbolic integration£#/li#£ £#li#£Area£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£
£#h5#£Further reading£#/h5#£ £#ul#££#li#£Introduction to Classical Real Analysis, by Karl R. Stromberg; Wadsworth, 1981 (see also)£#/li#£ £#li#£Historical Essay On Continuity Of Derivatives by Dave L. Renfro£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Wolfram Integrator — Free online symbolic integration with Mathematica£#/li#£ £#li#£Mathematical Assistant on Web — symbolic computations online. Allows users to integrate in small steps (with hints for next step (integration by parts, substitution, partial fractions, application of formulas and others), powered by Maxima£#/li#£ £#li#£Function Calculator from WIMS£#/li#£ £#li#£Integral at HyperPhysics£#/li#£ £#li#£Antiderivatives and indefinite integrals at the Khan Academy£#/li#£ £#li#£Integral calculator at Symbolab£#/li#£ £#li#£The Antiderivative at MIT£#/li#£ £#li#£Introduction to Integrals at SparkNotes£#/li#£ £#li#£Antiderivatives at Harvy Mudd College£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Calculus > Integrals > Indefinite Integrals £#/li#££#/ul#£




£#h3#£Antilaplacian£#/h3#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Operator Theory £#/li#££#/ul#£




£#h3#£Antilinear£#/h3#£

In mathematics, a function ${\displaystyle f:V\to W}$ between two complex vector spaces is said to be antilinear or conjugate-linear if

hold for all vectors ${\displaystyle x,y\in V}$ and every complex number ${\displaystyle s,}$ where ${\displaystyle {\overline {s}}}$ denotes the complex conjugate of ${\displaystyle s.}$
Antilinear maps stand in contrast to linear maps, which are additive maps that are homogeneous rather than conjugate homogeneous. If the vector spaces are real then antilinearity is the same as linearity.

Antilinear maps occur in quantum mechanics in the study of time reversal and in spinor calculus, where it is customary to replace the bars over the basis vectors and the components of geometric objects by dots put above the indices. Scalar-valued antilinear maps often arise when dealing with complex inner products and Hilbert spaces.


£#h5#£Definitions and characterizations£#/h5#£
A function is called antilinear or conjugate linear if it is additive and conjugate homogeneous. An antilinear functional on a vector space ${\displaystyle V}$ is a scalar-valued antilinear map.

A function ${\displaystyle f}$ is called additive if

while it is called conjugate homogeneous if In contrast, a linear map is a function that is additive and homogeneous, where ${\displaystyle f}$ is called homogeneous if
An antilinear map ${\displaystyle f:V\to W}$ may be equivalently described in terms of the linear map ${\displaystyle {\overline {f}}:V\to {\overline {W}}}$ from ${\displaystyle V}$ to the complex conjugate vector space ${\displaystyle {\overline {W}}.}$


£#h5#£Examples£#/h5#£
£#h5#£Anti-linear dual map£#/h5#£
Given a complex vector space ${\displaystyle V}$ of rank 1, we can construct an anti-linear dual map which is an anti-linear map

sending an element ${\displaystyle x_{1}+iy_{1}}$ for ${\displaystyle x_{1},y_{1}\in \mathbb {R} }$ to for some fixed real numbers ${\displaystyle a_{1},b_{1}.}$ We can extend this to any finite dimensional complex vector space, where if we write out the standard basis ${\displaystyle e_{1},\ldots ,e_{n}}$ and each standard basis element as then an anti-linear complex map to ${\displaystyle \mathbb {C} }$ will be of the form for ${\displaystyle a_{k},b_{k}\in \mathbb {R} .}$
£#h5#£Isomorphism of anti-linear dual with real dual£#/h5#£
The anti-linear dualpg 36 of a complex vector space ${\displaystyle V}$

is a special example because it is isomorphic to the real dual of the underlying real vector space of ${\displaystyle V,}$ ${\displaystyle {\text{Hom}}_{\mathbb {R} }(V,\mathbb {R} ).}$ This is given by the map sending an anti-linear map to In the other direction, there is the inverse map sending a real dual vector to giving the desired map.
£#h5#£Properties£#/h5#£
The composite of two antilinear maps is a linear map. The class of semilinear maps generalizes the class of antilinear maps.


£#h5#£Anti-dual space£#/h5#£
The vector space of all antilinear forms on a vector space ${\displaystyle X}$ is called the algebraic anti-dual space of ${\displaystyle X.}$ If ${\displaystyle X}$ is a topological vector space, then the vector space of all continuous antilinear functionals on ${\displaystyle X,}$ denoted by ${\textstyle {\overline {X}}^{\prime },}$ is called the continuous anti-dual space or simply the anti-dual space of ${\displaystyle X}$ if no confusion can arise.

When ${\displaystyle H}$ is a normed space then the canonical norm on the (continuous) anti-dual space ${\textstyle {\overline {X}}^{\prime },}$ denoted by ${\textstyle \|f\|_{{\overline {X}}^{\prime }},}$ is defined by using this same equation:

This formula is identical to the formula for the dual norm on the continuous dual space ${\displaystyle X^{\prime }}$ of ${\displaystyle X,}$ which is defined by

Canonical isometry between the dual and anti-dual

The complex conjugate ${\displaystyle {\overline {f}}}$ of a functional ${\displaystyle f}$ is defined by sending ${\displaystyle x\in \operatorname {domain} f}$ to ${\textstyle {\overline {f(x)}}.}$ It satisfies

for every ${\displaystyle f\in X^{\prime }}$ and every ${\textstyle g\in {\overline {X}}^{\prime }.}$ This says exactly that the canonical antilinear bijection defined by as well as its inverse ${\displaystyle \operatorname {Cong} ^{-1}~:~{\overline {X}}^{\prime }\to X^{\prime }}$ are antilinear isometries and consequently also homeomorphisms.
If ${\displaystyle \mathbb {F} =\mathbb {R} }$ then ${\displaystyle X^{\prime }={\overline {X}}^{\prime }}$ and this canonical map ${\displaystyle \operatorname {Cong} :X^{\prime }\to {\overline {X}}^{\prime }}$ reduces down to the identity map.

Inner product spaces

If ${\displaystyle X}$ is an inner product space then both the canonical norm on ${\displaystyle X^{\prime }}$ and on ${\displaystyle {\overline {X}}^{\prime }}$ satisfies the parallelogram law, which means that the polarization identity can be used to define a canonical inner product on ${\displaystyle X^{\prime }}$ and also on ${\displaystyle {\overline {X}}^{\prime },}$ which this article will denote by the notations

where this inner product makes ${\displaystyle X^{\prime }}$ and ${\displaystyle {\overline {X}}^{\prime }}$ into Hilbert spaces. The inner products ${\textstyle \langle f,g\rangle _{X^{\prime }}}$ and ${\textstyle \langle f,g\rangle _{{\overline {X}}^{\prime }}}$ are antilinear in their second arguments. Moreover, the canonical norm induced by this inner product (that is, the norm defined by ${\textstyle f\mapsto {\sqrt {\left\langle f,f\right\rangle _{X^{\prime }}}}}$ ) is consistent with the dual norm (that is, as defined above by the supremum over the unit ball); explicitly, this means that the following holds for every ${\displaystyle f\in X^{\prime }:}$
If ${\displaystyle X}$ is an inner product space then the inner products on the dual space ${\displaystyle X^{\prime }}$ and the anti-dual space ${\textstyle {\overline {X}}^{\prime },}$ denoted respectively by ${\textstyle \langle \,\cdot \,,\,\cdot \,\rangle _{X^{\prime }}}$ and ${\textstyle \langle \,\cdot \,,\,\cdot \,\rangle _{{\overline {X}}^{\prime }},}$ are related by

and
£#h5#£See also£#/h5#£ £#ul#££#li#£Cauchy's functional equation – Functional equation£#/li#£ £#li#£Complex conjugate – Fundamental operation on complex numbers£#/li#£ £#li#£Complex conjugate vector space£#/li#£ £#li#£Fundamental theorem of Hilbert spaces£#/li#£ £#li#£Inner product space – Generalization of the dot product; used to define Hilbert spaces£#/li#£ £#li#£Linear map – Mathematical function, in linear algebra£#/li#£ £#li#£Matrix consimilarity£#/li#£ £#li#£Riesz representation theorem – Theorem about the dual of a Hilbert space£#/li#£ £#li#£Sesquilinear form – Generalization of a bilinear form£#/li#£ £#li#£Time reversal – Time reversal symmetry in physics£#/li#££#/ul#£
£#h5#£Citations£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Budinich, P. and Trautman, A. The Spinorial Chessboard. Springer-Verlag, 1988. ISBN 0-387-19078-3. (antilinear maps are discussed in section 3.3).£#/li#£ £#li#£Horn and Johnson, Matrix Analysis, Cambridge University Press, 1985. ISBN 0-521-38632-2. (antilinear maps are discussed in section 4.6).£#/li#£ £#li#£Trèves, François (2006) [1967]. Topological Vector Spaces, Distributions and Kernels. Mineola, N.Y.: Dover Publications. ISBN 978-0-486-45352-1. OCLC 853623322.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Sakurai, J. J. Modern Quantum Mechanics. Menlo Park, CA: Benjamin/Cummings, 1985.£#/li#££#li#£ Sakurai, J. J. Modern Quantum Mechanics. Menlo Park, CA: Benjamin/Cummings, 1985. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Operator Theory £#/li#££#/ul#£




£#h3#£Antilogarithm£#/h3#£

In mathematics, the logarithm is the inverse function to exponentiation. That means the logarithm of a given number x is the exponent to which another fixed number, the base b, must be raised, to produce that number x. In the simplest case, the logarithm counts the number of occurrences of the same factor in repeated multiplication; e.g. since 1000 = 10 × 10 × 10 = 103, the "logarithm base 10" of 1000 is 3, or log10 (1000) = 3. The logarithm of x to base b is denoted as logb (x), or without parentheses, logb x, or even without the explicit base, log x, when no confusion is possible, or when the base does not matter such as in big O notation.

The logarithm base 10 (that is b = 10) is called the decimal or common logarithm and is commonly used in science and engineering. The natural logarithm has the number e (that is b ≈ 2.718) as its base; its use is widespread in mathematics and physics, because of its simpler integral and derivative. The binary logarithm uses base 2 (that is b = 2) and is frequently used in computer science.

Logarithms were introduced by John Napier in 1614 as a means of simplifying calculations. They were rapidly adopted by navigators, scientists, engineers, surveyors and others to perform high-accuracy computations more easily. Using logarithm tables, tedious multi-digit multiplication steps can be replaced by table look-ups and simpler addition. This is possible because of the fact—important in its own right—that the logarithm of a product is the sum of the logarithms of the factors:

${\displaystyle \log _{b}(xy)=\log _{b}x+\log _{b}y,}$
provided that b, x and y are all positive and b ≠ 1. The slide rule, also based on logarithms, allows quick calculations without tables, but at lower precision. The present-day notion of logarithms comes from Leonhard Euler, who connected them to the exponential function in the 18th century, and who also introduced the letter e as the base of natural logarithms.

Logarithmic scales reduce wide-ranging quantities to smaller scopes. For example, the decibel (dB) is a unit used to express ratio as logarithms, mostly for signal power and amplitude (of which sound pressure is a common example). In chemistry, pH is a logarithmic measure for the acidity of an aqueous solution. Logarithms are commonplace in scientific formulae, and in measurements of the complexity of algorithms and of geometric objects called fractals. They help to describe frequency ratios of musical intervals, appear in formulas counting prime numbers or approximating factorials, inform some models in psychophysics, and can aid in forensic accounting.

The concept of logarithm as the inverse of exponentiation extends to other mathematical structures as well. However, in general settings, the logarithm tends to be a multi-valued function. For example, the complex logarithm is the multi-valued inverse of the complex exponential function. Similarly, the discrete logarithm is the multi-valued inverse of the exponential function in finite groups; it has uses in public-key cryptography.


£#h5#£Motivation£#/h5#£
Addition, multiplication, and exponentiation are three of the most fundamental arithmetic operations. The inverse of addition is subtraction, and the inverse of multiplication is division. Similarly, a logarithm is the inverse operation of exponentiation. Exponentiation is when a number b, the base, is raised to a certain power y, the exponent, to give a value x; this is denoted

${\displaystyle b^{y}=x.}$
For example, raising 2 to the power of 3 gives 8: ${\displaystyle 2^{3}=8}$

The logarithm of base b is the inverse operation, that provides the output y from the input x. That is, ${\displaystyle y=\log _{b}x}$ is equivalent to ${\displaystyle x=b^{y}}$ if b is a positive real number. (If b is not a positive real number, both exponentiation and logarithm can be defined but may take several values, which makes definitions much more complicated.)

One of the main historical motivations of introducing logarithms is the formula

${\displaystyle \log _{b}(xy)=\log _{b}x+\log _{b}y,}$
which allowed (before the invention of computers) reducing computation of multiplications and divisions to additions, subtractions and logarithm table looking.


£#h5#£Definition£#/h5#£
Given a positive real number b such that b ≠ 1, the logarithm of a positive real number x with respect to base b is the exponent by which b must be raised to yield x. In other words, the logarithm of x to base b is the unique real number y such that ${\displaystyle b^{y}=x}$ .

The logarithm is denoted "logb x" (pronounced as "the logarithm of x to base b", "the base-b logarithm of x", or most commonly "the log, base b, of x").

An equivalent and more succinct definition is that the function logb is the inverse function to the function ${\displaystyle x\mapsto b^{x}}$ .


£#h5#£Examples£#/h5#£ £#ul#££#li#£log2 16 = 4, since 24 = 2 × 2 × 2 × 2 = 16.£#/li#£ £#li#£Logarithms can also be negative: ${\textstyle \log _{2}\!{\frac {1}{2}}=-1}$ since ${\textstyle 2^{-1}={\frac {1}{2^{1}}}={\frac {1}{2}}.}$ £#/li#£ £#li#£log10 150 is approximately 2.176, which lies between 2 and 3, just as 150 lies between 102 = 100 and 103 = 1000.£#/li#£ £#li#£For any base b, logb b = 1 and logb 1 = 0, since b1 = b and b0 = 1, respectively.£#/li#££#/ul#£
£#h5#£Logarithmic identities£#/h5#£
Several important formulas, sometimes called logarithmic identities or logarithmic laws, relate logarithms to one another.


£#h5#£Product, quotient, power, and root£#/h5#£
The logarithm of a product is the sum of the logarithms of the numbers being multiplied; the logarithm of the ratio of two numbers is the difference of the logarithms. The logarithm of the p-th power of a number is p times the logarithm of the number itself; the logarithm of a p-th root is the logarithm of the number divided by p. The following table lists these identities with examples. Each of the identities can be derived after substitution of the logarithm definitions ${\displaystyle x=b^{\,\log _{b}x}}$ or ${\displaystyle y=b^{\,\log _{b}y}}$ in the left hand sides.


£#h5#£Change of base£#/h5#£
The logarithm logb x can be computed from the logarithms of x and b with respect to an arbitrary base k using the following formula:

${\displaystyle \log _{b}x={\frac {\log _{k}x}{\log _{k}b}}.\,}$
Typical scientific calculators calculate the logarithms to bases 10 and e. Logarithms with respect to any base b can be determined using either of these two logarithms by the previous formula:

${\displaystyle \log _{b}x={\frac {\log _{10}x}{\log _{10}b}}={\frac {\log _{e}x}{\log _{e}b}}.}$
Given a number x and its logarithm y = logb x to an unknown base b, the base is given by:

${\displaystyle b=x^{\frac {1}{y}},}$
which can be seen from taking the defining equation ${\displaystyle x=b^{\,\log _{b}x}=b^{y}}$ to the power of ${\displaystyle {\tfrac {1}{y}}.}$


£#h5#£Particular bases£#/h5#£
Among all choices for the base, three are particularly common. These are b = 10, b = e (the irrational mathematical constant ≈ 2.71828), and b = 2 (the binary logarithm). In mathematical analysis, the logarithm base e is widespread because of analytical properties explained below. On the other hand, base-10 logarithms are easy to use for manual calculations in the decimal number system:

${\displaystyle \log _{10}(10x)=\log _{10}10+\log _{10}x=1+\log _{10}x.\ }$
Thus, log10 (x) is related to the number of decimal digits of a positive integer x: the number of digits is the smallest integer strictly bigger than log10 (x). For example, log10(1430) is approximately 3.15. The next integer is 4, which is the number of digits of 1430. Both the natural logarithm and the logarithm to base two are used in information theory, corresponding to the use of nats or bits as the fundamental units of information, respectively. Binary logarithms are also used in computer science, where the binary system is ubiquitous; in music theory, where a pitch ratio of two (the octave) is ubiquitous and the number of cents between any two pitches is the binary logarithm, times 1200, of their ratio (that is, 100 cents per equal-temperament semitone); and in photography to measure exposure values, light levels, exposure times, apertures, and film speeds in "stops".

The following table lists common notations for logarithms to these bases and the fields where they are used. Many disciplines write log x instead of logb x, when the intended base can be determined from the context. The notation blog x also occurs. The "ISO notation" column lists designations suggested by the International Organization for Standardization (ISO 80000-2). Because the notation log x has been used for all three bases (or when the base is indeterminate or immaterial), the intended base must often be inferred based on context or discipline. In computer science, log usually refers to log2, and in mathematics log usually refers to loge. In other contexts, log often means log10.


£#h5#£History£#/h5#£
The history of logarithms in seventeenth-century Europe is the discovery of a new function that extended the realm of analysis beyond the scope of algebraic methods. The method of logarithms was publicly propounded by John Napier in 1614, in a book titled Mirifici Logarithmorum Canonis Descriptio (Description of the Wonderful Rule of Logarithms). Prior to Napier's invention, there had been other techniques of similar scopes, such as the prosthaphaeresis or the use of tables of progressions, extensively developed by Jost Bürgi around 1600. Napier coined the term for logarithm in Middle Latin, “logarithmus,” derived from the Greek, literally meaning, “ratio-number,” from logos “proportion, ratio, word” + arithmos “number”.

The common logarithm of a number is the index of that power of ten which equals the number. Speaking of a number as requiring so many figures is a rough allusion to common logarithm, and was referred to by Archimedes as the “order of a number”. The first real logarithms were heuristic methods to turn multiplication into addition, thus facilitating rapid computation. Some of these methods used tables derived from trigonometric identities. Such methods are called prosthaphaeresis.

Invention of the function now known as the natural logarithm began as an attempt to perform a quadrature of a rectangular hyperbola by Grégoire de Saint-Vincent, a Belgian Jesuit residing in Prague. Archimedes had written The Quadrature of the Parabola in the third century BC, but a quadrature for the hyperbola eluded all efforts until Saint-Vincent published his results in 1647. The relation that the logarithm provides between a geometric progression in its argument and an arithmetic progression of values, prompted A. A. de Sarasa to make the connection of Saint-Vincent's quadrature and the tradition of logarithms in prosthaphaeresis, leading to the term “hyperbolic logarithm”, a synonym for natural logarithm. Soon the new function was appreciated by Christiaan Huygens, and James Gregory. The notation Log y was adopted by Leibniz in 1675, and the next year he connected it to the integral ${\textstyle \int {\frac {dy}{y}}.}$

Before Euler developed his modern conception of complex natural logarithms, Roger Cotes had a nearly equivalent result when he showed in 1714 that

${\displaystyle \log(\cos \theta +i\sin \theta )=i\theta }$ .

£#h5#£Logarithm tables, slide rules, and historical applications£#/h5#£
By simplifying difficult calculations before calculators and computers became available, logarithms contributed to the advance of science, especially astronomy. They were critical to advances in surveying, celestial navigation, and other domains. Pierre-Simon Laplace called logarithms

"...[a]n admirable artifice which, by reducing to a few days the labour of many months, doubles the life of the astronomer, and spares him the errors and disgust inseparable from long calculations."
As the function f(x) = bx is the inverse function of logb x, it has been called an antilogarithm. Nowadays, this function is more commonly called an exponential function.


£#h5#£Log tables£#/h5#£
A key tool that enabled the practical use of logarithms was the table of logarithms. The first such table was compiled by Henry Briggs in 1617, immediately after Napier's invention but with the innovation of using 10 as the base. Briggs' first table contained the common logarithms of all integers in the range from 1 to 1000, with a precision of 14 digits. Subsequently, tables with increasing scope were written. These tables listed the values of log10 x for any number x in a certain range, at a certain precision. Base-10 logarithms were universally used for computation, hence the name common logarithm, since numbers that differ by factors of 10 have logarithms that differ by integers. The common logarithm of x can be separated into an integer part and a fractional part, known as the characteristic and mantissa. Tables of logarithms need only include the mantissa, as the characteristic can be easily determined by counting digits from the decimal point. The characteristic of 10 · x is one plus the characteristic of x, and their mantissas are the same. Thus using a three-digit log table, the logarithm of 3542 is approximated by

${\displaystyle \log _{10}3542=\log _{10}(1000\cdot 3.542)=3+\log _{10}3.542\approx 3+\log _{10}3.54\,}$
Greater accuracy can be obtained by interpolation:

${\displaystyle \log _{10}3542\approx 3+\log _{10}3.54+0.2(\log _{10}3.55-\log _{10}3.54)\,}$
The value of 10x can be determined by reverse look up in the same table, since the logarithm is a monotonic function.


£#h5#£Computations£#/h5#£
The product and quotient of two positive numbers c and d were routinely calculated as the sum and difference of their logarithms. The product cd or quotient c/d came from looking up the antilogarithm of the sum or difference, via the same table:

${\displaystyle cd=10^{\,\log _{10}c}\,10^{\,\log _{10}d}=10^{\,\log _{10}c\,+\,\log _{10}d}}$
and

${\displaystyle {\frac {c}{d}}=cd^{-1}=10^{\,\log _{10}c\,-\,\log _{10}d}.}$
For manual calculations that demand any appreciable precision, performing the lookups of the two logarithms, calculating their sum or difference, and looking up the antilogarithm is much faster than performing the multiplication by earlier methods such as prosthaphaeresis, which relies on trigonometric identities.

Calculations of powers and roots are reduced to multiplications or divisions and lookups by

${\displaystyle c^{d}=\left(10^{\,\log _{10}c}\right)^{d}=10^{\,d\log _{10}c}}$
and

${\displaystyle {\sqrt[{d}]{c}}=c^{\frac {1}{d}}=10^{{\frac {1}{d}}\log _{10}c}.}$
Trigonometric calculations were facilitated by tables that contained the common logarithms of trigonometric functions.


£#h5#£Slide rules£#/h5#£
Another critical application was the slide rule, a pair of logarithmically divided scales used for calculation. The non-sliding logarithmic scale, Gunter's rule, was invented shortly after Napier's invention. William Oughtred enhanced it to create the slide rule—a pair of logarithmic scales movable with respect to each other. Numbers are placed on sliding scales at distances proportional to the differences between their logarithms. Sliding the upper scale appropriately amounts to mechanically adding logarithms, as illustrated here:

For example, adding the distance from 1 to 2 on the lower scale to the distance from 1 to 3 on the upper scale yields a product of 6, which is read off at the lower part. The slide rule was an essential calculating tool for engineers and scientists until the 1970s, because it allows, at the expense of precision, much faster computation than techniques based on tables.


£#h5#£Analytic properties£#/h5#£
A deeper study of logarithms requires the concept of a function. A function is a rule that, given one number, produces another number. An example is the function producing the x-th power of b from any real number x, where the base b is a fixed number. This function is written as f(x) = b x. When b is positive and unequal to 1, we show below that f is invertible when considered as a function from the reals to the positive reals.


£#h5#£Existence£#/h5#£
Let b be a positive real number not equal to 1 and let f(x) = b x.

It is a standard result in real analysis that any continuous strictly monotonic function is bijective between its domain and range. This fact follows from the intermediate value theorem. Now, f is strictly increasing (for b > 1), or strictly decreasing (for 0 < b < 1), is continuous, has domain ${\displaystyle \mathbb {R} }$ , and has range ${\displaystyle \mathbb {R} _{>0}}$ . Therefore, f is a bijection from ${\displaystyle \mathbb {R} }$ to ${\displaystyle \mathbb {R} _{>0}}$ . In other words, for each positive real number y, there is exactly one real number x such that ${\displaystyle b^{x}=y}$ .

We let ${\displaystyle \log _{b}\colon \mathbb {R} _{>0}\to \mathbb {R} }$ denote the inverse of f. That is, logb y is the unique real number x such that ${\displaystyle b^{x}=y}$ . This function is called the base-b logarithm function or logarithmic function (or just logarithm).


£#h5#£Characterization by the product formula£#/h5#£
The function logb x can also be essentially characterized by the product formula

${\displaystyle \log _{b}(xy)=\log _{b}x+\log _{b}y.}$
More precisely, the logarithm to any base b > 1 is the only increasing function f from the positive reals to the reals satisfying f(b) = 1 and

${\displaystyle f(xy)=f(x)+f(y).}$

£#h5#£Graph of the logarithm function£#/h5#£
As discussed above, the function logb is the inverse to the exponential function ${\displaystyle x\mapsto b^{x}}$ . Therefore, Their graphs correspond to each other upon exchanging the x- and the y-coordinates (or upon reflection at the diagonal line x = y), as shown at the right: a point (t, u = bt) on the graph of f yields a point (u, t = logb u) on the graph of the logarithm and vice versa. As a consequence, logb (x) diverges to infinity (gets bigger than any given number) if x grows to infinity, provided that b is greater than one. In that case, logb(x) is an increasing function. For b < 1, logb (x) tends to minus infinity instead. When x approaches zero, logb x goes to minus infinity for b > 1 (plus infinity for b < 1, respectively).


£#h5#£Derivative and antiderivative£#/h5#£
Analytic properties of functions pass to their inverses. Thus, as f(x) = bx is a continuous and differentiable function, so is logb y. Roughly, a continuous function is differentiable if its graph has no sharp "corners". Moreover, as the derivative of f(x) evaluates to ln(b) bx by the properties of the exponential function, the chain rule implies that the derivative of logb x is given by

${\displaystyle {\frac {d}{dx}}\log _{b}x={\frac {1}{x\ln b}}.}$
That is, the slope of the tangent touching the graph of the base-b logarithm at the point (x, logb (x)) equals 1/(x ln(b)).

The derivative of ln(x) is 1/x; this implies that ln(x) is the unique antiderivative of 1/x that has the value 0 for x = 1. It is this very simple formula that motivated to qualify as "natural" the natural logarithm; this is also one of the main reasons of the importance of the constant e.

The derivative with a generalized functional argument f(x) is

${\displaystyle {\frac {d}{dx}}\ln f(x)={\frac {f'(x)}{f(x)}}.}$
The quotient at the right hand side is called the logarithmic derivative of f. Computing f'(x) by means of the derivative of ln(f(x)) is known as logarithmic differentiation. The antiderivative of the natural logarithm ln(x) is:

${\displaystyle \int \ln(x)\,dx=x\ln(x)-x+C.}$
Related formulas, such as antiderivatives of logarithms to other bases can be derived from this equation using the change of bases.


£#h5#£Integral representation of the natural logarithm£#/h5#£
The natural logarithm of t can be defined as the definite integral:

${\displaystyle \ln t=\int _{1}^{t}{\frac {1}{x}}\,dx.}$
This definition has the advantage that it does not rely on the exponential function or any trigonometric functions; the definition is in terms of an integral of a simple reciprocal. As an integral, ln(t) equals the area between the x-axis and the graph of the function 1/x, ranging from x = 1 to x = t. This is a consequence of the fundamental theorem of calculus and the fact that the derivative of ln(x) is 1/x. Product and power logarithm formulas can be derived from this definition. For example, the product formula ln(tu) = ln(t) + ln(u) is deduced as:

${\displaystyle \ln(tu)=\int _{1}^{tu}{\frac {1}{x}}\,dx\ {\stackrel {(1)}{=}}\int _{1}^{t}{\frac {1}{x}}\,dx+\int _{t}^{tu}{\frac {1}{x}}\,dx\ {\stackrel {(2)}{=}}\ln(t)+\int _{1}^{u}{\frac {1}{w}}\,dw=\ln(t)+\ln(u).}$
The equality (1) splits the integral into two parts, while the equality (2) is a change of variable (w = x/t). In the illustration below, the splitting corresponds to dividing the area into the yellow and blue parts. Rescaling the left hand blue area vertically by the factor t and shrinking it by the same factor horizontally does not change its size. Moving it appropriately, the area fits the graph of the function f(x) = 1/x again. Therefore, the left hand blue area, which is the integral of f(x) from t to tu is the same as the integral from 1 to u. This justifies the equality (2) with a more geometric proof.

The power formula ln(tr) = r ln(t) may be derived in a similar way:

${\displaystyle \ln(t^{r})=\int _{1}^{t^{r}}{\frac {1}{x}}dx=\int _{1}^{t}{\frac {1}{w^{r}}}\left(rw^{r-1}\,dw\right)=r\int _{1}^{t}{\frac {1}{w}}\,dw=r\ln(t).}$
The second equality uses a change of variables (integration by substitution), w = x1/r.

The sum over the reciprocals of natural numbers,

${\displaystyle 1+{\frac {1}{2}}+{\frac {1}{3}}+\cdots +{\frac {1}{n}}=\sum _{k=1}^{n}{\frac {1}{k}},}$
is called the harmonic series. It is closely tied to the natural logarithm: as n tends to infinity, the difference,

${\displaystyle \sum _{k=1}^{n}{\frac {1}{k}}-\ln(n),}$
converges (i.e. gets arbitrarily close) to a number known as the Euler–Mascheroni constant γ = 0.5772.... This relation aids in analyzing the performance of algorithms such as quicksort.


£#h5#£Transcendence of the logarithm£#/h5#£
Real numbers that are not algebraic are called transcendental; for example, π and e are such numbers, but ${\displaystyle {\sqrt {2-{\sqrt {3}}}}}$ is not. Almost all real numbers are transcendental. The logarithm is an example of a transcendental function. The Gelfond–Schneider theorem asserts that logarithms usually take transcendental, i.e. "difficult" values.


£#h5#£Calculation£#/h5#£
Logarithms are easy to compute in some cases, such as log10 (1000) = 3. In general, logarithms can be calculated using power series or the arithmetic–geometric mean, or be retrieved from a precalculated logarithm table that provides a fixed precision. Newton's method, an iterative method to solve equations approximately, can also be used to calculate the logarithm, because its inverse function, the exponential function, can be computed efficiently. Using look-up tables, CORDIC-like methods can be used to compute logarithms by using only the operations of addition and bit shifts. Moreover, the binary logarithm algorithm calculates lb(x) recursively, based on repeated squarings of x, taking advantage of the relation

${\displaystyle \log _{2}\left(x^{2}\right)=2\log _{2}|x|.}$

£#h5#£Power series£#/h5#£
£#h5#£Taylor series£#/h5#£
For any real number z that satisfies 0 < z ≤ 2, the following formula holds:

${\displaystyle {\begin{aligned}\ln(z)&={\frac {(z-1)^{1}}{1}}-{\frac {(z-1)^{2}}{2}}+{\frac {(z-1)^{3}}{3}}-{\frac {(z-1)^{4}}{4}}+\cdots \\&=\sum _{k=1}^{\infty }(-1)^{k+1}{\frac {(z-1)^{k}}{k}}\end{aligned}}}$
This is a shorthand for saying that ln(z) can be approximated to a more and more accurate value by the following expressions:

${\displaystyle {\begin{array}{lllll}(z-1)&&\\(z-1)&-&{\frac {(z-1)^{2}}{2}}&\\(z-1)&-&{\frac {(z-1)^{2}}{2}}&+&{\frac {(z-1)^{3}}{3}}\\\vdots &\end{array}}}$
For example, with z = 1.5 the third approximation yields 0.4167, which is about 0.011 greater than ln(1.5) = 0.405465. This series approximates ln(z) with arbitrary precision, provided the number of summands is large enough. In elementary calculus, ln(z) is therefore the limit of this series. It is the Taylor series of the natural logarithm at z = 1. The Taylor series of ln(z) provides a particularly useful approximation to ln(1 + z) when z is small, |z| < 1, since then

${\displaystyle \ln(1+z)=z-{\frac {z^{2}}{2}}+{\frac {z^{3}}{3}}\cdots \approx z.}$
For example, with z = 0.1 the first-order approximation gives ln(1.1) ≈ 0.1, which is less than 5% off the correct value 0.0953.

Although the sequence for ${\displaystyle \ln(1+z)}$ only converges for ${\displaystyle |z|<1}$ , a neat trick can fix this.

${\displaystyle \ln(1+z)=-\ln \left({\frac {1}{1+z}}\right)=-\ln \left(1-{\frac {z}{z+1}}\right)}$
As ${\displaystyle \left|{\frac {z}{z+1}}\right|<1}$ for all ${\displaystyle |z|\geq 0}$ , the sequence converges for the same range of z.


£#h5#£Inverse hyperbolic tangent£#/h5#£
Another series is based on the inverse hyperbolic tangent function:

${\displaystyle \ln(z)=2\cdot \operatorname {artanh} \,{\frac {z-1}{z+1}}=2\left({\frac {z-1}{z+1}}+{\frac {1}{3}}{\left({\frac {z-1}{z+1}}\right)}^{3}+{\frac {1}{5}}{\left({\frac {z-1}{z+1}}\right)}^{5}+\cdots \right),}$
for any real number z > 0. Using sigma notation, this is also written as

${\displaystyle \ln(z)=2\sum _{k=0}^{\infty }{\frac {1}{2k+1}}\left({\frac {z-1}{z+1}}\right)^{2k+1}.}$
This series can be derived from the above Taylor series. It converges quicker than the Taylor series, especially if z is close to 1. For example, for z = 1.5, the first three terms of the second series approximate ln(1.5) with an error of about 3×10−6. The quick convergence for z close to 1 can be taken advantage of in the following way: given a low-accuracy approximation y ≈ ln(z) and putting

${\displaystyle A={\frac {z}{\exp(y)}},}$
the logarithm of z is:

${\displaystyle \ln(z)=y+\ln(A).}$
The better the initial approximation y is, the closer A is to 1, so its logarithm can be calculated efficiently. A can be calculated using the exponential series, which converges quickly provided y is not too large. Calculating the logarithm of larger z can be reduced to smaller values of z by writing z = a · 10b, so that ln(z) = ln(a) + b · ln(10).

A closely related method can be used to compute the logarithm of integers. Putting ${\displaystyle \textstyle z={\frac {n+1}{n}}}$ in the above series, it follows that:

${\displaystyle \ln(n+1)=\ln(n)+2\sum _{k=0}^{\infty }{\frac {1}{2k+1}}\left({\frac {1}{2n+1}}\right)^{2k+1}.}$
If the logarithm of a large integer n is known, then this series yields a fast converging series for log(n+1), with a rate of convergence of ${\textstyle \left({\frac {1}{2n+1}}\right)^{2}}$ .


£#h5#£Arithmetic–geometric mean approximation£#/h5#£
The arithmetic–geometric mean yields high precision approximations of the natural logarithm. Sasaki and Kanada showed in 1982 that it was particularly fast for precisions between 400 and 1000 decimal places, while Taylor series methods were typically faster when less precision was needed. In their work ln(x) is approximated to a precision of 2−p (or p precise bits) by the following formula (due to Carl Friedrich Gauss):

${\displaystyle \ln(x)\approx {\frac {\pi }{2\,\mathrm {M} \!\left(1,2^{2-m}/x\right)}}-m\ln(2).}$
Here M(x, y) denotes the arithmetic–geometric mean of x and y. It is obtained by repeatedly calculating the average (x + y)/2 (arithmetic mean) and ${\textstyle {\sqrt {xy}}}$ (geometric mean) of x and y then let those two numbers become the next x and y. The two numbers quickly converge to a common limit which is the value of M(x, y). m is chosen such that

${\displaystyle x\,2^{m}>2^{p/2}.\,}$
to ensure the required precision. A larger m makes the M(x, y) calculation take more steps (the initial x and y are farther apart so it takes more steps to converge) but gives more precision. The constants π and ln(2) can be calculated with quickly converging series.


£#h5#£Feynman's algorithm£#/h5#£
While at Los Alamos National Laboratory working on the Manhattan Project, Richard Feynman developed a bit-processing algorithm, to compute the logarithm, that is similar to long division and was later used in the Connection Machine. The algorithm uses the fact that every real number 1 < x < 2 is representable as a product of distinct factors of the form 1 + 2−k. The algorithm sequentially builds that product P, starting with P = 1 and k = 1: if P · (1 + 2−k) < x, then it changes P to P · (1 + 2−k). It then increases ${\displaystyle k}$ by one regardless. The algorithm stops when k is large enough to give the desired accuracy. Because log(x) is the sum of the terms of the form log(1 + 2−k) corresponding to those k for which the factor 1 + 2−k was included in the product P, log(x) may be computed by simple addition, using a table of log(1 + 2−k) for all k. Any base may be used for the logarithm table.


£#h5#£Applications£#/h5#£
Logarithms have many applications inside and outside mathematics. Some of these occurrences are related to the notion of scale invariance. For example, each chamber of the shell of a nautilus is an approximate copy of the next one, scaled by a constant factor. This gives rise to a logarithmic spiral. Benford's law on the distribution of leading digits can also be explained by scale invariance. Logarithms are also linked to self-similarity. For example, logarithms appear in the analysis of algorithms that solve a problem by dividing it into two similar smaller problems and patching their solutions. The dimensions of self-similar geometric shapes, that is, shapes whose parts resemble the overall picture are also based on logarithms. Logarithmic scales are useful for quantifying the relative change of a value as opposed to its absolute difference. Moreover, because the logarithmic function log(x) grows very slowly for large x, logarithmic scales are used to compress large-scale scientific data. Logarithms also occur in numerous scientific formulas, such as the Tsiolkovsky rocket equation, the Fenske equation, or the Nernst equation.


£#h5#£Logarithmic scale£#/h5#£
Scientific quantities are often expressed as logarithms of other quantities, using a logarithmic scale. For example, the decibel is a unit of measurement associated with logarithmic-scale quantities. It is based on the common logarithm of ratios—10 times the common logarithm of a power ratio or 20 times the common logarithm of a voltage ratio. It is used to quantify the loss of voltage levels in transmitting electrical signals, to describe power levels of sounds in acoustics, and the absorbance of light in the fields of spectrometry and optics. The signal-to-noise ratio describing the amount of unwanted noise in relation to a (meaningful) signal is also measured in decibels. In a similar vein, the peak signal-to-noise ratio is commonly used to assess the quality of sound and image compression methods using the logarithm.

The strength of an earthquake is measured by taking the common logarithm of the energy emitted at the quake. This is used in the moment magnitude scale or the Richter magnitude scale. For example, a 5.0 earthquake releases 32 times (101.5) and a 6.0 releases 1000 times (103) the energy of a 4.0. Apparent magnitude measures the brightness of stars logarithmically. In chemistry the negative of the decimal logarithm, the decimal cologarithm, is indicated by the letter p. For instance, pH is the decimal cologarithm of the activity of hydronium ions (the form hydrogen ions H+
take in water). The activity of hydronium ions in neutral water is 10−7 mol·L−1, hence a pH of 7. Vinegar typically has a pH of about 3. The difference of 4 corresponds to a ratio of 104 of the activity, that is, vinegar's hydronium ion activity is about 10−3 mol·L−1.

Semilog (log–linear) graphs use the logarithmic scale concept for visualization: one axis, typically the vertical one, is scaled logarithmically. For example, the chart at the right compresses the steep increase from 1 million to 1 trillion to the same space (on the vertical axis) as the increase from 1 to 1 million. In such graphs, exponential functions of the form f(x) = a · bx appear as straight lines with slope equal to the logarithm of b. Log-log graphs scale both axes logarithmically, which causes functions of the form f(x) = a · xk to be depicted as straight lines with slope equal to the exponent k. This is applied in visualizing and analyzing power laws.


£#h5#£Psychology£#/h5#£
Logarithms occur in several laws describing human perception: Hick's law proposes a logarithmic relation between the time individuals take to choose an alternative and the number of choices they have. Fitts's law predicts that the time required to rapidly move to a target area is a logarithmic function of the distance to and the size of the target. In psychophysics, the Weber–Fechner law proposes a logarithmic relationship between stimulus and sensation such as the actual vs. the perceived weight of an item a person is carrying. (This "law", however, is less realistic than more recent models, such as Stevens's power law.)

Psychological studies found that individuals with little mathematics education tend to estimate quantities logarithmically, that is, they position a number on an unmarked line according to its logarithm, so that 10 is positioned as close to 100 as 100 is to 1000. Increasing education shifts this to a linear estimate (positioning 1000 10 times as far away) in some circumstances, while logarithms are used when the numbers to be plotted are difficult to plot linearly.


£#h5#£Probability theory and statistics£#/h5#£
Logarithms arise in probability theory: the law of large numbers dictates that, for a fair coin, as the number of coin-tosses increases to infinity, the observed proportion of heads approaches one-half. The fluctuations of this proportion about one-half are described by the law of the iterated logarithm.

Logarithms also occur in log-normal distributions. When the logarithm of a random variable has a normal distribution, the variable is said to have a log-normal distribution. Log-normal distributions are encountered in many fields, wherever a variable is formed as the product of many independent positive random variables, for example in the study of turbulence.

Logarithms are used for maximum-likelihood estimation of parametric statistical models. For such a model, the likelihood function depends on at least one parameter that must be estimated. A maximum of the likelihood function occurs at the same parameter-value as a maximum of the logarithm of the likelihood (the "log likelihood"), because the logarithm is an increasing function. The log-likelihood is easier to maximize, especially for the multiplied likelihoods for independent random variables.

Benford's law describes the occurrence of digits in many data sets, such as heights of buildings. According to Benford's law, the probability that the first decimal-digit of an item in the data sample is d (from 1 to 9) equals log10 (d + 1) − log10 (d), regardless of the unit of measurement. Thus, about 30% of the data can be expected to have 1 as first digit, 18% start with 2, etc. Auditors examine deviations from Benford's law to detect fraudulent accounting.


£#h5#£Computational complexity£#/h5#£
Analysis of algorithms is a branch of computer science that studies the performance of algorithms (computer programs solving a certain problem). Logarithms are valuable for describing algorithms that divide a problem into smaller ones, and join the solutions of the subproblems.

For example, to find a number in a sorted list, the binary search algorithm checks the middle entry and proceeds with the half before or after the middle entry if the number is still not found. This algorithm requires, on average, log2 (N) comparisons, where N is the list's length. Similarly, the merge sort algorithm sorts an unsorted list by dividing the list into halves and sorting these first before merging the results. Merge sort algorithms typically require a time approximately proportional to N · log(N). The base of the logarithm is not specified here, because the result only changes by a constant factor when another base is used. A constant factor is usually disregarded in the analysis of algorithms under the standard uniform cost model.

A function f(x) is said to grow logarithmically if f(x) is (exactly or approximately) proportional to the logarithm of x. (Biological descriptions of organism growth, however, use this term for an exponential function.) For example, any natural number N can be represented in binary form in no more than log2 N + 1 bits. In other words, the amount of memory needed to store N grows logarithmically with N.


£#h5#£Entropy and chaos£#/h5#£
Entropy is broadly a measure of the disorder of some system. In statistical thermodynamics, the entropy S of some physical system is defined as

${\displaystyle S=-k\sum _{i}p_{i}\ln(p_{i}).\,}$
The sum is over all possible states i of the system in question, such as the positions of gas particles in a container. Moreover, pi is the probability that the state i is attained and k is the Boltzmann constant. Similarly, entropy in information theory measures the quantity of information. If a message recipient may expect any one of N possible messages with equal likelihood, then the amount of information conveyed by any one such message is quantified as log2 N bits.

Lyapunov exponents use logarithms to gauge the degree of chaoticity of a dynamical system. For example, for a particle moving on an oval billiard table, even small changes of the initial conditions result in very different paths of the particle. Such systems are chaotic in a deterministic way, because small measurement errors of the initial state predictably lead to largely different final states. At least one Lyapunov exponent of a deterministically chaotic system is positive.


£#h5#£Fractals£#/h5#£
Logarithms occur in definitions of the dimension of fractals. Fractals are geometric objects that are self-similar in the sense that small parts reproduce, at least roughly, the entire global structure. The Sierpinski triangle (pictured) can be covered by three copies of itself, each having sides half the original length. This makes the Hausdorff dimension of this structure ln(3)/ln(2) ≈ 1.58. Another logarithm-based notion of dimension is obtained by counting the number of boxes needed to cover the fractal in question.


£#h5#£Music£#/h5#£
Logarithms are related to musical tones and intervals. In equal temperament, the frequency ratio depends only on the interval between two tones, not on the specific frequency, or pitch, of the individual tones. For example, the note A has a frequency of 440 Hz and B-flat has a frequency of 466 Hz. The interval between A and B-flat is a semitone, as is the one between B-flat and B (frequency 493 Hz). Accordingly, the frequency ratios agree:

${\displaystyle {\frac {466}{440}}\approx {\frac {493}{466}}\approx 1.059\approx {\sqrt[{12}]{2}}.}$
Therefore, logarithms can be used to describe the intervals: an interval is measured in semitones by taking the base-21/12 logarithm of the frequency ratio, while the base-21/1200 logarithm of the frequency ratio expresses the interval in cents, hundredths of a semitone. The latter is used for finer encoding, as it is needed for non-equal temperaments.


£#h5#£Number theory£#/h5#£
Natural logarithms are closely linked to counting prime numbers (2, 3, 5, 7, 11, ...), an important topic in number theory. For any integer x, the quantity of prime numbers less than or equal to x is denoted π(x). The prime number theorem asserts that π(x) is approximately given by

${\displaystyle {\frac {x}{\ln(x)}},}$
in the sense that the ratio of π(x) and that fraction approaches 1 when x tends to infinity. As a consequence, the probability that a randomly chosen number between 1 and x is prime is inversely proportional to the number of decimal digits of x. A far better estimate of π(x) is given by the offset logarithmic integral function Li(x), defined by

${\displaystyle \mathrm {Li} (x)=\int _{2}^{x}{\frac {1}{\ln(t)}}\,dt.}$
The Riemann hypothesis, one of the oldest open mathematical conjectures, can be stated in terms of comparing π(x) and Li(x). The Erdős–Kac theorem describing the number of distinct prime factors also involves the natural logarithm.

The logarithm of n factorial, n! = 1 · 2 · ... · n, is given by

${\displaystyle \ln(n!)=\ln(1)+\ln(2)+\cdots +\ln(n).}$
This can be used to obtain Stirling's formula, an approximation of n! for large n.


£#h5#£Generalizations£#/h5#£
£#h5#£Complex logarithm£#/h5#£
All the complex numbers a that solve the equation

${\displaystyle e^{a}=z}$
are called complex logarithms of z, when z is (considered as) a complex number. A complex number is commonly represented as z = x + iy, where x and y are real numbers and i is an imaginary unit, the square of which is −1. Such a number can be visualized by a point in the complex plane, as shown at the right. The polar form encodes a non-zero complex number z by its absolute value, that is, the (positive, real) distance r to the origin, and an angle between the real (x) axis Re and the line passing through both the origin and z. This angle is called the argument of z.

The absolute value r of z is given by

${\displaystyle \textstyle r={\sqrt {x^{2}+y^{2}}}.}$
Using the geometrical interpretation of sine and cosine and their periodicity in 2π, any complex number z may be denoted as

${\displaystyle z=x+iy=r(\cos \varphi +i\sin \varphi )=r(\cos(\varphi +2k\pi )+i\sin(\varphi +2k\pi )),}$
for any integer number k. Evidently the argument of z is not uniquely specified: both φ and φ' = φ + 2kπ are valid arguments of z for all integers k, because adding 2kπ radians or k⋅360° to φ corresponds to "winding" around the origin counter-clock-wise by k turns. The resulting complex number is always z, as illustrated at the right for k = 1. One may select exactly one of the possible arguments of z as the so-called principal argument, denoted Arg(z), with a capital A, by requiring φ to belong to one, conveniently selected turn, e.g. −π < φ ≤ π or 0 ≤ φ < 2π. These regions, where the argument of z is uniquely determined are called branches of the argument function.

Euler's formula connects the trigonometric functions sine and cosine to the complex exponential:

${\displaystyle e^{i\varphi }=\cos \varphi +i\sin \varphi .}$
Using this formula, and again the periodicity, the following identities hold:

${\displaystyle {\begin{array}{lll}z&=&r\left(\cos \varphi +i\sin \varphi \right)\\&=&r\left(\cos(\varphi +2k\pi )+i\sin(\varphi +2k\pi )\right)\\&=&re^{i(\varphi +2k\pi )}\\&=&e^{\ln(r)}e^{i(\varphi +2k\pi )}\\&=&e^{\ln(r)+i(\varphi +2k\pi )}=e^{a_{k}},\end{array}}}$
where ln(r) is the unique real natural logarithm, ak denote the complex logarithms of z, and k is an arbitrary integer. Therefore, the complex logarithms of z, which are all those complex values ak for which the ak-th power of e equals z, are the infinitely many values

${\displaystyle a_{k}=\ln(r)+i(\varphi +2k\pi ),\quad }$ for arbitrary integers k.
Taking k such that φ + 2kπ is within the defined interval for the principal arguments, then ak is called the principal value of the logarithm, denoted Log(z), again with a capital L. The principal argument of any positive real number x is 0; hence Log(x) is a real number and equals the real (natural) logarithm. However, the above formulas for logarithms of products and powers do not generalize to the principal value of the complex logarithm.

The illustration at the right depicts Log(z), confining the arguments of z to the interval (−π, π]. This way the corresponding branch of the complex logarithm has discontinuities all along the negative real x axis, which can be seen in the jump in the hue there. This discontinuity arises from jumping to the other boundary in the same branch, when crossing a boundary, i.e. not changing to the corresponding k-value of the continuously neighboring branch. Such a locus is called a branch cut. Dropping the range restrictions on the argument makes the relations "argument of z", and consequently the "logarithm of z", multi-valued functions.


£#h5#£Inverses of other exponential functions£#/h5#£
Exponentiation occurs in many areas of mathematics and its inverse function is often referred to as the logarithm. For example, the logarithm of a matrix is the (multi-valued) inverse function of the matrix exponential. Another example is the p-adic logarithm, the inverse function of the p-adic exponential. Both are defined via Taylor series analogous to the real case. In the context of differential geometry, the exponential map maps the tangent space at a point of a manifold to a neighborhood of that point. Its inverse is also called the logarithmic (or log) map.

In the context of finite groups exponentiation is given by repeatedly multiplying one group element b with itself. The discrete logarithm is the integer n solving the equation

${\displaystyle b^{n}=x,}$
where x is an element of the group. Carrying out the exponentiation can be done efficiently, but the discrete logarithm is believed to be very hard to calculate in some groups. This asymmetry has important applications in public key cryptography, such as for example in the Diffie–Hellman key exchange, a routine that allows secure exchanges of cryptographic keys over unsecured information channels. Zech's logarithm is related to the discrete logarithm in the multiplicative group of non-zero elements of a finite field.

Further logarithm-like inverse functions include the double logarithm ln(ln(x)), the super- or hyper-4-logarithm (a slight variation of which is called iterated logarithm in computer science), the Lambert W function, and the logit. They are the inverse functions of the double exponential function, tetration, of f(w) = wew, and of the logistic function, respectively.


£#h5#£Related concepts£#/h5#£
From the perspective of group theory, the identity log(cd) = log(c) + log(d) expresses a group isomorphism between positive reals under multiplication and reals under addition. Logarithmic functions are the only continuous isomorphisms between these groups. By means of that isomorphism, the Haar measure (Lebesgue measure) dx on the reals corresponds to the Haar measure dx/x on the positive reals. The non-negative reals not only have a multiplication, but also have addition, and form a semiring, called the probability semiring; this is in fact a semifield. The logarithm then takes multiplication to addition (log multiplication), and takes addition to log addition (LogSumExp), giving an isomorphism of semirings between the probability semiring and the log semiring.

Logarithmic one-forms df/f appear in complex analysis and algebraic geometry as differential forms with logarithmic poles.

The polylogarithm is the function defined by

${\displaystyle \operatorname {Li} _{s}(z)=\sum _{k=1}^{\infty }{z^{k} \over k^{s}}.}$
It is related to the natural logarithm by Li1 (z) = −ln(1 − z). Moreover, Lis (1) equals the Riemann zeta function ζ(s).


£#h5#£See also£#/h5#£ £#ul#££#li#£Decimal exponent (dex)£#/li#£ £#li#£Exponential function£#/li#£ £#li#£Index of logarithm articles£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£
£#h5#£External links£#/h5#£ £#ul#££#li#£ Media related to Logarithm at Wikimedia Commons£#/li#£ £#li#£ The dictionary definition of logarithm at Wiktionary£#/li#£ £#li#£ A lesson on logarithms can be found on Wikiversity£#/li#£ £#li#£Weisstein, Eric W., "Logarithm", MathWorld£#/li#£ £#li#£Khan Academy: Logarithms, free online micro lectures£#/li#£ £#li#£"Logarithmic function", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#£ £#li#£Colin Byfleet, Educational video on logarithms, retrieved 12 October 2010£#/li#£ £#li#£Edward Wright, Translation of Napier's work on logarithms, archived from the original on 3 December 2002, retrieved 12 October 2010{{citation}}: CS1 maint: unfit URL (link)£#/li#£ £#li#£Glaisher, James Whitbread Lee (1911), "Logarithm" , in Chisholm, Hugh (ed.), Encyclopædia Britannica, vol. 16 (11th ed.), Cambridge University Press, pp. 868–77£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Logarithms £#/li#££#/ul#£




£#h3#£Antiperiodic Function£#/h3#£

A periodic function is a function that repeats its values at regular intervals. For example, the trigonometric functions, which repeat at intervals of ${\displaystyle 2\pi }$ radians, are periodic functions. Periodic functions are used throughout science to describe oscillations, waves, and other phenomena that exhibit periodicity. Any function that is not periodic is called aperiodic.


£#h5#£Definition£#/h5#£
A function f is said to be periodic if, for some nonzero constant P, it is the case that

${\displaystyle f(x+P)=f(x)}$
for all values of x in the domain. A nonzero constant P for which this is the case is called a period of the function. If there exists a least positive constant P with this property, it is called the fundamental period (also primitive period, basic period, or prime period.) Often, "the" period of a function is used to mean its fundamental period. A function with period P will repeat on intervals of length P, and these intervals are sometimes also referred to as periods of the function.

Geometrically, a periodic function can be defined as a function whose graph exhibits translational symmetry, i.e. a function f is periodic with period P if the graph of f is invariant under translation in the x-direction by a distance of P. This definition of periodicity can be extended to other geometric shapes and patterns, as well as be generalized to higher dimensions, such as periodic tessellations of the plane. A sequence can also be viewed as a function defined on the natural numbers, and for a periodic sequence these notions are defined accordingly.


£#h5#£Examples£#/h5#£
£#h5#£Real number examples£#/h5#£
The sine function is periodic with period ${\displaystyle 2\pi }$ , since

${\displaystyle \sin(x+2\pi )=\sin x}$
for all values of ${\displaystyle x}$ . This function repeats on intervals of length ${\displaystyle 2\pi }$ (see the graph to the right).

Everyday examples are seen when the variable is time; for instance the hands of a clock or the phases of the moon show periodic behaviour. Periodic motion is motion in which the position(s) of the system are expressible as periodic functions, all with the same period.

For a function on the real numbers or on the integers, that means that the entire graph can be formed from copies of one particular portion, repeated at regular intervals.

A simple example of a periodic function is the function ${\displaystyle f}$ that gives the "fractional part" of its argument. Its period is 1. In particular,

${\displaystyle f(0.5)=f(1.5)=f(2.5)=\cdots =0.5}$
The graph of the function ${\displaystyle f}$ is the sawtooth wave.

The trigonometric functions sine and cosine are common periodic functions, with period ${\displaystyle 2\pi }$ (see the figure on the right). The subject of Fourier series investigates the idea that an 'arbitrary' periodic function is a sum of trigonometric functions with matching periods.

According to the definition above, some exotic functions, for example the Dirichlet function, are also periodic; in the case of Dirichlet function, any nonzero rational number is a period.


£#h5#£Complex number examples£#/h5#£
Using complex variables we have the common period function:

${\displaystyle e^{ikx}=\cos kx+i\,\sin kx.}$
Since the cosine and sine functions are both periodic with period ${\displaystyle 2\pi }$ , the complex exponential is made up of cosine and sine waves. This means that Euler's formula (above) has the property such that if ${\displaystyle L}$ is the period of the function, then

${\displaystyle L={\frac {2\pi }{k}}.}$

£#h5#£Double-periodic functions£#/h5#£
A function whose domain is the complex numbers can have two incommensurate periods without being constant. The elliptic functions are such functions. ("Incommensurate" in this context means not real multiples of each other.)


£#h5#£Properties£#/h5#£
Periodic functions can take on values many times. More specifically, if a function ${\displaystyle f}$ is periodic with period ${\displaystyle P}$ , then for all ${\displaystyle x}$ in the domain of ${\displaystyle f}$ and all positive integers ${\displaystyle n}$ ,

${\displaystyle f(x+nP)=f(x)}$
If ${\displaystyle f(x)}$ is a function with period ${\displaystyle P}$ , then ${\displaystyle f(ax)}$ , where ${\displaystyle a}$ is a non-zero real number such that ${\displaystyle ax}$ is within the domain of ${\displaystyle f}$ , is periodic with period ${\textstyle {\frac {P}{a}}}$ . For example, ${\displaystyle f(x)=\sin(x)}$ has period ${\displaystyle 2\pi }$ therefore ${\displaystyle \sin(5x)}$ will have period ${\textstyle {\frac {2\pi }{5}}}$ .

Some periodic functions can be described by Fourier series. For instance, for L2 functions, Carleson's theorem states that they have a pointwise (Lebesgue) almost everywhere convergent Fourier series. Fourier series can only be used for periodic functions, or for functions on a bounded (compact) interval. If ${\displaystyle f}$ is a periodic function with period ${\displaystyle P}$ that can be described by a Fourier series, the coefficients of the series can be described by an integral over an interval of length ${\displaystyle P}$ .

Any function that consists only of periodic functions with the same period is also periodic (with period equal or smaller), including:

£#ul#££#li#£addition, subtraction, multiplication and division of periodic functions, and£#/li#£ £#li#£taking a power or a root of a periodic function (provided it is defined for all ${\displaystyle x}$ ).£#/li#££#/ul#£
£#h5#£Generalizations£#/h5#£
£#h5#£Antiperiodic functions£#/h5#£
One subset of periodic functions is that of antiperiodic functions. This is a function ${\displaystyle f}$ such that ${\displaystyle f(x+P)=-f(x)}$ for all ${\displaystyle x}$ . For example, the sine and cosine functions are ${\displaystyle \pi }$ -antiperiodic and ${\displaystyle 2\pi }$ -periodic. While a ${\displaystyle P}$ -antiperiodic function is a ${\displaystyle 2P}$ -periodic function, the converse is not necessarily true.


£#h5#£Bloch-periodic functions£#/h5#£
A further generalization appears in the context of Bloch's theorems and Floquet theory, which govern the solution of various periodic differential equations. In this context, the solution (in one dimension) is typically a function of the form

${\displaystyle f(x+P)=e^{ikP}f(x)~,}$
where ${\displaystyle k}$ is a real or complex number (the Bloch wavevector or Floquet exponent). Functions of this form are sometimes called Bloch-periodic in this context. A periodic function is the special case ${\displaystyle k=0}$ , and an antiperiodic function is the special case ${\displaystyle k=\pi /P}$ . Whenever ${\displaystyle kP/\pi }$ is rational, the function is also periodic.


£#h5#£Quotient spaces as domain£#/h5#£
In signal processing you encounter the problem, that Fourier series represent periodic functions and that Fourier series satisfy convolution theorems (i.e. convolution of Fourier series corresponds to multiplication of represented periodic function and vice versa), but periodic functions cannot be convolved with the usual definition, since the involved integrals diverge. A possible way out is to define a periodic function on a bounded but periodic domain. To this end you can use the notion of a quotient space:

${\displaystyle {\mathbb {R} /\mathbb {Z} }=\{x+\mathbb {Z} :x\in \mathbb {R} \}=\{\{y:y\in \mathbb {R} \land y-x\in \mathbb {Z} \}:x\in \mathbb {R} \}}$ .
That is, each element in ${\displaystyle {\mathbb {R} /\mathbb {Z} }}$ is an equivalence class of real numbers that share the same fractional part. Thus a function like ${\displaystyle f:{\mathbb {R} /\mathbb {Z} }\to \mathbb {R} }$ is a representation of a 1-periodic function.


£#h5#£Calculating period£#/h5#£
Consider a real waveform consisting of superimposed frequencies, expressed in a set as ratios to a fundamental frequency, f: F = 1⁄f [f1 f2 f3 ... fN] where all non-zero elements ≥1 and at least one of the elements of the set is 1. To find the period, T, first find the least common denominator of all the elements in the set. Period can be found as T = LCD⁄f. Consider that for a simple sinusoid, T = 1⁄f. Therefore, the LCD can be seen as a periodicity multiplier.

£#ul#££#li#£For set representing all notes of Western major scale: [1 9⁄8 5⁄4 4⁄3 3⁄2 5⁄3 15⁄8] the LCD is 24 therefore T = 24⁄f.£#/li#£ £#li#£For set representing all notes of a major triad: [1 5⁄4 3⁄2] the LCD is 4 therefore T = 4⁄f.£#/li#£ £#li#£For set representing all notes of a minor triad: [1 6⁄5 3⁄2] the LCD is 10 therefore T = 10⁄f.£#/li#££#/ul#£
If no least common denominator exists, for instance if one of the above elements were irrational, then the wave would not be periodic.


£#h5#£See also£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Ekeland, Ivar (1990). "One". Convexity methods in Hamiltonian mechanics. Ergebnisse der Mathematik und ihrer Grenzgebiete (3) [Results in Mathematics and Related Areas (3)]. Vol. 19. Berlin: Springer-Verlag. pp. x+247. ISBN 3-540-50613-6. MR 1051888.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Periodic function", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#£ £#li#£Periodic functions at MathWorld£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Demidovich, B. Problems in Mathematical Analysis. Moscow: Mir, p. 34, 1976.£#/li#££#li#£ Demidovich, B. Problems in Mathematical Analysis. Moscow: Mir, p. 34, 1976. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Functions £#/li#££#/ul#£




£#h3#£Antipodal Map£#/h3#£

In mathematics, antipodal points of a sphere are those diametrically opposite to each other (the specific qualities of such a definition are that a line drawn from the one to the other passes through the center of the sphere so forms a true diameter).

This term applies to opposite points on a circle or any n-sphere.

An antipodal point is sometimes called an antipode, a back-formation from the Greek loan word antipodes, meaning "opposite (the) feet", as the true word singular is antipus.


£#h5#£Theory£#/h5#£
In mathematics, the concept of antipodal points is generalized to spheres of any dimension: two points on the sphere are antipodal if they are opposite through the centre; for example, taking the centre as origin, they are points with related vectors v and −v. On a circle, such points are also called diametrically opposite. In other words, each line through the centre intersects the sphere in two points, one for each ray out from the centre, and these two points are antipodal.

The Borsuk–Ulam theorem is a result from algebraic topology dealing with such pairs of points. It says that any continuous function from Sn to Rn maps some pair of antipodal points in Sn to the same point in Rn. Here, Sn denotes the n-dimensional sphere in (n + 1)-dimensional space (so the "ordinary" sphere is S2 and a circle is S1).

The antipodal map A : Sn → Sn, defined by A(x) = −x, sends every point on the sphere to its antipodal point. It is homotopic to the identity map if n is odd, and its degree is (−1)n+1.

If one wants to consider antipodal points as identified, one passes to projective space (see also projective Hilbert space, for this idea as applied in quantum mechanics).


£#h5#£Antipodal pair of points on a convex polygon£#/h5#£
An antipodal pair of a convex polygon is a pair of 2 points admitting 2 infinite parallel lines being tangent to both points included in the antipodal without crossing any other line of the convex polygon.


£#h5#£References£#/h5#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Antipodes", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#£ £#li#£"antipodal". PlanetMath.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Topology > Manifolds £#/li#££#/ul#£




£#h3#£Antisymmetric£#/h3#£

Antisymmetric or skew-symmetric may refer to:

£#ul#££#li#£Antisymmetry in linguistics£#/li#£ £#li#£Antisymmetric relation in mathematics£#/li#£ £#li#£Skew-symmetric graph£#/li#£ £#li#£Self-complementary graph£#/li#££#/ul#£
In mathematics, especially linear algebra, and in theoretical physics, the adjective antisymmetric (or skew-symmetric) is used for matrices, tensors, and other objects that change sign if an appropriate operation (e.g. matrix transposition) is performed. See:

£#ul#££#li#£Skew-symmetric matrix (a matrix A for which AT = −A)£#/li#£ £#li#£Skew-symmetric bilinear form is a bilinear form B such that B(x, y) = −B(y, x) for all x and y.£#/li#£ £#li#£Antisymmetric tensor in matrices and index subsets.£#/li#£ £#li#£"antisymmetric function" – odd function£#/li#££#/ul#£
£#h5#£See also£#/h5#£ £#ul#££#li#£Symmetry in mathematics£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Operator Theory £#/li#££#/ul#£




£#h3#£Antisymmetric Part£#/h3#£

In mathematics, the tensor product of representations is a tensor product of vector spaces underlying representations together with the factor-wise group action on the product. This construction, together with the Clebsch–Gordan procedure, can be used to generate additional irreducible representations if one already knows a few.


£#h5#£Definition£#/h5#£
£#h5#£Group representations£#/h5#£
If ${\displaystyle V_{1},V_{2}}$ are linear representations of a group ${\displaystyle G}$ , then their tensor product is the tensor product of vector spaces ${\displaystyle V_{1}\otimes V_{2}}$ with the linear action of ${\displaystyle G}$ uniquely determined by the condition that

${\displaystyle g\cdot (v_{1}\otimes v_{2})=(g\cdot v_{1})\otimes (g\cdot v_{2})}$
for all ${\displaystyle v_{1}\in V_{1}}$ and ${\displaystyle v_{2}\in V_{2}}$ . Although not every element of ${\displaystyle V_{1}\otimes V_{2}}$ is expressible in the form ${\displaystyle v_{1}\otimes v_{2}}$ , the universal property of the tensor product operation guarantees that this action is well defined.

In the language of homomorphisms, if the actions of ${\displaystyle G}$ on ${\displaystyle V_{1}}$ and ${\displaystyle V_{2}}$ are given by homomorphisms ${\displaystyle \Pi _{1}:G\rightarrow \operatorname {GL} (V_{1})}$ and ${\displaystyle \Pi _{2}:G\rightarrow \operatorname {GL} (V_{2})}$ , then the tensor product representation is given by the homomorphism ${\displaystyle \Pi _{1}\otimes \Pi _{2}:G\rightarrow \operatorname {GL} (V_{1}\otimes V_{2})}$ given by

${\displaystyle \Pi _{1}\otimes \Pi _{2}(g)=\Pi _{1}(g)\otimes \Pi _{2}(g)}$ ,
where ${\displaystyle \Pi _{1}(g)\otimes \Pi _{2}(g)}$ is the tensor product of linear maps.

One can extend the notion of tensor products to any finite number of representations. If V is a linear representation of a group G, then with the above linear action, the tensor algebra ${\displaystyle T(V)}$ is an algebraic representation of G; i.e., each element of G acts as an algebra automorphism.


£#h5#£Lie algebra representations£#/h5#£
If ${\displaystyle (V_{1},\pi _{1})}$ and ${\displaystyle (V_{2},\pi _{2})}$ are representations of a Lie algebra ${\displaystyle {\mathfrak {g}}}$ , then the tensor product of these representations is the map ${\displaystyle \pi _{1}\otimes \pi _{2}:{\mathfrak {g}}\rightarrow \operatorname {End} (V_{1}\otimes V_{2})}$ given by

${\displaystyle \pi _{1}\otimes \pi _{2}(X)=\pi _{1}(X)\otimes I+I\otimes \pi _{2}(X)}$ ,
where ${\displaystyle I}$ is the identity endomorphism. The motivation for this definition comes from the case in which ${\displaystyle \pi _{1}}$ and ${\displaystyle \pi _{2}}$ come from representations ${\displaystyle \Pi _{1}}$ and ${\displaystyle \Pi _{2}}$ of a Lie group ${\displaystyle G}$ . In that case, a simple computation shows that the Lie algebra representation associated to ${\displaystyle \Pi _{1}\otimes \Pi _{2}}$ is given by the preceding formula.


£#h5#£Action on linear maps£#/h5#£
If ${\displaystyle (V_{1},\Pi _{1})}$ and ${\displaystyle (V_{2},\Pi _{2})}$ are representations of a group ${\displaystyle G}$ , let ${\displaystyle \operatorname {Hom} (V_{1},V_{2})}$ denote the space of all linear maps from ${\displaystyle V_{1}}$ to ${\displaystyle V_{2}}$ . Then ${\displaystyle \operatorname {Hom} (V_{1},V_{2})}$ can be given the structure of a representation by defining

${\displaystyle g\cdot A=\Pi _{2}(g)A\Pi _{1}(g)^{-1}}$
for all ${\displaystyle A\in \operatorname {Hom} (V,W)}$ . Now, there is a natural isomorphism

${\displaystyle \operatorname {Hom} (V,W)\cong V^{*}\otimes W}$
as vector spaces; this vector space isomorphism is in fact an isomorphism of representations.

The trivial subrepresentation ${\displaystyle \operatorname {Hom} (V,W)^{G}}$ consists of G-linear maps; i.e.,

${\displaystyle \operatorname {Hom} _{G}(V,W)=\operatorname {Hom} (V,W)^{G}.}$
Let ${\displaystyle E=\operatorname {End} (V)}$ denote the endomorphism algebra of V and let A denote the subalgebra of ${\displaystyle E^{\otimes m}}$ consisting of symmetric tensors. The main theorem of invariant theory states that A is semisimple when the characteristic of the base field is zero.


£#h5#£Clebsch–Gordan theory£#/h5#£
£#h5#£The general problem£#/h5#£
The tensor product of two irreducible representations ${\displaystyle V_{1},V_{2}}$ of a group or Lie algebra is usually not irreducible. It is therefore of interest to attempt to decompose ${\displaystyle V_{1}\otimes V_{2}}$ into irreducible pieces. This decomposition problem is known as the Clebsch–Gordan problem.


£#h5#£The SU(2) case£#/h5#£
The prototypical example of this problem is the case of the rotation group SO(3)—or its double cover, the special unitary group SU(2). The irreducible representations of SU(2) are described by a parameter ${\displaystyle \ell }$ , whose possible values are

${\displaystyle \ell =0,1/2,1,3/2,\ldots .}$
(The dimension of the representation is then ${\displaystyle 2\ell +1}$ .) Let us take two parameters ${\displaystyle \ell }$ and ${\displaystyle m}$ with ${\displaystyle \ell \geq m}$ . Then the tensor product representation ${\displaystyle V_{\ell }\otimes V_{m}}$ then decomposes as follows:

${\displaystyle V_{\ell }\otimes V_{m}\cong V_{\ell +m}\oplus V_{\ell +m-1}\oplus \cdots \oplus V_{\ell -m+1}\oplus V_{\ell -m}.}$
Consider, as an example, the tensor product of the four-dimensional representation ${\displaystyle V_{3/2}}$ and the three-dimensional representation ${\displaystyle V_{1}}$ . The tensor product representation ${\displaystyle V_{3/2}\otimes V_{1}}$ has dimension 12 and decomposes as

${\displaystyle V_{3/2}\otimes V_{1}\cong V_{5/2}\oplus V_{3/2}\oplus V_{1/2}}$ ,
where the representations on the right-hand side have dimension 6, 4, and 2, respectively. We may summarize this result arithmetically as ${\displaystyle 4\times 3=6+4+2}$ .


£#h5#£The SU(3) case£#/h5#£
In the case of the group SU(3), all the irreducible representations can be generated from the standard 3-dimensional representation and its dual, as follows. To generate the representation with label ${\displaystyle (m_{1},m_{2})}$ , one takes the tensor product of ${\displaystyle m_{1}}$ copies of the standard representation and ${\displaystyle m_{2}}$ copies of the dual of the standard representation, and then takes the invariant subspace generated by the tensor product of the highest weight vectors.

In contrast to the situation for SU(2), in the Clebsch–Gordan decomposition for SU(3), a given irreducible representation ${\displaystyle W}$ may occur more than once in the decomposition of ${\displaystyle U\otimes V}$ .


£#h5#£Tensor power£#/h5#£
As with vector spaces, one can define the kth tensor power of a representation V to be the vector space ${\displaystyle V^{\otimes k}}$ with the action given above.


£#h5#£The symmetric and alternating square£#/h5#£
Over a field of characteristic zero, the symmetric and alternating squares are subrepresentations of the second tensor power. They can be used to define the Frobenius–Schur indicator, which indicates whether a given irreducible character is real, complex, or quaternionic. They are examples of Schur functors. They are defined as follows.

Let V be a vector space. Define an endomorphism (self-map) T of ${\displaystyle V\otimes V}$ as follows:

${\displaystyle {\begin{aligned}T:V\otimes V&\longrightarrow V\otimes V\\v\otimes w&\longmapsto w\otimes v.\end{aligned}}}$
It is an involution (it is its own inverse), and so is an automorphism (self-isomorphism) of ${\displaystyle V\otimes V}$ .

Define two subsets of the second tensor power of V,

These are the symmetric square of V, ${\displaystyle V\odot V}$ , and the alternating square of V, ${\displaystyle V\wedge V}$ , respectively. The symmetric and alternating squares are also known as the symmetric part and antisymmetric part of the tensor product.


£#h5#£Properties£#/h5#£
The second tensor power of a linear representation V of a group G decomposes as the direct sum of the symmetric and alternating squares:

as representations. In particular, both are subrepresenations of the second tensor power. In the language of modules over the group ring, the symmetric and alternating squares are ${\displaystyle \mathbb {C} [G]}$ -submodules of ${\displaystyle V\otimes V}$ .

If V has a basis ${\displaystyle \{v_{1},v_{2},\ldots ,v_{n}\}}$ , then the symmetric square has a basis ${\displaystyle \{v_{i}\otimes v_{j}+v_{j}\otimes v_{i}\mid 1\leq i\leq j\leq n\}}$ and the alternating square has a basis ${\displaystyle \{v_{i}\otimes v_{j}-v_{j}\otimes v_{i}\mid 1\leq i<j\leq n\}}$ . Accordingly,

${\displaystyle {\begin{aligned}\dim \operatorname {Sym} ^{2}(V)&={\frac {\dim V(\dim V+1)}{2}},\\\dim \operatorname {Alt} ^{2}(V)&={\frac {\dim V(\dim V-1)}{2}}.\end{aligned}}}$
Let ${\displaystyle \chi :G\to \mathbb {C} }$ be the character of ${\displaystyle V}$ . Then we can calculate the characters of the symmetric and alternating squares as follows: for all g in G,

${\displaystyle {\begin{aligned}\chi _{\operatorname {Sym} ^{2}(V)}(g)&={\frac {1}{2}}(\chi (g)^{2}+\chi (g^{2})),\\\chi _{\operatorname {Alt} ^{2}(V)}(g)&={\frac {1}{2}}(\chi (g)^{2}-\chi (g^{2})).\end{aligned}}}$

£#h5#£The symmetric and exterior powers£#/h5#£
As in multilinear algebra, over a field of characteristic zero, one can more generally define the kth symmetric power ${\displaystyle \operatorname {Sym} ^{n}(V)}$ and kth exterior power ${\displaystyle \Lambda ^{n}(V)}$ , which are subspaces of the kth tensor power (see those pages for more detail on this construction). They are also subrepresentations, but higher tensor powers no longer decompose as their direct sum.

The Schur–Weyl duality computes the irreducible representations occurring in tensor powers of representations of the general linear group ${\displaystyle G=\operatorname {GL} (V)}$ . Precisely, as an ${\displaystyle S_{n}\times G}$ -module

${\displaystyle V^{\otimes n}\simeq \bigoplus _{\lambda }M_{\lambda }\otimes S^{\lambda }(V)}$
where

£#ul#££#li#£ ${\displaystyle M_{\lambda }}$ is an irreducible representation of the symmetric group ${\displaystyle S_{n}}$ corresponding to a partition ${\displaystyle \lambda }$ of n (in decreasing order),£#/li#£ £#li#£ ${\displaystyle S^{\lambda }(V)}$ is the image of the Young symmetrizer ${\displaystyle c_{\lambda }:V^{\otimes n}\to V^{\otimes n}}$ .£#/li#££#/ul#£
The mapping ${\displaystyle V\mapsto S^{\lambda }(V)}$ is a functor called the Schur functor. It generalizes the constructions of symmetric and exterior powers:

${\displaystyle S^{(n)}(V)=\operatorname {Sym} ^{n}V,\,\,S^{(1,1,\dots ,1)}(V)=\wedge ^{n}V.}$
In particular, as an G-module, the above simplifies to

${\displaystyle V^{\otimes n}\simeq \bigoplus _{\lambda }S^{\lambda }(V)^{\oplus m_{\lambda }}}$
where ${\displaystyle m_{\lambda }=\dim M_{\lambda }}$ . Moreover, the multiplicity ${\displaystyle m_{\lambda }}$ may be computed by the Frobenius formula (or the hook length formula). For example, take ${\displaystyle n=3}$ . Then there are exactly three partitions: ${\displaystyle 3=3=2+1=1+1+1}$ and, as it turns out, ${\displaystyle m_{(3)}=m_{(1,1,1)}=1,\,m_{(2,1)}=2}$ . Hence,

${\displaystyle V^{\otimes 3}\simeq \operatorname {Sym} ^{3}V\bigoplus \wedge ^{3}V\bigoplus S^{(2,1)}(V)^{\oplus 2}.}$

£#h5#£Tensor products involving Schur functors£#/h5#£
Let ${\displaystyle S^{\lambda }}$ denote the Schur functor defined according to a partition ${\displaystyle \lambda }$ . Then there is the following decomposition:

${\displaystyle S^{\lambda }V\otimes S^{\mu }V\simeq \bigoplus _{\nu }(S^{\nu }V)^{\oplus N_{\lambda \mu \nu }}}$
where the multiplicities ${\displaystyle N_{\lambda \mu \nu }}$ are given by the Littlewood–Richardson rule.

Given finite-dimensional vector spaces V, W, the Schur functors Sλ give the decomposition

${\displaystyle \operatorname {Sym} (W^{*}\otimes V)\simeq \bigoplus _{\lambda }S^{\lambda }(W^{*})\otimes S^{\lambda }(V)}$
The left-hand side can be identified with the ring k[Hom(V, W)] = k[V * ⊗ W] of polynomial functions on Hom(V, W) and so the above also gives the decomposition of k[Hom(V, W)].


£#h5#£Tensor products representations as representations of product groups£#/h5#£
Let G, H be two groups and let ${\displaystyle (\pi ,V)}$ and ${\displaystyle (\rho ,W)}$ be representations of G and H, respectively. Then we can let the direct product group ${\displaystyle G\times H}$ act on the tensor product space ${\displaystyle V\otimes W}$ by the formula

${\displaystyle (g,h)\cdot (v\otimes w)=\pi (g)v\otimes \rho (h)w.}$
Even if ${\displaystyle G=H}$ , we can still perform this construction, so that the tensor product of two representations of ${\displaystyle G}$ could, alternatively, be viewed as a representation of ${\displaystyle G\times G}$ rather than a representation of ${\displaystyle G}$ . It is therefore important to clarify whether the tensor product of two representations of ${\displaystyle G}$ is being viewed as a representation of ${\displaystyle G}$ or as a representation of ${\displaystyle G\times G}$ .

In contrast to the Clebsch–Gordan problem discussed above, the tensor product of two irreducible representations of ${\displaystyle G}$ is irreducible when viewed as a representation of the product group ${\displaystyle G\times G}$ .


£#h5#£See also£#/h5#£ £#ul#££#li#£Dual representation£#/li#£ £#li#£Hermite reciprocity£#/li#£ £#li#£Clebsch–Gordan coefficients£#/li#£ £#li#£Lie group representation£#/li#£ £#li#£Lie algebra representation£#/li#£ £#li#£Kronecker product£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Fulton, William; Harris, Joe (1991). Representation theory. A first course. Graduate Texts in Mathematics, Readings in Mathematics. Vol. 129. New York: Springer-Verlag. doi:10.1007/978-1-4612-0979-9. ISBN 978-0-387-97495-8. MR 1153249. OCLC 246650103.£#/li#£ £#li#£Hall, Brian C. (2015), Lie Groups, Lie Algebras, and Representations: An Elementary Introduction, Graduate Texts in Mathematics, vol. 222 (2nd ed.), Springer, ISBN 978-3319134666.£#/li#£ £#li#£James, Gordon Douglas (2001). Representations and characters of groups. Liebeck, Martin W. (2nd ed.). Cambridge, UK: Cambridge University Press. ISBN 978-0521003926. OCLC 52220683.£#/li#£ £#li#£Claudio Procesi (2007) Lie Groups: an approach through invariants and representation, Springer, ISBN 9780387260402 .£#/li#£ £#li#£Serre, Jean-Pierre (1977). Linear Representations of Finite Groups. Springer-Verlag. ISBN 978-0-387-90190-9. OCLC 2202385.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Wald, R. M. General Relativity. Chicago, IL: University of Chicago Press, 1984.£#/li#££#li#£ Wald, R. M. General Relativity. Chicago, IL: University of Chicago Press, 1984. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Algebra > Linear Algebra > Matrices > Matrix Operations £#/li#££#li#£ Calculus and Analysis > Differential Geometry > Tensor Analysis £#/li#££#/ul#£




£#h3#£Antisymmetric Tensor£#/h3#£

In mathematics and theoretical physics, a tensor is antisymmetric on (or with respect to) an index subset if it alternates sign (+/−) when any two indices of the subset are interchanged. The index subset must generally either be all covariant or all contravariant.

For example,

holds when the tensor is antisymmetric with respect to its first three indices.
If a tensor changes sign under exchange of each pair of its indices, then the tensor is completely (or totally) antisymmetric. A completely antisymmetric covariant tensor field of order ${\displaystyle k}$ may be referred to as a differential ${\displaystyle k}$ -form, and a completely antisymmetric contravariant tensor field may be referred to as a ${\displaystyle k}$ -vector field.


£#h5#£Antisymmetric and symmetric tensors£#/h5#£
A tensor A that is antisymmetric on indices ${\displaystyle i}$ and ${\displaystyle j}$ has the property that the contraction with a tensor B that is symmetric on indices ${\displaystyle i}$ and ${\displaystyle j}$ is identically 0.

For a general tensor U with components ${\displaystyle U_{ijk\dots }}$ and a pair of indices ${\displaystyle i}$ and ${\displaystyle j,}$ U has symmetric and antisymmetric parts defined as:

Similar definitions can be given for other pairs of indices. As the term "part" suggests, a tensor is the sum of its symmetric part and antisymmetric part for a given pair of indices, as in


£#h5#£Notation£#/h5#£
A shorthand notation for anti-symmetrization is denoted by a pair of square brackets. For example, in arbitrary dimensions, for an order 2 covariant tensor M,

and for an order 3 covariant tensor T,
In any 2 and 3 dimensions, these can be written as

where ${\displaystyle \delta _{ab\dots }^{cd\dots }}$ is the generalized Kronecker delta, and we use the Einstein notation to summation over like indices.
More generally, irrespective of the number of dimensions, antisymmetrization over ${\displaystyle p}$ indices may be expressed as

In general, every tensor of rank 2 can be decomposed into a symmetric and anti-symmetric pair as:

This decomposition is not in general true for tensors of rank 3 or more, which have more complex symmetries.


£#h5#£Examples£#/h5#£
Totally antisymmetric tensors include:

£#ul#££#li#£Trivially, all scalars and vectors (tensors of order 0 and 1) are totally antisymmetric (as well as being totally symmetric).£#/li#£ £#li#£The electromagnetic tensor, ${\displaystyle F_{\mu \nu }}$ in electromagnetism.£#/li#£ £#li#£The Riemannian volume form on a pseudo-Riemannian manifold.£#/li#££#/ul#£
£#h5#£See also£#/h5#£ £#ul#££#li#£Antisymmetric matrix£#/li#£ £#li#£Exterior algebra – Algebraic construction used in multilinear algebra and geometry£#/li#£ £#li#£Levi-Civita symbol – Antisymmetric permutation object acting on tensors£#/li#£ £#li#£Ricci calculus – Tensor index notation for tensor-based calculations£#/li#£ £#li#£Symmetric tensor – Tensor invariant under permutations of vectors it acts on£#/li#£ £#li#£Symmetrization£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Penrose, Roger (2007). The Road to Reality. Vintage books. ISBN 0-679-77631-1.£#/li#£ £#li#£J.A. Wheeler; C. Misner; K.S. Thorne (1973). Gravitation. W.H. Freeman & Co. pp. 85–86, §3.5. ISBN 0-7167-0344-0.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Antisymmetric Tensor – mathworld.wolfram.com£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Wald, R. M. General Relativity. Chicago, IL: University of Chicago Press, 1984.£#/li#££#li#£ Wald, R. M. General Relativity. Chicago, IL: University of Chicago Press, 1984. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Differential Geometry > Tensor Analysis £#/li#££#/ul#£




£#h3#£Antiunitary£#/h3#£

In mathematics, an antiunitary transformation, is a bijective antilinear map

${\displaystyle U:H_{1}\to H_{2}\,}$
between two complex Hilbert spaces such that

${\displaystyle \langle Ux,Uy\rangle ={\overline {\langle x,y\rangle }}}$
for all ${\displaystyle x}$ and ${\displaystyle y}$ in ${\displaystyle H_{1}}$ , where the horizontal bar represents the complex conjugate. If additionally one has ${\displaystyle H_{1}=H_{2}}$ then ${\displaystyle U}$ is called an antiunitary operator.

Antiunitary operators are important in quantum theory because they are used to represent certain symmetries, such as time reversal. Their fundamental importance in quantum physics is further demonstrated by Wigner's theorem.


£#h5#£Invariance transformations£#/h5#£
In quantum mechanics, the invariance transformations of complex Hilbert space ${\displaystyle H}$ leave the absolute value of scalar product invariant:

${\displaystyle |\langle Tx,Ty\rangle |=|\langle x,y\rangle |}$
for all ${\displaystyle x}$ and ${\displaystyle y}$ in ${\displaystyle H}$ .

Due to Wigner's theorem these transformations can either be unitary or antiunitary.


£#h5#£Geometric Interpretation£#/h5#£
Congruences of the plane form two distinct classes. The first conserves the orientation and is generated by translations and rotations. The second does not conserve the orientation and is obtained from the first class by applying a reflection. On the complex plane these two classes correspond (up to translation) to unitaries and antiunitaries, respectively.


£#h5#£Properties£#/h5#£ £#ul#££#li#£ ${\displaystyle \langle Ux,Uy\rangle ={\overline {\langle x,y\rangle }}=\langle y,x\rangle }$ holds for all elements ${\displaystyle x,y}$ of the Hilbert space and an antiunitary ${\displaystyle U}$ .£#/li#£ £#li#£When ${\displaystyle U}$ is antiunitary then ${\displaystyle U^{2}}$ is unitary. This follows from £#/li#£ £#li#£For unitary operator ${\displaystyle V}$ the operator ${\displaystyle VK}$ , where ${\displaystyle K}$ is complex conjugate operator, is antiunitary. The reverse is also true, for antiunitary ${\displaystyle U}$ the operator ${\displaystyle UK}$ is unitary.£#/li#£ £#li#£For antiunitary ${\displaystyle U}$ the definition of the adjoint operator ${\displaystyle U^{*}}$ is changed to compensate the complex conjugation, becoming £#/li#£ £#li#£The adjoint of an antiunitary ${\displaystyle U}$ is also antiunitary and (This is not to be confused with the definition of unitary operators, as the antiunitary operator ${\displaystyle U}$ is not complex linear.)£#/li#££#/ul#£
£#h5#£Examples£#/h5#£ £#ul#££#li#£The complex conjugate operator ${\displaystyle K,}$ ${\displaystyle Kz={\overline {z}},}$ is an antiunitary operator on the complex plane.£#/li#£ £#li#£The operator where ${\displaystyle \sigma _{y}}$ is the second Pauli matrix and ${\displaystyle K}$ is the complex conjugate operator, is antiunitary. It satisfies ${\displaystyle U^{2}=-1}$ .£#/li#££#/ul#£
£#h5#£Decomposition of an antiunitary operator into a direct sum of elementary Wigner antiunitaries£#/h5#£
An antiunitary operator on a finite-dimensional space may be decomposed as a direct sum of elementary Wigner antiunitaries ${\displaystyle W_{\theta }}$ , ${\displaystyle 0\leq \theta \leq \pi }$ . The operator ${\displaystyle W_{0}:\mathbb {C} \to \mathbb {C} }$ is just simple complex conjugation on ${\displaystyle \mathbb {C} }$

${\displaystyle W_{0}(z)={\overline {z}}}$
For ${\displaystyle 0<\theta \leq \pi }$ , the operator ${\displaystyle W_{\theta }}$ acts on two-dimensional complex Hilbert space. It is defined by

${\displaystyle W_{\theta }\left(\left(z_{1},z_{2}\right)\right)=\left(e^{{\frac {i}{2}}\theta }{\overline {z_{2}}},\;e^{-{\frac {i}{2}}\theta }{\overline {z_{1}}}\right).}$
Note that for ${\displaystyle 0<\theta \leq \pi }$

${\displaystyle W_{\theta }\left(W_{\theta }\left(\left(z_{1},z_{2}\right)\right)\right)=\left(e^{i\theta }z_{1},e^{-i\theta }z_{2}\right),}$
so such ${\displaystyle W_{\theta }}$ may not be further decomposed into ${\displaystyle W_{0}}$ 's, which square to the identity map.

Note that the above decomposition of antiunitary operators contrasts with the spectral decomposition of unitary operators. In particular, a unitary operator on a complex Hilbert space may be decomposed into a direct sum of unitaries acting on 1-dimensional complex spaces (eigenspaces), but an antiunitary operator may only be decomposed into a direct sum of elementary operators on 1- and 2-dimensional complex spaces.


£#h5#£References£#/h5#£ £#ul#££#li#£Wigner, E. "Normal Form of Antiunitary Operators", Journal of Mathematical Physics Vol 1, no 5, 1960, pp. 409–412£#/li#£ £#li#£Wigner, E. "Phenomenological Distinction between Unitary and Antiunitary Symmetry Operators", Journal of Mathematical Physics Vol1, no5, 1960, pp.414–416£#/li#££#/ul#£
£#h5#£See also£#/h5#£ £#ul#££#li#£Unitary operator£#/li#£ £#li#£Wigner's Theorem£#/li#£ £#li#£Particle physics and representation theory£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Sakurai, J. J. Modern Quantum Mechanics. Menlo Park, CA: Benjamin/Cummings, 1985.£#/li#££#li#£ Sakurai, J. J. Modern Quantum Mechanics. Menlo Park, CA: Benjamin/Cummings, 1985. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Operator Theory £#/li#££#/ul#£




£#h3#£Apéry's Constant£#/h3#£

£#h5#£ References £#/h5#£ £#ul#££#li#£Amdeberhan, T. "Faster and Faster Convergent Series for zeta(3)." Electronic J. Combinatorics 3, No. 1, R13, 1-2, 1996. http://www.combinatorics.org/Volume_3/Abstracts/v3i1r13.html.£#/li#££#li#£Amdeberhan, T. and Zeilberger, D. "Hypergeometric Series Acceleration via the WZ Method." Electronic J. Combinatorics 4, No. 2, R3, 1-3, 1997. http://www.combinatorics.org/Volume_4/Abstracts/v4i2r3.html. Also available at http://www.math.temple.edu/~zeilberg/mamarim/mamarimhtml/accel.html.£#/li#££#li#£Apéry, R. "Irrationalité de zeta(2) et zeta(3)." Astérisque 61, 11-13, 1979.£#/li#££#li#£Bailey, D. H.; Borwein, J. M.; Calkin, N. J.; Girgensohn, R.; Luke, D. R.; and Moll, V. H. Experimental Mathematics in Action. Wellesley, MA: A K Peters, 2007.£#/li#££#li#£Bailey, D. H. and Crandall, R. E. "Random Generators and Normal Numbers." Exper. Math. 11, 527-546, 2002.£#/li#££#li#£Preprint dated Feb. 22, 2003 available at http://www.nersc.gov/~dhbailey/dhbpapers/bcnormal.pdf.£#/li#££#li#£Berndt, B. C. Ramanujan's Notebooks: Part I. New York: Springer-Verlag, 1985.£#/li#££#li#£Beukers, F. "A Note on the Irrationality of zeta(2) and zeta(3)." Bull. London Math. Soc. 11, 268-272, 1979.£#/li#££#li#£Beukers, F. "Another Congruence for the Apéry Numbers." J. Number Th. 25, 201-210, 1987.£#/li#££#li#£Boros, G. and Moll, V. Irresistible Integrals: Symbolics, Analysis and Experiments in the Evaluation of Integrals. Cambridge, England: Cambridge University Press, 2004.£#/li#££#li#£Borwein, J. M. and Borwein, P. B. Pi & the AGM: A Study in Analytic Number Theory and Computational Complexity. New York: Wiley, 1987.£#/li#££#li#£Castellanos, D. "The Ubiquitous Pi. Part I." Math. Mag. 61, 67-98, 1988.£#/li#££#li#£Conway, J. H. and Guy, R. K. "The Great Enigma." In The Book of Numbers. New York: Springer-Verlag, pp. 261-262, 1996.£#/li#££#li#£Derbyshire, J. Prime Obsession: Bernhard Riemann and the Greatest Unsolved Problem in Mathematics. New York: Penguin, pp. 76 and 371, 2004.£#/li#££#li#£Dvornicich, R. and Viola, C. "Some Remarks on Beukers' Integrals." In Number Theory, Colloq. Math. Soc. János Bolyai, Vol. 51. Amsterdam, Netherlands: North-Holland, pp. 637-657, 1987.£#/li#££#li#£Ewell, J. A. "A New Series Representation for zeta(3)." Amer. Math. Monthly 97, 219-220, 1990.£#/li#££#li#£Finch, S. R. "Apéry's Constant." §1.6 in Mathematical Constants. Cambridge, England: Cambridge University Press, pp. 40-53, 2003.£#/li#££#li#£Gosper, R. W. "Strip Mining in the Abandoned Orefields of Nineteenth Century Mathematics." In Computers in Mathematics (Ed. D. V. Chudnovsky and R. D. Jenks). New York: Dekker, 1990.£#/li#££#li#£Gosper, R. W. "Zeta(3) to 250000 digits." math-fun@cs.arizona.edu posting, Sept. 1, 1996.£#/li#££#li#£Gourevitch, P. "L'univers de pi." http://www.pi314.net/hypergse11.php.£#/li#££#li#£Gutnik, L. A. "On the Irrationality of Some Quantities Containing zeta(3)." Acta Arith. 42, 255-264, 1983. English translation in Amer. Math. Soc. Transl. 140, 45-55, 1988.£#/li#££#li#£Haible, B. and Papanikolaou, T. "Fast Multiprecision Evaluation of Series of Rational Numbers." Technical Report TI-97-7. Darmstadt, Germany: Darmstadt University of Technology, Apr. 1997.£#/li#££#li#£Hata, M. "A New Irrationality Measure for zeta(3)." Acta Arith. 92, 47-57, 2000.£#/li#££#li#£Havil, J. Gamma: Exploring Euler's Constant. Princeton, NJ: Princeton University Press, p. 42, 2003.£#/li#££#li#£Huvent, G. "Formules d'ordre supérieur." Pi314.net, 2002. http://s146372241.onlinehome.fr/web/pi314.net/hypergse11.php#x13-107002r480.£#/li#££#li#£Huylebrouck, D. "Similarities in Irrationality Proofs for pi, ln2, zeta(2), and zeta(3)." Amer. Math. Monthly 108, 222-231, 2001.£#/li#££#li#£Jin, Y. and Dickinson, H. "Apéry Sequences and Legendre Transforms." J. Austral. Math. Soc. Ser. A 68, 349-356, 2000.£#/li#££#li#£Le Lionnais, F. Les nombres remarquables. Paris: Hermann, p. 36, 1983.£#/li#££#li#£Nesterenko, Yu. V. "A Few Remarks on zeta(3)." Mat. Zametki 59, 865-880, 1996. English translation in Math. Notes 59, 625-636, 1996.£#/li#££#li#£Plouffe, S. "Table of Current Records for the Computation of Constants." http://pi.lacim.uqam.ca/eng/records_en.html.£#/li#££#li#£Prévost, M. "A New Proof of the Irrationality of zeta(2) and zeta(3) using Padé Approximants." J. Comput. Appl. Math. 67, 219-235, 1996.£#/li#££#li#£Rhin, G. and Viola, C. "The Group Structure for zeta(3)." Acta Arith. 97, 269-293, 2001.£#/li#££#li#£Sloane, N. J. A. Sequence A002117/M0020 in "The On-Line Encyclopedia of Integer Sequences."£#/li#££#li#£Sorokin, V. N. "Hermite-Padé Approximations for Nikishin Systems and the Irrationality of zeta(3)." Uspekhi Mat. Nauk 49, 167-168, 1994. English translation in Russian Math. Surveys 49, 176-177, 1994.£#/li#££#li#£Srivastava, H. M. "Some Simple Algorithms for the Evaluations and Representations of the Riemann Zeta Function at Positive Integer Arguments." J. Math. Anal. Appl. 246, 331-351, 2000.£#/li#££#li#£van der Poorten, A. "A Proof that Euler Missed... Apéry's Proof of the Irrationality of zeta(3)." Math. Intel. 1, 196-203, 1979.£#/li#££#li#£Wedeniwski, S. "128000026 Digits of Zeta(3)." http://pi.lacim.uqam.ca/piDATA/Zeta3.txt.£#/li#££#li#£Wells, D. The Penguin Dictionary of Curious and Interesting Numbers. Middlesex, England: Penguin Books, p. 33, 1986.£#/li#££#li#£Zeilberger, D. "The Method of Creative Telescoping." J. Symb. Comput. 11, 195-204, 1991.£#/li#££#li#£ Amdeberhan, T. "Faster and Faster Convergent Series for ." Electronic J. Combinatorics 3, No. 1, R13, 1-2, 1996. http://www.combinatorics.org/Volume_3/Abstracts/v3i1r13.html. £#/li#££#li#£ Amdeberhan, T. and Zeilberger, D. "Hypergeometric Series Acceleration via the WZ Method." Electronic J. Combinatorics 4, No. 2, R3, 1-3, 1997. http://www.combinatorics.org/Volume_4/Abstracts/v4i2r3.html. Also available at http://www.math.temple.edu/~zeilberg/mamarim/mamarimhtml/accel.html. £#/li#££#li#£ Apéry, R. "Irrationalité de et ." Astérisque 61, 11-13, 1979. £#/li#££#li#£ Bailey, D. H.; Borwein, J. M.; Calkin, N. J.; Girgensohn, R.; Luke, D. R.; and Moll, V. H. Experimental Mathematics in Action. Wellesley, MA: A K Peters, 2007. £#/li#££#li#£ Bailey, D. H. and Crandall, R. E. "Random Generators and Normal Numbers." Exper. Math. 11, 527-546, 2002. £#/li#££#li#£ Preprint dated Feb. 22, 2003 available at http://www.nersc.gov/~dhbailey/dhbpapers/bcnormal.pdf. £#/li#££#li#£ Berndt, B. C. Ramanujan's Notebooks: Part I. New York: Springer-Verlag, 1985. £#/li#££#li#£ Beukers, F. "A Note on the Irrationality of and ." Bull. London Math. Soc. 11, 268-272, 1979. £#/li#££#li#£ Beukers, F. "Another Congruence for the Apéry Numbers." J. Number Th. 25, 201-210, 1987. £#/li#££#li#£ Boros, G. and Moll, V. Irresistible Integrals: Symbolics, Analysis and Experiments in the Evaluation of Integrals. Cambridge, England: Cambridge University Press, 2004. £#/li#££#li#£ Borwein, J. M. and Borwein, P. B. Pi & the AGM: A Study in Analytic Number Theory and Computational Complexity. New York: Wiley, 1987. £#/li#££#li#£ Castellanos, D. "The Ubiquitous Pi. Part I." Math. Mag. 61, 67-98, 1988. £#/li#££#li#£ Conway, J. H. and Guy, R. K. "The Great Enigma." In The Book of Numbers. New York: Springer-Verlag, pp. 261-262, 1996. £#/li#££#li#£ Derbyshire, J. Prime Obsession: Bernhard Riemann and the Greatest Unsolved Problem in Mathematics. New York: Penguin, pp. 76 and 371, 2004. £#/li#££#li#£ Dvornicich, R. and Viola, C. "Some Remarks on Beukers' Integrals." In Number Theory, Colloq. Math. Soc. János Bolyai, Vol. 51. Amsterdam, Netherlands: North-Holland, pp. 637-657, 1987. £#/li#££#li#£ Ewell, J. A. "A New Series Representation for ." Amer. Math. Monthly 97, 219-220, 1990. £#/li#££#li#£ Finch, S. R. "Apéry's Constant." §1.6 in Mathematical Constants. Cambridge, England: Cambridge University Press, pp. 40-53, 2003. £#/li#££#li#£ Gosper, R. W. "Strip Mining in the Abandoned Orefields of Nineteenth Century Mathematics." In Computers in Mathematics (Ed. D. V. Chudnovsky and R. D. Jenks). New York: Dekker, 1990. £#/li#££#li#£ Gosper, R. W. "Zeta(3) to digits." math-fun@cs.arizona.edu posting, Sept. 1, 1996. £#/li#££#li#£ Gourevitch, P. "L'univers de ." http://www.pi314.net/hypergse11.php. £#/li#££#li#£ Gutnik, L. A. "On the Irrationality of Some Quantities Containing ." Acta Arith. 42, 255-264, 1983. English translation in Amer. Math. Soc. Transl. 140, 45-55, 1988. £#/li#££#li#£ Haible, B. and Papanikolaou, T. "Fast Multiprecision Evaluation of Series of Rational Numbers." Technical Report TI-97-7. Darmstadt, Germany: Darmstadt University of Technology, Apr. 1997. £#/li#££#li#£ Hata, M. "A New Irrationality Measure for ." Acta Arith. 92, 47-57, 2000. £#/li#££#li#£ Havil, J. Gamma: Exploring Euler's Constant. Princeton, NJ: Princeton University Press, p. 42, 2003. £#/li#££#li#£ Huvent, G. "Formules d'ordre supérieur." Pi314.net, 2002. http://s146372241.onlinehome.fr/web/pi314.net/hypergse11.php#x13-107002r480. £#/li#££#li#£ Huylebrouck, D. "Similarities in Irrationality Proofs for , , , and ." Amer. Math. Monthly 108, 222-231, 2001. £#/li#££#li#£ Jin, Y. and Dickinson, H. "Apéry Sequences and Legendre Transforms." J. Austral. Math. Soc. Ser. A 68, 349-356, 2000. £#/li#££#li#£ Le Lionnais, F. Les nombres remarquables. Paris: Hermann, p. 36, 1983. £#/li#££#li#£ Nesterenko, Yu. V. "A Few Remarks on ." Mat. Zametki 59, 865-880, 1996. English translation in Math. Notes 59, 625-636, 1996. £#/li#££#li#£ Plouffe, S. "Table of Current Records for the Computation of Constants." http://pi.lacim.uqam.ca/eng/records_en.html. £#/li#££#li#£ Prévost, M. "A New Proof of the Irrationality of and using Padé Approximants." J. Comput. Appl. Math. 67, 219-235, 1996. £#/li#££#li#£ Rhin, G. and Viola, C. "The Group Structure for ." Acta Arith. 97, 269-293, 2001. £#/li#££#li#£ Sloane, N. J. A. Sequence A002117/M0020 in "The On-Line Encyclopedia of Integer Sequences." £#/li#££#li#£ Sorokin, V. N. "Hermite-Padé Approximations for Nikishin Systems and the Irrationality of ." Uspekhi Mat. Nauk 49, 167-168, 1994. English translation in Russian Math. Surveys 49, 176-177, 1994. £#/li#££#li#£ Srivastava, H. M. "Some Simple Algorithms for the Evaluations and Representations of the Riemann Zeta Function at Positive Integer Arguments." J. Math. Anal. Appl. 246, 331-351, 2000. £#/li#££#li#£ van der Poorten, A. "A Proof that Euler Missed... Apéry's Proof of the Irrationality of ." Math. Intel. 1, 196-203, 1979. £#/li#££#li#£ Wedeniwski, S. " Digits of Zeta(3)." http://pi.lacim.uqam.ca/piDATA/Zeta3.txt. £#/li#££#li#£ Wells, D. The Penguin Dictionary of Curious and Interesting Numbers. Middlesex, England: Penguin Books, p. 33, 1986. £#/li#££#li#£ Zeilberger, D. "The Method of Creative Telescoping." J. Symb. Comput. 11, 195-204, 1991. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Number Theory > Constants > Apery's Constant £#/li#££#li#£ Calculus and Analysis > Calculus > Integrals > Definite Integrals £#/li#££#li#£ Calculus and Analysis > Series > BBP Formulas £#/li#££#li#£ Number Theory > Prime Numbers > Prime Number Sequences £#/li#££#li#£ Recreational Mathematics > Mathematical Records £#/li#££#/ul#£




£#h3#£Apodization£#/h3#£

Apodization is a signal processing technique. Its literal translation from Greek is "removing the foot". It is the technical term for changing the shape of a mathematical function, an electrical signal, an optical transmission or a mechanical structure. In optics, it is primarily used to remove Airy disks caused by diffraction around an intensity peak, improving the focus.


£#h5#£Apodization in electronics£#/h5#£
£#h5#£Apodization in signal processing£#/h5#£
The term apodization is used frequently in publications on Fourier-transform infrared (FTIR) signal processing. An example of apodization is the use of the Hann window in the fast Fourier transform analyzer to smooth the discontinuities at the beginning and end of the sampled time record.


£#h5#£Apodization in digital audio£#/h5#£
An apodizing filter can be used in digital audio processing instead of the more common brickwall filters, in order to avoid the pre-ringing that the latter introduces.


£#h5#£Apodization in mass spectrometry£#/h5#£
During oscillation within an Orbitrap, ion transient signal may not be stable until the ions settle into their oscillations. Toward the end, subtle ion collisions have added up to cause noticeable dephasing. This presents a problem for the Fourier transformation, as it averages the oscillatory signal across the length of the time-domain measurement. Software allows “apodization”, the removal of the front and back section of the transient signal from consideration in the FT calculation. Thus, apodization improves the resolution of the resulting mass spectrum. Another way to improve the quality of the transient is to wait to collect data until ions have settled into stable oscillatory motion within the trap.


£#h5#£Apodization in nuclear magnetic resonance spectroscopy£#/h5#£
Apodization is applied to NMR signals before discrete Fourier Transformation. Typically, NMR signals are truncated due to time constraints (indirect dimension) or to obtain a higher signal-to-noise ratio. In order to reduce truncation artefacts, the signals are subjected to apodization with different types of window functions.


£#h5#£Apodization in optics£#/h5#£
In optical design jargon, an apodization function is used to purposely change the input intensity profile of an optical system, and may be a complicated function to tailor the system to certain properties. Usually it refers to a non-uniform illumination or transmission profile that approaches zero at the edges.


£#h5#£Apodization in imaging£#/h5#£
Since side lobes of the Airy disk are responsible for degrading the image, techniques for suppressing them are utilized. In case the imaging beam has Gaussian distribution, when the truncation ratio (the ratio of the diameter of the Gaussian beam to the diameter of the truncating aperture) is set to 1, the side-lobes become negligible and the beam profile becomes purely Gaussian.

In medical ultrasonography, the effect of grating lobes can be reduced by activiating ultrasonic transducer elements using variable voltages in apodization process.


£#h5#£Apodization in photography£#/h5#£
Most camera lenses contain diaphragms which decrease the amount of light coming into the camera. These are not strictly an example of apodization, since the diaphragm does not produce a smooth transition to zero intensity, nor does it provide shaping of the intensity profile (beyond the obvious all-or-nothing, "top hat" transmission of its aperture).

Some lenses use other methods to reduce the amount of light let in. For example, the Minolta/Sony STF 135mm f/2.8 T4.5 lens however, has a special design introduced in 1999, which accomplishes this by utilizing a concave neutral-gray tinted lens element as an apodization filter, thereby producing a pleasant bokeh. The same optical effect can be achieved combining depth-of-field bracketing with multi exposure, as implemented in the Minolta Maxxum 7's STF function. In 2014, Fujifilm announced a lens utilizing a similar apodization filter in the Fujinon XF 56mm F1.2 R APD lens. In 2017, Sony introduced the E-mount full-frame lens Sony FE 100mm F2.8 STF GM OSS (SEL-100F28GM) based on the same optical Smooth Trans Focus principle.

Simulation of a Gaussian laser beam input profile is also an example of apodization.

Photon sieves provide a relatively easy way to achieve tailored optical apodization.


£#h5#£Apodization in astronomy£#/h5#£
Apodization is used in telescope optics in order to improve the dynamic range of the image. For example, stars with low intensity in the close vicinity of very bright stars can be made visible using this technique, and even images of planets can be obtained when otherwise obscured by the bright atmosphere of the star they orbit. Generally, apodization reduces the resolution of an optical image; however, because it reduces diffraction edge effects, it can actually enhance certain small details. In fact the notion of resolution, as it is commonly defined with the Rayleigh criterion, is in this case partially irrelevant. One has to understand that the image formed in the focal plane of a lens (or a mirror) is modelled through the Fresnel diffraction formalism. The classical diffraction pattern, the Airy disk, is connected to a circular pupil, without any obstruction and with a uniform transmission. Any change in the shape of the pupil (for example a square instead of a circle), or in its transmission, results in an alteration in the associated diffraction pattern.


£#h5#£See also£#/h5#£ £#ul#££#li#£Apodization function£#/li#££#/ul#£
£#h5#£References£#/h5#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Integral Transforms > Apodization Functions £#/li#££#/ul#£




£#h3#£Apodization Function£#/h3#£

In signal processing and statistics, a window function (also known as an apodization function or tapering function) is a mathematical function that is zero-valued outside of some chosen interval, normally symmetric around the middle of the interval, usually near a maximum in the middle, and usually tapering away from the middle. Mathematically, when another function or waveform/data-sequence is "multiplied" by a window function, the product is also zero-valued outside the interval: all that is left is the part where they overlap, the "view through the window". Equivalently, and in actual practice, the segment of data within the window is first isolated, and then only that data is multiplied by the window function values. Thus, tapering, not segmentation, is the main purpose of window functions.

The reasons for examining segments of a longer function include detection of transient events and time-averaging of frequency spectra. The duration of the segments is determined in each application by requirements like time and frequency resolution. But that method also changes the frequency content of the signal by an effect called spectral leakage. Window functions allow us to distribute the leakage spectrally in different ways, according to the needs of the particular application. There are many choices detailed in this article, but many of the differences are so subtle as to be insignificant in practice.

In typical applications, the window functions used are non-negative, smooth, "bell-shaped" curves. Rectangle, triangle, and other functions can also be used. A more general definition of window functions does not require them to be identically zero outside an interval, as long as the product of the window multiplied by its argument is square integrable, and, more specifically, that the function goes sufficiently rapidly toward zero.


£#h5#£Applications£#/h5#£
Window functions are used in spectral analysis/modification/resynthesis, the design of finite impulse response filters, as well as beamforming and antenna design.


£#h5#£Spectral analysis£#/h5#£
The Fourier transform of the function cos(ωt) is zero, except at frequency ±ω. However, many other functions and waveforms do not have convenient closed-form transforms. Alternatively, one might be interested in their spectral content only during a certain time period.

In either case, the Fourier transform (or a similar transform) can be applied on one or more finite intervals of the waveform. In general, the transform is applied to the product of the waveform and a window function. Any window (including rectangular) affects the spectral estimate computed by this method.


£#h5#£Choice of window function£#/h5#£
Windowing of a simple waveform like cos(ωt) causes its Fourier transform to develop non-zero values (commonly called spectral leakage) at frequencies other than ω. The leakage tends to be worst (highest) near ω and least at frequencies farthest from ω.

If the waveform under analysis comprises two sinusoids of different frequencies, leakage can interfere with our ability to distinguish them spectrally. Possible types of interference are often broken down into two opposing classes as follows: If the component frequencies are dissimilar and one component is weaker, then leakage from the stronger component can obscure the weaker one's presence. But if the frequencies are too similar, leakage can render them unresolvable even when the sinusoids are of equal strength. Windows that are effective against the first type of interference, namely where components have dissimilar frequencies and amplitudes, are called high dynamic range. Conversely, windows that can distinguish components with similar frequencies and amplitudes are called high resolution.

The rectangular window is an example of a window that is high resolution but low dynamic range, meaning it is good for distinguishing components of similar amplitude even when the frequencies are also close, but poor at distinguishing components of different amplitude even when the frequencies are far away. High-resolution, low-dynamic-range windows such as the rectangular window also have the property of high sensitivity, which is the ability to reveal relatively weak sinusoids in the presence of additive random noise. That is because the noise produces a stronger response with high-dynamic-range windows than with high-resolution windows.

At the other extreme of the range of window types are windows with high dynamic range but low resolution and sensitivity. High-dynamic-range windows are most often justified in wideband applications, where the spectrum being analyzed is expected to contain many different components of various amplitudes.

In between the extremes are moderate windows, such as Hamming and Hann. They are commonly used in narrowband applications, such as the spectrum of a telephone channel.

In summary, spectral analysis involves a trade-off between resolving comparable strength components with similar frequencies (high resolution / sensitivity) and resolving disparate strength components with dissimilar frequencies (high dynamic range). That trade-off occurs when the window function is chosen.: p. 90 


£#h5#£Discrete-time signals£#/h5#£
When the input waveform is time-sampled, instead of continuous, the analysis is usually done by applying a window function and then a discrete Fourier transform (DFT). But the DFT provides only a sparse sampling of the actual discrete-time Fourier transform (DTFT) spectrum. Figure 2, row 3 shows a DTFT for a rectangularly-windowed sinusoid. The actual frequency of the sinusoid is indicated as "13" on the horizontal axis. Everything else is leakage, exaggerated by the use of a logarithmic presentation. The unit of frequency is "DFT bins"; that is, the integer values on the frequency axis correspond to the frequencies sampled by the DFT. So the figure depicts a case where the actual frequency of the sinusoid coincides with a DFT sample, and the maximum value of the spectrum is accurately measured by that sample. In row 4, it misses the maximum value by ½ bin, and the resultant measurement error is referred to as scalloping loss (inspired by the shape of the peak). For a known frequency, such as a musical note or a sinusoidal test signal, matching the frequency to a DFT bin can be prearranged by choices of a sampling rate and a window length that results in an integer number of cycles within the window.


£#h5#£Noise bandwidth£#/h5#£
The concepts of resolution and dynamic range tend to be somewhat subjective, depending on what the user is actually trying to do. But they also tend to be highly correlated with the total leakage, which is quantifiable. It is usually expressed as an equivalent bandwidth, B. It can be thought of as redistributing the DTFT into a rectangular shape with height equal to the spectral maximum and width B. The more the leakage, the greater the bandwidth. It is sometimes called noise equivalent bandwidth or equivalent noise bandwidth, because it is proportional to the average power that will be registered by each DFT bin when the input signal contains a random noise component (or is just random noise). A graph of the power spectrum, averaged over time, typically reveals a flat noise floor, caused by this effect. The height of the noise floor is proportional to B. So two different window functions can produce different noise floors.


£#h5#£Processing gain and losses£#/h5#£
In signal processing, operations are chosen to improve some aspect of quality of a signal by exploiting the differences between the signal and the corrupting influences. When the signal is a sinusoid corrupted by additive random noise, spectral analysis distributes the signal and noise components differently, often making it easier to detect the signal's presence or measure certain characteristics, such as amplitude and frequency. Effectively, the signal-to-noise ratio (SNR) is improved by distributing the noise uniformly, while concentrating most of the sinusoid's energy around one frequency. Processing gain is a term often used to describe an SNR improvement. The processing gain of spectral analysis depends on the window function, both its noise bandwidth (B) and its potential scalloping loss. These effects partially offset, because windows with the least scalloping naturally have the most leakage.

Figure 3 depicts the effects of three different window functions on the same data set, comprising two equal strength sinusoids in additive noise. The frequencies of the sinusoids are chosen such that one encounters no scalloping and the other encounters maximum scalloping. Both sinusoids suffer less SNR loss under the Hann window than under the Blackman–Harris window. In general (as mentioned earlier), this is a deterrent to using high-dynamic-range windows in low-dynamic-range applications.


£#h5#£Symmetry£#/h5#£
The formulas provided in this article produce discrete sequences, as if a continuous window function has been "sampled". (See an example at Kaiser window.) Window sequences for spectral analysis are either symmetric or 1-sample short of symmetric (called periodic, DFT-even, or DFT-symmetric: p. 52 ). For instance, a true symmetric sequence, with its maximum at a single center-point, is generated by the MATLAB function hann(9,'symmetric'). Deleting the last sample produces a sequence identical to hann(8,'periodic'). Similarly, the sequence hann(8,'symmetric') has two equal center-points.

Some functions have one or two zero-valued end-points, which are unnecessary in most applications. Deleting a zero-valued end-point has no effect on its DTFT (spectral leakage). But the function designed for N + 1 or N + 2 samples, in anticipation of deleting one or both end points, typically has a slightly narrower main lobe, slightly higher sidelobes, and a slightly smaller noise-bandwidth.

DFT-symmetry
The predecessor of the DFT is the finite Fourier transform, and window functions were "always an odd number of points and exhibit even symmetry about the origin".: p.52  In that case, the DTFT is entirely real-valued. When the same sequence is shifted into a DFT data window, ${\displaystyle [0\leq n\leq N],}$ the DTFT becomes complex-valued except at frequencies spaced at regular intervals of ${\displaystyle 1/N.}$   Thus, when sampled by an ${\displaystyle N}$ -length DFT, the samples (called DFT coefficients) are still real-valued. An approximation is to truncate the N+1-length sequence (effectively ${\displaystyle w[N]=0}$ ), and compute an ${\displaystyle N}$ -length DFT. The DTFT (spectral leakage) is slightly affected, but the samples remain real-valued. The terms DFT-even and periodic refer to the idea that if the truncated sequence were repeated periodically, it would be even-symmetric about ${\displaystyle n=0,}$ and its DTFT would be entirely real-valued. But the actual DTFT is generally complex-valued, except for the ${\displaystyle N}$ DFT coefficients. In subsequent sections, we sample the DTFT much more densely and display only the magnitude component of the complex numbers.

Periodic summation
An exact method to sample the DTFT of an N+1-length sequence at intervals of ${\displaystyle 1/N}$ is described at DTFT § L=N+1. Essentially, ${\displaystyle w[N]}$ is combined with ${\displaystyle w[0]}$ (by addition), and an ${\displaystyle N}$ -point DFT is done on the truncated sequence. Similarly, spectral analysis would be done by combining the ${\displaystyle n=0}$ and ${\displaystyle n=N}$ data samples before applying the truncated symmetric window. That is not a common practice, even though truncated windows are very popular.

Convolution
The appeal of DFT-symmetric windows is explained by the popularity of the fast Fourier transform (FFT) algorithm for implementation of the DFT, because truncation of an odd-length sequence results in an even-length sequence. Their real-valued DFT coefficients are also an advantage in certain esoteric applications where windowing is achieved by means of convolution between the DFT coefficients and an unwindowed DFT of the data.: p.62 : p.85  In those applications, DFT-symmetric windows (even or odd length) from the Cosine-sum family are preferred, because most of their DFT coefficients are zero-valued, making the convolution very efficient.: p.85 


£#h5#£Filter design£#/h5#£
Windows are sometimes used in the design of digital filters, in particular to convert an "ideal" impulse response of infinite duration, such as a sinc function, to a finite impulse response (FIR) filter design. That is called the window method.


£#h5#£Statistics and curve fitting£#/h5#£
Window functions are sometimes used in the field of statistical analysis to restrict the set of data being analyzed to a range near a given point, with a weighting factor that diminishes the effect of points farther away from the portion of the curve being fit. In the field of Bayesian analysis and curve fitting, this is often referred to as the kernel.


£#h5#£Rectangular window applications£#/h5#£
£#h5#£Analysis of transients£#/h5#£
When analyzing a transient signal in modal analysis, such as an impulse, a shock response, a sine burst, a chirp burst, or noise burst, where the energy vs time distribution is extremely uneven, the rectangular window may be most appropriate. For instance, when most of the energy is located at the beginning of the recording, a non-rectangular window attenuates most of the energy, degrading the signal-to-noise ratio.


£#h5#£Harmonic analysis£#/h5#£
One might wish to measure the harmonic content of a musical note from a particular instrument or the harmonic distortion of an amplifier at a given frequency. Referring again to Figure 2, we can observe that there is no leakage at a discrete set of harmonically-related frequencies sampled by the DFT. (The spectral nulls are actually zero-crossings, which cannot be shown on a logarithmic scale such as this.) This property is unique to the rectangular window, and it must be appropriately configured for the signal frequency, as described above.


£#h5#£A list of window functions£#/h5#£
Conventions:

£#ul#££#li#£ ${\displaystyle w_{0}(x)}$ is a zero-phase function (symmetrical about ${\displaystyle x=0}$ ), continuous for ${\displaystyle x\in [-N/2,N/2],}$ where ${\displaystyle N}$ is a positive integer (even or odd).£#/li#£ £#li#£The sequence   ${\displaystyle \{w[n]=w_{0}(n-N/2),\quad 0\leq n\leq N\}}$   is symmetric, of length ${\displaystyle N+1.}$ £#/li#£ £#li#£ ${\displaystyle \{w[n],\quad 0\leq n\leq N-1\}}$   is DFT-symmetric, of length ${\displaystyle N.}$ £#/li#££#/ul#££#ul#££#li#£The parameter B displayed on each spectral plot is the function's noise equivalent bandwidth metric, in units of DFT bins.£#/li#££#/ul#£
The sparse sampling of a DTFT (such as the DFTs in Fig 2) only reveals the leakage into the DFT bins from a sinusoid whose frequency is also an integer DFT bin. The unseen sidelobes reveal the leakage to expect from sinusoids at other frequencies. Therefore, when choosing a window function, it is usually important to sample the DTFT more densely (as we do throughout this section) and choose a window that suppresses the sidelobes to an acceptable level.


£#h5#£Rectangular window£#/h5#£
The rectangular window (sometimes known as the boxcar or Dirichlet window) is the simplest window, equivalent to replacing all but N values of a data sequence by zeros, making it appear as though the waveform suddenly turns on and off:

${\displaystyle w[n]=1.}$
Other windows are designed to moderate these sudden changes, which reduces scalloping loss and improves dynamic range, as described above (§ Spectral analysis).

The rectangular window is the 1st order B-spline window as well as the 0th power power-of-sine window.

The rectangular window provides the minimum mean square error estimate of the Discrete-time Fourier transform, at the cost of other issues discussed.


£#h5#£B-spline windows£#/h5#£
B-spline windows can be obtained as k-fold convolutions of the rectangular window. They include the rectangular window itself (k = 1), the § Triangular window (k = 2) and the § Parzen window (k = 4). Alternative definitions sample the appropriate normalized B-spline basis functions instead of convolving discrete-time windows.  A  kth-order B-spline basis function is a piece-wise polynomial function of degree k−1 that is obtained by k-fold self-convolution of the rectangular function.


£#h5#£Triangular window£#/h5#£
Triangular windows are given by:

${\displaystyle w[n]=1-\left|{\frac {n-{\frac {N}{2}}}{\frac {L}{2}}}\right|,\quad 0\leq n\leq N}$
where L can be N, N + 1, or N + 2.  The first one is also known as Bartlett window or Fejér window. All three definitions converge at large N.

The triangular window is the 2nd order B-spline window. The L = N form can be seen as the convolution of two N/2-width rectangular windows. The Fourier transform of the result is the squared values of the transform of the half-width rectangular window.


£#h5#£Parzen window£#/h5#£
Defining  L ≜ N + 1,  the Parzen window, also known as the de la Vallée Poussin window, is the 4th order B-spline window given by:

${\displaystyle w_{0}(n)\triangleq \left\{{\begin{array}{ll}1-6\left({\frac {n}{L/2}}\right)^{2}\left(1-{\frac {|n|}{L/2}}\right),&0\leq |n|\leq {\frac {L}{4}}\\2\left(1-{\frac {|n|}{L/2}}\right)^{3}&{\frac {L}{4}}<|n|\leq {\frac {L}{2}}\\\end{array}}\right\}}$
${\displaystyle w[n]=\ w_{0}\left(n-{\tfrac {N}{2}}\right),\ 0\leq n\leq N}$

£#h5#£Other polynomial windows£#/h5#£
£#h5#£Welch window£#/h5#£
The Welch window consists of a single parabolic section:

${\displaystyle w[n]=1-\left({\frac {n-{\frac {N}{2}}}{\frac {N}{2}}}\right)^{2},\quad 0\leq n\leq N.}$
The defining quadratic polynomial reaches a value of zero at the samples just outside the span of the window.


£#h5#£Sine window£#/h5#£
${\displaystyle w[n]=\sin \left({\frac {\pi n}{N}}\right)=\cos \left({\frac {\pi n}{N}}-{\frac {\pi }{2}}\right),\quad 0\leq n\leq N.}$
The corresponding ${\displaystyle w_{0}(n)\,}$ function is a cosine without the π/2 phase offset. So the sine window is sometimes also called cosine window. As it represents half a cycle of a sinusoidal function, it is also known variably as half-sine window or half-cosine window.

The autocorrelation of a sine window produces a function known as the Bohman window.


£#h5#£Power-of-sine/cosine windows£#/h5#£
These window functions have the form:

${\displaystyle w[n]=\sin ^{\alpha }\left({\frac {\pi n}{N}}\right)=\cos ^{\alpha }\left({\frac {\pi n}{N}}-{\frac {\pi }{2}}\right),\quad 0\leq n\leq N.}$
The rectangular window (α = 0), the sine window (α = 1), and the Hann window (α = 2) are members of this family.

For even-integer values of α these functions can also be expressed in cosine-sum form:

${\displaystyle w[n]=a_{0}-a_{1}\cos \left({\frac {2\pi n}{N}}\right)+a_{2}\cos \left({\frac {4\pi n}{N}}\right)-a_{3}\cos \left({\frac {6\pi n}{N}}\right)+a_{4}\cos \left({\frac {8\pi n}{N}}\right)-...}$
${\displaystyle {\begin{array}{l|llll}\hline \alpha &a_{0}&a_{1}&a_{2}&a_{3}&a_{4}\\\hline 0&1\\2&0.5&0.5\\4&0.375&0.5&0.125\\6&0.3125&0.46875&0.1875&0.03125\\8&0.2734375&0.4375&0.21875&0.0625&7.8125\times 10^{-3}\\\hline \end{array}}}$

£#h5#£Cosine-sum windows£#/h5#£
This family is also known as generalized cosine windows.

In most cases, including the examples below, all coefficients ak ≥ 0.  These windows have only 2K + 1 non-zero N-point DFT coefficients.


£#h5#£Hann and Hamming windows£#/h5#£
The customary cosine-sum windows for case K = 1 have the form:

${\displaystyle w[n]=a_{0}-\underbrace {(1-a_{0})} _{a_{1}}\cdot \cos \left({\tfrac {2\pi n}{N}}\right),\quad 0\leq n\leq N,}$
which is easily (and often) confused with its zero-phase version:

${\displaystyle {\begin{aligned}w_{0}(n)\ &=w\left[n+{\tfrac {N}{2}}\right]\\&=a_{0}+a_{1}\cdot \cos \left({\tfrac {2\pi n}{N}}\right),\quad -{\tfrac {N}{2}}\leq n\leq {\tfrac {N}{2}}.\end{aligned}}}$
Setting   ${\displaystyle a_{0}=0.5}$   produces a Hann window:

${\displaystyle w[n]=0.5\;\left[1-\cos \left({\frac {2\pi n}{N}}\right)\right]=\sin ^{2}\left({\frac {\pi n}{N}}\right),}$
named after Julius von Hann, and sometimes erroneously referred to as Hanning, presumably due to its linguistic and formulaic similarities to the Hamming window. It is also known as raised cosine, because the zero-phase version, ${\displaystyle w_{0}(n),}$ is one lobe of an elevated cosine function.

This function is a member of both the cosine-sum and power-of-sine families. Unlike the Hamming window, the end points of the Hann window just touch zero. The resulting side-lobes roll off at about 18 dB per octave.

Setting   ${\displaystyle a_{0}}$   to approximately 0.54, or more precisely 25/46, produces the Hamming window, proposed by Richard W. Hamming. That choice places a zero-crossing at frequency 5π/(N − 1), which cancels the first sidelobe of the Hann window, giving it a height of about one-fifth that of the Hann window. The Hamming window is often called the Hamming blip when used for pulse shaping.

Approximation of the coefficients to two decimal places substantially lowers the level of sidelobes, to a nearly equiripple condition. In the equiripple sense, the optimal values for the coefficients are a0 = 0.53836 and a1 = 0.46164.


£#h5#£Blackman window£#/h5#£
Blackman windows are defined as:

${\displaystyle w[n]=a_{0}-a_{1}\cos \left({\frac {2\pi n}{N}}\right)+a_{2}\cos \left({\frac {4\pi n}{N}}\right)}$
${\displaystyle a_{0}={\frac {1-\alpha }{2}};\quad a_{1}={\frac {1}{2}};\quad a_{2}={\frac {\alpha }{2}}.}$
By common convention, the unqualified term Blackman window refers to Blackman's "not very serious proposal" of α = 0.16 (a0 = 0.42, a1 = 0.5, a2 = 0.08), which closely approximates the exact Blackman, with a0 = 7938/18608 ≈ 0.42659, a1 = 9240/18608 ≈ 0.49656, and a2 = 1430/18608 ≈ 0.076849. These exact values place zeros at the third and fourth sidelobes, but result in a discontinuity at the edges and a 6 dB/oct fall-off. The truncated coefficients do not null the sidelobes as well, but have an improved 18 dB/oct fall-off.


£#h5#£Nuttall window, continuous first derivative£#/h5#£
The continuous form of the Nuttall window, ${\displaystyle w_{0}(x),}$ and its first derivative are continuous everywhere, like the Hann function. That is, the function goes to 0 at x = ±N/2, unlike the Blackman–Nuttall, Blackman–Harris, and Hamming windows. The Blackman window (α = 0.16) is also continuous with continuous derivative at the edge, but the "exact Blackman window" is not.

${\displaystyle w[n]=a_{0}-a_{1}\cos \left({\frac {2\pi n}{N}}\right)+a_{2}\cos \left({\frac {4\pi n}{N}}\right)-a_{3}\cos \left({\frac {6\pi n}{N}}\right)}$
${\displaystyle a_{0}=0.355768;\quad a_{1}=0.487396;\quad a_{2}=0.144232;\quad a_{3}=0.012604.}$

£#h5#£Blackman–Nuttall window£#/h5#£
${\displaystyle w[n]=a_{0}-a_{1}\cos \left({\frac {2\pi n}{N}}\right)+a_{2}\cos \left({\frac {4\pi n}{N}}\right)-a_{3}\cos \left({\frac {6\pi n}{N}}\right)}$
${\displaystyle a_{0}=0.3635819;\quad a_{1}=0.4891775;\quad a_{2}=0.1365995;\quad a_{3}=0.0106411.}$

£#h5#£Blackman–Harris window£#/h5#£
A generalization of the Hamming family, produced by adding more shifted sinc functions, meant to minimize side-lobe levels

${\displaystyle w[n]=a_{0}-a_{1}\cos \left({\frac {2\pi n}{N}}\right)+a_{2}\cos \left({\frac {4\pi n}{N}}\right)-a_{3}\cos \left({\frac {6\pi n}{N}}\right)}$
${\displaystyle a_{0}=0.35875;\quad a_{1}=0.48829;\quad a_{2}=0.14128;\quad a_{3}=0.01168.}$

£#h5#£Flat top window£#/h5#£
A flat top window is a partially negative-valued window that has minimal scalloping loss in the frequency domain. That property is desirable for the measurement of amplitudes of sinusoidal frequency components. Drawbacks of the broad bandwidth are poor frequency resolution and high § Noise bandwidth.

Flat top windows can be designed using low-pass filter design methods, or they may be of the usual cosine-sum variety:

${\displaystyle {\begin{aligned}w[n]=a_{0}&{}-a_{1}\cos \left({\frac {2\pi n}{N}}\right)+a_{2}\cos \left({\frac {4\pi n}{N}}\right)\\&{}-a_{3}\cos \left({\frac {6\pi n}{N}}\right)+a_{4}\cos \left({\frac {8\pi n}{N}}\right).\end{aligned}}}$
The Matlab variant has these coefficients:

${\displaystyle a_{0}=0.21557895;\quad a_{1}=0.41663158;\quad a_{2}=0.277263158;\quad a_{3}=0.083578947;\quad a_{4}=0.006947368.}$
Other variations are available, such as sidelobes that roll off at the cost of higher values near the main lobe.


£#h5#£Rife–Vincent windows£#/h5#£
Rife–Vincent windows are customarily scaled for unity average value, instead of unity peak value. The coefficient values below, applied to Eq.1, reflect that custom.

Class I, Order 1 (K = 1):  ${\displaystyle a_{0}=1;\quad a_{1}=1}$       Functionally equivalent to the Hann window.

Class I, Order 2 (K = 2):  ${\displaystyle a_{0}=1;\quad a_{1}={\tfrac {4}{3}};\quad a_{2}={\tfrac {1}{3}}}$

Class I is defined by minimizing the high-order sidelobe amplitude. Coefficients for orders up to K=4 are tabulated.

Class II minimizes the main-lobe width for a given maximum side-lobe.

Class III is a compromise for which order K = 2 resembles the § Blackman window.


£#h5#£Adjustable windows£#/h5#£
£#h5#£Gaussian window£#/h5#£
The Fourier transform of a Gaussian is also a Gaussian. Since the support of a Gaussian function extends to infinity, it must either be truncated at the ends of the window, or itself windowed with another zero-ended window.

Since the log of a Gaussian produces a parabola, this can be used for nearly exact quadratic interpolation in frequency estimation.

${\displaystyle w[n]=\exp \left(-{\frac {1}{2}}\left({\frac {n-N/2}{\sigma N/2}}\right)^{2}\right),\quad 0\leq n\leq N.}$
${\displaystyle \sigma \leq \;0.5\,}$
The standard deviation of the Gaussian function is σ · N/2 sampling periods.


£#h5#£Confined Gaussian window£#/h5#£
The confined Gaussian window yields the smallest possible root mean square frequency width σω for a given temporal width  (N + 1) σt. These windows optimize the RMS time-frequency bandwidth products. They are computed as the minimum eigenvectors of a parameter-dependent matrix. The confined Gaussian window family contains the § Sine window and the § Gaussian window in the limiting cases of large and small σt, respectively.


£#h5#£Approximate confined Gaussian window£#/h5#£
Defining  L ≜ N + 1,  a confined Gaussian window of temporal width  L × σt  is well approximated by:

${\displaystyle w[n]=G(n)-{\frac {G(-{\tfrac {1}{2}})[G(n+L)+G(n-L)]}{G(-{\tfrac {1}{2}}+L)+G(-{\tfrac {1}{2}}-L)}}}$
where ${\displaystyle G}$ is a Gaussian function:

${\displaystyle G(x)=\exp \left(-\left({\cfrac {x-{\frac {N}{2}}}{2L\sigma _{t}}}\right)^{2}\right)}$
The standard deviation of the approximate window is asymptotically equal (i.e. large values of N) to  L × σt  for  σt < 0.14.


£#h5#£Generalized normal window£#/h5#£
A more generalized version of the Gaussian window is the generalized normal window. Retaining the notation from the Gaussian window above, we can represent this window as

${\displaystyle w[n,p]=\exp \left(-\left({\frac {n-N/2}{\sigma N/2}}\right)^{p}\right)}$
for any even ${\displaystyle p}$ . At ${\displaystyle p=2}$ , this is a Gaussian window and as ${\displaystyle p}$ approaches ${\displaystyle \infty }$ , this approximates to a rectangular window. The Fourier transform of this window does not exist in a closed form for a general ${\displaystyle p}$ . However, it demonstrates the other benefits of being smooth, adjustable bandwidth. Like the § Tukey window, this window naturally offers a "flat top" to control the amplitude attenuation of a time-series (on which we don't have a control with Gaussian window). In essence, it offers a good (controllable) compromise, in terms of spectral leakage, frequency resolution and amplitude attenuation, between the Gaussian window and the rectangular window. See also for a study on time-frequency representation of this window (or function).


£#h5#£Tukey window£#/h5#£
The Tukey window, also known as the cosine-tapered window, can be regarded as a cosine lobe of width Nα/2 (spanning Nα/2 + 1 observations) that is convolved with a rectangular window of width N(1 − α/2).

${\displaystyle \left.{\begin{array}{lll}w[n]={\frac {1}{2}}\left[1-\cos \left({\frac {2\pi n}{\alpha N}}\right)\right],\quad &0\leq n<{\frac {\alpha N}{2}}\\w[n]=1,\quad &{\frac {\alpha N}{2}}\leq n\leq {\frac {N}{2}}\\w[N-n]=w[n],\quad &0\leq n\leq {\frac {N}{2}}\end{array}}\right\}}$  
At α = 0 it becomes rectangular, and at α = 1 it becomes a Hann window.


£#h5#£Planck-taper window£#/h5#£
The so-called "Planck-taper" window is a bump function that has been widely used in the theory of partitions of unity in manifolds. It is smooth (a ${\displaystyle C^{\infty }}$ function) everywhere, but is exactly zero outside of a compact region, exactly one over an interval within that region, and varies smoothly and monotonically between those limits. Its use as a window function in signal processing was first suggested in the context of gravitational-wave astronomy, inspired by the Planck distribution. It is defined as a piecewise function:

${\displaystyle \left.{\begin{array}{lll}w[0]=0,\\w[n]=\left(1+\exp \left({\frac {\varepsilon N}{n}}-{\frac {\varepsilon N}{\varepsilon N-n}}\right)\right)^{-1},\quad &1\leq n<\varepsilon N\\w[n]=1,\quad &\varepsilon N\leq n\leq {\frac {N}{2}}\\w[N-n]=w[n],\quad &0\leq n\leq {\frac {N}{2}}\end{array}}\right\}}$
The amount of tapering is controlled by the parameter ε, with smaller values giving sharper transitions.


£#h5#£DPSS or Slepian window£#/h5#£
The DPSS (discrete prolate spheroidal sequence) or Slepian window maximizes the energy concentration in the main lobe, and is used in multitaper spectral analysis, which averages out noise in the spectrum and reduces information loss at the edges of the window.

The main lobe ends at a frequency bin given by the parameter α.

The Kaiser windows below are created by a simple approximation to the DPSS windows:


£#h5#£Kaiser window£#/h5#£
The Kaiser, or Kaiser–Bessel, window is a simple approximation of the DPSS window using Bessel functions, discovered by James Kaiser.

${\displaystyle w[n]={\frac {I_{0}\left(\pi \alpha {\sqrt {1-\left({\frac {2n}{N}}-1\right)^{2}}}\right)}{I_{0}(\pi \alpha )}},\quad 0\leq n\leq N}$    : p. 73 
${\displaystyle w_{0}(n)={\frac {I_{0}\left(\pi \alpha {\sqrt {1-\left({\frac {2n}{N}}\right)^{2}}}\right)}{I_{0}(\pi \alpha )}},\quad -N/2\leq n\leq N/2}$
where ${\displaystyle I_{0}}$ is the zero-th order modified Bessel function of the first kind. Variable parameter ${\displaystyle \alpha }$ determines the tradeoff between main lobe width and side lobe levels of the spectral leakage pattern.  The main lobe width, in between the nulls, is given by   ${\displaystyle 2{\sqrt {1+\alpha ^{2}}},}$   in units of DFT bins,  and a typical value of ${\displaystyle \alpha }$ is 3.


£#h5#£Dolph–Chebyshev window£#/h5#£
Minimizes the Chebyshev norm of the side-lobes for a given main lobe width.

The zero-phase Dolph–Chebyshev window function ${\displaystyle w_{0}[n]}$ is usually defined in terms of its real-valued discrete Fourier transform, ${\displaystyle W_{0}[k]}$ :

${\displaystyle W_{0}(k)={\frac {T_{N}{\big (}\beta \cos \left({\frac {\pi k}{N+1}}\right){\big )}}{T_{N}(\beta )}}={\frac {T_{N}{\big (}\beta \cos \left({\frac {\pi k}{N+1}}\right){\big )}}{10^{\alpha }}},\ 0\leq k\leq N.}$
Tn(x) is the n-th Chebyshev polynomial of the first kind evaluated in x, which can be computed using

${\displaystyle T_{n}(x)={\begin{cases}\cos \!{\big (}n\cos ^{-1}(x){\big )}&{\text{if }}-1\leq x\leq 1\\\cosh \!{\big (}n\cosh ^{-1}(x){\big )}&{\text{if }}x\geq 1\\(-1)^{n}\cosh \!{\big (}n\cosh ^{-1}(-x){\big )}&{\text{if }}x\leq -1,\end{cases}}}$
and

${\displaystyle \beta =\cosh \!{\big (}{\tfrac {1}{N}}\cosh ^{-1}(10^{\alpha }){\big )}}$
is the unique positive real solution to ${\displaystyle T_{N}(\beta )=10^{\alpha }}$ , where the parameter α sets the Chebyshev norm of the sidelobes to −20α decibels.

The window function can be calculated from W0(k) by an inverse discrete Fourier transform (DFT):

${\displaystyle w_{0}(n)={\frac {1}{N+1}}\sum _{k=0}^{N}W_{0}(k)\cdot e^{i2\pi kn/(N+1)},\ -N/2\leq n\leq N/2.}$
The lagged version of the window can be obtained by:

${\displaystyle w[n]=w_{0}\left(n-{\frac {N}{2}}\right),\quad 0\leq n\leq N,}$
which for even values of N must be computed as follows:

${\displaystyle {\begin{aligned}w_{0}\left(n-{\frac {N}{2}}\right)={\frac {1}{N+1}}\sum _{k=0}^{N}W_{0}(k)\cdot e^{\frac {i2\pi k(n-N/2)}{N+1}}={\frac {1}{N+1}}\sum _{k=0}^{N}\left[\left(-e^{\frac {i\pi }{N+1}}\right)^{k}\cdot W_{0}(k)\right]e^{\frac {i2\pi kn}{N+1}},\end{aligned}}}$
which is an inverse DFT of   ${\displaystyle \left(-e^{\frac {i\pi }{N+1}}\right)^{k}\cdot W_{0}(k).}$

Variations:

£#ul#££#li#£Due to the equiripple condition, the time-domain window has discontinuities at the edges. An approximation that avoids them, by allowing the equiripples to drop off at the edges, is a Taylor window.£#/li#£ £#li#£An alternative to the inverse DFT definition is also available.[1].£#/li#££#/ul#£
£#h5#£Ultraspherical window£#/h5#£
The Ultraspherical window was introduced in 1984 by Roy Streit and has application in antenna array design, non-recursive filter design, and spectrum analysis.

Like other adjustable windows, the Ultraspherical window has parameters that can be used to control its Fourier transform main-lobe width and relative side-lobe amplitude. Uncommon to other windows, it has an additional parameter which can be used to set the rate at which side-lobes decrease (or increase) in amplitude.

The window can be expressed in the time-domain as follows:

${\displaystyle w[n]={\frac {1}{N+1}}\left[C_{N}^{\mu }(x_{0})+\sum _{k=1}^{\frac {N}{2}}C_{N}^{\mu }\left(x_{0}\cos {\frac {k\pi }{N+1}}\right)\cos {\frac {2n\pi k}{N+1}}\right]}$
where ${\displaystyle C_{N}^{\mu }}$ is the Ultraspherical polynomial of degree N, and ${\displaystyle x_{0}}$ and ${\displaystyle \mu }$ control the side-lobe patterns.

Certain specific values of ${\displaystyle \mu }$ yield other well-known windows: ${\displaystyle \mu =0}$ and ${\displaystyle \mu =1}$ give the Dolph–Chebyshev and Saramäki windows respectively. See here for illustration of Ultraspherical windows with varied parametrization.


£#h5#£Exponential or Poisson window£#/h5#£
The Poisson window, or more generically the exponential window increases exponentially towards the center of the window and decreases exponentially in the second half. Since the exponential function never reaches zero, the values of the window at its limits are non-zero (it can be seen as the multiplication of an exponential function by a rectangular window ). It is defined by

${\displaystyle w[n]=e^{-\left|n-{\frac {N}{2}}\right|{\frac {1}{\tau }}},}$
where τ is the time constant of the function. The exponential function decays as e ≃ 2.71828 or approximately 8.69 dB per time constant. This means that for a targeted decay of D dB over half of the window length, the time constant τ is given by

${\displaystyle \tau ={\frac {N}{2}}{\frac {8.69}{D}}.}$

£#h5#£Hybrid windows£#/h5#£
Window functions have also been constructed as multiplicative or additive combinations of other windows.


£#h5#£Bartlett–Hann window£#/h5#£
${\displaystyle w[n]=a_{0}-a_{1}\left|{\frac {n}{N}}-{\frac {1}{2}}\right|-a_{2}\cos \left({\frac {2\pi n}{N}}\right)}$
${\displaystyle a_{0}=0.62;\quad a_{1}=0.48;\quad a_{2}=0.38\,}$

£#h5#£Planck–Bessel window£#/h5#£
A § Planck-taper window multiplied by a Kaiser window which is defined in terms of a modified Bessel function. This hybrid window function was introduced to decrease the peak side-lobe level of the Planck-taper window while still exploiting its good asymptotic decay. It has two tunable parameters, ε from the Planck-taper and α from the Kaiser window, so it can be adjusted to fit the requirements of a given signal.


£#h5#£Hann–Poisson window£#/h5#£
A Hann window multiplied by a Poisson window. For ${\displaystyle \alpha \geqslant 2}$ it has no side-lobes, as its Fourier transform drops off forever away from the main lobe without local minima. It can thus be used in hill climbing algorithms like Newton's method. The Hann–Poisson window is defined by:

${\displaystyle w[n]={\frac {1}{2}}\left(1-\cos \left({\frac {2\pi n}{N}}\right)\right)e^{\frac {-\alpha \left|N-2n\right|}{N}}\,}$
where α is a parameter that controls the slope of the exponential.


£#h5#£Other windows£#/h5#£
£#h5#£Generalized adaptive polynomial (GAP) window£#/h5#£
The GAP window is a family of adjustable window functions that are based on a symmetrical polynomial expansion of order ${\displaystyle K}$ . It is continuous with continuous derivative everywhere. With the appropriate set of expansion coefficients and expansion order, the GAP window can mimic all the known window functions, reproducing accurately their spectral properties.

${\displaystyle w_{0}[n]=a_{0}+\sum _{k=1}^{K}a_{2k}\left({\frac {n}{\sigma }}\right)^{2k},\quad -{\frac {N}{2}}\leq n\leq {\frac {N}{2}},}$  
where ${\displaystyle \sigma }$ is the standard deviation of the ${\displaystyle \{n\}}$ sequence.

Additionally, starting with a set of expansion coefficients ${\displaystyle a_{2k}}$ that mimics a certain known window function, the GAP window can be optimized by minimization procedures to get a new set of coefficients that improve one or more spectral properties, such as the main lobe width, side lobe attenuation, and side lobe falloff rate. Therefore, a GAP window function can be developed with designed spectral properties depending on the specific application.


£#h5#£Lanczos window£#/h5#£
£#ul#££#li#£used in Lanczos resampling£#/li#£ £#li#£for the Lanczos window, ${\displaystyle \operatorname {sinc} (x)}$ is defined as ${\displaystyle \sin(\pi x)/\pi x}$ £#/li#£ £#li#£also known as a sinc window, because: is the main lobe of a normalized sinc function£#/li#££#/ul#£
£#h5#£Comparison of windows£#/h5#£
When selecting an appropriate window function for an application, this comparison graph may be useful. The frequency axis has units of FFT "bins" when the window of length N is applied to data and a transform of length N is computed. For instance, the value at frequency ½ "bin" is the response that would be measured in bins k and k + 1 to a sinusoidal signal at frequency k + ½. It is relative to the maximum possible response, which occurs when the signal frequency is an integer number of bins. The value at frequency ½ is referred to as the maximum scalloping loss of the window, which is one metric used to compare windows. The rectangular window is noticeably worse than the others in terms of that metric.

Other metrics that can be seen are the width of the main lobe and the peak level of the sidelobes, which respectively determine the ability to resolve comparable strength signals and disparate strength signals. The rectangular window (for instance) is the best choice for the former and the worst choice for the latter. What cannot be seen from the graphs is that the rectangular window has the best noise bandwidth, which makes it a good candidate for detecting low-level sinusoids in an otherwise white noise environment. Interpolation techniques, such as zero-padding and frequency-shifting, are available to mitigate its potential scalloping loss.


£#h5#£Overlapping windows£#/h5#£
When the length of a data set to be transformed is larger than necessary to provide the desired frequency resolution, a common practice is to subdivide it into smaller sets and window them individually. To mitigate the "loss" at the edges of the window, the individual sets may overlap in time. See Welch method of power spectral analysis and the modified discrete cosine transform.


£#h5#£Two-dimensional windows£#/h5#£
Two-dimensional windows are commonly used in image processing to reduce unwanted high-frequencies in the image Fourier transform. They can be constructed from one-dimensional windows in either of two forms. The separable form, ${\displaystyle W(m,n)=w(m)w(n)}$ is trivial to compute. The radial form, ${\displaystyle W(m,n)=w(r)}$ , which involves the radius ${\displaystyle r={\sqrt {(m-M/2)^{2}+(n-N/2)^{2}}}}$ , is isotropic, independent on the orientation of the coordinate axes. Only the Gaussian function is both separable and isotropic. The separable forms of all other window functions have corners that depend on the choice of the coordinate axes. The isotropy/anisotropy of a two-dimensional window function is shared by its two-dimensional Fourier transform. The difference between the separable and radial forms is akin to the result of diffraction from rectangular vs. circular apertures, which can be visualized in terms of the product of two sinc functions vs. an Airy function, respectively.


£#h5#£See also£#/h5#£ £#ul#££#li#£Spectral leakage£#/li#£ £#li#£Multitaper£#/li#£ £#li#£Apodization£#/li#£ £#li#£Welch method£#/li#£ £#li#£Short-time Fourier transform£#/li#£ £#li#£Window design method£#/li#£ £#li#£Kolmogorov–Zurbenko filter£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£Page citations£#/h5#£
£#h5#£References£#/h5#£
£#h5#£Further reading£#/h5#£ £#ul#££#li#£Harris, Frederic J. (September 1976). "Windows, Harmonic Analysis, and the Discrete Fourier Transform" (PDF). apps.dtic.mil. Naval Undersea Center, San Diego. Archived (PDF) from the original on April 8, 2019. Retrieved 2019-04-08.£#/li#£ £#li#£Albrecht, Hans-Helge (2012). Tailored minimum sidelobe and minimum sidelobe cosine-sum windows. Version 1.0. Vol. ISBN 978-3-86918-281-0 ). editor: Physikalisch-Technische Bundesanstalt. Physikalisch-Technische Bundesanstalt. doi:10.7795/110.20121022aa. ISBN 978-3-86918-281-0.£#/li#£ £#li#£Bergen, S.W.A.; Antoniou, A. (2005). "Design of Nonrecursive Digital Filters Using the Ultraspherical Window Function". EURASIP Journal on Applied Signal Processing. 2005 (12): 1910–1922. Bibcode:2005EJASP2005...44B. doi:10.1155/ASP.2005.1910.£#/li#£ £#li#£Prabhu, K. M. M. (2014). Window Functions and Their Applications in Signal Processing. Boca Raton, FL: CRC Press. ISBN 978-1-4665-1583-3.£#/li#£ £#li#£US patent 7065150, Park, Young-Seo, "System and method for generating a root raised cosine orthogonal frequency division multiplexing (RRC OFDM) modulation", published 2003, issued 2006 £#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£ Media related to Window function at Wikimedia Commons£#/li#£ £#li#£LabView Help, Characteristics of Smoothing Filters, http://zone.ni.com/reference/en-XX/help/371361B-01/lvanlsconcepts/char_smoothing_windows/£#/li#£ £#li#£Creation and properties of Cosine-sum Window functions, http://electronicsart.weebly.com/fftwindows.html£#/li#£ £#li#£Online Interactive FFT, Windows, Resolution, and Leakage Simulation | RITEC | Library & Tools£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Ball, J. A. "The Spectral Resolution in a Correlator System" §4.3.5 in Astrophysics, Part C: Radio Observations (Ed. M. L. Meeks). New York: Academic Press, pp. 55-57, 1976.£#/li#££#li#£Blackman, R. B. and Tukey, J. W. "Particular Pairs of Windows." In The Measurement of Power Spectra, From the Point of View of Communications Engineering. New York: Dover, pp. 95-101, 1959.£#/li#££#li#£Brault, J. W. "Fourier Transform Spectrometry." In High Resolution in Astronomy: 15th Advanced Course of the Swiss Society of Astronomy and Astrophysics (Ed. A. Benz, M. Huber, and M. Mayor). Geneva Observatory, Sauverny, Switzerland, pp. 31-32, 1985.£#/li#££#li#£Harris, F. J. "On the Use of Windows for Harmonic Analysis with the Discrete Fourier Transform." Proc. IEEE 66, 51-83, 1978.£#/li#££#li#£Norton, R. H. and Beer, R. "New Apodizing Functions for Fourier Spectroscopy." J. Opt. Soc. Amer. 66, 259-264, 1976.£#/li#££#li#£Press, W. H.; Flannery, B. P.; Teukolsky, S. A.; and Vetterling, W. T. Numerical Recipes in FORTRAN: The Art of Scientific Computing, 2nd ed. Cambridge, England: Cambridge University Press, pp. 547-548, 1992.£#/li#££#li#£Schnopper, H. W. and Thompson, R. I. "Fourier Spectrometers." In Astrophysics, Part A: Optical and Infrared (Ed. N. P. Carleton). New York: Academic Press, pp. 491-529, 1974.£#/li#££#li#£ Ball, J. A. "The Spectral Resolution in a Correlator System" §4.3.5 in Astrophysics, Part C: Radio Observations (Ed. M. L. Meeks). New York: Academic Press, pp. 55-57, 1976. £#/li#££#li#£ Blackman, R. B. and Tukey, J. W. "Particular Pairs of Windows." In The Measurement of Power Spectra, From the Point of View of Communications Engineering. New York: Dover, pp. 95-101, 1959. £#/li#££#li#£ Brault, J. W. "Fourier Transform Spectrometry." In High Resolution in Astronomy: 15th Advanced Course of the Swiss Society of Astronomy and Astrophysics (Ed. A. Benz, M. Huber, and M. Mayor). Geneva Observatory, Sauverny, Switzerland, pp. 31-32, 1985. £#/li#££#li#£ Harris, F. J. "On the Use of Windows for Harmonic Analysis with the Discrete Fourier Transform." Proc. IEEE 66, 51-83, 1978. £#/li#££#li#£ Norton, R. H. and Beer, R. "New Apodizing Functions for Fourier Spectroscopy." J. Opt. Soc. Amer. 66, 259-264, 1976. £#/li#££#li#£ Press, W. H.; Flannery, B. P.; Teukolsky, S. A.; and Vetterling, W. T. Numerical Recipes in FORTRAN: The Art of Scientific Computing, 2nd ed. Cambridge, England: Cambridge University Press, pp. 547-548, 1992. £#/li#££#li#£ Schnopper, H. W. and Thompson, R. I. "Fourier Spectrometers." In Astrophysics, Part A: Optical and Infrared (Ed. N. P. Carleton). New York: Academic Press, pp. 491-529, 1974. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Integral Transforms > Apodization Functions £#/li#££#/ul#£




£#h3#£Apparatus Function£#/h3#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Integral Transforms > Apodization Functions £#/li#££#/ul#£




£#h3#£Appell Hypergeometric Function£#/h3#£

In mathematics, Appell series are a set of four hypergeometric series F1, F2, F3, F4 of two variables that were introduced by Paul Appell (1880) and that generalize Gauss's hypergeometric series 2F1 of one variable. Appell established the set of partial differential equations of which these functions are solutions, and found various reduction formulas and expressions of these series in terms of hypergeometric series of one variable.


£#h5#£Definitions£#/h5#£
The Appell series F1 is defined for |x| < 1, |y| < 1 by the double series

${\displaystyle F_{1}(a,b_{1},b_{2};c;x,y)=\sum _{m,n=0}^{\infty }{\frac {(a)_{m+n}(b_{1})_{m}(b_{2})_{n}}{(c)_{m+n}\,m!\,n!}}\,x^{m}y^{n}~,}$
where ${\displaystyle (q)_{n}}$ is the Pochhammer symbol. For other values of x and y the function F1 can be defined by analytic continuation. It can be shown that

${\displaystyle F_{1}(a,b_{1},b_{2};c;x,y)=\sum _{r=0}^{\infty }{\frac {(a)_{r}(b_{1})_{r}(b_{2})_{r}(c-a)_{r}}{(c+r-1)_{r}(c)_{2r}r!}}\,x^{r}y^{r}{}_{2}F_{1}\left(a+r,b_{1}+r;c+2r;x\right){}_{2}F_{1}\left(a+r,b_{2}+r;c+2r;y\right)~.}$
Similarly, the function F2 is defined for |x| + |y| < 1 by the series

${\displaystyle F_{2}(a,b_{1},b_{2};c_{1},c_{2};x,y)=\sum _{m,n=0}^{\infty }{\frac {(a)_{m+n}(b_{1})_{m}(b_{2})_{n}}{(c_{1})_{m}(c_{2})_{n}\,m!\,n!}}\,x^{m}y^{n}}$
and it can be shown that

${\displaystyle F_{2}(a,b_{1},b_{2};c_{1},c_{2};x,y)=\sum _{r=0}^{\infty }{\frac {(a)_{r}(b_{1})_{r}(b_{2})_{r}}{(c_{1})_{r}(c_{2})_{r}r!}}\,x^{r}y^{r}{}_{2}F_{1}\left(a+r,b_{1}+r;c_{1}+r;x\right){}_{2}F_{1}\left(a+r,b_{2}+r;c_{2}+r;y\right)~.}$
Also the function F3 for |x| < 1, |y| < 1 can be defined by the series

${\displaystyle F_{3}(a_{1},a_{2},b_{1},b_{2};c;x,y)=\sum _{m,n=0}^{\infty }{\frac {(a_{1})_{m}(a_{2})_{n}(b_{1})_{m}(b_{2})_{n}}{(c)_{m+n}\,m!\,n!}}\,x^{m}y^{n}~,}$
and the function F4 for |x|½ + |y|½ < 1 by the series

${\displaystyle F_{4}(a,b;c_{1},c_{2};x,y)=\sum _{m,n=0}^{\infty }{\frac {(a)_{m+n}(b)_{m+n}}{(c_{1})_{m}(c_{2})_{n}\,m!\,n!}}\,x^{m}y^{n}~.}$

£#h5#£Recurrence relations£#/h5#£
Like the Gauss hypergeometric series 2F1, the Appell double series entail recurrence relations among contiguous functions. For example, a basic set of such relations for Appell's F1 is given by:

${\displaystyle (a-b_{1}-b_{2})F_{1}(a,b_{1},b_{2},c;x,y)-a\,F_{1}(a+1,b_{1},b_{2},c;x,y)+b_{1}F_{1}(a,b_{1}+1,b_{2},c;x,y)+b_{2}F_{1}(a,b_{1},b_{2}+1,c;x,y)=0~,}$
${\displaystyle c\,F_{1}(a,b_{1},b_{2},c;x,y)-(c-a)F_{1}(a,b_{1},b_{2},c+1;x,y)-a\,F_{1}(a+1,b_{1},b_{2},c+1;x,y)=0~,}$
${\displaystyle c\,F_{1}(a,b_{1},b_{2},c;x,y)+c(x-1)F_{1}(a,b_{1}+1,b_{2},c;x,y)-(c-a)x\,F_{1}(a,b_{1}+1,b_{2},c+1;x,y)=0~,}$
${\displaystyle c\,F_{1}(a,b_{1},b_{2},c;x,y)+c(y-1)F_{1}(a,b_{1},b_{2}+1,c;x,y)-(c-a)y\,F_{1}(a,b_{1},b_{2}+1,c+1;x,y)=0~.}$
Any other relation valid for F1 can be derived from these four.

Similarly, all recurrence relations for Appell's F3 follow from this set of five:

${\displaystyle c\,F_{3}(a_{1},a_{2},b_{1},b_{2},c;x,y)+(a_{1}+a_{2}-c)F_{3}(a_{1},a_{2},b_{1},b_{2},c+1;x,y)-a_{1}F_{3}(a_{1}+1,a_{2},b_{1},b_{2},c+1;x,y)-a_{2}F_{3}(a_{1},a_{2}+1,b_{1},b_{2},c+1;x,y)=0~,}$
${\displaystyle c\,F_{3}(a_{1},a_{2},b_{1},b_{2},c;x,y)-c\,F_{3}(a_{1}+1,a_{2},b_{1},b_{2},c;x,y)+b_{1}x\,F_{3}(a_{1}+1,a_{2},b_{1}+1,b_{2},c+1;x,y)=0~,}$
${\displaystyle c\,F_{3}(a_{1},a_{2},b_{1},b_{2},c;x,y)-c\,F_{3}(a_{1},a_{2}+1,b_{1},b_{2},c;x,y)+b_{2}y\,F_{3}(a_{1},a_{2}+1,b_{1},b_{2}+1,c+1;x,y)=0~,}$
${\displaystyle c\,F_{3}(a_{1},a_{2},b_{1},b_{2},c;x,y)-c\,F_{3}(a_{1},a_{2},b_{1}+1,b_{2},c;x,y)+a_{1}x\,F_{3}(a_{1}+1,a_{2},b_{1}+1,b_{2},c+1;x,y)=0~,}$
${\displaystyle c\,F_{3}(a_{1},a_{2},b_{1},b_{2},c;x,y)-c\,F_{3}(a_{1},a_{2},b_{1},b_{2}+1,c;x,y)+a_{2}y\,F_{3}(a_{1},a_{2}+1,b_{1},b_{2}+1,c+1;x,y)=0~.}$

£#h5#£Derivatives and differential equations£#/h5#£
For Appell's F1, the following derivatives result from the definition by a double series:

${\displaystyle {\frac {\partial ^{n}}{\partial x^{n}}}F_{1}(a,b_{1},b_{2},c;x,y)={\frac {\left(a\right)_{n}\left(b_{1}\right)_{n}}{\left(c\right)_{n}}}F_{1}(a+n,b_{1}+n,b_{2},c+n;x,y)}$
${\displaystyle {\frac {\partial ^{n}}{\partial y^{n}}}F_{1}(a,b_{1},b_{2},c;x,y)={\frac {\left(a\right)_{n}\left(b_{2}\right)_{n}}{\left(c\right)_{n}}}F_{1}(a+n,b_{1},b_{2}+n,c+n;x,y)}$
From its definition, Appell's F1 is further found to satisfy the following system of second-order differential equations:

${\displaystyle x(1-x){\frac {\partial ^{2}F_{1}(x,y)}{\partial x^{2}}}+y(1-x){\frac {\partial ^{2}F_{1}(x,y)}{\partial x\partial y}}+[c-(a+b_{1}+1)x]{\frac {\partial F_{1}(x,y)}{\partial x}}-b_{1}y{\frac {\partial F_{1}(x,y)}{\partial y}}-ab_{1}F_{1}(x,y)=0}$
${\displaystyle y(1-y){\frac {\partial ^{2}F_{1}(x,y)}{\partial y^{2}}}+x(1-y){\frac {\partial ^{2}F_{1}(x,y)}{\partial x\partial y}}+[c-(a+b_{2}+1)y]{\frac {\partial F_{1}(x,y)}{\partial y}}-b_{2}x{\frac {\partial F_{1}(x,y)}{\partial x}}-ab_{2}F_{1}(x,y)=0}$
A system partial differential equations for F2 is

${\displaystyle x(1-x){\frac {\partial ^{2}F_{2}(x,y)}{\partial x^{2}}}-xy{\frac {\partial ^{2}F_{2}(x,y)}{\partial x\partial y}}+[c_{1}-(a+b_{1}+1)x]{\frac {\partial F_{2}(x,y)}{\partial x}}-b_{1}y{\frac {\partial F_{2}(x,y)}{\partial y}}-ab_{1}F_{2}(x,y)=0}$
${\displaystyle y(1-y){\frac {\partial ^{2}F_{2}(x,y)}{\partial y^{2}}}-xy{\frac {\partial ^{2}F_{2}(x,y)}{\partial x\partial y}}+[c_{2}-(a+b_{2}+1)x]{\frac {\partial F_{2}(x,y)}{\partial y}}-b_{2}x{\frac {\partial F_{2}(x,y)}{\partial x}}-ab_{2}F_{2}(x,y)=0}$
The system have solution

${\displaystyle F_{2}(x,y)=C_{1}F_{2}(a,b_{1},b_{2},c_{1},c_{2};x,y)+C_{2}x^{1-c_{1}}F_{2}(a-c_{1}+1,b_{1}-c_{1}+1,b_{2},2-c_{1},c_{2};x,y)+C_{3}y^{1-c_{2}}F_{2}(a-c_{2}+1,b_{1},b_{2}-c_{2}+1,c_{1},2-c_{2};x,y)+C_{4}x^{1-c_{1}}y^{1-c_{2}}F_{2}(a-c_{1}-c_{2}+2,b_{1}-c_{1}+1,b_{2}-c_{2}+1,2-c_{1},2-c_{2};x,y)}$
Similarly, for F3 the following derivatives result from the definition:

${\displaystyle {\frac {\partial }{\partial x}}F_{3}(a_{1},a_{2},b_{1},b_{2},c;x,y)={\frac {a_{1}b_{1}}{c}}F_{3}(a_{1}+1,a_{2},b_{1}+1,b_{2},c+1;x,y)}$
${\displaystyle {\frac {\partial }{\partial y}}F_{3}(a_{1},a_{2},b_{1},b_{2},c;x,y)={\frac {a_{2}b_{2}}{c}}F_{3}(a_{1},a_{2}+1,b_{1},b_{2}+1,c+1;x,y)}$
And for F3 the following system of differential equations is obtained:

${\displaystyle x(1-x){\frac {\partial ^{2}F_{3}(x,y)}{\partial x^{2}}}+y{\frac {\partial ^{2}F_{3}(x,y)}{\partial x\partial y}}+[c-(a_{1}+b_{1}+1)x]{\frac {\partial F_{3}(x,y)}{\partial x}}-a_{1}b_{1}F_{3}(x,y)=0}$
${\displaystyle y(1-y){\frac {\partial ^{2}F_{3}(x,y)}{\partial y^{2}}}+x{\frac {\partial ^{2}F_{3}(x,y)}{\partial x\partial y}}+[c-(a_{2}+b_{2}+1)y]{\frac {\partial F_{3}(x,y)}{\partial y}}-a_{2}b_{2}F_{3}(x,y)=0}$
A system partial differential equations for F4 is

${\displaystyle x(1-x){\frac {\partial ^{2}F_{4}(x,y)}{\partial x^{2}}}-y^{2}{\frac {\partial ^{2}F_{4}(x,y)}{\partial y^{2}}}-2xy{\frac {\partial ^{2}F_{4}(x,y)}{\partial x\partial y}}+[c_{1}-(a+b+1)x]{\frac {\partial F_{4}(x,y)}{\partial x}}-(a+b+1)y{\frac {\partial F_{4}(x,y)}{\partial y}}-abF_{4}(x,y)=0}$
${\displaystyle y(1-y){\frac {\partial ^{2}F_{4}(x,y)}{\partial y^{2}}}-x^{2}{\frac {\partial ^{2}F_{4}(x,y)}{\partial x^{2}}}-2xy{\frac {\partial ^{2}F_{4}(x,y)}{\partial x\partial y}}+[c_{2}-(a+b+1)y]{\frac {\partial F_{4}(x,y)}{\partial y}}-(a+b+1)x{\frac {\partial F_{4}(x,y)}{\partial x}}-abF_{4}(x,y)=0}$
The system have solution

${\displaystyle F_{4}(x,y)=C_{1}F_{4}(a,b,c_{1},c_{2};x,y)+C_{2}x^{1-c_{1}}F_{4}(a-c_{1}+1,b-c_{1}+1,2-c_{1},c_{2};x,y)+C_{3}y^{1-c_{2}}F_{4}(a-c_{2}+1,b-c_{2}+1,c_{1},2-c_{2};x,y)+C_{4}x^{1-c_{1}}y^{1-c_{2}}F_{4}(2+a-c_{1}-c_{2},2+b-c_{1}-c_{2},2-c_{1},2-c_{2};x,y)}$

£#h5#£Integral representations£#/h5#£
The four functions defined by Appell's double series can be represented in terms of double integrals involving elementary functions only (Gradshteyn et al. 2015, §9.184). However, Émile Picard (1881) discovered that Appell's F1 can also be written as a one-dimensional Euler-type integral:

${\displaystyle F_{1}(a,b_{1},b_{2},c;x,y)={\frac {\Gamma (c)}{\Gamma (a)\Gamma (c-a)}}\int _{0}^{1}t^{a-1}(1-t)^{c-a-1}(1-xt)^{-b_{1}}(1-yt)^{-b_{2}}\,\mathrm {d} t,\quad \Re \,c>\Re \,a>0~.}$
This representation can be verified by means of Taylor expansion of the integrand, followed by termwise integration.


£#h5#£Special cases£#/h5#£
Picard's integral representation implies that the incomplete elliptic integrals F and E as well as the complete elliptic integral Π are special cases of Appell's F1:

${\displaystyle F(\phi ,k)=\int _{0}^{\phi }{\frac {\mathrm {d} \theta }{\sqrt {1-k^{2}\sin ^{2}\theta }}}=\sin(\phi )\,F_{1}({\tfrac {1}{2}},{\tfrac {1}{2}},{\tfrac {1}{2}},{\tfrac {3}{2}};\sin ^{2}\phi ,k^{2}\sin ^{2}\phi ),\quad |\Re \,\phi |<{\frac {\pi }{2}}~,}$
${\displaystyle E(\phi ,k)=\int _{0}^{\phi }{\sqrt {1-k^{2}\sin ^{2}\theta }}\,\mathrm {d} \theta =\sin(\phi )\,F_{1}({\tfrac {1}{2}},{\tfrac {1}{2}},-{\tfrac {1}{2}},{\tfrac {3}{2}};\sin ^{2}\phi ,k^{2}\sin ^{2}\phi ),\quad |\Re \,\phi |<{\frac {\pi }{2}}~,}$
${\displaystyle \Pi (n,k)=\int _{0}^{\pi /2}{\frac {\mathrm {d} \theta }{(1-n\sin ^{2}\theta ){\sqrt {1-k^{2}\sin ^{2}\theta }}}}={\frac {\pi }{2}}\,F_{1}({\tfrac {1}{2}},1,{\tfrac {1}{2}},1;n,k^{2})~.}$

£#h5#£Related series£#/h5#£ £#ul#££#li#£There are seven related series of two variables, Φ1, Φ2, Φ3, Ψ1, Ψ2, Ξ1, and Ξ2, which generalize Kummer's confluent hypergeometric function 1F1 of one variable and the confluent hypergeometric limit function 0F1 of one variable in a similar manner. The first of these was introduced by Pierre Humbert in 1920.£#/li#£ £#li#£Giuseppe Lauricella (1893) defined four functions similar to the Appell series, but depending on many variables rather than just the two variables x and y. These series were also studied by Appell. They satisfy certain partial differential equations, and can also be given in terms of Euler-type integrals and contour integrals.£#/li#££#/ul#£
£#h5#£References£#/h5#£ £#ul#££#li#£Appell, Paul (1880). "Sur les séries hypergéométriques de deux variables et sur des équations différentielles linéaires aux dérivées partielles". Comptes rendus hebdomadaires des séances de l'Académie des sciences (in French). 90: 296–298 and 731–735. JFM 12.0296.01. (see also "Sur la série F3(α,α',β,β',γ; x,y)" in C. R. Acad. Sci. 90, pp. 977–980)£#/li#£ £#li#£Appell, Paul (1882). "Sur les fonctions hypergéométriques de deux variables". Journal de Mathématiques Pures et Appliquées. (3ème série) (in French). 8: 173–216. Archived from the original on April 12, 2013.£#/li#£ £#li#£Appell, Paul; Kampé de Fériet, Joseph (1926). Fonctions hypergéométriques et hypersphériques; Polynômes d'Hermite (in French). Paris: Gauthier–Villars. JFM 52.0361.13. (see p. 14)£#/li#£ £#li#£Askey, R. A.; Olde Daalhuis, A. B. (2010), "Appell series", in Olver, Frank W. J.; Lozier, Daniel M.; Boisvert, Ronald F.; Clark, Charles W. (eds.), NIST Handbook of Mathematical Functions, Cambridge University Press, ISBN 978-0-521-19225-5, MR 2723248£#/li#£ £#li#£Burchnall, J. L.; Chaundy, T. W. (1940). "Expansions of Appell's double hypergeometric functions". Q. J. Math. First Series. 11: 249–270. doi:10.1093/qmath/os-11.1.249.£#/li#£ £#li#£Erdélyi, A. (1953). Higher Transcendental Functions, Vol. I (PDF). New York: McGraw–Hill. (see p. 224)£#/li#£ £#li#£Gradshteyn, Izrail Solomonovich; Ryzhik, Iosif Moiseevich; Geronimus, Yuri Veniaminovich; Tseytlin, Michail Yulyevich; Jeffrey, Alan (2015) [October 2014]. "9.18.". In Zwillinger, Daniel; Moll, Victor Hugo (eds.). Table of Integrals, Series, and Products. Translated by Scripta Technica, Inc. (8 ed.). Academic Press, Inc. ISBN 978-0-12-384933-5. LCCN 2014010276.£#/li#£ £#li#£Humbert, Pierre (1920). "Sur les fonctions hypercylindriques". Comptes rendus hebdomadaires des séances de l'Académie des sciences (in French). 171: 490–492. JFM 47.0348.01.£#/li#£ £#li#£Lauricella, Giuseppe (1893). "Sulle funzioni ipergeometriche a più variabili". Rendiconti del Circolo Matematico di Palermo (in Italian). 7: 111–158. doi:10.1007/BF03012437. JFM 25.0756.01. S2CID 122316343.£#/li#£ £#li#£Picard, Émile (1881). "Sur une extension aux fonctions de deux variables du problème de Riemann relativ aux fonctions hypergéométriques". Annales Scientifiques de l'École Normale Supérieure. Série 2 (in French). 10: 305–322. doi:10.24033/asens.203. JFM 13.0389.01. (see also C. R. Acad. Sci. 90 (1880), pp. 1119–1121 and 1267–1269)£#/li#£ £#li#£Slater, Lucy Joan (1966). Generalized hypergeometric functions. Cambridge, UK: Cambridge University Press. ISBN 0-521-06483-X. MR 0201688. (there is a 2008 paperback with ISBN 978-0-521-09061-2)£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Aarts, Ronald M. "Lauricella Functions". MathWorld.£#/li#£ £#li#£Weisstein, Eric W. "Appell Hypergeometric Function". MathWorld.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Appell, P. "Sur les fonctions hypergéométriques de plusieurs variables." In Mémoir. Sci. Math. Paris: Gauthier-Villars, 1925.£#/li#££#li#£Appell, P. and Kampé de Fériet, J. Fonctions hypergéométriques et hypersphériques: polynomes d'Hermite. Paris: Gauthier-Villars, 1926.£#/li#££#li#£Bailey, W. N. "A Reducible Case of the Fourth Type of Appell's Hypergeometric Functions of Two Variables." Quart. J. Math. (Oxford) 4, 305-308, 1933.£#/li#££#li#£Bailey, W. N. "On the Reducibility of Appell's Function F_4." Quart. J. Math. (Oxford) 5, 291-292, 1934.£#/li#££#li#£Bailey, W. N. "Appell's Hypergeometric Functions of Two Variables." Ch. 9 in Generalised Hypergeometric Series. Cambridge, England: Cambridge University Press, pp. 73-83 and 99-101, 1935.£#/li#££#li#£Erdélyi, A.; Magnus, W.; Oberhettinger, F.; and Tricomi, F. G. Higher Transcendental Functions, Vol. 1. New York: Krieger, pp. 222 and 224, 1981.£#/li#££#li#£Exton, H. Handbook of Hypergeometric Integrals: Theory, Applications, Tables, Computer Programs. Chichester, England: Ellis Horwood, p. 27, 1978.£#/li#££#li#£Goursat, E. "Extension du problème de Riemann à des fonctions hypergéométriques de deux variables." Comptes Rendus Acad. Sci. Paris 95, 903 and 1044, 1882.£#/li#££#li#£Iyanaga, S. and Kawada, Y. (Eds.). Encyclopedic Dictionary of Mathematics. Cambridge, MA: MIT Press, p. 1461, 1980.£#/li#££#li#£Picard, E. "Sur une classe de fonctions de deux variables indépendantes." Comptes Rendus Acad. Sci. Paris 90, 1119-1121, 1880a.£#/li#££#li#£Picard, E. "Sur une extension aux fonctions de deux variables du problème de Riemann relatif aux fonctions hypergéométriques." Comptes Rendus Acad. Sci. Paris 90, 1267-1269, 1880b.£#/li#££#li#£Picard, E. "Sur une extension aux fonctions de deux variables du problème de Riemann relatif aux fonctions hypergéométriques." Ann. Ecole Norm. Sup. (2) 10, 305-322, 1881.£#/li#££#li#£Watson, G. N. "The Product of Two Hypergeometric Functions." Proc. London Math. Soc. 20, 189-195, 1922.£#/li#££#li#£Whittaker, E. T. and Watson, G. N. A Course in Modern Analysis, 4th ed. Cambridge, England: Cambridge University Press, 1990.£#/li#££#li#£ Appell, P. "Sur les fonctions hypergéométriques de plusieurs variables." In Mémoir. Sci. Math. Paris: Gauthier-Villars, 1925. £#/li#££#li#£ Appell, P. and Kampé de Fériet, J. Fonctions hypergéométriques et hypersphériques: polynomes d'Hermite. Paris: Gauthier-Villars, 1926. £#/li#££#li#£ Bailey, W. N. "A Reducible Case of the Fourth Type of Appell's Hypergeometric Functions of Two Variables." Quart. J. Math. (Oxford) 4, 305-308, 1933. £#/li#££#li#£ Bailey, W. N. "On the Reducibility of Appell's Function ." Quart. J. Math. (Oxford) 5, 291-292, 1934. £#/li#££#li#£ Bailey, W. N. "Appell's Hypergeometric Functions of Two Variables." Ch. 9 in Generalised Hypergeometric Series. Cambridge, England: Cambridge University Press, pp. 73-83 and 99-101, 1935. £#/li#££#li#£ Erdélyi, A.; Magnus, W.; Oberhettinger, F.; and Tricomi, F. G. Higher Transcendental Functions, Vol. 1. New York: Krieger, pp. 222 and 224, 1981. £#/li#££#li#£ Exton, H. Handbook of Hypergeometric Integrals: Theory, Applications, Tables, Computer Programs. Chichester, England: Ellis Horwood, p. 27, 1978. £#/li#££#li#£ Goursat, E. "Extension du problème de Riemann à des fonctions hypergéométriques de deux variables." Comptes Rendus Acad. Sci. Paris 95, 903 and 1044, 1882. £#/li#££#li#£ Iyanaga, S. and Kawada, Y. (Eds.). Encyclopedic Dictionary of Mathematics. Cambridge, MA: MIT Press, p. 1461, 1980. £#/li#££#li#£ Picard, E. "Sur une classe de fonctions de deux variables indépendantes." Comptes Rendus Acad. Sci. Paris 90, 1119-1121, 1880a. £#/li#££#li#£ Picard, E. "Sur une extension aux fonctions de deux variables du problème de Riemann relatif aux fonctions hypergéométriques." Comptes Rendus Acad. Sci. Paris 90, 1267-1269, 1880b. £#/li#££#li#£ Picard, E. "Sur une extension aux fonctions de deux variables du problème de Riemann relatif aux fonctions hypergéométriques." Ann. Ecole Norm. Sup. (2) 10, 305-322, 1881. £#/li#££#li#£ Watson, G. N. "The Product of Two Hypergeometric Functions." Proc. London Math. Soc. 20, 189-195, 1922. £#/li#££#li#£ Whittaker, E. T. and Watson, G. N. A Course in Modern Analysis, 4th ed. Cambridge, England: Cambridge University Press, 1990. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Hypergeometric Functions > Generalized Hypergeometric Functions £#/li#££#li#£ Calculus and Analysis > Special Functions > Multivariate Functions £#/li#££#li#£ Calculus and Analysis > Calculus > Integrals > Definite Integrals £#/li#££#/ul#£




£#h3#£Appell Polynomial£#/h3#£

In mathematics, an Appell sequence, named after Paul Émile Appell, is any polynomial sequence ${\displaystyle \{p_{n}(x)\}_{n=0,1,2,\ldots }}$ satisfying the identity

${\displaystyle {\frac {d}{dx}}p_{n}(x)=np_{n-1}(x),}$
and in which ${\displaystyle p_{0}(x)}$ is a non-zero constant.

Among the most notable Appell sequences besides the trivial example ${\displaystyle \{x^{n}\}}$ are the Hermite polynomials, the Bernoulli polynomials, and the Euler polynomials. Every Appell sequence is a Sheffer sequence, but most Sheffer sequences are not Appell sequences. Appell sequences have a probabilistic interpretation as systems of moments.


£#h5#£Equivalent characterizations of Appell sequences£#/h5#£
The following conditions on polynomial sequences can easily be seen to be equivalent:

£#ul#££#li#£For ${\displaystyle n=1,2,3,\ldots }$ ,£#/li#££#/ul#£
${\displaystyle {\frac {d}{dx}}p_{n}(x)=np_{n-1}(x)}$
and ${\displaystyle p_{0}(x)}$ is a non-zero constant;
£#ul#££#li#£For some sequence ${\textstyle \{c_{n}\}_{n=0}^{\infty }}$ of scalars with ${\displaystyle c_{0}\neq 0}$ ,£#/li#££#/ul#£
${\displaystyle p_{n}(x)=\sum _{k=0}^{n}{\binom {n}{k}}c_{k}x^{n-k};}$
£#ul#££#li#£For the same sequence of scalars,£#/li#££#/ul#£
${\displaystyle p_{n}(x)=\left(\sum _{k=0}^{\infty }{\frac {c_{k}}{k!}}D^{k}\right)x^{n},}$
where
${\displaystyle D={\frac {d}{dx}};}$
£#ul#££#li#£For ${\displaystyle n=0,1,2,\ldots }$ ,£#/li#££#/ul#£
${\displaystyle p_{n}(x+y)=\sum _{k=0}^{n}{\binom {n}{k}}p_{k}(x)y^{n-k}.}$

£#h5#£Recursion formula£#/h5#£
Suppose

${\displaystyle p_{n}(x)=\left(\sum _{k=0}^{\infty }{c_{k} \over k!}D^{k}\right)x^{n}=Sx^{n},}$
where the last equality is taken to define the linear operator ${\displaystyle S}$ on the space of polynomials in ${\displaystyle x}$ . Let

${\displaystyle T=S^{-1}=\left(\sum _{k=0}^{\infty }{\frac {c_{k}}{k!}}D^{k}\right)^{-1}=\sum _{k=1}^{\infty }{\frac {a_{k}}{k!}}D^{k}}$
be the inverse operator, the coefficients ${\displaystyle a_{k}}$ being those of the usual reciprocal of a formal power series, so that

${\displaystyle Tp_{n}(x)=x^{n}.\,}$
In the conventions of the umbral calculus, one often treats this formal power series ${\displaystyle T}$ as representing the Appell sequence ${\displaystyle p_{n}}$ . One can define

${\displaystyle \log T=\log \left(\sum _{k=0}^{\infty }{\frac {a_{k}}{k!}}D^{k}\right)}$
by using the usual power series expansion of the ${\displaystyle \log(x)}$ and the usual definition of composition of formal power series. Then we have

${\displaystyle p_{n+1}(x)=(x-(\log T)')p_{n}(x).\,}$
(This formal differentiation of a power series in the differential operator ${\displaystyle D}$ is an instance of Pincherle differentiation.)

In the case of Hermite polynomials, this reduces to the conventional recursion formula for that sequence.


£#h5#£Subgroup of the Sheffer polynomials£#/h5#£
The set of all Appell sequences is closed under the operation of umbral composition of polynomial sequences, defined as follows. Suppose ${\displaystyle \{p_{n}(x)\colon n=0,1,2,\ldots \}}$ and ${\displaystyle \{q_{n}(x)\colon n=0,1,2,\ldots \}}$ are polynomial sequences, given by

${\displaystyle p_{n}(x)=\sum _{k=0}^{n}a_{n,k}x^{k}{\text{ and }}q_{n}(x)=\sum _{k=0}^{n}b_{n,k}x^{k}.}$
Then the umbral composition ${\displaystyle p\circ q}$ is the polynomial sequence whose ${\displaystyle n}$ th term is

${\displaystyle (p_{n}\circ q)(x)=\sum _{k=0}^{n}a_{n,k}q_{k}(x)=\sum _{0\leq \ell \leq k\leq n}a_{n,k}b_{k,\ell }x^{\ell }}$
(the subscript ${\displaystyle n}$ appears in ${\displaystyle p_{n}}$ , since this is the ${\displaystyle n}$ th term of that sequence, but not in ${\displaystyle q}$ , since this refers to the sequence as a whole rather than one of its terms).

Under this operation, the set of all Sheffer sequences is a non-abelian group, but the set of all Appell sequences is an abelian subgroup. That it is abelian can be seen by considering the fact that every Appell sequence is of the form

${\displaystyle p_{n}(x)=\left(\sum _{k=0}^{\infty }{\frac {c_{k}}{k!}}D^{k}\right)x^{n},}$
and that umbral composition of Appell sequences corresponds to multiplication of these formal power series in the operator ${\displaystyle D}$ .


£#h5#£Different convention£#/h5#£
Another convention followed by some authors (see Chihara) defines this concept in a different way, conflicting with Appell's original definition, by using the identity

${\displaystyle {d \over dx}p_{n}(x)=p_{n-1}(x)}$
instead.


£#h5#£See also£#/h5#£ £#ul#££#li#£Sheffer sequence£#/li#£ £#li#£Umbral calculus£#/li#£ £#li#£Generalized Appell polynomials£#/li#£ £#li#£Wick product£#/li#££#/ul#£
£#h5#£References£#/h5#£ £#ul#££#li#£Appell, Paul (1880). "Sur une classe de polynômes". Annales Scientifiques de l'École Normale Supérieure. 2e Série. 9: 119–144.£#/li#£ £#li#£Roman, Steven; Rota, Gian-Carlo (1978). "The Umbral Calculus". Advances in Mathematics. 27 (2): 95–188. doi:10.1016/0001-8708(78)90087-7..£#/li#£ £#li#£Rota, Gian-Carlo; Kahaner, D.; Odlyzko, Andrew (1973). "Finite Operator Calculus". Journal of Mathematical Analysis and Applications. 42 (3): 685–760. doi:10.1016/0022-247X(73)90172-8. Reprinted in the book with the same title, Academic Press, New York, 1975.£#/li#£ £#li#£Steven Roman. The Umbral Calculus. Dover Publications.£#/li#£ £#li#£Theodore Seio Chihara (1978). An Introduction to Orthogonal Polynomials. Gordon and Breach, New York. ISBN 978-0-677-04150-6.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Appell polynomials", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#£ £#li#£Appell Sequence at MathWorld£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Suetin, P. K. "Classical Appell's Orthogonal Polynomials." Ch. 3 in Orthogonal Polynomials in Two Variables. Amsterdam, Netherlands: Gordon and Breach, pp. 63-86, 1999.£#/li#££#li#£ Suetin, P. K. "Classical Appell's Orthogonal Polynomials." Ch. 3 in Orthogonal Polynomials in Two Variables. Amsterdam, Netherlands: Gordon and Breach, pp. 63-86, 1999. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Orthogonal Polynomials £#/li#££#/ul#£




£#h3#£Approximate Identity£#/h3#£

In mathematics, particularly in functional analysis and ring theory, an approximate identity is a net in a Banach algebra or ring (generally without an identity) that acts as a substitute for an identity element.


£#h5#£Definition£#/h5#£
A right approximate identity in a Banach algebra A is a net ${\displaystyle \{e_{\lambda }:\lambda \in \Lambda \}}$ such that for every element a of A, ${\displaystyle \lim _{\lambda \in \Lambda }\lVert ae_{\lambda }-a\rVert =0.}$ Similarly, a left approximate identity in a Banach algebra A is a net ${\displaystyle \{e_{\lambda }:\lambda \in \Lambda \}}$ such that for every element a of A, ${\displaystyle \lim _{\lambda \in \Lambda }\lVert e_{\lambda }a-a\rVert =0.}$ An approximate identity is a net which is both a right approximate identity and a left approximate identity.


£#h5#£C*-algebras£#/h5#£
For C*-algebras, a right (or left) approximate identity consisting of self-adjoint elements is the same as an approximate identity. The net of all positive elements in A of norm ≤ 1 with its natural order is an approximate identity for any C*-algebra. This is called the canonical approximate identity of a C*-algebra. Approximate identities are not unique. For example, for compact operators acting on a Hilbert space, the net consisting of finite rank projections would be another approximate identity.

If an approximate identity is a sequence, we call it a sequential approximate identity and a C*-algebra with a sequential approximate identity is called σ-unital. Every separable C*-algebra is σ-unital, though the converse is false. A commutative C*-algebra is σ-unital if and only if its spectrum is σ-compact. In general, a C*-algebra A is σ-unital if and only if A contains a strictly positive element, i.e. there exists h in A+ such that the hereditary C*-subalgebra generated by h is A.

One sometimes considers approximate identities consisting of specific types of elements. For example, a C*-algebra has real rank zero if and only if every hereditary C*-subalgebra has an approximate identity consisting of projections. This was known as property (HP) in earlier literature.


£#h5#£Convolution algebras£#/h5#£
An approximate identity in a convolution algebra plays the same role as a sequence of function approximations to the Dirac delta function (which is the identity element for convolution). For example, the Fejér kernels of Fourier series theory give rise to an approximate identity.


£#h5#£Rings£#/h5#£
In ring theory, an approximate identity is defined in a similar way, except that the ring is given the discrete topology so that a = aeλ for some λ.

A module over a ring with approximate identity is called non-degenerate if for every m in the module there is some λ with m = meλ.


£#h5#£See also£#/h5#£ £#ul#££#li#£Mollifier£#/li#£ £#li#£Nascent delta function£#/li#£ £#li#£Summability kernel£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Blackadar, B. K-Theory for Operator Algebras. New York: Cambridge University Press, 1998.£#/li#££#li#£Conway, J. A Course in Functional Analysis. New York: Springer-Verlag, 1990.£#/li#££#li#£ Blackadar, B. K-Theory for Operator Algebras. New York: Cambridge University Press, 1998. £#/li#££#li#£ Conway, J. A Course in Functional Analysis. New York: Springer-Verlag, 1990. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Functional Analysis £#/li#££#/ul#£




£#h3#£Approximate Unit£#/h3#£

£#h5#£ References £#/h5#£ £#ul#££#li#£Kadison, R. V. and Ringrose, J. R. Fundamentals of the Theory of Operator Algebras, Vol. 1: Elementary Theory. Providence, RI: Amer. Math. Soc., 1997.£#/li#££#li#£Murphy, G. J. C-*-Algebras and Operator Theory. New York: Academic Press, 1990.£#/li#££#li#£C-*-Algebras and Operator Theory.£#/li#££#li#£ Kadison, R. V. and Ringrose, J. R. Fundamentals of the Theory of Operator Algebras, Vol. 1: Elementary Theory. Providence, RI: Amer. Math. Soc., 1997. £#/li#££#li#£ Murphy, G. J. C-*-Algebras and Operator Theory. New York: Academic Press, 1990. £#/li#££#li#£ C-*-Algebras and Operator Theory. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Functional Analysis £#/li#££#/ul#£




£#h3#£Approximation Problem£#/h3#£

In functional analysis, a branch of mathematics, a compact operator is a linear operator ${\displaystyle T:X\to Y}$ , where ${\displaystyle X,Y}$ are normed vector spaces, with the property that ${\displaystyle T}$ maps bounded subsets of ${\displaystyle X}$ to relatively compact subsets of ${\displaystyle Y}$ (subsets with compact closure in ${\displaystyle Y}$ ). Such an operator is necessarily a bounded operator, and so continuous. Some authors require that ${\displaystyle X,Y}$ are Banach, but the definition can be extended to more general spaces.

Any bounded operator ${\displaystyle T}$ that has finite rank is a compact operator; indeed, the class of compact operators is a natural generalization of the class of finite-rank operators in an infinite-dimensional setting. When ${\displaystyle Y}$ is a Hilbert space, it is true that any compact operator is a limit of finite-rank operators, so that the class of compact operators can be defined alternatively as the closure of the set of finite-rank operators in the norm topology. Whether this was true in general for Banach spaces (the approximation property) was an unsolved question for many years; in 1973 Per Enflo gave a counter-example, building on work by Grothendieck and Banach.

The origin of the theory of compact operators is in the theory of integral equations, where integral operators supply concrete examples of such operators. A typical Fredholm integral equation gives rise to a compact operator K on function spaces; the compactness property is shown by equicontinuity. The method of approximation by finite-rank operators is basic in the numerical solution of such equations. The abstract idea of Fredholm operator is derived from this connection.


£#h5#£Equivalent formulations£#/h5#£
A linear map ${\displaystyle T:X\to Y}$ between two topological vector spaces is said to be compact if there exists a neighborhood ${\displaystyle U}$ of the origin in ${\displaystyle X}$ such that ${\displaystyle T(U)}$ is a relatively compact subset of ${\displaystyle Y}$ .

Let ${\displaystyle X,Y}$ be normed spaces and ${\displaystyle T:X\to Y}$ a linear operator. Then the following statements are equivalent, and some of them are used as the principal definition by different authors

£#ul#££#li#£ ${\displaystyle T}$ is a compact operator;£#/li#£ £#li#£the image of the unit ball of ${\displaystyle X}$ under ${\displaystyle T}$ is relatively compact in ${\displaystyle Y}$ ;£#/li#£ £#li#£the image of any bounded subset of ${\displaystyle X}$ under ${\displaystyle T}$ is relatively compact in ${\displaystyle Y}$ ;£#/li#£ £#li#£there exists a neighbourhood ${\displaystyle U}$ of the origin in ${\displaystyle X}$ and a compact subset ${\displaystyle V\subseteq Y}$ such that ${\displaystyle T(U)\subseteq V}$ ;£#/li#£ £#li#£for any bounded sequence ${\displaystyle (x_{n})_{n\in \mathbb {N} }}$ in ${\displaystyle X}$ , the sequence ${\displaystyle (Tx_{n})_{n\in \mathbb {N} }}$ contains a converging subsequence.£#/li#££#/ul#£
If in addition ${\displaystyle Y}$ is Banach, these statements are also equivalent to:

£#ul#££#li#£the image of any bounded subset of ${\displaystyle X}$ under ${\displaystyle T}$ is totally bounded in ${\displaystyle Y}$ .£#/li#££#/ul#£
If a linear operator is compact, then it is continuous.


£#h5#£Important properties£#/h5#£
In the following, ${\displaystyle X,Y,Z,W}$ are Banach spaces, ${\displaystyle B(X,Y)}$ is the space of bounded operators ${\displaystyle X\to Y}$ under the operator norm, and ${\displaystyle K(X,Y)}$ denotes the space of compact operators ${\displaystyle X\to Y}$ . ${\displaystyle \operatorname {Id} _{X}}$ denotes the identity operator on ${\displaystyle X}$ , ${\displaystyle B(X)=B(X,X)}$ , and ${\displaystyle K(X)=K(X,X)}$ .

£#ul#££#li#£ ${\displaystyle K(X,Y)}$ is a closed subspace of ${\displaystyle B(X,Y)}$ (in the norm topology). Equivalently,£#ul#££#li#£given a sequence of compact operators ${\displaystyle (T_{n})_{n\in \mathbf {N} }}$ mapping ${\displaystyle X\to Y}$ (where ${\displaystyle X,Y}$ are Banach) and given that ${\displaystyle (T_{n})_{n\in \mathbf {N} }}$ converges to ${\displaystyle T}$ with respect to the operator norm, ${\displaystyle T}$ is then compact.£#/li#££#/ul#££#/li#£ £#li#£Conversely, if ${\displaystyle X,Y}$ are Hilbert spaces, then every compact operator from ${\displaystyle X\to Y}$ is the limit of finite rank operators. Notably, this "approximation property" is false for general Banach spaces X and Y.£#/li#£ £#li#£ ${\displaystyle B(Y,Z)\circ K(X,Y)\circ B(W,X)\subseteq K(W,Z).}$   In particular, ${\displaystyle K(X)}$ forms a two-sided ideal in ${\displaystyle B(X)}$ .£#/li#£ £#li#£Any compact operator is strictly singular, but not vice versa.£#/li#£ £#li#£A bounded linear operator between Banach spaces is compact if and only if its adjoint is compact (Schauder's theorem). £#ul#££#li#£If ${\displaystyle T:X\to Y}$ is bounded and compact, then: £#ul#££#li#£the closure of the range of ${\displaystyle T}$ is separable.£#/li#£ £#li#£if the range of ${\displaystyle T}$ is closed in Y, then the range of ${\displaystyle T}$ is finite-dimensional.£#/li#££#/ul#££#/li#££#/ul#££#/li#£ £#li#£If ${\displaystyle X}$ is a Banach space and there exists an invertible bounded compact operator ${\displaystyle T:X\to X}$ then ${\displaystyle X}$ is necessarily finite-dimensional.£#/li#££#/ul#£
Now suppose that ${\displaystyle X}$ is a Banach space and ${\displaystyle T:X\to X}$ is a compact linear operator, and ${\displaystyle T^{*}:X^{*}\to X^{*}}$ is the adjoint or transpose of T.

£#ul#££#li#£For any ${\displaystyle T\in K(X)}$ , then ${\displaystyle {\operatorname {Id} _{X}}-T}$   is a Fredholm operator of index 0. In particular, ${\displaystyle \operatorname {Im} \,({\operatorname {Id} _{X}}-T)}$   is closed. This is essential in developing the spectral properties of compact operators. One can notice the similarity between this property and the fact that, if ${\displaystyle M}$ and ${\displaystyle N}$ are subspaces of ${\displaystyle X}$ where ${\displaystyle M}$ is closed and ${\displaystyle N}$ is finite-dimensional, then ${\displaystyle M+N}$ is also closed.£#/li#£ £#li#£If ${\displaystyle S:X\to X}$ is any bounded linear operator then both ${\displaystyle S\circ T}$ and ${\displaystyle T\circ S}$ are compact operators.£#/li#£ £#li#£If ${\displaystyle \lambda \neq 0}$ then the range of ${\displaystyle T-\lambda \operatorname {Id} _{X}}$ is closed and the kernel of ${\displaystyle T-\lambda \operatorname {Id} _{X}}$ is finite-dimensional.£#/li#£ £#li#£If ${\displaystyle \lambda \neq 0}$ then the following are finite and equal: ${\displaystyle \dim \ker \left(T-\lambda \operatorname {Id} _{X}\right)=\dim X/\operatorname {Im} \left(T-\lambda \operatorname {Id} _{X}\right)=\dim \ker \left(T^{*}-\lambda \operatorname {Id} _{X^{*}}\right)=\dim X^{*}/\operatorname {Im} \left(T^{*}-\lambda \operatorname {Id} _{X^{*}}\right)}$ £#/li#£ £#li#£The spectrum ${\displaystyle \sigma (T)}$ of ${\displaystyle T}$ , is compact, countable, and has at most one limit point, which would necessarily be the origin.£#/li#£ £#li#£If ${\displaystyle X}$ is infinite-dimensional then ${\displaystyle 0\in \sigma (T)}$ .£#/li#£ £#li#£If ${\displaystyle \lambda \neq 0}$ and ${\displaystyle \lambda \in \sigma (T)}$ then ${\displaystyle \lambda }$ is an eigenvalue of both ${\displaystyle T}$ and ${\displaystyle T^{*}}$ .£#/li#£ £#li#£For every ${\displaystyle r>0}$ the set ${\displaystyle E_{r}=\left\{\lambda \in \sigma (T):|\lambda |>r\right\}}$ is finite, and for every non-zero ${\displaystyle \lambda \in \sigma (T)}$ the range of ${\displaystyle T-\lambda \operatorname {Id} _{X}}$ is a proper subset of X.£#/li#££#/ul#£
£#h5#£Origins in integral equation theory£#/h5#£
A crucial property of compact operators is the Fredholm alternative, which asserts that the existence of solution of linear equations of the form

${\displaystyle (\lambda K+I)u=f}$

(where K is a compact operator, f is a given function, and u is the unknown function to be solved for) behaves much like as in finite dimensions. The spectral theory of compact operators then follows, and it is due to Frigyes Riesz (1918). It shows that a compact operator K on an infinite-dimensional Banach space has spectrum that is either a finite subset of C which includes 0, or the spectrum is a countably infinite subset of C which has 0 as its only limit point. Moreover, in either case the non-zero elements of the spectrum are eigenvalues of K with finite multiplicities (so that K − λI has a finite-dimensional kernel for all complex λ ≠ 0).

An important example of a compact operator is compact embedding of Sobolev spaces, which, along with the Gårding inequality and the Lax–Milgram theorem, can be used to convert an elliptic boundary value problem into a Fredholm integral equation. Existence of the solution and spectral properties then follow from the theory of compact operators; in particular, an elliptic boundary value problem on a bounded domain has infinitely many isolated eigenvalues. One consequence is that a solid body can vibrate only at isolated frequencies, given by the eigenvalues, and arbitrarily high vibration frequencies always exist.

The compact operators from a Banach space to itself form a two-sided ideal in the algebra of all bounded operators on the space. Indeed, the compact operators on an infinite-dimensional separable Hilbert space form a maximal ideal, so the quotient algebra, known as the Calkin algebra, is simple. More generally, the compact operators form an operator ideal.


£#h5#£Compact operator on Hilbert spaces£#/h5#£
For Hilbert spaces, another equivalent definition of compact operators is given as follows.

An operator ${\displaystyle T}$ on an infinite-dimensional Hilbert space ${\displaystyle {\mathcal {H}}}$

${\displaystyle T:{\mathcal {H}}\to {\mathcal {H}}}$
is said to be compact if it can be written in the form

${\displaystyle T=\sum _{n=1}^{\infty }\lambda _{n}\langle f_{n},\cdot \rangle g_{n}\,,}$
where ${\displaystyle \{f_{1},f_{2},\ldots \}}$ and ${\displaystyle \{g_{1},g_{2},\ldots \}}$ are orthonormal sets (not necessarily complete), and ${\displaystyle \lambda _{1},\lambda _{2},\ldots }$ is a sequence of positive numbers with limit zero, called the singular values of the operator. The singular values can accumulate only at zero. If the sequence becomes stationary at zero, that is ${\displaystyle \lambda _{N+k}=0}$ for some ${\displaystyle N\in \mathbb {N} ,}$ and every ${\displaystyle k=1,2,\dots }$ , then the operator has finite rank, i.e, a finite-dimensional range and can be written as

${\displaystyle T=\sum _{n=1}^{N}\lambda _{n}\langle f_{n},\cdot \rangle g_{n}\,.}$
The bracket ${\displaystyle \langle \cdot ,\cdot \rangle }$ is the scalar product on the Hilbert space; the sum on the right hand side converges in the operator norm.

An important subclass of compact operators is the trace-class or nuclear operators.


£#h5#£Completely continuous operators£#/h5#£
Let X and Y be Banach spaces. A bounded linear operator T : X → Y is called completely continuous if, for every weakly convergent sequence ${\displaystyle (x_{n})}$ from X, the sequence ${\displaystyle (Tx_{n})}$ is norm-convergent in Y (Conway 1985, §VI.3). Compact operators on a Banach space are always completely continuous. If X is a reflexive Banach space, then every completely continuous operator T : X → Y is compact.

Somewhat confusingly, compact operators are sometimes referred to as "completely continuous" in older literature, even though they are not necessarily completely continuous by the definition of that phrase in modern terminology.


£#h5#£Examples£#/h5#£ £#ul#££#li#£Every finite rank operator is compact.£#/li#£ £#li#£For ${\displaystyle \ell ^{p}}$ and a sequence (tn) converging to zero, the multiplication operator (Tx)n = tn xn is compact.£#/li#£ £#li#£For some fixed g ∈ C([0, 1]; R), define the linear operator T from C([0, 1]; R) to C([0, 1]; R) by That the operator T is indeed compact follows from the Ascoli theorem.£#/li#£ £#li#£More generally, if Ω is any domain in Rn and the integral kernel k : Ω × Ω → R is a Hilbert–Schmidt kernel, then the operator T on L2(Ω; R) defined by is a compact operator.£#/li#£ £#li#£By Riesz's lemma, the identity operator is a compact operator if and only if the space is finite-dimensional.£#/li#££#/ul#£
£#h5#£See also£#/h5#£ £#ul#££#li#£Compact embedding£#/li#£ £#li#£Compact operator on Hilbert space£#/li#£ £#li#£Fredholm alternative£#/li#£ £#li#£Fredholm integral equations£#/li#£ £#li#£Fredholm operator£#/li#£ £#li#£Spectral theory of compact operators£#/li#£ £#li#£Strictly singular operator£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Conway, John B. (1985). A course in functional analysis. Springer-Verlag. Section 2.4. ISBN 978-3-540-96042-3.£#/li#£ £#li#£Conway, John B. (1990). A Course in Functional Analysis. Graduate Texts in Mathematics. Vol. 96 (2nd ed.). New York: Springer-Verlag. ISBN 978-0-387-97245-9. OCLC 21195908.£#/li#£ £#li#£Enflo, P. (1973). "A counterexample to the approximation problem in Banach spaces". Acta Mathematica. 130 (1): 309–317. doi:10.1007/BF02392270. ISSN 0001-5962. MR 0402468.£#/li#£ £#li#£Kreyszig, Erwin (1978). Introductory functional analysis with applications. John Wiley & Sons. ISBN 978-0-471-50731-4.£#/li#£ £#li#£Kutateladze, S.S. (1996). Fundamentals of Functional Analysis. Texts in Mathematical Sciences. Vol. 12 (2nd ed.). New York: Springer-Verlag. p. 292. ISBN 978-0-7923-3898-7.£#/li#£ £#li#£Lax, Peter (2002). Functional Analysis. New York: Wiley-Interscience. ISBN 978-0-471-55604-6. OCLC 47767143.£#/li#£ £#li#£Narici, Lawrence; Beckenstein, Edward (2011). Topological Vector Spaces. Pure and applied mathematics (Second ed.). Boca Raton, FL: CRC Press. ISBN 978-1584888666. OCLC 144216834.£#/li#£ £#li#£Renardy, M.; Rogers, R. C. (2004). An introduction to partial differential equations. Texts in Applied Mathematics. Vol. 13 (2nd ed.). New York: Springer-Verlag. p. 356. ISBN 978-0-387-00444-0. (Section 7.5)£#/li#£ £#li#£Rudin, Walter (1991). Functional Analysis. International Series in Pure and Applied Mathematics. Vol. 8 (Second ed.). New York, NY: McGraw-Hill Science/Engineering/Math. ISBN 978-0-07-054236-5. OCLC 21163277.£#/li#£ £#li#£Schaefer, Helmut H.; Wolff, Manfred P. (1999). Topological Vector Spaces. GTM. Vol. 8 (Second ed.). New York, NY: Springer New York Imprint Springer. ISBN 978-1-4612-7155-0. OCLC 840278135.£#/li#£ £#li#£Trèves, François (2006) [1967]. Topological Vector Spaces, Distributions and Kernels. Mineola, N.Y.: Dover Publications. ISBN 978-0-486-45352-1. OCLC 853623322.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Enflo, P. "A Counterexample to the Approximation Problem in Banach Spaces." Acta Math. 130, 309-317, 1973.£#/li#££#li#£Grothendieck, A. Produits tensoriels topologiques et espaces nucléaires. Providence, RI: Amer. Math. Soc., 1955.£#/li#££#li#£ Enflo, P. "A Counterexample to the Approximation Problem in Banach Spaces." Acta Math. 130, 309-317, 1973. £#/li#££#li#£ Grothendieck, A. Produits tensoriels topologiques et espaces nucléaires. Providence, RI: Amer. Math. Soc., 1955. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Functional Analysis £#/li#££#li#£ Topology > Spaces £#/li#££#/ul#£




£#h3#£Arbitrary Constant of Integration£#/h3#£

In calculus, the constant of integration, often denoted by ${\displaystyle C}$ , is a constant term added to an antiderivative of a function ${\displaystyle f(x)}$ to indicate that the indefinite integral of ${\displaystyle f(x)}$ (i.e., the set of all antiderivatives of ${\displaystyle f(x)}$ ), on a connected domain, is only defined up to an additive constant. This constant expresses an ambiguity inherent in the construction of antiderivatives.

More specifically, if a function ${\displaystyle f(x)}$ is defined on an interval, and ${\displaystyle F(x)}$ is an antiderivative of ${\displaystyle f(x)}$ , then the set of all antiderivatives of ${\displaystyle f(x)}$ is given by the functions ${\displaystyle F(x)+C}$ , where ${\displaystyle C}$ is an arbitrary constant (meaning that any value of ${\displaystyle C}$ would make ${\displaystyle F(x)+C}$ a valid antiderivative). For that reason, the indefinite integral is often written as ${\textstyle \int f(x)\,dx=F(x)+C}$ , although the constant of integration might be sometimes omitted in lists of integrals for simplicity.


£#h5#£Origin£#/h5#£
The derivative of any constant function is zero. Once one has found one antiderivative ${\displaystyle F(x)}$ for a function ${\displaystyle f(x)}$ , adding or subtracting any constant ${\displaystyle C}$ will give us another antiderivative, because ${\textstyle {\frac {d}{dx}}(F(x)+C)={\frac {d}{dx}}F(x)+{\frac {d}{dx}}C=F'(x)=f(x)}$ . The constant is a way of expressing that every function with at least one antiderivative will have an infinite number of them.

Let ${\displaystyle F:\mathbb {R} \to \mathbb {R} }$ and ${\displaystyle G:\mathbb {R} \to \mathbb {R} }$ be two everywhere differentiable functions. Suppose that ${\displaystyle F\,'(x)=G\,'(x)}$ for every real number x. Then there exists a real number ${\displaystyle C}$ such that ${\displaystyle F(x)-G(x)=C}$ for every real number x.

To prove this, notice that ${\displaystyle [F(x)-G(x)]'=0}$ . So ${\displaystyle F}$ can be replaced by ${\displaystyle F-G}$ , and ${\displaystyle G}$ by the constant function ${\displaystyle 0}$ , making the goal to prove that an everywhere differentiable function whose derivative is always zero must be constant:

Choose a real number ${\displaystyle a}$ , and let ${\displaystyle C=F(a)}$ . For any x, the fundamental theorem of calculus, together with the assumption that the derivative of ${\displaystyle F}$ vanishes, implies that

${\displaystyle {\begin{aligned}&0=\int _{a}^{x}F'(t)\ dt\\&0=F(x)-F(a)\\&0=F(x)-C\\&F(x)=C\\\end{aligned}}}$
thereby showing that ${\displaystyle F}$ is a constant function.

Two facts are crucial in this proof. First, the real line is connected. If the real line were not connected, we would not always be able to integrate from our fixed a to any given x. For example, if we were to ask for functions defined on the union of intervals [0,1] and [2,3], and if a were 0, then it would not be possible to integrate from 0 to 3, because the function is not defined between 1 and 2. Here, there will be two constants, one for each connected component of the domain. In general, by replacing constants with locally constant functions, we can extend this theorem to disconnected domains. For example, there are two constants of integration for ${\textstyle \int dx/x}$ , and infinitely many for ${\textstyle \int \tan x\,dx}$ , so for example, the general form for the integral of 1/x is:

${\displaystyle \int {\frac {dx}{x}}={\begin{cases}\ln \left|x\right|+C^{-}&x<0\\\ln \left|x\right|+C^{+}&x>0\end{cases}}}$
Second, ${\displaystyle F}$ and ${\displaystyle G}$ were assumed to be everywhere differentiable. If ${\displaystyle F}$ and ${\displaystyle G}$ are not differentiable at even one point, then the theorem might fail. As an example, let ${\displaystyle F(x)}$ be the Heaviside step function, which is zero for negative values of x and one for non-negative values of x, and let ${\displaystyle G(x)=0}$ . Then the derivative of ${\displaystyle F}$ is zero where it is defined, and the derivative of ${\displaystyle G}$ is always zero. Yet it's clear that ${\displaystyle F}$ and ${\displaystyle G}$ do not differ by a constant, even if it is assumed that ${\displaystyle F}$ and ${\displaystyle G}$ are everywhere continuous and almost everywhere differentiable the theorem still fails. As an example, take ${\displaystyle F}$ to be the Cantor function and again let ${\displaystyle G=0}$ .

For example, suppose one wants to find antiderivatives of ${\displaystyle \cos(x)}$ . One such antiderivative is ${\displaystyle \sin(x)}$ . Another one is ${\displaystyle \sin(x)+1}$ . A third is ${\displaystyle \sin(x)-\pi }$ . Each of these has derivative ${\displaystyle \cos(x)}$ , so they are all antiderivatives of ${\displaystyle \cos(x)}$ .

It turns out that adding and subtracting constants is the only flexibility we have in finding different antiderivatives of the same function. That is, all antiderivatives are the same up to a constant. To express this fact for ${\displaystyle \cos(x)}$ , we write:

${\displaystyle \int \cos(x)\,dx=\sin(x)+C.}$
Replacing ${\displaystyle C}$ by a number will produce an antiderivative. By writing ${\displaystyle C}$ instead of a number, however, a compact description of all the possible antiderivatives of ${\displaystyle \cos(x)}$ is obtained. ${\displaystyle C}$ is called the constant of integration. It is easily determined that all of these functions are indeed antiderivatives of ${\displaystyle \cos(x)}$ :

${\displaystyle {\begin{aligned}{\frac {d}{dx}}[\sin(x)+C]&={\frac {d}{dx}}\sin(x)+{\frac {d}{dx}}C\\&=\cos(x)+0\\&=\cos(x)\end{aligned}}}$

£#h5#£Necessity£#/h5#£
At first glance, it may seem that the constant is unnecessary, since it can be set to zero. Furthermore, when evaluating definite integrals using the fundamental theorem of calculus, the constant will always cancel with itself.

However, trying to set the constant to zero does not always make sense. For example, ${\displaystyle 2\sin(x)\cos(x)}$ can be integrated in at least three different ways:

${\displaystyle {\begin{aligned}\int 2\sin(x)\cos(x)\,dx&=&\sin ^{2}(x)+C&=&-\cos ^{2}(x)+1+C&=&-{\frac {1}{2}}\cos(2x)+C\\\int 2\sin(x)\cos(x)\,dx&=&-\cos ^{2}(x)+C&=&\sin ^{2}(x)-1+C&=&-{\frac {1}{2}}\cos(2x)+C\\\int 2\sin(x)\cos(x)\,dx&=&-{\frac {1}{2}}\cos(2x)+C&=&\sin ^{2}(x)+C&=&-\cos ^{2}(x)+C\end{aligned}}}$
So setting ${\displaystyle C}$ to zero can still leave a constant. This means that, for a given function, there is no "simplest antiderivative".

Another problem with setting ${\displaystyle C}$ equal to zero is that sometimes we want to find an antiderivative that has a given value at a given point (as in an initial value problem). For example, to obtain the antiderivative of ${\displaystyle \cos(x)}$ that has the value 100 at x = π, then only one value of ${\displaystyle C}$ will work (in this case ${\displaystyle C=100}$ ).

This restriction can be rephrased in the language of differential equations. Finding an indefinite integral of a function ${\displaystyle f(x)}$ is the same as solving the differential equation ${\textstyle {\frac {dy}{dx}}=f(x)}$ . Any differential equation will have many solutions, and each constant represents the unique solution of a well-posed initial value problem. Imposing the condition that our antiderivative takes the value 100 at x = π is an initial condition. Each initial condition corresponds to one and only one value of ${\displaystyle C}$ , so without ${\displaystyle C}$ it would be impossible to solve the problem.

There is another justification, coming from abstract algebra. The space of all (suitable) real-valued functions on the real numbers is a vector space, and the differential operator ${\textstyle {\frac {d}{dx}}}$ is a linear operator. The operator ${\textstyle {\frac {d}{dx}}}$ maps a function to zero if and only if that function is constant. Consequently, the kernel of ${\textstyle {\frac {d}{dx}}}$ is the space of all constant functions. The process of indefinite integration amounts to finding a pre-image of a given function. There is no canonical pre-image for a given function, but the set of all such pre-images forms a coset. Choosing a constant is the same as choosing an element of the coset. In this context, solving an initial value problem is interpreted as lying in the hyperplane given by the initial conditions.


£#h5#£References£#/h5#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Calculus > Integrals > Indefinite Integrals £#/li#££#/ul#£




£#h3#£Arccos£#/h3#£

Arccos, ArcCos, ARCCOS, arccos, or ARccOS may refer to:

£#ul#££#li#£arccos (trigonometry), the inverse trigonometric function of cosine£#/li#£ £#li#£ARccOS protection, a copyright protection mechanism by Sony£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Trigonometric Functions £#/li#££#/ul#£




£#h3#£Arccosec£#/h3#£

In mathematics, the inverse trigonometric functions (occasionally also called arcus functions, antitrigonometric functions or cyclometric functions) are the inverse functions of the trigonometric functions (with suitably restricted domains). Specifically, they are the inverses of the sine, cosine, tangent, cotangent, secant, and cosecant functions, and are used to obtain an angle from any of the angle's trigonometric ratios. Inverse trigonometric functions are widely used in engineering, navigation, physics, and geometry.


£#h5#£Notation£#/h5#£
Several notations for the inverse trigonometric functions exist. The most common convention is to name inverse trigonometric functions using an arc- prefix: arcsin(x), arccos(x), arctan(x), etc. (This convention is used throughout this article.) This notation arises from the following geometric relationships: when measuring in radians, an angle of θ radians will correspond to an arc whose length is rθ, where r is the radius of the circle. Thus in the unit circle, "the arc whose cosine is x" is the same as "the angle whose cosine is x", because the length of the arc of the circle in radii is the same as the measurement of the angle in radians. In computer programming languages, the inverse trigonometric functions are often called by the abbreviated forms asin, acos, atan.

The notations sin−1(x), cos−1(x), tan−1(x), etc., as introduced by John Herschel in 1813, are often used as well in English-language sources, much more than the also established sin[−1](x), cos[−1](x), tan[−1](x),—conventions consistent with the notation of an inverse function, that is useful e.g. to define the multivalued version of each inverse trigonometric function: e.g. ${\displaystyle \tan ^{-1}(x)=\{\arctan(x)+\pi k\mid k\in \mathbb {Z} \}}$ . However, this might appear to conflict logically with the common semantics for expressions such as sin2(x) (although only sin2 x, without parentheses, is the really common one), which refer to numeric power rather than function composition, and therefore may result in confusion between multiplicative inverse or reciprocal and compositional inverse. The confusion is somewhat mitigated by the fact that each of the reciprocal trigonometric functions has its own name—for example, (cos(x))−1 = sec(x). Nevertheless, certain authors advise against using it for its ambiguity. Another precarious convention used by a tiny number of authors is to use an uppercase first letter, along with a −1 superscript: Sin−1(x), Cos−1(x), Tan−1(x), etc. Although its intention is to avoid confusion with the multiplicative inverse, which should be represented by sin−1(x), cos−1(x), etc., or, better, by sin−1 x, cos−1 x, etc., it in turn creates yet another major source of ambiguity: especially in light of the fact that many popular high-level programming languages (e.g. Wolfram's Mathematica, and University of Sydney's MAGMA) use those very same capitalised representations for the standard trig functions; whereas others (Python (ie SymPy and NumPy), Matlab, MAPLE etc) use lower-case.

Hence, since 2009, the ISO 80000-2 standard has specified solely the "arc" prefix for the inverse functions.


£#h5#£Basic concepts£#/h5#£
£#h5#£Principal values£#/h5#£
Since none of the six trigonometric functions are one-to-one, they must be restricted in order to have inverse functions. Therefore, the result ranges of the inverse functions are proper (i.e. strict) subsets of the domains of the original functions.

For example, using function in the sense of multivalued functions, just as the square root function ${\displaystyle y={\sqrt {x}}}$ could be defined from ${\displaystyle y^{2}=x,}$ the function ${\displaystyle y=\arcsin(x)}$ is defined so that ${\displaystyle \sin(y)=x.}$ For a given real number ${\displaystyle x,}$ with ${\displaystyle -1\leq x\leq 1,}$ there are multiple (in fact, countably infinitely many) numbers ${\displaystyle y}$ such that ${\displaystyle \sin(y)=x}$ ; for example, ${\displaystyle \sin(0)=0,}$ but also ${\displaystyle \sin(\pi )=0,}$ ${\displaystyle \sin(2\pi )=0,}$ etc. When only one value is desired, the function may be restricted to its principal branch. With this restriction, for each ${\displaystyle x}$ in the domain, the expression ${\displaystyle \arcsin(x)}$ will evaluate only to a single value, called its principal value. These properties apply to all the inverse trigonometric functions.

The principal inverses are listed in the following table.

Note: Some authors define the range of arcsecant to be ${\textstyle (0\leq y<{\frac {\pi }{2}}{\text{ or }}\pi \leq y<{\frac {3\pi }{2}})}$ , because the tangent function is nonnegative on this domain. This makes some computations more consistent. For example, using this range, ${\displaystyle \tan(\operatorname {arcsec}(x))={\sqrt {x^{2}-1}},}$ whereas with the range ${\textstyle (0\leq y<{\frac {\pi }{2}}{\text{ or }}{\frac {\pi }{2}}<y\leq \pi )}$ , we would have to write ${\displaystyle \tan(\operatorname {arcsec}(x))=\pm {\sqrt {x^{2}-1}},}$ since tangent is nonnegative on ${\textstyle 0\leq y<{\frac {\pi }{2}},}$ but nonpositive on ${\textstyle {\frac {\pi }{2}}<y\leq \pi .}$ For a similar reason, the same authors define the range of arccosecant to be ${\textstyle (-\pi <y\leq -{\frac {\pi }{2}}}$ or ${\textstyle 0<y\leq {\frac {\pi }{2}}).}$

If ${\displaystyle x}$ is allowed to be a complex number, then the range of ${\displaystyle y}$ applies only to its real part.

The table below displays names and domains of the inverse trigonometric functions along with the range of their usual principal values in radians.

The symbol ${\displaystyle \mathbb {R} =(-\infty ,\infty )}$ denotes the set of all real numbers and ${\displaystyle \mathbb {Z} =\{\ldots ,\,-2,\,-1,\,0,\,1,\,2,\,\ldots \}}$ denotes the set of all integers. The set of all integer multiples of ${\displaystyle \pi }$ is denoted by

The symbol ${\displaystyle \,\setminus \,}$ denotes set subtraction so that, for instance, ${\displaystyle \mathbb {R} \setminus (-1,1)=(-\infty ,-1]\cup [1,\infty )}$ is the set of points in ${\displaystyle \mathbb {R} }$ (that is, real numbers) that are not in the interval ${\displaystyle (-1,1).}$

The Minkowski sum notation ${\textstyle \pi \mathbb {Z} +(0,\pi )}$ and ${\displaystyle \pi \mathbb {Z} +{\bigl (}{-{\tfrac {\pi }{2}}},{\tfrac {\pi }{2}}{\bigr )}}$ that is used above to concisely write the domains of ${\displaystyle \cot ,\csc ,\tan ,{\text{ and }}\sec }$ is now explained.

Domain of cotangent ${\displaystyle \cot }$ and cosecant ${\displaystyle \csc }$ : The domains of ${\displaystyle \,\cot \,}$ and ${\displaystyle \,\csc \,}$ are the same. They are the set of all angles ${\displaystyle \theta }$ at which ${\displaystyle \sin \theta \neq 0,}$ i.e. all real numbers that are not of the form ${\displaystyle \pi n}$ for some integer ${\displaystyle n,}$

Domain of tangent ${\displaystyle \tan }$ and secant ${\displaystyle \sec }$ : The domains of ${\displaystyle \,\tan \,}$ and ${\displaystyle \,\sec \,}$ are the same. They are the set of all angles ${\displaystyle \theta }$ at which ${\displaystyle \cos \theta \neq 0,}$


£#h5#£Solutions to elementary trigonometric equations£#/h5#£
Each of the trigonometric functions is periodic in the real part of its argument, running through all its values twice in each interval of ${\displaystyle 2\pi }$ :

£#ul#££#li#£Sine and cosecant begin their period at ${\textstyle 2\pi k-{\frac {\pi }{2}}}$ (where ${\displaystyle k}$ is an integer), finish it at ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ and then reverse themselves over ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ to ${\displaystyle 2\pi k+{\frac {3\pi }{2}}.}$ £#/li#£ £#li#£Cosine and secant begin their period at ${\displaystyle 2\pi k,}$ finish it at ${\displaystyle 2\pi k+\pi .}$ and then reverse themselves over ${\displaystyle 2\pi k+\pi }$ to ${\displaystyle 2\pi k+2\pi .}$ £#/li#£ £#li#£Tangent begins its period at ${\textstyle 2\pi k-{\frac {\pi }{2}},}$ finishes it at ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ and then repeats it (forward) over ${\textstyle 2\pi k+{\frac {\pi }{2}}}$ to ${\textstyle 2\pi k+{\frac {3\pi }{2}}.}$ £#/li#£ £#li#£Cotangent begins its period at ${\displaystyle 2\pi k,}$ finishes it at ${\displaystyle 2\pi k+\pi ,}$ and then repeats it (forward) over ${\displaystyle 2\pi k+\pi }$ to ${\displaystyle 2\pi k+2\pi .}$ £#/li#££#/ul#£
This periodicity is reflected in the general inverses, where ${\displaystyle k}$ is some integer.

The following table shows how inverse trigonometric functions may be used to solve equalities involving the six standard trigonometric functions. It is assumed that the given values ${\displaystyle \theta ,}$ ${\displaystyle r,}$ ${\displaystyle s,}$ ${\displaystyle x,}$ and ${\displaystyle y}$ all lie within appropriate ranges so that the relevant expressions below are well-defined. Note that "for some ${\displaystyle k\in \mathbb {Z} }$ " is just another way of saying "for some integer ${\displaystyle k.}$ "

The symbol ${\displaystyle \,\iff \,}$ is logical equality. The expression "LHS ${\displaystyle \,\iff \,}$ RHS" indicates that either (a) the left hand side (i.e. LHS) and right hand side (i.e. RHS) are both true, or else (b) the left hand side and right hand side are both false; there is no option (c) (e.g. it is not possible for the LHS statement to be true and also simultaneously for the RHS statement to false), because otherwise "LHS ${\displaystyle \,\iff \,}$ RHS" would not have been written (see this footnote for an example illustrating this concept).

For example, if ${\displaystyle \cos \theta =-1}$ then ${\displaystyle \theta =\pi +2\pi k=-\pi +2\pi (1+k)}$ for some ${\displaystyle k\in \mathbb {Z} .}$ While if ${\displaystyle \sin \theta =\pm 1}$ then ${\textstyle \theta ={\frac {\pi }{2}}+\pi k=-{\frac {\pi }{2}}+\pi (k+1)}$ for some ${\displaystyle k\in \mathbb {Z} ,}$ where ${\displaystyle k}$ will be even if ${\displaystyle \sin \theta =1}$ and it will be odd if ${\displaystyle \sin \theta =-1.}$ The equations ${\displaystyle \sec \theta =-1}$ and ${\displaystyle \csc \theta =\pm 1}$ have the same solutions as ${\displaystyle \cos \theta =-1}$ and ${\displaystyle \sin \theta =\pm 1,}$ respectively. In all equations above except for those just solved (i.e. except for ${\displaystyle \sin }$ / ${\displaystyle \csc \theta =\pm 1}$ and ${\displaystyle \cos }$ / ${\displaystyle \sec \theta =-1}$ ), the integer ${\displaystyle k}$ in the solution's formula is uniquely determined by ${\displaystyle \theta }$ (for fixed ${\displaystyle r,s,x,}$ and ${\displaystyle y}$ ).

Detailed example and explanation of the "plus or minus" symbol ${\displaystyle \pm }$
The solutions to ${\displaystyle \cos \theta =x}$ and ${\displaystyle \sec \theta =x}$ involve the "plus or minus" symbol ${\displaystyle \,\pm ,\,}$ whose meaning is now clarified. Only the solution to ${\displaystyle \cos \theta =x}$ will be discussed since the discussion for ${\displaystyle \sec \theta =x}$ is the same. We are given ${\displaystyle x}$ between ${\displaystyle -1\leq x\leq 1}$ and we know that there is an angle ${\displaystyle \theta }$ in some give interval that satisfies ${\displaystyle \cos \theta =x.}$ We want to find this ${\displaystyle \theta .}$ The formula for the solution involves:

If ${\displaystyle \,\arccos x=0\,}$ (which only happens when ${\displaystyle x=1}$ ) then ${\displaystyle \,+\arccos x=0\,}$ and ${\displaystyle \,-\arccos x=0\,}$ so either way, ${\displaystyle \,\pm \arccos x\,}$ can only be equal to ${\displaystyle 0.}$ But if ${\displaystyle \,\arccos x\neq 0,\,}$ which will now be assumed, then the solution to ${\displaystyle \cos \theta =x,}$ which is written above as is shorthand for the following statement:
Either £#ul#££#li#£ ${\displaystyle \,\theta =\arccos x+2\pi k\,}$ for some integer ${\displaystyle k,}$
or else£#/li#£ £#li#£ ${\displaystyle \,\theta =-\arccos x+2\pi k\,}$ for some integer ${\displaystyle k.}$ £#/li#££#/ul#£
Because ${\displaystyle \,\arccos x\neq 0\,}$ and ${\displaystyle \,0<\arccos x\leq \pi \,}$ exactly one of these two equalities can hold. Additional information about ${\displaystyle \theta }$ is needed to determine which one holds. For example, suppose that ${\displaystyle x=0}$ and that all that is known about ${\displaystyle \theta }$ is that ${\displaystyle \,-\pi \leq \theta \leq \pi \,}$ (and nothing more is known). Then

and moreover, in this particular case ${\displaystyle k=0}$ (for both the ${\displaystyle \,+\,}$ case and the ${\displaystyle \,-\,}$ case) and so consequently, This means that ${\displaystyle \theta }$ could be either ${\displaystyle \,\pi /2\,}$ or ${\displaystyle \,-\pi /2.}$ Without additional information it is not possible to determine which of these values ${\displaystyle \theta }$ has. An example of some additional information that could determine the value of ${\displaystyle \theta }$ would be knowing that the angle is above the ${\displaystyle x}$ -axis(in which case ${\displaystyle \theta =\pi /2}$ ) or alternatively, knowing that it is below the ${\displaystyle x}$ -axis(in which case ${\displaystyle \theta =-\pi /2}$ ).
Transforming equations

The equations above can be transformed by using the reflection and shift identities:

These formulas imply, in particular, that the following hold:

where swapping ${\displaystyle \sin \leftrightarrow \csc ,}$ swapping ${\displaystyle \cos \leftrightarrow \sec ,}$ and swapping ${\displaystyle \tan \leftrightarrow \cot }$ gives the analogous equations for ${\displaystyle \csc ,\sec ,{\text{ and }}\cot ,}$ respectively.
So for example, by using the equality ${\textstyle \sin \left({\frac {\pi }{2}}-\theta \right)=\cos \theta ,}$ the equation ${\displaystyle \cos \theta =x}$ can be transformed into ${\textstyle \sin \left({\frac {\pi }{2}}-\theta \right)=x,}$ which allows for the solution to the equation ${\displaystyle \;\sin \varphi =x\;}$ (where ${\textstyle \varphi :={\frac {\pi }{2}}-\theta }$ ) to be used; that solution being: ${\displaystyle \varphi =(-1)^{k}\arcsin(x)+\pi k\;{\text{ for some }}k\in \mathbb {Z} ,}$ which becomes:

where using the fact that ${\displaystyle (-1)^{k}=(-1)^{-k}}$ and substituting ${\displaystyle h:=-k}$ proves that another solution to ${\displaystyle \;\cos \theta =x\;}$ is: The substitution ${\displaystyle \;\arcsin x={\frac {\pi }{2}}-\arccos x\;}$ may be used express the right hand side of the above formula in terms of ${\displaystyle \;\arccos x\;}$ instead of ${\displaystyle \;\arcsin x.\;}$
£#h5#£Equal identical trigonometric functions£#/h5#£
The table below shows how two angles ${\displaystyle \theta }$ and ${\displaystyle \varphi }$ must be related if their values under a given trigonometric function are equal or negatives of each other.


£#h5#£Relationships between trigonometric functions and inverse trigonometric functions£#/h5#£
Trigonometric functions of inverse trigonometric functions are tabulated below. A quick way to derive them is by considering the geometry of a right-angled triangle, with one side of length 1 and another side of length ${\displaystyle x,}$ then applying the Pythagorean theorem and definitions of the trigonometric ratios. Purely algebraic derivations are longer. It is worth noting that for arcsecant and arccosecant, the diagram assumes that ${\displaystyle x}$ is positive, and thus the result has to be corrected through the use of absolute values and the signum (sgn) operation.


£#h5#£Relationships among the inverse trigonometric functions£#/h5#£
Complementary angles:

${\displaystyle {\begin{aligned}\arccos(x)&={\frac {\pi }{2}}-\arcsin(x)\\[0.5em]\operatorname {arccot}(x)&={\frac {\pi }{2}}-\arctan(x)\\[0.5em]\operatorname {arccsc}(x)&={\frac {\pi }{2}}-\operatorname {arcsec}(x)\end{aligned}}}$
Negative arguments:

${\displaystyle {\begin{aligned}\arcsin(-x)&=-\arcsin(x)\\\arccos(-x)&=\pi -\arccos(x)\\\arctan(-x)&=-\arctan(x)\\\operatorname {arccot}(-x)&=\pi -\operatorname {arccot}(x)\\\operatorname {arcsec}(-x)&=\pi -\operatorname {arcsec}(x)\\\operatorname {arccsc}(-x)&=-\operatorname {arccsc}(x)\end{aligned}}}$
Reciprocal arguments:

${\displaystyle {\begin{aligned}\arccos \left({\frac {1}{x}}\right)&=\operatorname {arcsec}(x)\\[0.3em]\arcsin \left({\frac {1}{x}}\right)&=\operatorname {arccsc}(x)\\[0.3em]\arctan \left({\frac {1}{x}}\right)&={\frac {\pi }{2}}-\arctan(x)=\operatorname {arccot}(x)\,,{\text{ if }}x>0\\[0.3em]\arctan \left({\frac {1}{x}}\right)&=-{\frac {\pi }{2}}-\arctan(x)=\operatorname {arccot}(x)-\pi \,,{\text{ if }}x<0\\[0.3em]\operatorname {arccot} \left({\frac {1}{x}}\right)&={\frac {\pi }{2}}-\operatorname {arccot}(x)=\arctan(x)\,,{\text{ if }}x>0\\[0.3em]\operatorname {arccot} \left({\frac {1}{x}}\right)&={\frac {3\pi }{2}}-\operatorname {arccot}(x)=\pi +\arctan(x)\,,{\text{ if }}x<0\\[0.3em]\operatorname {arcsec} \left({\frac {1}{x}}\right)&=\arccos(x)\\[0.3em]\operatorname {arccsc} \left({\frac {1}{x}}\right)&=\arcsin(x)\end{aligned}}}$
Useful identities if one only has a fragment of a sine table:

${\displaystyle {\begin{aligned}\arccos(x)&=\arcsin \left({\sqrt {1-x^{2}}}\right)\,,{\text{ if }}0\leq x\leq 1{\text{ , from which you get }}\\\arccos &\left({\frac {1-x^{2}}{1+x^{2}}}\right)=\arcsin \left({\frac {2x}{1+x^{2}}}\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin &\left({\sqrt {1-x^{2}}}\right)={\frac {\pi }{2}}-\operatorname {sgn}(x)\arcsin(x)\\\arccos(x)&={\frac {1}{2}}\arccos \left(2x^{2}-1\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin(x)&={\frac {1}{2}}\arccos \left(1-2x^{2}\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin(x)&=\arctan \left({\frac {x}{\sqrt {1-x^{2}}}}\right)\\\arccos(x)&=\arctan \left({\frac {\sqrt {1-x^{2}}}{x}}\right)\\\arctan(x)&=\arcsin \left({\frac {x}{\sqrt {1+x^{2}}}}\right)\\\operatorname {arccot}(x)&=\arccos \left({\frac {x}{\sqrt {1+x^{2}}}}\right)\end{aligned}}}$
Whenever the square root of a complex number is used here, we choose the root with the positive real part (or positive imaginary part if the square was negative real).

A useful form that follows directly from the table above is

${\displaystyle \arctan \left(x\right)=\arccos \left({\sqrt {\frac {1}{1+x^{2}}}}\right)\,,{\text{ if }}x\geq 0}$ .
It is obtained by recognizing that ${\displaystyle \cos \left(\arctan \left(x\right)\right)={\sqrt {\frac {1}{1+x^{2}}}}=\cos \left(\arccos \left({\sqrt {\frac {1}{1+x^{2}}}}\right)\right)}$ .

From the half-angle formula, ${\displaystyle \tan \left({\tfrac {\theta }{2}}\right)={\tfrac {\sin(\theta )}{1+\cos(\theta )}}}$ , we get:

${\displaystyle {\begin{aligned}\arcsin(x)&=2\arctan \left({\frac {x}{1+{\sqrt {1-x^{2}}}}}\right)\\[0.5em]\arccos(x)&=2\arctan \left({\frac {\sqrt {1-x^{2}}}{1+x}}\right)\,,{\text{ if }}-1<x\leq 1\\[0.5em]\arctan(x)&=2\arctan \left({\frac {x}{1+{\sqrt {1+x^{2}}}}}\right)\end{aligned}}}$

£#h5#£Arctangent addition formula£#/h5#£
${\displaystyle \arctan(u)\pm \arctan(v)=\arctan \left({\frac {u\pm v}{1\mp uv}}\right){\pmod {\pi }}\,,\quad uv\neq 1\,.}$
This is derived from the tangent addition formula

${\displaystyle \tan(\alpha \pm \beta )={\frac {\tan(\alpha )\pm \tan(\beta )}{1\mp \tan(\alpha )\tan(\beta )}}\,,}$
by letting

${\displaystyle \alpha =\arctan(u)\,,\quad \beta =\arctan(v)\,.}$

£#h5#£In calculus£#/h5#£
£#h5#£Derivatives of inverse trigonometric functions£#/h5#£
The derivatives for complex values of z are as follows:

${\displaystyle {\begin{aligned}{\frac {d}{dz}}\arcsin(z)&{}={\frac {1}{\sqrt {1-z^{2}}}}\;;&z&{}\neq -1,+1\\{\frac {d}{dz}}\arccos(z)&{}=-{\frac {1}{\sqrt {1-z^{2}}}}\;;&z&{}\neq -1,+1\\{\frac {d}{dz}}\arctan(z)&{}={\frac {1}{1+z^{2}}}\;;&z&{}\neq -i,+i\\{\frac {d}{dz}}\operatorname {arccot}(z)&{}=-{\frac {1}{1+z^{2}}}\;;&z&{}\neq -i,+i\\{\frac {d}{dz}}\operatorname {arcsec}(z)&{}={\frac {1}{z^{2}{\sqrt {1-{\frac {1}{z^{2}}}}}}}\;;&z&{}\neq -1,0,+1\\{\frac {d}{dz}}\operatorname {arccsc}(z)&{}=-{\frac {1}{z^{2}{\sqrt {1-{\frac {1}{z^{2}}}}}}}\;;&z&{}\neq -1,0,+1\end{aligned}}}$
Only for real values of x:

${\displaystyle {\begin{aligned}{\frac {d}{dx}}\operatorname {arcsec}(x)&{}={\frac {1}{|x|{\sqrt {x^{2}-1}}}}\;;&|x|>1\\{\frac {d}{dx}}\operatorname {arccsc}(x)&{}=-{\frac {1}{|x|{\sqrt {x^{2}-1}}}}\;;&|x|>1\end{aligned}}}$
For a sample derivation: if ${\displaystyle \theta =\arcsin(x)}$ , we get:

${\displaystyle {\frac {d\arcsin(x)}{dx}}={\frac {d\theta }{d\sin(\theta )}}={\frac {d\theta }{\cos(\theta )\,d\theta }}={\frac {1}{\cos(\theta )}}={\frac {1}{\sqrt {1-\sin ^{2}(\theta )}}}={\frac {1}{\sqrt {1-x^{2}}}}}$

£#h5#£Expression as definite integrals£#/h5#£
Integrating the derivative and fixing the value at one point gives an expression for the inverse trigonometric function as a definite integral:

${\displaystyle {\begin{aligned}\arcsin(x)&{}=\int _{0}^{x}{\frac {1}{\sqrt {1-z^{2}}}}\,dz\;,&|x|&{}\leq 1\\\arccos(x)&{}=\int _{x}^{1}{\frac {1}{\sqrt {1-z^{2}}}}\,dz\;,&|x|&{}\leq 1\\\arctan(x)&{}=\int _{0}^{x}{\frac {1}{z^{2}+1}}\,dz\;,\\\operatorname {arccot}(x)&{}=\int _{x}^{\infty }{\frac {1}{z^{2}+1}}\,dz\;,\\\operatorname {arcsec}(x)&{}=\int _{1}^{x}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz=\pi +\int _{-x}^{-1}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz\;,&x&{}\geq 1\\\operatorname {arccsc}(x)&{}=\int _{x}^{\infty }{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz=\int _{-\infty }^{-x}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz\;,&x&{}\geq 1\\\end{aligned}}}$
When x equals 1, the integrals with limited domains are improper integrals, but still well-defined.


£#h5#£Infinite series£#/h5#£
Similar to the sine and cosine functions, the inverse trigonometric functions can also be calculated using power series, as follows. For arcsine, the series can be derived by expanding its derivative, ${\textstyle {\tfrac {1}{\sqrt {1-z^{2}}}}}$ , as a binomial series, and integrating term by term (using the integral definition as above). The series for arctangent can similarly be derived by expanding its derivative ${\textstyle {\frac {1}{1+z^{2}}}}$ in a geometric series, and applying the integral definition above (see Leibniz series).

${\displaystyle {\begin{aligned}\arcsin(z)&=z+\left({\frac {1}{2}}\right){\frac {z^{3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {z^{5}}{5}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {z^{7}}{7}}+\cdots \\[5pt]&=\sum _{n=0}^{\infty }{\frac {(2n-1)!!}{(2n)!!}}{\frac {z^{2n+1}}{2n+1}}\\[5pt]&=\sum _{n=0}^{\infty }{\frac {(2n)!}{(2^{n}n!)^{2}}}{\frac {z^{2n+1}}{2n+1}}\,;\qquad |z|\leq 1\end{aligned}}}$
${\displaystyle \arctan(z)=z-{\frac {z^{3}}{3}}+{\frac {z^{5}}{5}}-{\frac {z^{7}}{7}}+\cdots =\sum _{n=0}^{\infty }{\frac {(-1)^{n}z^{2n+1}}{2n+1}}\,;\qquad |z|\leq 1\qquad z\neq i,-i}$
Series for the other inverse trigonometric functions can be given in terms of these according to the relationships given above. For example, ${\displaystyle \arccos(x)=\pi /2-\arcsin(x)}$ , ${\displaystyle \operatorname {arccsc}(x)=\arcsin(1/x)}$ , and so on. Another series is given by:

${\displaystyle 2\left(\arcsin \left({\frac {x}{2}}\right)\right)^{2}=\sum _{n=1}^{\infty }{\frac {x^{2n}}{n^{2}{\binom {2n}{n}}}}.}$
Leonhard Euler found a series for the arctangent that converges more quickly than its Taylor series:

${\displaystyle \arctan(z)={\frac {z}{1+z^{2}}}\sum _{n=0}^{\infty }\prod _{k=1}^{n}{\frac {2kz^{2}}{(2k+1)(1+z^{2})}}.}$
(The term in the sum for n = 0 is the empty product, so is 1.)

Alternatively, this can be expressed as

${\displaystyle \arctan(z)=\sum _{n=0}^{\infty }{\frac {2^{2n}(n!)^{2}}{(2n+1)!}}{\frac {z^{2n+1}}{(1+z^{2})^{n+1}}}.}$
Another series for the arctangent function is given by

${\displaystyle \arctan(z)=i\sum _{n=1}^{\infty }{\frac {1}{2n-1}}\left({\frac {1}{(1+2i/z)^{2n-1}}}-{\frac {1}{(1-2i/z)^{2n-1}}}\right),}$
where ${\displaystyle i={\sqrt {-1}}}$ is the imaginary unit.


£#h5#£Continued fractions for arctangent£#/h5#£
Two alternatives to the power series for arctangent are these generalized continued fractions:

${\displaystyle \arctan(z)={\frac {z}{1+{\cfrac {(1z)^{2}}{3-1z^{2}+{\cfrac {(3z)^{2}}{5-3z^{2}+{\cfrac {(5z)^{2}}{7-5z^{2}+{\cfrac {(7z)^{2}}{9-7z^{2}+\ddots }}}}}}}}}}={\frac {z}{1+{\cfrac {(1z)^{2}}{3+{\cfrac {(2z)^{2}}{5+{\cfrac {(3z)^{2}}{7+{\cfrac {(4z)^{2}}{9+\ddots }}}}}}}}}}}$
The second of these is valid in the cut complex plane. There are two cuts, from −i to the point at infinity, going down the imaginary axis, and from i to the point at infinity, going up the same axis. It works best for real numbers running from −1 to 1. The partial denominators are the odd natural numbers, and the partial numerators (after the first) are just (nz)2, with each perfect square appearing once. The first was developed by Leonhard Euler; the second by Carl Friedrich Gauss utilizing the Gaussian hypergeometric series.


£#h5#£Indefinite integrals of inverse trigonometric functions£#/h5#£
For real and complex values of z:

${\displaystyle {\begin{aligned}\int \arcsin(z)\,dz&{}=z\,\arcsin(z)+{\sqrt {1-z^{2}}}+C\\\int \arccos(z)\,dz&{}=z\,\arccos(z)-{\sqrt {1-z^{2}}}+C\\\int \arctan(z)\,dz&{}=z\,\arctan(z)-{\frac {1}{2}}\ln \left(1+z^{2}\right)+C\\\int \operatorname {arccot}(z)\,dz&{}=z\,\operatorname {arccot}(z)+{\frac {1}{2}}\ln \left(1+z^{2}\right)+C\\\int \operatorname {arcsec}(z)\,dz&{}=z\,\operatorname {arcsec}(z)-\ln \left[z\left(1+{\sqrt {\frac {z^{2}-1}{z^{2}}}}\right)\right]+C\\\int \operatorname {arccsc}(z)\,dz&{}=z\,\operatorname {arccsc}(z)+\ln \left[z\left(1+{\sqrt {\frac {z^{2}-1}{z^{2}}}}\right)\right]+C\end{aligned}}}$
For real x ≥ 1:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\ln \left(x+{\sqrt {x^{2}-1}}\right)+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\ln \left(x+{\sqrt {x^{2}-1}}\right)+C\end{aligned}}}$
For all real x not between -1 and 1:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\operatorname {sgn}(x)\ln \left|x+{\sqrt {x^{2}-1}}\right|+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\operatorname {sgn}(x)\ln \left|x+{\sqrt {x^{2}-1}}\right|+C\end{aligned}}}$
The absolute value is necessary to compensate for both negative and positive values of the arcsecant and arccosecant functions. The signum function is also necessary due to the absolute values in the derivatives of the two functions, which create two different solutions for positive and negative values of x. These can be further simplified using the logarithmic definitions of the inverse hyperbolic functions:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\operatorname {arcosh} (|x|)+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\operatorname {arcosh} (|x|)+C\\\end{aligned}}}$
The absolute value in the argument of the arcosh function creates a negative half of its graph, making it identical to the signum logarithmic function shown above.

All of these antiderivatives can be derived using integration by parts and the simple derivative forms shown above.


£#h5#£Example£#/h5#£
Using ${\displaystyle \int u\,dv=uv-\int v\,du}$ (i.e. integration by parts), set

${\displaystyle {\begin{aligned}u&=\arcsin(x)&dv&=dx\\du&={\frac {dx}{\sqrt {1-x^{2}}}}&v&=x\end{aligned}}}$
Then

${\displaystyle \int \arcsin(x)\,dx=x\arcsin(x)-\int {\frac {x}{\sqrt {1-x^{2}}}}\,dx,}$
which by the simple substitution ${\displaystyle w=1-x^{2},\ dw=-2x\,dx}$ yields the final result:

${\displaystyle \int \arcsin(x)\,dx=x\arcsin(x)+{\sqrt {1-x^{2}}}+C}$

£#h5#£Extension to complex plane£#/h5#£
Since the inverse trigonometric functions are analytic functions, they can be extended from the real line to the complex plane. This results in functions with multiple sheets and branch points. One possible way of defining the extension is:

${\displaystyle \arctan(z)=\int _{0}^{z}{\frac {dx}{1+x^{2}}}\quad z\neq -i,+i}$
where the part of the imaginary axis which does not lie strictly between the branch points (−i and +i) is the branch cut between the principal sheet and other sheets. The path of the integral must not cross a branch cut. For z not on a branch cut, a straight line path from 0 to z is such a path. For z on a branch cut, the path must approach from Re[x] > 0 for the upper branch cut and from Re[x] < 0 for the lower branch cut.

The arcsine function may then be defined as:

${\displaystyle \arcsin(z)=\arctan \left({\frac {z}{\sqrt {1-z^{2}}}}\right)\quad z\neq -1,+1}$
where (the square-root function has its cut along the negative real axis and) the part of the real axis which does not lie strictly between −1 and +1 is the branch cut between the principal sheet of arcsin and other sheets;

${\displaystyle \arccos(z)={\frac {\pi }{2}}-\arcsin(z)\quad z\neq -1,+1}$
which has the same cut as arcsin;

${\displaystyle \operatorname {arccot}(z)={\frac {\pi }{2}}-\arctan(z)\quad z\neq -i,i}$
which has the same cut as arctan;

${\displaystyle \operatorname {arcsec}(z)=\arccos \left({\frac {1}{z}}\right)\quad z\neq -1,0,+1}$
where the part of the real axis between −1 and +1 inclusive is the cut between the principal sheet of arcsec and other sheets;

${\displaystyle \operatorname {arccsc}(z)=\arcsin \left({\frac {1}{z}}\right)\quad z\neq -1,0,+1}$
which has the same cut as arcsec.


£#h5#£Logarithmic forms£#/h5#£
These functions may also be expressed using complex logarithms. This extends their domains to the complex plane in a natural fashion. The following identities for principal values of the functions hold everywhere that they are defined, even on their branch cuts.

${\displaystyle {\begin{aligned}\arcsin(z)&{}=-i\ln \left({\sqrt {1-z^{2}}}+iz\right)=i\ln \left({\sqrt {1-z^{2}}}-iz\right)&{}=\operatorname {arccsc} \left({\frac {1}{z}}\right)\\[10pt]\arccos(z)&{}=-i\ln \left(i{\sqrt {1-z^{2}}}+z\right)={\frac {\pi }{2}}-\arcsin(z)&{}=\operatorname {arcsec} \left({\frac {1}{z}}\right)\\[10pt]\arctan(z)&{}=-{\frac {i}{2}}\ln \left({\frac {i-z}{i+z}}\right)=-{\frac {i}{2}}\ln \left({\frac {1+iz}{1-iz}}\right)&{}=\operatorname {arccot} \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arccot}(z)&{}=-{\frac {i}{2}}\ln \left({\frac {z+i}{z-i}}\right)=-{\frac {i}{2}}\ln \left({\frac {iz-1}{iz+1}}\right)&{}=\arctan \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arcsec}(z)&{}=-i\ln \left(i{\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {1}{z}}\right)={\frac {\pi }{2}}-\operatorname {arccsc}(z)&{}=\arccos \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arccsc}(z)&{}=-i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {i}{z}}\right)=i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}-{\frac {i}{z}}\right)&{}=\arcsin \left({\frac {1}{z}}\right)\end{aligned}}}$

£#h5#£Generalization£#/h5#£
Because all of the inverse trigonometric functions output an angle of a right triangle, they can be generalized by using Euler's formula to form a right triangle in the complex plane. Algebraically, this gives us:

${\displaystyle ce^{\theta i}=c\cos(\theta )+ci\sin(\theta )}$
or

${\displaystyle ce^{\theta i}=a+bi}$
where ${\displaystyle a}$ is the adjacent side, ${\displaystyle b}$ is the opposite side, and ${\displaystyle c}$ is the hypotenuse. From here, we can solve for ${\displaystyle \theta }$ .

${\displaystyle {\begin{aligned}e^{\ln(c)+\theta i}&=a+bi\\\ln c+\theta i&=\ln(a+bi)\\\theta &=\operatorname {Im} \left(\ln(a+bi)\right)\end{aligned}}}$
or

${\displaystyle \theta =-i\ln \left({\frac {a+bi}{c}}\right)=i\ln \left({\frac {c}{a+bi}}\right)}$
Simply taking the imaginary part works for any real-valued ${\displaystyle a}$ and ${\displaystyle b}$ , but if ${\displaystyle a}$ or ${\displaystyle b}$ is complex-valued, we have to use the final equation so that the real part of the result isn't excluded. Since the length of the hypotenuse doesn't change the angle, ignoring the real part of ${\displaystyle \ln(a+bi)}$ also removes ${\displaystyle c}$ from the equation. In the final equation, we see that the angle of the triangle in the complex plane can be found by inputting the lengths of each side. By setting one of the three sides equal to 1 and one of the remaining sides equal to our input ${\displaystyle z}$ , we obtain a formula for one of the inverse trig functions, for a total of six equations. Because the inverse trig functions require only one input, we must put the final side of the triangle in terms of the other two using the Pythagorean Theorem relation

${\displaystyle a^{2}+b^{2}=c^{2}}$
The table below shows the values of a, b, and c for each of the inverse trig functions and the equivalent expressions for ${\displaystyle \theta }$ that result from plugging the values into the equations above and simplifying.

${\displaystyle {\begin{aligned}&a&&b&&c&&\theta &&\theta _{\text{simplified}}&&\theta _{a,b\in \mathbb {R} }\\\arcsin(z)\ \ &{\sqrt {1-z^{2}}}&&z&&1&&-i\ln \left({\frac {{\sqrt {1-z^{2}}}+zi}{1}}\right)&&=-i\ln \left({\sqrt {1-z^{2}}}+zi\right)&&\operatorname {Im} \left(\ln \left({\sqrt {1-z^{2}}}+zi\right)\right)\\\arccos(z)\ \ &z&&{\sqrt {1-z^{2}}}&&1&&-i\ln \left({\frac {z+i{\sqrt {1-z^{2}}}}{1}}\right)&&=-i\ln \left(z+{\sqrt {z^{2}-1}}\right)&&\operatorname {Im} \left(\ln \left(z+{\sqrt {z^{2}-1}}\right)\right)\\\arctan(z)\ \ &1&&z&&{\sqrt {1+z^{2}}}&&i\ln \left({\frac {\sqrt {1+z^{2}}}{1+zi}}\right)&&={\frac {i}{2}}\ln \left({\frac {i+z}{i-z}}\right)&&\operatorname {Im} \left(\ln \left(1+zi\right)\right)\\\operatorname {arccot}(z)\ \ &z&&1&&{\sqrt {z^{2}+1}}&&i\ln \left({\frac {\sqrt {z^{2}+1}}{z+i}}\right)&&={\frac {i}{2}}\ln \left({\frac {z-i}{z+i}}\right)&&\operatorname {Im} \left(\ln \left(z+i\right)\right)\\\operatorname {arcsec}(z)\ \ &1&&{\sqrt {z^{2}-1}}&&z&&-i\ln \left({\frac {1+i{\sqrt {z^{2}-1}}}{z}}\right)&&=-i\ln \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z^{2}}}-1}}\right)&&\operatorname {Im} \left(\ln \left(1+{\sqrt {1-z^{2}}}\right)\right)\\\operatorname {arccsc}(z)\ \ &{\sqrt {z^{2}-1}}&&1&&z&&-i\ln \left({\frac {{\sqrt {z^{2}-1}}+i}{z}}\right)&&=-i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {i}{z}}\right)&&\operatorname {Im} \left(\ln \left({\sqrt {z^{2}-1}}+i\right)\right)\\\end{aligned}}}$
In this sense, all of the inverse trig functions can be thought of as specific cases of the complex-valued log function. Since this definition works for any complex-valued ${\displaystyle z}$ , this definition allows for hyperbolic angles as outputs and can be used to further define the inverse hyperbolic functions. Elementary proofs of the relations may also proceed via expansion to exponential forms of the trigonometric functions.


£#h5#£Example proof£#/h5#£
${\displaystyle {\begin{aligned}\sin(\phi )&=z\\\phi &=\arcsin(z)\end{aligned}}}$
Using the exponential definition of sine, and letting ${\displaystyle \xi =e^{\phi i},}$

${\displaystyle {\begin{aligned}z&={\frac {e^{\phi i}-e^{-\phi i}}{2i}}\\[10mu]2iz&=\xi -{\frac {1}{\xi }}\\[5mu]0&=\xi ^{2}-2i\xi z-1\\[5mu]\xi &=iz\pm {\sqrt {1-z^{2}}}\\[5mu]\phi &=-i\ln \left(iz\pm {\sqrt {1-z^{2}}}\right)\end{aligned}}}$
(the positive branch is chosen)

${\displaystyle \phi =\arcsin(z)=-i\ln \left(iz+{\sqrt {1-z^{2}}}\right)}$

£#h5#£Applications£#/h5#£
£#h5#£Finding the angle of a right triangle£#/h5#£
Inverse trigonometric functions are useful when trying to determine the remaining two angles of a right triangle when the lengths of the sides of the triangle are known. Recalling the right-triangle definitions of sine and cosine, it follows that

${\displaystyle \theta =\arcsin \left({\frac {\text{opposite}}{\text{hypotenuse}}}\right)=\arccos \left({\frac {\text{adjacent}}{\text{hypotenuse}}}\right).}$
Often, the hypotenuse is unknown and would need to be calculated before using arcsine or arccosine using the Pythagorean Theorem: ${\displaystyle a^{2}+b^{2}=h^{2}}$ where ${\displaystyle h}$ is the length of the hypotenuse. Arctangent comes in handy in this situation, as the length of the hypotenuse is not needed.

${\displaystyle \theta =\arctan \left({\frac {\text{opposite}}{\text{adjacent}}}\right)\,.}$
For example, suppose a roof drops 8 feet as it runs out 20 feet. The roof makes an angle θ with the horizontal, where θ may be computed as follows:

${\displaystyle \theta =\arctan \left({\frac {\text{opposite}}{\text{adjacent}}}\right)=\arctan \left({\frac {\text{rise}}{\text{run}}}\right)=\arctan \left({\frac {8}{20}}\right)\approx 21.8^{\circ }\,.}$

£#h5#£In computer science and engineering£#/h5#£
£#h5#£Two-argument variant of arctangent£#/h5#£

The two-argument atan2 function computes the arctangent of y / x given y and x, but with a range of (−π, π]. In other words, atan2(y, x) is the angle between the positive x-axis of a plane and the point (x, y) on it, with positive sign for counter-clockwise angles (upper half-plane, y > 0), and negative sign for clockwise angles (lower half-plane, y < 0). It was first introduced in many computer programming languages, but it is now also common in other fields of science and engineering.

In terms of the standard arctan function, that is with range of (−π/2, π/2), it can be expressed as follows:

${\displaystyle \operatorname {atan2} (y,x)={\begin{cases}\arctan \left({\frac {y}{x}}\right)&\quad x>0\\\arctan \left({\frac {y}{x}}\right)+\pi &\quad y\geq 0,\;x<0\\\arctan \left({\frac {y}{x}}\right)-\pi &\quad y<0,\;x<0\\{\frac {\pi }{2}}&\quad y>0,\;x=0\\-{\frac {\pi }{2}}&\quad y<0,\;x=0\\{\text{undefined}}&\quad y=0,\;x=0\end{cases}}}$
It also equals the principal value of the argument of the complex number x + iy.

This limited version of the function above may also be defined using the tangent half-angle formulae as follows:

${\displaystyle \operatorname {atan2} (y,x)=2\arctan \left({\frac {y}{{\sqrt {x^{2}+y^{2}}}+x}}\right)}$
provided that either x > 0 or y ≠ 0. However this fails if given x ≤ 0 and y = 0 so the expression is unsuitable for computational use.

The above argument order (y, x) seems to be the most common, and in particular is used in ISO standards such as the C programming language, but a few authors may use the opposite convention (x, y) so some caution is warranted. These variations are detailed at atan2.


£#h5#£Arctangent function with location parameter£#/h5#£
In many applications the solution ${\displaystyle y}$ of the equation ${\displaystyle x=\tan(y)}$ is to come as close as possible to a given value ${\displaystyle -\infty <\eta <\infty }$ . The adequate solution is produced by the parameter modified arctangent function

${\displaystyle y=\arctan _{\eta }(x):=\arctan(x)+\pi \,\operatorname {rni} \left({\frac {\eta -\arctan(x)}{\pi }}\right)\,.}$
The function ${\displaystyle \operatorname {rni} }$ rounds to the nearest integer.


£#h5#£Numerical accuracy£#/h5#£
For angles near 0 and π, arccosine is ill-conditioned and will thus calculate the angle with reduced accuracy in a computer implementation (due to the limited number of digits). Similarly, arcsine is inaccurate for angles near −π/2 and π/2.


£#h5#£See also£#/h5#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Abramowitz, Milton; Stegun, Irene A., eds. (1972). Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables. New York: Dover Publications. ISBN 978-0-486-61272-0.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Inverse Tangent". MathWorld.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Trigonometric Functions £#/li#££#/ul#£




£#h3#£Arccosecant£#/h3#£

In mathematics, the inverse trigonometric functions (occasionally also called arcus functions, antitrigonometric functions or cyclometric functions) are the inverse functions of the trigonometric functions (with suitably restricted domains). Specifically, they are the inverses of the sine, cosine, tangent, cotangent, secant, and cosecant functions, and are used to obtain an angle from any of the angle's trigonometric ratios. Inverse trigonometric functions are widely used in engineering, navigation, physics, and geometry.


£#h5#£Notation£#/h5#£
Several notations for the inverse trigonometric functions exist. The most common convention is to name inverse trigonometric functions using an arc- prefix: arcsin(x), arccos(x), arctan(x), etc. (This convention is used throughout this article.) This notation arises from the following geometric relationships: when measuring in radians, an angle of θ radians will correspond to an arc whose length is rθ, where r is the radius of the circle. Thus in the unit circle, "the arc whose cosine is x" is the same as "the angle whose cosine is x", because the length of the arc of the circle in radii is the same as the measurement of the angle in radians. In computer programming languages, the inverse trigonometric functions are often called by the abbreviated forms asin, acos, atan.

The notations sin−1(x), cos−1(x), tan−1(x), etc., as introduced by John Herschel in 1813, are often used as well in English-language sources, much more than the also established sin[−1](x), cos[−1](x), tan[−1](x),—conventions consistent with the notation of an inverse function, that is useful e.g. to define the multivalued version of each inverse trigonometric function: e.g. ${\displaystyle \tan ^{-1}(x)=\{\arctan(x)+\pi k\mid k\in \mathbb {Z} \}}$ . However, this might appear to conflict logically with the common semantics for expressions such as sin2(x) (although only sin2 x, without parentheses, is the really common one), which refer to numeric power rather than function composition, and therefore may result in confusion between multiplicative inverse or reciprocal and compositional inverse. The confusion is somewhat mitigated by the fact that each of the reciprocal trigonometric functions has its own name—for example, (cos(x))−1 = sec(x). Nevertheless, certain authors advise against using it for its ambiguity. Another precarious convention used by a tiny number of authors is to use an uppercase first letter, along with a −1 superscript: Sin−1(x), Cos−1(x), Tan−1(x), etc. Although its intention is to avoid confusion with the multiplicative inverse, which should be represented by sin−1(x), cos−1(x), etc., or, better, by sin−1 x, cos−1 x, etc., it in turn creates yet another major source of ambiguity: especially in light of the fact that many popular high-level programming languages (e.g. Wolfram's Mathematica, and University of Sydney's MAGMA) use those very same capitalised representations for the standard trig functions; whereas others (Python (ie SymPy and NumPy), Matlab, MAPLE etc) use lower-case.

Hence, since 2009, the ISO 80000-2 standard has specified solely the "arc" prefix for the inverse functions.


£#h5#£Basic concepts£#/h5#£
£#h5#£Principal values£#/h5#£
Since none of the six trigonometric functions are one-to-one, they must be restricted in order to have inverse functions. Therefore, the result ranges of the inverse functions are proper (i.e. strict) subsets of the domains of the original functions.

For example, using function in the sense of multivalued functions, just as the square root function ${\displaystyle y={\sqrt {x}}}$ could be defined from ${\displaystyle y^{2}=x,}$ the function ${\displaystyle y=\arcsin(x)}$ is defined so that ${\displaystyle \sin(y)=x.}$ For a given real number ${\displaystyle x,}$ with ${\displaystyle -1\leq x\leq 1,}$ there are multiple (in fact, countably infinitely many) numbers ${\displaystyle y}$ such that ${\displaystyle \sin(y)=x}$ ; for example, ${\displaystyle \sin(0)=0,}$ but also ${\displaystyle \sin(\pi )=0,}$ ${\displaystyle \sin(2\pi )=0,}$ etc. When only one value is desired, the function may be restricted to its principal branch. With this restriction, for each ${\displaystyle x}$ in the domain, the expression ${\displaystyle \arcsin(x)}$ will evaluate only to a single value, called its principal value. These properties apply to all the inverse trigonometric functions.

The principal inverses are listed in the following table.

Note: Some authors define the range of arcsecant to be ${\textstyle (0\leq y<{\frac {\pi }{2}}{\text{ or }}\pi \leq y<{\frac {3\pi }{2}})}$ , because the tangent function is nonnegative on this domain. This makes some computations more consistent. For example, using this range, ${\displaystyle \tan(\operatorname {arcsec}(x))={\sqrt {x^{2}-1}},}$ whereas with the range ${\textstyle (0\leq y<{\frac {\pi }{2}}{\text{ or }}{\frac {\pi }{2}}<y\leq \pi )}$ , we would have to write ${\displaystyle \tan(\operatorname {arcsec}(x))=\pm {\sqrt {x^{2}-1}},}$ since tangent is nonnegative on ${\textstyle 0\leq y<{\frac {\pi }{2}},}$ but nonpositive on ${\textstyle {\frac {\pi }{2}}<y\leq \pi .}$ For a similar reason, the same authors define the range of arccosecant to be ${\textstyle (-\pi <y\leq -{\frac {\pi }{2}}}$ or ${\textstyle 0<y\leq {\frac {\pi }{2}}).}$

If ${\displaystyle x}$ is allowed to be a complex number, then the range of ${\displaystyle y}$ applies only to its real part.

The table below displays names and domains of the inverse trigonometric functions along with the range of their usual principal values in radians.

The symbol ${\displaystyle \mathbb {R} =(-\infty ,\infty )}$ denotes the set of all real numbers and ${\displaystyle \mathbb {Z} =\{\ldots ,\,-2,\,-1,\,0,\,1,\,2,\,\ldots \}}$ denotes the set of all integers. The set of all integer multiples of ${\displaystyle \pi }$ is denoted by

The symbol ${\displaystyle \,\setminus \,}$ denotes set subtraction so that, for instance, ${\displaystyle \mathbb {R} \setminus (-1,1)=(-\infty ,-1]\cup [1,\infty )}$ is the set of points in ${\displaystyle \mathbb {R} }$ (that is, real numbers) that are not in the interval ${\displaystyle (-1,1).}$

The Minkowski sum notation ${\textstyle \pi \mathbb {Z} +(0,\pi )}$ and ${\displaystyle \pi \mathbb {Z} +{\bigl (}{-{\tfrac {\pi }{2}}},{\tfrac {\pi }{2}}{\bigr )}}$ that is used above to concisely write the domains of ${\displaystyle \cot ,\csc ,\tan ,{\text{ and }}\sec }$ is now explained.

Domain of cotangent ${\displaystyle \cot }$ and cosecant ${\displaystyle \csc }$ : The domains of ${\displaystyle \,\cot \,}$ and ${\displaystyle \,\csc \,}$ are the same. They are the set of all angles ${\displaystyle \theta }$ at which ${\displaystyle \sin \theta \neq 0,}$ i.e. all real numbers that are not of the form ${\displaystyle \pi n}$ for some integer ${\displaystyle n,}$

Domain of tangent ${\displaystyle \tan }$ and secant ${\displaystyle \sec }$ : The domains of ${\displaystyle \,\tan \,}$ and ${\displaystyle \,\sec \,}$ are the same. They are the set of all angles ${\displaystyle \theta }$ at which ${\displaystyle \cos \theta \neq 0,}$


£#h5#£Solutions to elementary trigonometric equations£#/h5#£
Each of the trigonometric functions is periodic in the real part of its argument, running through all its values twice in each interval of ${\displaystyle 2\pi }$ :

£#ul#££#li#£Sine and cosecant begin their period at ${\textstyle 2\pi k-{\frac {\pi }{2}}}$ (where ${\displaystyle k}$ is an integer), finish it at ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ and then reverse themselves over ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ to ${\displaystyle 2\pi k+{\frac {3\pi }{2}}.}$ £#/li#£ £#li#£Cosine and secant begin their period at ${\displaystyle 2\pi k,}$ finish it at ${\displaystyle 2\pi k+\pi .}$ and then reverse themselves over ${\displaystyle 2\pi k+\pi }$ to ${\displaystyle 2\pi k+2\pi .}$ £#/li#£ £#li#£Tangent begins its period at ${\textstyle 2\pi k-{\frac {\pi }{2}},}$ finishes it at ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ and then repeats it (forward) over ${\textstyle 2\pi k+{\frac {\pi }{2}}}$ to ${\textstyle 2\pi k+{\frac {3\pi }{2}}.}$ £#/li#£ £#li#£Cotangent begins its period at ${\displaystyle 2\pi k,}$ finishes it at ${\displaystyle 2\pi k+\pi ,}$ and then repeats it (forward) over ${\displaystyle 2\pi k+\pi }$ to ${\displaystyle 2\pi k+2\pi .}$ £#/li#££#/ul#£
This periodicity is reflected in the general inverses, where ${\displaystyle k}$ is some integer.

The following table shows how inverse trigonometric functions may be used to solve equalities involving the six standard trigonometric functions. It is assumed that the given values ${\displaystyle \theta ,}$ ${\displaystyle r,}$ ${\displaystyle s,}$ ${\displaystyle x,}$ and ${\displaystyle y}$ all lie within appropriate ranges so that the relevant expressions below are well-defined. Note that "for some ${\displaystyle k\in \mathbb {Z} }$ " is just another way of saying "for some integer ${\displaystyle k.}$ "

The symbol ${\displaystyle \,\iff \,}$ is logical equality. The expression "LHS ${\displaystyle \,\iff \,}$ RHS" indicates that either (a) the left hand side (i.e. LHS) and right hand side (i.e. RHS) are both true, or else (b) the left hand side and right hand side are both false; there is no option (c) (e.g. it is not possible for the LHS statement to be true and also simultaneously for the RHS statement to false), because otherwise "LHS ${\displaystyle \,\iff \,}$ RHS" would not have been written (see this footnote for an example illustrating this concept).

For example, if ${\displaystyle \cos \theta =-1}$ then ${\displaystyle \theta =\pi +2\pi k=-\pi +2\pi (1+k)}$ for some ${\displaystyle k\in \mathbb {Z} .}$ While if ${\displaystyle \sin \theta =\pm 1}$ then ${\textstyle \theta ={\frac {\pi }{2}}+\pi k=-{\frac {\pi }{2}}+\pi (k+1)}$ for some ${\displaystyle k\in \mathbb {Z} ,}$ where ${\displaystyle k}$ will be even if ${\displaystyle \sin \theta =1}$ and it will be odd if ${\displaystyle \sin \theta =-1.}$ The equations ${\displaystyle \sec \theta =-1}$ and ${\displaystyle \csc \theta =\pm 1}$ have the same solutions as ${\displaystyle \cos \theta =-1}$ and ${\displaystyle \sin \theta =\pm 1,}$ respectively. In all equations above except for those just solved (i.e. except for ${\displaystyle \sin }$ / ${\displaystyle \csc \theta =\pm 1}$ and ${\displaystyle \cos }$ / ${\displaystyle \sec \theta =-1}$ ), the integer ${\displaystyle k}$ in the solution's formula is uniquely determined by ${\displaystyle \theta }$ (for fixed ${\displaystyle r,s,x,}$ and ${\displaystyle y}$ ).

Detailed example and explanation of the "plus or minus" symbol ${\displaystyle \pm }$
The solutions to ${\displaystyle \cos \theta =x}$ and ${\displaystyle \sec \theta =x}$ involve the "plus or minus" symbol ${\displaystyle \,\pm ,\,}$ whose meaning is now clarified. Only the solution to ${\displaystyle \cos \theta =x}$ will be discussed since the discussion for ${\displaystyle \sec \theta =x}$ is the same. We are given ${\displaystyle x}$ between ${\displaystyle -1\leq x\leq 1}$ and we know that there is an angle ${\displaystyle \theta }$ in some give interval that satisfies ${\displaystyle \cos \theta =x.}$ We want to find this ${\displaystyle \theta .}$ The formula for the solution involves:

If ${\displaystyle \,\arccos x=0\,}$ (which only happens when ${\displaystyle x=1}$ ) then ${\displaystyle \,+\arccos x=0\,}$ and ${\displaystyle \,-\arccos x=0\,}$ so either way, ${\displaystyle \,\pm \arccos x\,}$ can only be equal to ${\displaystyle 0.}$ But if ${\displaystyle \,\arccos x\neq 0,\,}$ which will now be assumed, then the solution to ${\displaystyle \cos \theta =x,}$ which is written above as is shorthand for the following statement:
Either £#ul#££#li#£ ${\displaystyle \,\theta =\arccos x+2\pi k\,}$ for some integer ${\displaystyle k,}$
or else£#/li#£ £#li#£ ${\displaystyle \,\theta =-\arccos x+2\pi k\,}$ for some integer ${\displaystyle k.}$ £#/li#££#/ul#£
Because ${\displaystyle \,\arccos x\neq 0\,}$ and ${\displaystyle \,0<\arccos x\leq \pi \,}$ exactly one of these two equalities can hold. Additional information about ${\displaystyle \theta }$ is needed to determine which one holds. For example, suppose that ${\displaystyle x=0}$ and that all that is known about ${\displaystyle \theta }$ is that ${\displaystyle \,-\pi \leq \theta \leq \pi \,}$ (and nothing more is known). Then

and moreover, in this particular case ${\displaystyle k=0}$ (for both the ${\displaystyle \,+\,}$ case and the ${\displaystyle \,-\,}$ case) and so consequently, This means that ${\displaystyle \theta }$ could be either ${\displaystyle \,\pi /2\,}$ or ${\displaystyle \,-\pi /2.}$ Without additional information it is not possible to determine which of these values ${\displaystyle \theta }$ has. An example of some additional information that could determine the value of ${\displaystyle \theta }$ would be knowing that the angle is above the ${\displaystyle x}$ -axis(in which case ${\displaystyle \theta =\pi /2}$ ) or alternatively, knowing that it is below the ${\displaystyle x}$ -axis(in which case ${\displaystyle \theta =-\pi /2}$ ).
Transforming equations

The equations above can be transformed by using the reflection and shift identities:

These formulas imply, in particular, that the following hold:

where swapping ${\displaystyle \sin \leftrightarrow \csc ,}$ swapping ${\displaystyle \cos \leftrightarrow \sec ,}$ and swapping ${\displaystyle \tan \leftrightarrow \cot }$ gives the analogous equations for ${\displaystyle \csc ,\sec ,{\text{ and }}\cot ,}$ respectively.
So for example, by using the equality ${\textstyle \sin \left({\frac {\pi }{2}}-\theta \right)=\cos \theta ,}$ the equation ${\displaystyle \cos \theta =x}$ can be transformed into ${\textstyle \sin \left({\frac {\pi }{2}}-\theta \right)=x,}$ which allows for the solution to the equation ${\displaystyle \;\sin \varphi =x\;}$ (where ${\textstyle \varphi :={\frac {\pi }{2}}-\theta }$ ) to be used; that solution being: ${\displaystyle \varphi =(-1)^{k}\arcsin(x)+\pi k\;{\text{ for some }}k\in \mathbb {Z} ,}$ which becomes:

where using the fact that ${\displaystyle (-1)^{k}=(-1)^{-k}}$ and substituting ${\displaystyle h:=-k}$ proves that another solution to ${\displaystyle \;\cos \theta =x\;}$ is: The substitution ${\displaystyle \;\arcsin x={\frac {\pi }{2}}-\arccos x\;}$ may be used express the right hand side of the above formula in terms of ${\displaystyle \;\arccos x\;}$ instead of ${\displaystyle \;\arcsin x.\;}$
£#h5#£Equal identical trigonometric functions£#/h5#£
The table below shows how two angles ${\displaystyle \theta }$ and ${\displaystyle \varphi }$ must be related if their values under a given trigonometric function are equal or negatives of each other.


£#h5#£Relationships between trigonometric functions and inverse trigonometric functions£#/h5#£
Trigonometric functions of inverse trigonometric functions are tabulated below. A quick way to derive them is by considering the geometry of a right-angled triangle, with one side of length 1 and another side of length ${\displaystyle x,}$ then applying the Pythagorean theorem and definitions of the trigonometric ratios. Purely algebraic derivations are longer. It is worth noting that for arcsecant and arccosecant, the diagram assumes that ${\displaystyle x}$ is positive, and thus the result has to be corrected through the use of absolute values and the signum (sgn) operation.


£#h5#£Relationships among the inverse trigonometric functions£#/h5#£
Complementary angles:

${\displaystyle {\begin{aligned}\arccos(x)&={\frac {\pi }{2}}-\arcsin(x)\\[0.5em]\operatorname {arccot}(x)&={\frac {\pi }{2}}-\arctan(x)\\[0.5em]\operatorname {arccsc}(x)&={\frac {\pi }{2}}-\operatorname {arcsec}(x)\end{aligned}}}$
Negative arguments:

${\displaystyle {\begin{aligned}\arcsin(-x)&=-\arcsin(x)\\\arccos(-x)&=\pi -\arccos(x)\\\arctan(-x)&=-\arctan(x)\\\operatorname {arccot}(-x)&=\pi -\operatorname {arccot}(x)\\\operatorname {arcsec}(-x)&=\pi -\operatorname {arcsec}(x)\\\operatorname {arccsc}(-x)&=-\operatorname {arccsc}(x)\end{aligned}}}$
Reciprocal arguments:

${\displaystyle {\begin{aligned}\arccos \left({\frac {1}{x}}\right)&=\operatorname {arcsec}(x)\\[0.3em]\arcsin \left({\frac {1}{x}}\right)&=\operatorname {arccsc}(x)\\[0.3em]\arctan \left({\frac {1}{x}}\right)&={\frac {\pi }{2}}-\arctan(x)=\operatorname {arccot}(x)\,,{\text{ if }}x>0\\[0.3em]\arctan \left({\frac {1}{x}}\right)&=-{\frac {\pi }{2}}-\arctan(x)=\operatorname {arccot}(x)-\pi \,,{\text{ if }}x<0\\[0.3em]\operatorname {arccot} \left({\frac {1}{x}}\right)&={\frac {\pi }{2}}-\operatorname {arccot}(x)=\arctan(x)\,,{\text{ if }}x>0\\[0.3em]\operatorname {arccot} \left({\frac {1}{x}}\right)&={\frac {3\pi }{2}}-\operatorname {arccot}(x)=\pi +\arctan(x)\,,{\text{ if }}x<0\\[0.3em]\operatorname {arcsec} \left({\frac {1}{x}}\right)&=\arccos(x)\\[0.3em]\operatorname {arccsc} \left({\frac {1}{x}}\right)&=\arcsin(x)\end{aligned}}}$
Useful identities if one only has a fragment of a sine table:

${\displaystyle {\begin{aligned}\arccos(x)&=\arcsin \left({\sqrt {1-x^{2}}}\right)\,,{\text{ if }}0\leq x\leq 1{\text{ , from which you get }}\\\arccos &\left({\frac {1-x^{2}}{1+x^{2}}}\right)=\arcsin \left({\frac {2x}{1+x^{2}}}\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin &\left({\sqrt {1-x^{2}}}\right)={\frac {\pi }{2}}-\operatorname {sgn}(x)\arcsin(x)\\\arccos(x)&={\frac {1}{2}}\arccos \left(2x^{2}-1\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin(x)&={\frac {1}{2}}\arccos \left(1-2x^{2}\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin(x)&=\arctan \left({\frac {x}{\sqrt {1-x^{2}}}}\right)\\\arccos(x)&=\arctan \left({\frac {\sqrt {1-x^{2}}}{x}}\right)\\\arctan(x)&=\arcsin \left({\frac {x}{\sqrt {1+x^{2}}}}\right)\\\operatorname {arccot}(x)&=\arccos \left({\frac {x}{\sqrt {1+x^{2}}}}\right)\end{aligned}}}$
Whenever the square root of a complex number is used here, we choose the root with the positive real part (or positive imaginary part if the square was negative real).

A useful form that follows directly from the table above is

${\displaystyle \arctan \left(x\right)=\arccos \left({\sqrt {\frac {1}{1+x^{2}}}}\right)\,,{\text{ if }}x\geq 0}$ .
It is obtained by recognizing that ${\displaystyle \cos \left(\arctan \left(x\right)\right)={\sqrt {\frac {1}{1+x^{2}}}}=\cos \left(\arccos \left({\sqrt {\frac {1}{1+x^{2}}}}\right)\right)}$ .

From the half-angle formula, ${\displaystyle \tan \left({\tfrac {\theta }{2}}\right)={\tfrac {\sin(\theta )}{1+\cos(\theta )}}}$ , we get:

${\displaystyle {\begin{aligned}\arcsin(x)&=2\arctan \left({\frac {x}{1+{\sqrt {1-x^{2}}}}}\right)\\[0.5em]\arccos(x)&=2\arctan \left({\frac {\sqrt {1-x^{2}}}{1+x}}\right)\,,{\text{ if }}-1<x\leq 1\\[0.5em]\arctan(x)&=2\arctan \left({\frac {x}{1+{\sqrt {1+x^{2}}}}}\right)\end{aligned}}}$

£#h5#£Arctangent addition formula£#/h5#£
${\displaystyle \arctan(u)\pm \arctan(v)=\arctan \left({\frac {u\pm v}{1\mp uv}}\right){\pmod {\pi }}\,,\quad uv\neq 1\,.}$
This is derived from the tangent addition formula

${\displaystyle \tan(\alpha \pm \beta )={\frac {\tan(\alpha )\pm \tan(\beta )}{1\mp \tan(\alpha )\tan(\beta )}}\,,}$
by letting

${\displaystyle \alpha =\arctan(u)\,,\quad \beta =\arctan(v)\,.}$

£#h5#£In calculus£#/h5#£
£#h5#£Derivatives of inverse trigonometric functions£#/h5#£
The derivatives for complex values of z are as follows:

${\displaystyle {\begin{aligned}{\frac {d}{dz}}\arcsin(z)&{}={\frac {1}{\sqrt {1-z^{2}}}}\;;&z&{}\neq -1,+1\\{\frac {d}{dz}}\arccos(z)&{}=-{\frac {1}{\sqrt {1-z^{2}}}}\;;&z&{}\neq -1,+1\\{\frac {d}{dz}}\arctan(z)&{}={\frac {1}{1+z^{2}}}\;;&z&{}\neq -i,+i\\{\frac {d}{dz}}\operatorname {arccot}(z)&{}=-{\frac {1}{1+z^{2}}}\;;&z&{}\neq -i,+i\\{\frac {d}{dz}}\operatorname {arcsec}(z)&{}={\frac {1}{z^{2}{\sqrt {1-{\frac {1}{z^{2}}}}}}}\;;&z&{}\neq -1,0,+1\\{\frac {d}{dz}}\operatorname {arccsc}(z)&{}=-{\frac {1}{z^{2}{\sqrt {1-{\frac {1}{z^{2}}}}}}}\;;&z&{}\neq -1,0,+1\end{aligned}}}$
Only for real values of x:

${\displaystyle {\begin{aligned}{\frac {d}{dx}}\operatorname {arcsec}(x)&{}={\frac {1}{|x|{\sqrt {x^{2}-1}}}}\;;&|x|>1\\{\frac {d}{dx}}\operatorname {arccsc}(x)&{}=-{\frac {1}{|x|{\sqrt {x^{2}-1}}}}\;;&|x|>1\end{aligned}}}$
For a sample derivation: if ${\displaystyle \theta =\arcsin(x)}$ , we get:

${\displaystyle {\frac {d\arcsin(x)}{dx}}={\frac {d\theta }{d\sin(\theta )}}={\frac {d\theta }{\cos(\theta )\,d\theta }}={\frac {1}{\cos(\theta )}}={\frac {1}{\sqrt {1-\sin ^{2}(\theta )}}}={\frac {1}{\sqrt {1-x^{2}}}}}$

£#h5#£Expression as definite integrals£#/h5#£
Integrating the derivative and fixing the value at one point gives an expression for the inverse trigonometric function as a definite integral:

${\displaystyle {\begin{aligned}\arcsin(x)&{}=\int _{0}^{x}{\frac {1}{\sqrt {1-z^{2}}}}\,dz\;,&|x|&{}\leq 1\\\arccos(x)&{}=\int _{x}^{1}{\frac {1}{\sqrt {1-z^{2}}}}\,dz\;,&|x|&{}\leq 1\\\arctan(x)&{}=\int _{0}^{x}{\frac {1}{z^{2}+1}}\,dz\;,\\\operatorname {arccot}(x)&{}=\int _{x}^{\infty }{\frac {1}{z^{2}+1}}\,dz\;,\\\operatorname {arcsec}(x)&{}=\int _{1}^{x}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz=\pi +\int _{-x}^{-1}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz\;,&x&{}\geq 1\\\operatorname {arccsc}(x)&{}=\int _{x}^{\infty }{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz=\int _{-\infty }^{-x}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz\;,&x&{}\geq 1\\\end{aligned}}}$
When x equals 1, the integrals with limited domains are improper integrals, but still well-defined.


£#h5#£Infinite series£#/h5#£
Similar to the sine and cosine functions, the inverse trigonometric functions can also be calculated using power series, as follows. For arcsine, the series can be derived by expanding its derivative, ${\textstyle {\tfrac {1}{\sqrt {1-z^{2}}}}}$ , as a binomial series, and integrating term by term (using the integral definition as above). The series for arctangent can similarly be derived by expanding its derivative ${\textstyle {\frac {1}{1+z^{2}}}}$ in a geometric series, and applying the integral definition above (see Leibniz series).

${\displaystyle {\begin{aligned}\arcsin(z)&=z+\left({\frac {1}{2}}\right){\frac {z^{3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {z^{5}}{5}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {z^{7}}{7}}+\cdots \\[5pt]&=\sum _{n=0}^{\infty }{\frac {(2n-1)!!}{(2n)!!}}{\frac {z^{2n+1}}{2n+1}}\\[5pt]&=\sum _{n=0}^{\infty }{\frac {(2n)!}{(2^{n}n!)^{2}}}{\frac {z^{2n+1}}{2n+1}}\,;\qquad |z|\leq 1\end{aligned}}}$
${\displaystyle \arctan(z)=z-{\frac {z^{3}}{3}}+{\frac {z^{5}}{5}}-{\frac {z^{7}}{7}}+\cdots =\sum _{n=0}^{\infty }{\frac {(-1)^{n}z^{2n+1}}{2n+1}}\,;\qquad |z|\leq 1\qquad z\neq i,-i}$
Series for the other inverse trigonometric functions can be given in terms of these according to the relationships given above. For example, ${\displaystyle \arccos(x)=\pi /2-\arcsin(x)}$ , ${\displaystyle \operatorname {arccsc}(x)=\arcsin(1/x)}$ , and so on. Another series is given by:

${\displaystyle 2\left(\arcsin \left({\frac {x}{2}}\right)\right)^{2}=\sum _{n=1}^{\infty }{\frac {x^{2n}}{n^{2}{\binom {2n}{n}}}}.}$
Leonhard Euler found a series for the arctangent that converges more quickly than its Taylor series:

${\displaystyle \arctan(z)={\frac {z}{1+z^{2}}}\sum _{n=0}^{\infty }\prod _{k=1}^{n}{\frac {2kz^{2}}{(2k+1)(1+z^{2})}}.}$
(The term in the sum for n = 0 is the empty product, so is 1.)

Alternatively, this can be expressed as

${\displaystyle \arctan(z)=\sum _{n=0}^{\infty }{\frac {2^{2n}(n!)^{2}}{(2n+1)!}}{\frac {z^{2n+1}}{(1+z^{2})^{n+1}}}.}$
Another series for the arctangent function is given by

${\displaystyle \arctan(z)=i\sum _{n=1}^{\infty }{\frac {1}{2n-1}}\left({\frac {1}{(1+2i/z)^{2n-1}}}-{\frac {1}{(1-2i/z)^{2n-1}}}\right),}$
where ${\displaystyle i={\sqrt {-1}}}$ is the imaginary unit.


£#h5#£Continued fractions for arctangent£#/h5#£
Two alternatives to the power series for arctangent are these generalized continued fractions:

${\displaystyle \arctan(z)={\frac {z}{1+{\cfrac {(1z)^{2}}{3-1z^{2}+{\cfrac {(3z)^{2}}{5-3z^{2}+{\cfrac {(5z)^{2}}{7-5z^{2}+{\cfrac {(7z)^{2}}{9-7z^{2}+\ddots }}}}}}}}}}={\frac {z}{1+{\cfrac {(1z)^{2}}{3+{\cfrac {(2z)^{2}}{5+{\cfrac {(3z)^{2}}{7+{\cfrac {(4z)^{2}}{9+\ddots }}}}}}}}}}}$
The second of these is valid in the cut complex plane. There are two cuts, from −i to the point at infinity, going down the imaginary axis, and from i to the point at infinity, going up the same axis. It works best for real numbers running from −1 to 1. The partial denominators are the odd natural numbers, and the partial numerators (after the first) are just (nz)2, with each perfect square appearing once. The first was developed by Leonhard Euler; the second by Carl Friedrich Gauss utilizing the Gaussian hypergeometric series.


£#h5#£Indefinite integrals of inverse trigonometric functions£#/h5#£
For real and complex values of z:

${\displaystyle {\begin{aligned}\int \arcsin(z)\,dz&{}=z\,\arcsin(z)+{\sqrt {1-z^{2}}}+C\\\int \arccos(z)\,dz&{}=z\,\arccos(z)-{\sqrt {1-z^{2}}}+C\\\int \arctan(z)\,dz&{}=z\,\arctan(z)-{\frac {1}{2}}\ln \left(1+z^{2}\right)+C\\\int \operatorname {arccot}(z)\,dz&{}=z\,\operatorname {arccot}(z)+{\frac {1}{2}}\ln \left(1+z^{2}\right)+C\\\int \operatorname {arcsec}(z)\,dz&{}=z\,\operatorname {arcsec}(z)-\ln \left[z\left(1+{\sqrt {\frac {z^{2}-1}{z^{2}}}}\right)\right]+C\\\int \operatorname {arccsc}(z)\,dz&{}=z\,\operatorname {arccsc}(z)+\ln \left[z\left(1+{\sqrt {\frac {z^{2}-1}{z^{2}}}}\right)\right]+C\end{aligned}}}$
For real x ≥ 1:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\ln \left(x+{\sqrt {x^{2}-1}}\right)+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\ln \left(x+{\sqrt {x^{2}-1}}\right)+C\end{aligned}}}$
For all real x not between -1 and 1:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\operatorname {sgn}(x)\ln \left|x+{\sqrt {x^{2}-1}}\right|+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\operatorname {sgn}(x)\ln \left|x+{\sqrt {x^{2}-1}}\right|+C\end{aligned}}}$
The absolute value is necessary to compensate for both negative and positive values of the arcsecant and arccosecant functions. The signum function is also necessary due to the absolute values in the derivatives of the two functions, which create two different solutions for positive and negative values of x. These can be further simplified using the logarithmic definitions of the inverse hyperbolic functions:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\operatorname {arcosh} (|x|)+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\operatorname {arcosh} (|x|)+C\\\end{aligned}}}$
The absolute value in the argument of the arcosh function creates a negative half of its graph, making it identical to the signum logarithmic function shown above.

All of these antiderivatives can be derived using integration by parts and the simple derivative forms shown above.


£#h5#£Example£#/h5#£
Using ${\displaystyle \int u\,dv=uv-\int v\,du}$ (i.e. integration by parts), set

${\displaystyle {\begin{aligned}u&=\arcsin(x)&dv&=dx\\du&={\frac {dx}{\sqrt {1-x^{2}}}}&v&=x\end{aligned}}}$
Then

${\displaystyle \int \arcsin(x)\,dx=x\arcsin(x)-\int {\frac {x}{\sqrt {1-x^{2}}}}\,dx,}$
which by the simple substitution ${\displaystyle w=1-x^{2},\ dw=-2x\,dx}$ yields the final result:

${\displaystyle \int \arcsin(x)\,dx=x\arcsin(x)+{\sqrt {1-x^{2}}}+C}$

£#h5#£Extension to complex plane£#/h5#£
Since the inverse trigonometric functions are analytic functions, they can be extended from the real line to the complex plane. This results in functions with multiple sheets and branch points. One possible way of defining the extension is:

${\displaystyle \arctan(z)=\int _{0}^{z}{\frac {dx}{1+x^{2}}}\quad z\neq -i,+i}$
where the part of the imaginary axis which does not lie strictly between the branch points (−i and +i) is the branch cut between the principal sheet and other sheets. The path of the integral must not cross a branch cut. For z not on a branch cut, a straight line path from 0 to z is such a path. For z on a branch cut, the path must approach from Re[x] > 0 for the upper branch cut and from Re[x] < 0 for the lower branch cut.

The arcsine function may then be defined as:

${\displaystyle \arcsin(z)=\arctan \left({\frac {z}{\sqrt {1-z^{2}}}}\right)\quad z\neq -1,+1}$
where (the square-root function has its cut along the negative real axis and) the part of the real axis which does not lie strictly between −1 and +1 is the branch cut between the principal sheet of arcsin and other sheets;

${\displaystyle \arccos(z)={\frac {\pi }{2}}-\arcsin(z)\quad z\neq -1,+1}$
which has the same cut as arcsin;

${\displaystyle \operatorname {arccot}(z)={\frac {\pi }{2}}-\arctan(z)\quad z\neq -i,i}$
which has the same cut as arctan;

${\displaystyle \operatorname {arcsec}(z)=\arccos \left({\frac {1}{z}}\right)\quad z\neq -1,0,+1}$
where the part of the real axis between −1 and +1 inclusive is the cut between the principal sheet of arcsec and other sheets;

${\displaystyle \operatorname {arccsc}(z)=\arcsin \left({\frac {1}{z}}\right)\quad z\neq -1,0,+1}$
which has the same cut as arcsec.


£#h5#£Logarithmic forms£#/h5#£
These functions may also be expressed using complex logarithms. This extends their domains to the complex plane in a natural fashion. The following identities for principal values of the functions hold everywhere that they are defined, even on their branch cuts.

${\displaystyle {\begin{aligned}\arcsin(z)&{}=-i\ln \left({\sqrt {1-z^{2}}}+iz\right)=i\ln \left({\sqrt {1-z^{2}}}-iz\right)&{}=\operatorname {arccsc} \left({\frac {1}{z}}\right)\\[10pt]\arccos(z)&{}=-i\ln \left(i{\sqrt {1-z^{2}}}+z\right)={\frac {\pi }{2}}-\arcsin(z)&{}=\operatorname {arcsec} \left({\frac {1}{z}}\right)\\[10pt]\arctan(z)&{}=-{\frac {i}{2}}\ln \left({\frac {i-z}{i+z}}\right)=-{\frac {i}{2}}\ln \left({\frac {1+iz}{1-iz}}\right)&{}=\operatorname {arccot} \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arccot}(z)&{}=-{\frac {i}{2}}\ln \left({\frac {z+i}{z-i}}\right)=-{\frac {i}{2}}\ln \left({\frac {iz-1}{iz+1}}\right)&{}=\arctan \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arcsec}(z)&{}=-i\ln \left(i{\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {1}{z}}\right)={\frac {\pi }{2}}-\operatorname {arccsc}(z)&{}=\arccos \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arccsc}(z)&{}=-i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {i}{z}}\right)=i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}-{\frac {i}{z}}\right)&{}=\arcsin \left({\frac {1}{z}}\right)\end{aligned}}}$

£#h5#£Generalization£#/h5#£
Because all of the inverse trigonometric functions output an angle of a right triangle, they can be generalized by using Euler's formula to form a right triangle in the complex plane. Algebraically, this gives us:

${\displaystyle ce^{\theta i}=c\cos(\theta )+ci\sin(\theta )}$
or

${\displaystyle ce^{\theta i}=a+bi}$
where ${\displaystyle a}$ is the adjacent side, ${\displaystyle b}$ is the opposite side, and ${\displaystyle c}$ is the hypotenuse. From here, we can solve for ${\displaystyle \theta }$ .

${\displaystyle {\begin{aligned}e^{\ln(c)+\theta i}&=a+bi\\\ln c+\theta i&=\ln(a+bi)\\\theta &=\operatorname {Im} \left(\ln(a+bi)\right)\end{aligned}}}$
or

${\displaystyle \theta =-i\ln \left({\frac {a+bi}{c}}\right)=i\ln \left({\frac {c}{a+bi}}\right)}$
Simply taking the imaginary part works for any real-valued ${\displaystyle a}$ and ${\displaystyle b}$ , but if ${\displaystyle a}$ or ${\displaystyle b}$ is complex-valued, we have to use the final equation so that the real part of the result isn't excluded. Since the length of the hypotenuse doesn't change the angle, ignoring the real part of ${\displaystyle \ln(a+bi)}$ also removes ${\displaystyle c}$ from the equation. In the final equation, we see that the angle of the triangle in the complex plane can be found by inputting the lengths of each side. By setting one of the three sides equal to 1 and one of the remaining sides equal to our input ${\displaystyle z}$ , we obtain a formula for one of the inverse trig functions, for a total of six equations. Because the inverse trig functions require only one input, we must put the final side of the triangle in terms of the other two using the Pythagorean Theorem relation

${\displaystyle a^{2}+b^{2}=c^{2}}$
The table below shows the values of a, b, and c for each of the inverse trig functions and the equivalent expressions for ${\displaystyle \theta }$ that result from plugging the values into the equations above and simplifying.

${\displaystyle {\begin{aligned}&a&&b&&c&&\theta &&\theta _{\text{simplified}}&&\theta _{a,b\in \mathbb {R} }\\\arcsin(z)\ \ &{\sqrt {1-z^{2}}}&&z&&1&&-i\ln \left({\frac {{\sqrt {1-z^{2}}}+zi}{1}}\right)&&=-i\ln \left({\sqrt {1-z^{2}}}+zi\right)&&\operatorname {Im} \left(\ln \left({\sqrt {1-z^{2}}}+zi\right)\right)\\\arccos(z)\ \ &z&&{\sqrt {1-z^{2}}}&&1&&-i\ln \left({\frac {z+i{\sqrt {1-z^{2}}}}{1}}\right)&&=-i\ln \left(z+{\sqrt {z^{2}-1}}\right)&&\operatorname {Im} \left(\ln \left(z+{\sqrt {z^{2}-1}}\right)\right)\\\arctan(z)\ \ &1&&z&&{\sqrt {1+z^{2}}}&&i\ln \left({\frac {\sqrt {1+z^{2}}}{1+zi}}\right)&&={\frac {i}{2}}\ln \left({\frac {i+z}{i-z}}\right)&&\operatorname {Im} \left(\ln \left(1+zi\right)\right)\\\operatorname {arccot}(z)\ \ &z&&1&&{\sqrt {z^{2}+1}}&&i\ln \left({\frac {\sqrt {z^{2}+1}}{z+i}}\right)&&={\frac {i}{2}}\ln \left({\frac {z-i}{z+i}}\right)&&\operatorname {Im} \left(\ln \left(z+i\right)\right)\\\operatorname {arcsec}(z)\ \ &1&&{\sqrt {z^{2}-1}}&&z&&-i\ln \left({\frac {1+i{\sqrt {z^{2}-1}}}{z}}\right)&&=-i\ln \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z^{2}}}-1}}\right)&&\operatorname {Im} \left(\ln \left(1+{\sqrt {1-z^{2}}}\right)\right)\\\operatorname {arccsc}(z)\ \ &{\sqrt {z^{2}-1}}&&1&&z&&-i\ln \left({\frac {{\sqrt {z^{2}-1}}+i}{z}}\right)&&=-i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {i}{z}}\right)&&\operatorname {Im} \left(\ln \left({\sqrt {z^{2}-1}}+i\right)\right)\\\end{aligned}}}$
In this sense, all of the inverse trig functions can be thought of as specific cases of the complex-valued log function. Since this definition works for any complex-valued ${\displaystyle z}$ , this definition allows for hyperbolic angles as outputs and can be used to further define the inverse hyperbolic functions. Elementary proofs of the relations may also proceed via expansion to exponential forms of the trigonometric functions.


£#h5#£Example proof£#/h5#£
${\displaystyle {\begin{aligned}\sin(\phi )&=z\\\phi &=\arcsin(z)\end{aligned}}}$
Using the exponential definition of sine, and letting ${\displaystyle \xi =e^{\phi i},}$

${\displaystyle {\begin{aligned}z&={\frac {e^{\phi i}-e^{-\phi i}}{2i}}\\[10mu]2iz&=\xi -{\frac {1}{\xi }}\\[5mu]0&=\xi ^{2}-2i\xi z-1\\[5mu]\xi &=iz\pm {\sqrt {1-z^{2}}}\\[5mu]\phi &=-i\ln \left(iz\pm {\sqrt {1-z^{2}}}\right)\end{aligned}}}$
(the positive branch is chosen)

${\displaystyle \phi =\arcsin(z)=-i\ln \left(iz+{\sqrt {1-z^{2}}}\right)}$

£#h5#£Applications£#/h5#£
£#h5#£Finding the angle of a right triangle£#/h5#£
Inverse trigonometric functions are useful when trying to determine the remaining two angles of a right triangle when the lengths of the sides of the triangle are known. Recalling the right-triangle definitions of sine and cosine, it follows that

${\displaystyle \theta =\arcsin \left({\frac {\text{opposite}}{\text{hypotenuse}}}\right)=\arccos \left({\frac {\text{adjacent}}{\text{hypotenuse}}}\right).}$
Often, the hypotenuse is unknown and would need to be calculated before using arcsine or arccosine using the Pythagorean Theorem: ${\displaystyle a^{2}+b^{2}=h^{2}}$ where ${\displaystyle h}$ is the length of the hypotenuse. Arctangent comes in handy in this situation, as the length of the hypotenuse is not needed.

${\displaystyle \theta =\arctan \left({\frac {\text{opposite}}{\text{adjacent}}}\right)\,.}$
For example, suppose a roof drops 8 feet as it runs out 20 feet. The roof makes an angle θ with the horizontal, where θ may be computed as follows:

${\displaystyle \theta =\arctan \left({\frac {\text{opposite}}{\text{adjacent}}}\right)=\arctan \left({\frac {\text{rise}}{\text{run}}}\right)=\arctan \left({\frac {8}{20}}\right)\approx 21.8^{\circ }\,.}$

£#h5#£In computer science and engineering£#/h5#£
£#h5#£Two-argument variant of arctangent£#/h5#£

The two-argument atan2 function computes the arctangent of y / x given y and x, but with a range of (−π, π]. In other words, atan2(y, x) is the angle between the positive x-axis of a plane and the point (x, y) on it, with positive sign for counter-clockwise angles (upper half-plane, y > 0), and negative sign for clockwise angles (lower half-plane, y < 0). It was first introduced in many computer programming languages, but it is now also common in other fields of science and engineering.

In terms of the standard arctan function, that is with range of (−π/2, π/2), it can be expressed as follows:

${\displaystyle \operatorname {atan2} (y,x)={\begin{cases}\arctan \left({\frac {y}{x}}\right)&\quad x>0\\\arctan \left({\frac {y}{x}}\right)+\pi &\quad y\geq 0,\;x<0\\\arctan \left({\frac {y}{x}}\right)-\pi &\quad y<0,\;x<0\\{\frac {\pi }{2}}&\quad y>0,\;x=0\\-{\frac {\pi }{2}}&\quad y<0,\;x=0\\{\text{undefined}}&\quad y=0,\;x=0\end{cases}}}$
It also equals the principal value of the argument of the complex number x + iy.

This limited version of the function above may also be defined using the tangent half-angle formulae as follows:

${\displaystyle \operatorname {atan2} (y,x)=2\arctan \left({\frac {y}{{\sqrt {x^{2}+y^{2}}}+x}}\right)}$
provided that either x > 0 or y ≠ 0. However this fails if given x ≤ 0 and y = 0 so the expression is unsuitable for computational use.

The above argument order (y, x) seems to be the most common, and in particular is used in ISO standards such as the C programming language, but a few authors may use the opposite convention (x, y) so some caution is warranted. These variations are detailed at atan2.


£#h5#£Arctangent function with location parameter£#/h5#£
In many applications the solution ${\displaystyle y}$ of the equation ${\displaystyle x=\tan(y)}$ is to come as close as possible to a given value ${\displaystyle -\infty <\eta <\infty }$ . The adequate solution is produced by the parameter modified arctangent function

${\displaystyle y=\arctan _{\eta }(x):=\arctan(x)+\pi \,\operatorname {rni} \left({\frac {\eta -\arctan(x)}{\pi }}\right)\,.}$
The function ${\displaystyle \operatorname {rni} }$ rounds to the nearest integer.


£#h5#£Numerical accuracy£#/h5#£
For angles near 0 and π, arccosine is ill-conditioned and will thus calculate the angle with reduced accuracy in a computer implementation (due to the limited number of digits). Similarly, arcsine is inaccurate for angles near −π/2 and π/2.


£#h5#£See also£#/h5#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Abramowitz, Milton; Stegun, Irene A., eds. (1972). Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables. New York: Dover Publications. ISBN 978-0-486-61272-0.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Inverse Tangent". MathWorld.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Trigonometric Functions £#/li#££#/ul#£




£#h3#£Arccosh£#/h3#£


In mathematics, the inverse hyperbolic functions are the inverse functions of the hyperbolic functions.

For a given value of a hyperbolic function, the corresponding inverse hyperbolic function provides the corresponding hyperbolic angle. The size of the hyperbolic angle is equal to the area of the corresponding hyperbolic sector of the hyperbola xy = 1, or twice the area of the corresponding sector of the unit hyperbola x2 − y2 = 1, just as a circular angle is twice the area of the circular sector of the unit circle. Some authors have called inverse hyperbolic functions "area functions" to realize the hyperbolic angles.

Hyperbolic functions occur in the calculations of angles and distances in hyperbolic geometry. It also occurs in the solutions of many linear differential equations (such as the equation defining a catenary), cubic equations, and Laplace's equation in Cartesian coordinates. Laplace's equations are important in many areas of physics, including electromagnetic theory, heat transfer, fluid dynamics, and special relativity.


£#h5#£Notation£#/h5#£
The ISO 80000-2 standard abbreviations consist of ar- followed by the abbreviation of the corresponding hyperbolic function (e.g., arsinh, arcosh). The prefix arc- followed by the corresponding hyperbolic function (e.g., arcsinh, arccosh) is also commonly seen, by analogy with the nomenclature for inverse trigonometric functions. These are misnomers, since the prefix arc is the abbreviation for arcus, while the prefix ar stands for area; the hyperbolic functions are not directly related to arcs.

Other authors prefer to use the notation argsinh, argcosh, argtanh, and so on, where the prefix arg is the abbreviation of the Latin argumentum. In computer science, this is often shortened to asinh.

The notation sinh−1(x), cosh−1(x), etc., is also used, despite the fact that care must be taken to avoid misinterpretations of the superscript −1 as a power, as opposed to a shorthand to denote the inverse function (e.g., cosh−1(x) versus cosh(x)−1).


£#h5#£Definitions in terms of logarithms£#/h5#£
Since the hyperbolic functions are rational functions of ex whose numerator and denominator are of degree at most two, these functions may be solved in terms of ex, by using the quadratic formula; then, taking the natural logarithm gives the following expressions for the inverse hyperbolic functions.

For complex arguments, the inverse hyperbolic functions, the square root and the logarithm are multi-valued functions, and the equalities of the next subsections may be viewed as equalities of multi-valued functions.

For all inverse hyperbolic functions (save the inverse hyperbolic cotangent and the inverse hyperbolic cosecant), the domain of the real function is connected.


£#h5#£Inverse hyperbolic sine£#/h5#£
Inverse hyperbolic sine (a.k.a. area hyperbolic sine) (Latin: Area sinus hyperbolicus):

${\displaystyle \operatorname {arsinh} x=\ln \left(x+{\sqrt {x^{2}+1}}\right)}$
The domain is the whole real line.


£#h5#£Inverse hyperbolic cosine£#/h5#£
Inverse hyperbolic cosine (a.k.a. area hyperbolic cosine) (Latin: Area cosinus hyperbolicus):

${\displaystyle \operatorname {arcosh} x=\ln \left(x+{\sqrt {x^{2}-1}}\right)}$
The domain is the closed interval [1, +∞ ).


£#h5#£Inverse hyperbolic tangent£#/h5#£
Inverse hyperbolic tangent (a.k.a. area hyperbolic tangent) (Latin: Area tangens hyperbolicus):

${\displaystyle \operatorname {artanh} x={\frac {1}{2}}\ln \left({\frac {1+x}{1-x}}\right)}$
The domain is the open interval (−1, 1).


£#h5#£Inverse hyperbolic cotangent£#/h5#£
Inverse hyperbolic cotangent (a.k.a., area hyperbolic cotangent) (Latin: Area cotangens hyperbolicus):

${\displaystyle \operatorname {arcoth} x={\frac {1}{2}}\ln \left({\frac {x+1}{x-1}}\right)}$
The domain is the union of the open intervals (−∞, −1) and (1, +∞).


£#h5#£Inverse hyperbolic secant£#/h5#£
Inverse hyperbolic secant (a.k.a., area hyperbolic secant) (Latin: Area secans hyperbolicus):

${\displaystyle \operatorname {arsech} x=\ln \left({\frac {1}{x}}+{\sqrt {{\frac {1}{x^{2}}}-1}}\right)=\ln \left({\frac {1+{\sqrt {1-x^{2}}}}{x}}\right)}$
The domain is the semi-open interval (0, 1].


£#h5#£Inverse hyperbolic cosecant£#/h5#£
Inverse hyperbolic cosecant (a.k.a., area hyperbolic cosecant) (Latin: Area cosecans hyperbolicus):

${\displaystyle \operatorname {arcsch} x=\ln \left({\frac {1}{x}}+{\sqrt {{\frac {1}{x^{2}}}+1}}\right)}$
The domain is the real line with 0 removed.


£#h5#£Addition formulae£#/h5#£
${\displaystyle \operatorname {arsinh} u\pm \operatorname {arsinh} v=\operatorname {arsinh} \left(u{\sqrt {1+v^{2}}}\pm v{\sqrt {1+u^{2}}}\right)}$
${\displaystyle \operatorname {arcosh} u\pm \operatorname {arcosh} v=\operatorname {arcosh} \left(uv\pm {\sqrt {(u^{2}-1)(v^{2}-1)}}\right)}$
${\displaystyle \operatorname {artanh} u\pm \operatorname {artanh} v=\operatorname {artanh} \left({\frac {u\pm v}{1\pm uv}}\right)}$
${\displaystyle \operatorname {arcoth} u\pm \operatorname {arcoth} v=\operatorname {arcoth} \left({\frac {1\pm uv}{u\pm v}}\right)}$
${\displaystyle {\begin{aligned}\operatorname {arsinh} u+\operatorname {arcosh} v&=\operatorname {arsinh} \left(uv+{\sqrt {(1+u^{2})(v^{2}-1)}}\right)\\&=\operatorname {arcosh} \left(v{\sqrt {1+u^{2}}}+u{\sqrt {v^{2}-1}}\right)\end{aligned}}}$

£#h5#£Other identities£#/h5#£
${\displaystyle {\begin{aligned}2\operatorname {arcosh} x&=\operatorname {arcosh} (2x^{2}-1)&\quad {\hbox{ for }}x\geq 1\\4\operatorname {arcosh} x&=\operatorname {arcosh} (8x^{4}-8x^{2}+1)&\quad {\hbox{ for }}x\geq 1\\2\operatorname {arsinh} x&=\operatorname {arcosh} (2x^{2}+1)&\quad {\hbox{ for }}x\geq 0\\4\operatorname {arsinh} x&=\operatorname {arcosh} (8x^{4}+8x^{2}+1)&\quad {\hbox{ for }}x\geq 0\end{aligned}}}$
${\displaystyle \ln(x)=\operatorname {arcosh} \left({\frac {x^{2}+1}{2x}}\right)=\operatorname {arsinh} \left({\frac {x^{2}-1}{2x}}\right)=\operatorname {artanh} \left({\frac {x^{2}-1}{x^{2}+1}}\right)}$

£#h5#£Composition of hyperbolic and inverse hyperbolic functions£#/h5#£
${\displaystyle {\begin{aligned}&\sinh(\operatorname {arcosh} x)={\sqrt {x^{2}-1}}\quad {\text{for}}\quad |x|>1\\&\sinh(\operatorname {artanh} x)={\frac {x}{\sqrt {1-x^{2}}}}\quad {\text{for}}\quad -1<x<1\\&\cosh(\operatorname {arsinh} x)={\sqrt {1+x^{2}}}\\&\cosh(\operatorname {artanh} x)={\frac {1}{\sqrt {1-x^{2}}}}\quad {\text{for}}\quad -1<x<1\\&\tanh(\operatorname {arsinh} x)={\frac {x}{\sqrt {1+x^{2}}}}\\&\tanh(\operatorname {arcosh} x)={\frac {\sqrt {x^{2}-1}}{x}}\quad {\text{for}}\quad |x|>1\end{aligned}}}$

£#h5#£Composition of inverse hyperbolic and trigonometric functions£#/h5#£
${\displaystyle \operatorname {arsinh} \left(\tan \alpha \right)=\operatorname {artanh} \left(\sin \alpha \right)=\ln \left({\frac {1+\sin \alpha }{\cos \alpha }}\right)=\pm \operatorname {arcosh} \left({\frac {1}{\cos \alpha }}\right)}$
${\displaystyle \ln \left(\left|\tan \alpha \right|\right)=-\operatorname {artanh} \left(\cos 2\alpha \right)}$

£#h5#£Conversions£#/h5#£
${\displaystyle \ln x=\operatorname {artanh} \left({\frac {x^{2}-1}{x^{2}+1}}\right)=\operatorname {arsinh} \left({\frac {x^{2}-1}{2x}}\right)=\pm \operatorname {arcosh} \left({\frac {x^{2}+1}{2x}}\right)}$
${\displaystyle \operatorname {artanh} x=\operatorname {arsinh} \left({\frac {x}{\sqrt {1-x^{2}}}}\right)=\pm \operatorname {arcosh} \left({\frac {1}{\sqrt {1-x^{2}}}}\right)}$
${\displaystyle \operatorname {arsinh} x=\operatorname {artanh} \left({\frac {x}{\sqrt {1+x^{2}}}}\right)=\pm \operatorname {arcosh} \left({\sqrt {1+x^{2}}}\right)}$
${\displaystyle \operatorname {arcosh} x=\left|\operatorname {arsinh} \left({\sqrt {x^{2}-1}}\right)\right|=\left|\operatorname {artanh} \left({\frac {\sqrt {x^{2}-1}}{x}}\right)\right|}$

£#h5#£Derivatives£#/h5#£
${\displaystyle {\begin{aligned}{\frac {d}{dx}}\operatorname {arsinh} x&{}={\frac {1}{\sqrt {x^{2}+1}}},{\text{ for all real }}x\\{\frac {d}{dx}}\operatorname {arcosh} x&{}={\frac {1}{\sqrt {x^{2}-1}}},{\text{ for all real }}x>1\\{\frac {d}{dx}}\operatorname {artanh} x&{}={\frac {1}{1-x^{2}}},{\text{ for all real }}|x|<1\\{\frac {d}{dx}}\operatorname {arcoth} x&{}={\frac {1}{1-x^{2}}},{\text{ for all real }}|x|>1\\{\frac {d}{dx}}\operatorname {arsech} x&{}={\frac {-1}{x{\sqrt {1-x^{2}}}}},{\text{ for all real }}x\in (0,1)\\{\frac {d}{dx}}\operatorname {arcsch} x&{}={\frac {-1}{|x|{\sqrt {1+x^{2}}}}},{\text{ for all real }}x{\text{, except }}0\\\end{aligned}}}$
For an example differentiation: let θ = arsinh x, so (where sinh2 θ = (sinh θ)2):

${\displaystyle {\frac {d\,\operatorname {arsinh} x}{dx}}={\frac {d\theta }{d\sinh \theta }}={\frac {1}{\cosh \theta }}={\frac {1}{\sqrt {1+\sinh ^{2}\theta }}}={\frac {1}{\sqrt {1+x^{2}}}}.}$

£#h5#£Series expansions£#/h5#£
Expansion series can be obtained for the above functions:

${\displaystyle {\begin{aligned}\operatorname {arsinh} x&=x-\left({\frac {1}{2}}\right){\frac {x^{3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{5}}{5}}-\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{7}}{7}}\pm \cdots \\&=\sum _{n=0}^{\infty }\left({\frac {(-1)^{n}(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{2n+1}}{2n+1}},\qquad \left|x\right|<1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcosh} x&=\ln(2x)-\left(\left({\frac {1}{2}}\right){\frac {x^{-2}}{2}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{-4}}{4}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{-6}}{6}}+\cdots \right)\\&=\ln(2x)-\sum _{n=1}^{\infty }\left({\frac {(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{-2n}}{2n}},\qquad \left|x\right|>1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {artanh} x&=x+{\frac {x^{3}}{3}}+{\frac {x^{5}}{5}}+{\frac {x^{7}}{7}}+\cdots \\&=\sum _{n=0}^{\infty }{\frac {x^{2n+1}}{2n+1}},\qquad \left|x\right|<1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcsch} x=\operatorname {arsinh} {\frac {1}{x}}&=x^{-1}-\left({\frac {1}{2}}\right){\frac {x^{-3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{-5}}{5}}-\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{-7}}{7}}\pm \cdots \\&=\sum _{n=0}^{\infty }\left({\frac {(-1)^{n}(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{-(2n+1)}}{2n+1}},\qquad \left|x\right|>1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arsech} x=\operatorname {arcosh} {\frac {1}{x}}&=\ln {\frac {2}{x}}-\left(\left({\frac {1}{2}}\right){\frac {x^{2}}{2}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{4}}{4}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{6}}{6}}+\cdots \right)\\&=\ln {\frac {2}{x}}-\sum _{n=1}^{\infty }\left({\frac {(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{2n}}{2n}},\qquad 0<x\leq 1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcoth} x=\operatorname {artanh} {\frac {1}{x}}&=x^{-1}+{\frac {x^{-3}}{3}}+{\frac {x^{-5}}{5}}+{\frac {x^{-7}}{7}}+\cdots \\&=\sum _{n=0}^{\infty }{\frac {x^{-(2n+1)}}{2n+1}},\qquad \left|x\right|>1\end{aligned}}}$
Asymptotic expansion for the arsinh x is given by

${\displaystyle \operatorname {arsinh} x=\ln(2x)+\sum \limits _{n=1}^{\infty }{\left({-1}\right)^{n-1}{\frac {\left({2n-1}\right)!!}{2n\left({2n}\right)!!}}}{\frac {1}{x^{2n}}}}$



£#h5#£Principal values in the complex plane£#/h5#£
As functions of a complex variable, inverse hyperbolic functions are multivalued functions that are analytic, except at a finite number of points. For such a function, it is common to define a principal value, which is a single valued analytic function which coincides with one specific branch of the multivalued function, over a domain consisting of the complex plane in which a finite number of arcs (usually half lines or line segments) have been removed. These arcs are called branch cuts. For specifying the branch, that is, defining which value of the multivalued function is considered at each point, one generally define it at a particular point, and deduce the value everywhere in the domain of definition of the principal value by analytic continuation. When possible, it is better to define the principal value directly—without referring to analytic continuation.

For example, for the square root, the principal value is defined as the square root that has a positive real part. This defines a single valued analytic function, which is defined everywhere, except for non-positive real values of the variables (where the two square roots have a zero real part). This principal value of the square root function is denoted ${\displaystyle {\sqrt {x}}}$ in what follows. Similarly, the principal value of the logarithm, denoted ${\displaystyle \operatorname {Log} }$ in what follows, is defined as the value for which the imaginary part has the smallest absolute value. It is defined everywhere except for non-positive real values of the variable, for which two different values of the logarithm reach the minimum.

For all inverse hyperbolic functions, the principal value may be defined in terms of principal values of the square root and the logarithm function. However, in some cases, the formulas of § Definitions in terms of logarithms do not give a correct principal value, as giving a domain of definition which is too small and, in one case non-connected.


£#h5#£Principal value of the inverse hyperbolic sine£#/h5#£
The principal value of the inverse hyperbolic sine is given by

${\displaystyle \operatorname {arsinh} z=\operatorname {Log} (z+{\sqrt {z^{2}+1}}\,)\,.}$
The argument of the square root is a non-positive real number, if and only if z belongs to one of the intervals [i, +i∞) and (−i∞, −i] of the imaginary axis. If the argument of the logarithm is real, then it is positive. Thus this formula defines a principal value for arsinh, with branch cuts [i, +i∞) and (−i∞, −i]. This is optimal, as the branch cuts must connect the singular points i and −i to the infinity.


£#h5#£Principal value of the inverse hyperbolic cosine£#/h5#£
The formula for the inverse hyperbolic cosine given in § Inverse hyperbolic cosine is not convenient, since similar to the principal values of the logarithm and the square root, the principal value of arcosh would not be defined for imaginary z. Thus the square root has to be factorized, leading to

${\displaystyle \operatorname {arcosh} z=\operatorname {Log} (z+{\sqrt {z+1}}{\sqrt {z-1}}\,)\,.}$
The principal values of the square roots are both defined, except if z belongs to the real interval (−∞, 1]. If the argument of the logarithm is real, then z is real and has the same sign. Thus, the above formula defines a principal value of arcosh outside the real interval (−∞, 1], which is thus the unique branch cut.


£#h5#£Principal values of the inverse hyperbolic tangent and cotangent£#/h5#£
The formulas given in § Definitions in terms of logarithms suggests

${\displaystyle {\begin{aligned}\operatorname {artanh} z&={\frac {1}{2}}\operatorname {Log} \left({\frac {1+z}{1-z}}\right)\\\operatorname {arcoth} z&={\frac {1}{2}}\operatorname {Log} \left({\frac {z+1}{z-1}}\right)\end{aligned}}}$
for the definition of the principal values of the inverse hyperbolic tangent and cotangent. In these formulas, the argument of the logarithm is real if and only if z is real. For artanh, this argument is in the real interval (−∞, 0], if z belongs either to (−∞, −1] or to [1, ∞). For arcoth, the argument of the logarithm is in (−∞, 0], if and only if z belongs to the real interval [−1, 1].

Therefore, these formulas define convenient principal values, for which the branch cuts are (−∞, −1] and [1, ∞) for the inverse hyperbolic tangent, and [−1, 1] for the inverse hyperbolic cotangent.

In view of a better numerical evaluation near the branch cuts, some authors use the following definitions of the principal values, although the second one introduces a removable singularity at z = 0. The two definitions of ${\displaystyle \operatorname {artanh} }$ differ for real values of ${\displaystyle z}$ with ${\displaystyle z>1}$ . The ones of ${\displaystyle \operatorname {arcoth} }$ differ for real values of ${\displaystyle z}$ with ${\displaystyle z\in [0,1)}$ .

${\displaystyle {\begin{aligned}\operatorname {artanh} z&={\tfrac {1}{2}}\operatorname {Log} \left({1+z}\right)-{\tfrac {1}{2}}\operatorname {Log} \left({1-z}\right)\\\operatorname {arcoth} z&={\tfrac {1}{2}}\operatorname {Log} \left({1+{\frac {1}{z}}}\right)-{\tfrac {1}{2}}\operatorname {Log} \left({1-{\frac {1}{z}}}\right)\end{aligned}}}$

£#h5#£Principal value of the inverse hyperbolic cosecant£#/h5#£
For the inverse hyperbolic cosecant, the principal value is defined as

${\displaystyle \operatorname {arcsch} z=\operatorname {Log} \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z^{2}}}+1}}\,\right)}$ .
It is defined when the arguments of the logarithm and the square root are not non-positive real numbers. The principal value of the square root is thus defined outside the interval [−i, i] of the imaginary line. If the argument of the logarithm is real, then z is a non-zero real number, and this implies that the argument of the logarithm is positive.

Thus, the principal value is defined by the above formula outside the branch cut, consisting of the interval [−i, i] of the imaginary line.

For z = 0, there is a singular point that is included in the branch cut.


£#h5#£Principal value of the inverse hyperbolic secant£#/h5#£
Here, as in the case of the inverse hyperbolic cosine, we have to factorize the square root. This gives the principal value

${\displaystyle \operatorname {arsech} z=\operatorname {Log} \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z}}+1}}\,{\sqrt {{\frac {1}{z}}-1}}\right).}$
If the argument of a square root is real, then z is real, and it follows that both principal values of square roots are defined, except if z is real and belongs to one of the intervals (−∞, 0] and [1, +∞). If the argument of the logarithm is real and negative, then z is also real and negative. It follows that the principal value of arsech is well defined, by the above formula outside two branch cuts, the real intervals (−∞, 0] and [1, +∞).

For z = 0, there is a singular point that is included in one of the branch cuts.


£#h5#£Graphical representation£#/h5#£
In the following graphical representation of the principal values of the inverse hyperbolic functions, the branch cuts appear as discontinuities of the color. The fact that the whole branch cuts appear as discontinuities, shows that these principal values may not be extended into analytic functions defined over larger domains. In other words, the above defined branch cuts are minimal.


£#h5#£See also£#/h5#£ £#ul#££#li#£Complex logarithm£#/li#£ £#li#£Hyperbolic secant distribution£#/li#£ £#li#£ISO 80000-2£#/li#£ £#li#£List of integrals of inverse hyperbolic functions£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£Bibliography£#/h5#£ £#ul#££#li#£Herbert Busemann and Paul J. Kelly (1953) Projective Geometry and Projective Metrics, page 207, Academic Press.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Inverse hyperbolic functions", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Hyperbolic Functions £#/li#££#/ul#£




£#h3#£Arccosine£#/h3#£

In mathematics, the inverse trigonometric functions (occasionally also called arcus functions, antitrigonometric functions or cyclometric functions) are the inverse functions of the trigonometric functions (with suitably restricted domains). Specifically, they are the inverses of the sine, cosine, tangent, cotangent, secant, and cosecant functions, and are used to obtain an angle from any of the angle's trigonometric ratios. Inverse trigonometric functions are widely used in engineering, navigation, physics, and geometry.


£#h5#£Notation£#/h5#£
Several notations for the inverse trigonometric functions exist. The most common convention is to name inverse trigonometric functions using an arc- prefix: arcsin(x), arccos(x), arctan(x), etc. (This convention is used throughout this article.) This notation arises from the following geometric relationships: when measuring in radians, an angle of θ radians will correspond to an arc whose length is rθ, where r is the radius of the circle. Thus in the unit circle, "the arc whose cosine is x" is the same as "the angle whose cosine is x", because the length of the arc of the circle in radii is the same as the measurement of the angle in radians. In computer programming languages, the inverse trigonometric functions are often called by the abbreviated forms asin, acos, atan.

The notations sin−1(x), cos−1(x), tan−1(x), etc., as introduced by John Herschel in 1813, are often used as well in English-language sources, much more than the also established sin[−1](x), cos[−1](x), tan[−1](x),—conventions consistent with the notation of an inverse function, that is useful e.g. to define the multivalued version of each inverse trigonometric function: e.g. ${\displaystyle \tan ^{-1}(x)=\{\arctan(x)+\pi k\mid k\in \mathbb {Z} \}}$ . However, this might appear to conflict logically with the common semantics for expressions such as sin2(x) (although only sin2 x, without parentheses, is the really common one), which refer to numeric power rather than function composition, and therefore may result in confusion between multiplicative inverse or reciprocal and compositional inverse. The confusion is somewhat mitigated by the fact that each of the reciprocal trigonometric functions has its own name—for example, (cos(x))−1 = sec(x). Nevertheless, certain authors advise against using it for its ambiguity. Another precarious convention used by a tiny number of authors is to use an uppercase first letter, along with a −1 superscript: Sin−1(x), Cos−1(x), Tan−1(x), etc. Although its intention is to avoid confusion with the multiplicative inverse, which should be represented by sin−1(x), cos−1(x), etc., or, better, by sin−1 x, cos−1 x, etc., it in turn creates yet another major source of ambiguity: especially in light of the fact that many popular high-level programming languages (e.g. Wolfram's Mathematica, and University of Sydney's MAGMA) use those very same capitalised representations for the standard trig functions; whereas others (Python (ie SymPy and NumPy), Matlab, MAPLE etc) use lower-case.

Hence, since 2009, the ISO 80000-2 standard has specified solely the "arc" prefix for the inverse functions.


£#h5#£Basic concepts£#/h5#£
£#h5#£Principal values£#/h5#£
Since none of the six trigonometric functions are one-to-one, they must be restricted in order to have inverse functions. Therefore, the result ranges of the inverse functions are proper (i.e. strict) subsets of the domains of the original functions.

For example, using function in the sense of multivalued functions, just as the square root function ${\displaystyle y={\sqrt {x}}}$ could be defined from ${\displaystyle y^{2}=x,}$ the function ${\displaystyle y=\arcsin(x)}$ is defined so that ${\displaystyle \sin(y)=x.}$ For a given real number ${\displaystyle x,}$ with ${\displaystyle -1\leq x\leq 1,}$ there are multiple (in fact, countably infinitely many) numbers ${\displaystyle y}$ such that ${\displaystyle \sin(y)=x}$ ; for example, ${\displaystyle \sin(0)=0,}$ but also ${\displaystyle \sin(\pi )=0,}$ ${\displaystyle \sin(2\pi )=0,}$ etc. When only one value is desired, the function may be restricted to its principal branch. With this restriction, for each ${\displaystyle x}$ in the domain, the expression ${\displaystyle \arcsin(x)}$ will evaluate only to a single value, called its principal value. These properties apply to all the inverse trigonometric functions.

The principal inverses are listed in the following table.

Note: Some authors define the range of arcsecant to be ${\textstyle (0\leq y<{\frac {\pi }{2}}{\text{ or }}\pi \leq y<{\frac {3\pi }{2}})}$ , because the tangent function is nonnegative on this domain. This makes some computations more consistent. For example, using this range, ${\displaystyle \tan(\operatorname {arcsec}(x))={\sqrt {x^{2}-1}},}$ whereas with the range ${\textstyle (0\leq y<{\frac {\pi }{2}}{\text{ or }}{\frac {\pi }{2}}<y\leq \pi )}$ , we would have to write ${\displaystyle \tan(\operatorname {arcsec}(x))=\pm {\sqrt {x^{2}-1}},}$ since tangent is nonnegative on ${\textstyle 0\leq y<{\frac {\pi }{2}},}$ but nonpositive on ${\textstyle {\frac {\pi }{2}}<y\leq \pi .}$ For a similar reason, the same authors define the range of arccosecant to be ${\textstyle (-\pi <y\leq -{\frac {\pi }{2}}}$ or ${\textstyle 0<y\leq {\frac {\pi }{2}}).}$

If ${\displaystyle x}$ is allowed to be a complex number, then the range of ${\displaystyle y}$ applies only to its real part.

The table below displays names and domains of the inverse trigonometric functions along with the range of their usual principal values in radians.

The symbol ${\displaystyle \mathbb {R} =(-\infty ,\infty )}$ denotes the set of all real numbers and ${\displaystyle \mathbb {Z} =\{\ldots ,\,-2,\,-1,\,0,\,1,\,2,\,\ldots \}}$ denotes the set of all integers. The set of all integer multiples of ${\displaystyle \pi }$ is denoted by

The symbol ${\displaystyle \,\setminus \,}$ denotes set subtraction so that, for instance, ${\displaystyle \mathbb {R} \setminus (-1,1)=(-\infty ,-1]\cup [1,\infty )}$ is the set of points in ${\displaystyle \mathbb {R} }$ (that is, real numbers) that are not in the interval ${\displaystyle (-1,1).}$

The Minkowski sum notation ${\textstyle \pi \mathbb {Z} +(0,\pi )}$ and ${\displaystyle \pi \mathbb {Z} +{\bigl (}{-{\tfrac {\pi }{2}}},{\tfrac {\pi }{2}}{\bigr )}}$ that is used above to concisely write the domains of ${\displaystyle \cot ,\csc ,\tan ,{\text{ and }}\sec }$ is now explained.

Domain of cotangent ${\displaystyle \cot }$ and cosecant ${\displaystyle \csc }$ : The domains of ${\displaystyle \,\cot \,}$ and ${\displaystyle \,\csc \,}$ are the same. They are the set of all angles ${\displaystyle \theta }$ at which ${\displaystyle \sin \theta \neq 0,}$ i.e. all real numbers that are not of the form ${\displaystyle \pi n}$ for some integer ${\displaystyle n,}$

Domain of tangent ${\displaystyle \tan }$ and secant ${\displaystyle \sec }$ : The domains of ${\displaystyle \,\tan \,}$ and ${\displaystyle \,\sec \,}$ are the same. They are the set of all angles ${\displaystyle \theta }$ at which ${\displaystyle \cos \theta \neq 0,}$


£#h5#£Solutions to elementary trigonometric equations£#/h5#£
Each of the trigonometric functions is periodic in the real part of its argument, running through all its values twice in each interval of ${\displaystyle 2\pi }$ :

£#ul#££#li#£Sine and cosecant begin their period at ${\textstyle 2\pi k-{\frac {\pi }{2}}}$ (where ${\displaystyle k}$ is an integer), finish it at ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ and then reverse themselves over ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ to ${\displaystyle 2\pi k+{\frac {3\pi }{2}}.}$ £#/li#£ £#li#£Cosine and secant begin their period at ${\displaystyle 2\pi k,}$ finish it at ${\displaystyle 2\pi k+\pi .}$ and then reverse themselves over ${\displaystyle 2\pi k+\pi }$ to ${\displaystyle 2\pi k+2\pi .}$ £#/li#£ £#li#£Tangent begins its period at ${\textstyle 2\pi k-{\frac {\pi }{2}},}$ finishes it at ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ and then repeats it (forward) over ${\textstyle 2\pi k+{\frac {\pi }{2}}}$ to ${\textstyle 2\pi k+{\frac {3\pi }{2}}.}$ £#/li#£ £#li#£Cotangent begins its period at ${\displaystyle 2\pi k,}$ finishes it at ${\displaystyle 2\pi k+\pi ,}$ and then repeats it (forward) over ${\displaystyle 2\pi k+\pi }$ to ${\displaystyle 2\pi k+2\pi .}$ £#/li#££#/ul#£
This periodicity is reflected in the general inverses, where ${\displaystyle k}$ is some integer.

The following table shows how inverse trigonometric functions may be used to solve equalities involving the six standard trigonometric functions. It is assumed that the given values ${\displaystyle \theta ,}$ ${\displaystyle r,}$ ${\displaystyle s,}$ ${\displaystyle x,}$ and ${\displaystyle y}$ all lie within appropriate ranges so that the relevant expressions below are well-defined. Note that "for some ${\displaystyle k\in \mathbb {Z} }$ " is just another way of saying "for some integer ${\displaystyle k.}$ "

The symbol ${\displaystyle \,\iff \,}$ is logical equality. The expression "LHS ${\displaystyle \,\iff \,}$ RHS" indicates that either (a) the left hand side (i.e. LHS) and right hand side (i.e. RHS) are both true, or else (b) the left hand side and right hand side are both false; there is no option (c) (e.g. it is not possible for the LHS statement to be true and also simultaneously for the RHS statement to false), because otherwise "LHS ${\displaystyle \,\iff \,}$ RHS" would not have been written (see this footnote for an example illustrating this concept).

For example, if ${\displaystyle \cos \theta =-1}$ then ${\displaystyle \theta =\pi +2\pi k=-\pi +2\pi (1+k)}$ for some ${\displaystyle k\in \mathbb {Z} .}$ While if ${\displaystyle \sin \theta =\pm 1}$ then ${\textstyle \theta ={\frac {\pi }{2}}+\pi k=-{\frac {\pi }{2}}+\pi (k+1)}$ for some ${\displaystyle k\in \mathbb {Z} ,}$ where ${\displaystyle k}$ will be even if ${\displaystyle \sin \theta =1}$ and it will be odd if ${\displaystyle \sin \theta =-1.}$ The equations ${\displaystyle \sec \theta =-1}$ and ${\displaystyle \csc \theta =\pm 1}$ have the same solutions as ${\displaystyle \cos \theta =-1}$ and ${\displaystyle \sin \theta =\pm 1,}$ respectively. In all equations above except for those just solved (i.e. except for ${\displaystyle \sin }$ / ${\displaystyle \csc \theta =\pm 1}$ and ${\displaystyle \cos }$ / ${\displaystyle \sec \theta =-1}$ ), the integer ${\displaystyle k}$ in the solution's formula is uniquely determined by ${\displaystyle \theta }$ (for fixed ${\displaystyle r,s,x,}$ and ${\displaystyle y}$ ).

Detailed example and explanation of the "plus or minus" symbol ${\displaystyle \pm }$
The solutions to ${\displaystyle \cos \theta =x}$ and ${\displaystyle \sec \theta =x}$ involve the "plus or minus" symbol ${\displaystyle \,\pm ,\,}$ whose meaning is now clarified. Only the solution to ${\displaystyle \cos \theta =x}$ will be discussed since the discussion for ${\displaystyle \sec \theta =x}$ is the same. We are given ${\displaystyle x}$ between ${\displaystyle -1\leq x\leq 1}$ and we know that there is an angle ${\displaystyle \theta }$ in some give interval that satisfies ${\displaystyle \cos \theta =x.}$ We want to find this ${\displaystyle \theta .}$ The formula for the solution involves:

If ${\displaystyle \,\arccos x=0\,}$ (which only happens when ${\displaystyle x=1}$ ) then ${\displaystyle \,+\arccos x=0\,}$ and ${\displaystyle \,-\arccos x=0\,}$ so either way, ${\displaystyle \,\pm \arccos x\,}$ can only be equal to ${\displaystyle 0.}$ But if ${\displaystyle \,\arccos x\neq 0,\,}$ which will now be assumed, then the solution to ${\displaystyle \cos \theta =x,}$ which is written above as is shorthand for the following statement:
Either £#ul#££#li#£ ${\displaystyle \,\theta =\arccos x+2\pi k\,}$ for some integer ${\displaystyle k,}$
or else£#/li#£ £#li#£ ${\displaystyle \,\theta =-\arccos x+2\pi k\,}$ for some integer ${\displaystyle k.}$ £#/li#££#/ul#£
Because ${\displaystyle \,\arccos x\neq 0\,}$ and ${\displaystyle \,0<\arccos x\leq \pi \,}$ exactly one of these two equalities can hold. Additional information about ${\displaystyle \theta }$ is needed to determine which one holds. For example, suppose that ${\displaystyle x=0}$ and that all that is known about ${\displaystyle \theta }$ is that ${\displaystyle \,-\pi \leq \theta \leq \pi \,}$ (and nothing more is known). Then

and moreover, in this particular case ${\displaystyle k=0}$ (for both the ${\displaystyle \,+\,}$ case and the ${\displaystyle \,-\,}$ case) and so consequently, This means that ${\displaystyle \theta }$ could be either ${\displaystyle \,\pi /2\,}$ or ${\displaystyle \,-\pi /2.}$ Without additional information it is not possible to determine which of these values ${\displaystyle \theta }$ has. An example of some additional information that could determine the value of ${\displaystyle \theta }$ would be knowing that the angle is above the ${\displaystyle x}$ -axis(in which case ${\displaystyle \theta =\pi /2}$ ) or alternatively, knowing that it is below the ${\displaystyle x}$ -axis(in which case ${\displaystyle \theta =-\pi /2}$ ).
Transforming equations

The equations above can be transformed by using the reflection and shift identities:

These formulas imply, in particular, that the following hold:

where swapping ${\displaystyle \sin \leftrightarrow \csc ,}$ swapping ${\displaystyle \cos \leftrightarrow \sec ,}$ and swapping ${\displaystyle \tan \leftrightarrow \cot }$ gives the analogous equations for ${\displaystyle \csc ,\sec ,{\text{ and }}\cot ,}$ respectively.
So for example, by using the equality ${\textstyle \sin \left({\frac {\pi }{2}}-\theta \right)=\cos \theta ,}$ the equation ${\displaystyle \cos \theta =x}$ can be transformed into ${\textstyle \sin \left({\frac {\pi }{2}}-\theta \right)=x,}$ which allows for the solution to the equation ${\displaystyle \;\sin \varphi =x\;}$ (where ${\textstyle \varphi :={\frac {\pi }{2}}-\theta }$ ) to be used; that solution being: ${\displaystyle \varphi =(-1)^{k}\arcsin(x)+\pi k\;{\text{ for some }}k\in \mathbb {Z} ,}$ which becomes:

where using the fact that ${\displaystyle (-1)^{k}=(-1)^{-k}}$ and substituting ${\displaystyle h:=-k}$ proves that another solution to ${\displaystyle \;\cos \theta =x\;}$ is: The substitution ${\displaystyle \;\arcsin x={\frac {\pi }{2}}-\arccos x\;}$ may be used express the right hand side of the above formula in terms of ${\displaystyle \;\arccos x\;}$ instead of ${\displaystyle \;\arcsin x.\;}$
£#h5#£Equal identical trigonometric functions£#/h5#£
The table below shows how two angles ${\displaystyle \theta }$ and ${\displaystyle \varphi }$ must be related if their values under a given trigonometric function are equal or negatives of each other.


£#h5#£Relationships between trigonometric functions and inverse trigonometric functions£#/h5#£
Trigonometric functions of inverse trigonometric functions are tabulated below. A quick way to derive them is by considering the geometry of a right-angled triangle, with one side of length 1 and another side of length ${\displaystyle x,}$ then applying the Pythagorean theorem and definitions of the trigonometric ratios. Purely algebraic derivations are longer. It is worth noting that for arcsecant and arccosecant, the diagram assumes that ${\displaystyle x}$ is positive, and thus the result has to be corrected through the use of absolute values and the signum (sgn) operation.


£#h5#£Relationships among the inverse trigonometric functions£#/h5#£
Complementary angles:

${\displaystyle {\begin{aligned}\arccos(x)&={\frac {\pi }{2}}-\arcsin(x)\\[0.5em]\operatorname {arccot}(x)&={\frac {\pi }{2}}-\arctan(x)\\[0.5em]\operatorname {arccsc}(x)&={\frac {\pi }{2}}-\operatorname {arcsec}(x)\end{aligned}}}$
Negative arguments:

${\displaystyle {\begin{aligned}\arcsin(-x)&=-\arcsin(x)\\\arccos(-x)&=\pi -\arccos(x)\\\arctan(-x)&=-\arctan(x)\\\operatorname {arccot}(-x)&=\pi -\operatorname {arccot}(x)\\\operatorname {arcsec}(-x)&=\pi -\operatorname {arcsec}(x)\\\operatorname {arccsc}(-x)&=-\operatorname {arccsc}(x)\end{aligned}}}$
Reciprocal arguments:

${\displaystyle {\begin{aligned}\arccos \left({\frac {1}{x}}\right)&=\operatorname {arcsec}(x)\\[0.3em]\arcsin \left({\frac {1}{x}}\right)&=\operatorname {arccsc}(x)\\[0.3em]\arctan \left({\frac {1}{x}}\right)&={\frac {\pi }{2}}-\arctan(x)=\operatorname {arccot}(x)\,,{\text{ if }}x>0\\[0.3em]\arctan \left({\frac {1}{x}}\right)&=-{\frac {\pi }{2}}-\arctan(x)=\operatorname {arccot}(x)-\pi \,,{\text{ if }}x<0\\[0.3em]\operatorname {arccot} \left({\frac {1}{x}}\right)&={\frac {\pi }{2}}-\operatorname {arccot}(x)=\arctan(x)\,,{\text{ if }}x>0\\[0.3em]\operatorname {arccot} \left({\frac {1}{x}}\right)&={\frac {3\pi }{2}}-\operatorname {arccot}(x)=\pi +\arctan(x)\,,{\text{ if }}x<0\\[0.3em]\operatorname {arcsec} \left({\frac {1}{x}}\right)&=\arccos(x)\\[0.3em]\operatorname {arccsc} \left({\frac {1}{x}}\right)&=\arcsin(x)\end{aligned}}}$
Useful identities if one only has a fragment of a sine table:

${\displaystyle {\begin{aligned}\arccos(x)&=\arcsin \left({\sqrt {1-x^{2}}}\right)\,,{\text{ if }}0\leq x\leq 1{\text{ , from which you get }}\\\arccos &\left({\frac {1-x^{2}}{1+x^{2}}}\right)=\arcsin \left({\frac {2x}{1+x^{2}}}\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin &\left({\sqrt {1-x^{2}}}\right)={\frac {\pi }{2}}-\operatorname {sgn}(x)\arcsin(x)\\\arccos(x)&={\frac {1}{2}}\arccos \left(2x^{2}-1\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin(x)&={\frac {1}{2}}\arccos \left(1-2x^{2}\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin(x)&=\arctan \left({\frac {x}{\sqrt {1-x^{2}}}}\right)\\\arccos(x)&=\arctan \left({\frac {\sqrt {1-x^{2}}}{x}}\right)\\\arctan(x)&=\arcsin \left({\frac {x}{\sqrt {1+x^{2}}}}\right)\\\operatorname {arccot}(x)&=\arccos \left({\frac {x}{\sqrt {1+x^{2}}}}\right)\end{aligned}}}$
Whenever the square root of a complex number is used here, we choose the root with the positive real part (or positive imaginary part if the square was negative real).

A useful form that follows directly from the table above is

${\displaystyle \arctan \left(x\right)=\arccos \left({\sqrt {\frac {1}{1+x^{2}}}}\right)\,,{\text{ if }}x\geq 0}$ .
It is obtained by recognizing that ${\displaystyle \cos \left(\arctan \left(x\right)\right)={\sqrt {\frac {1}{1+x^{2}}}}=\cos \left(\arccos \left({\sqrt {\frac {1}{1+x^{2}}}}\right)\right)}$ .

From the half-angle formula, ${\displaystyle \tan \left({\tfrac {\theta }{2}}\right)={\tfrac {\sin(\theta )}{1+\cos(\theta )}}}$ , we get:

${\displaystyle {\begin{aligned}\arcsin(x)&=2\arctan \left({\frac {x}{1+{\sqrt {1-x^{2}}}}}\right)\\[0.5em]\arccos(x)&=2\arctan \left({\frac {\sqrt {1-x^{2}}}{1+x}}\right)\,,{\text{ if }}-1<x\leq 1\\[0.5em]\arctan(x)&=2\arctan \left({\frac {x}{1+{\sqrt {1+x^{2}}}}}\right)\end{aligned}}}$

£#h5#£Arctangent addition formula£#/h5#£
${\displaystyle \arctan(u)\pm \arctan(v)=\arctan \left({\frac {u\pm v}{1\mp uv}}\right){\pmod {\pi }}\,,\quad uv\neq 1\,.}$
This is derived from the tangent addition formula

${\displaystyle \tan(\alpha \pm \beta )={\frac {\tan(\alpha )\pm \tan(\beta )}{1\mp \tan(\alpha )\tan(\beta )}}\,,}$
by letting

${\displaystyle \alpha =\arctan(u)\,,\quad \beta =\arctan(v)\,.}$

£#h5#£In calculus£#/h5#£
£#h5#£Derivatives of inverse trigonometric functions£#/h5#£
The derivatives for complex values of z are as follows:

${\displaystyle {\begin{aligned}{\frac {d}{dz}}\arcsin(z)&{}={\frac {1}{\sqrt {1-z^{2}}}}\;;&z&{}\neq -1,+1\\{\frac {d}{dz}}\arccos(z)&{}=-{\frac {1}{\sqrt {1-z^{2}}}}\;;&z&{}\neq -1,+1\\{\frac {d}{dz}}\arctan(z)&{}={\frac {1}{1+z^{2}}}\;;&z&{}\neq -i,+i\\{\frac {d}{dz}}\operatorname {arccot}(z)&{}=-{\frac {1}{1+z^{2}}}\;;&z&{}\neq -i,+i\\{\frac {d}{dz}}\operatorname {arcsec}(z)&{}={\frac {1}{z^{2}{\sqrt {1-{\frac {1}{z^{2}}}}}}}\;;&z&{}\neq -1,0,+1\\{\frac {d}{dz}}\operatorname {arccsc}(z)&{}=-{\frac {1}{z^{2}{\sqrt {1-{\frac {1}{z^{2}}}}}}}\;;&z&{}\neq -1,0,+1\end{aligned}}}$
Only for real values of x:

${\displaystyle {\begin{aligned}{\frac {d}{dx}}\operatorname {arcsec}(x)&{}={\frac {1}{|x|{\sqrt {x^{2}-1}}}}\;;&|x|>1\\{\frac {d}{dx}}\operatorname {arccsc}(x)&{}=-{\frac {1}{|x|{\sqrt {x^{2}-1}}}}\;;&|x|>1\end{aligned}}}$
For a sample derivation: if ${\displaystyle \theta =\arcsin(x)}$ , we get:

${\displaystyle {\frac {d\arcsin(x)}{dx}}={\frac {d\theta }{d\sin(\theta )}}={\frac {d\theta }{\cos(\theta )\,d\theta }}={\frac {1}{\cos(\theta )}}={\frac {1}{\sqrt {1-\sin ^{2}(\theta )}}}={\frac {1}{\sqrt {1-x^{2}}}}}$

£#h5#£Expression as definite integrals£#/h5#£
Integrating the derivative and fixing the value at one point gives an expression for the inverse trigonometric function as a definite integral:

${\displaystyle {\begin{aligned}\arcsin(x)&{}=\int _{0}^{x}{\frac {1}{\sqrt {1-z^{2}}}}\,dz\;,&|x|&{}\leq 1\\\arccos(x)&{}=\int _{x}^{1}{\frac {1}{\sqrt {1-z^{2}}}}\,dz\;,&|x|&{}\leq 1\\\arctan(x)&{}=\int _{0}^{x}{\frac {1}{z^{2}+1}}\,dz\;,\\\operatorname {arccot}(x)&{}=\int _{x}^{\infty }{\frac {1}{z^{2}+1}}\,dz\;,\\\operatorname {arcsec}(x)&{}=\int _{1}^{x}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz=\pi +\int _{-x}^{-1}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz\;,&x&{}\geq 1\\\operatorname {arccsc}(x)&{}=\int _{x}^{\infty }{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz=\int _{-\infty }^{-x}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz\;,&x&{}\geq 1\\\end{aligned}}}$
When x equals 1, the integrals with limited domains are improper integrals, but still well-defined.


£#h5#£Infinite series£#/h5#£
Similar to the sine and cosine functions, the inverse trigonometric functions can also be calculated using power series, as follows. For arcsine, the series can be derived by expanding its derivative, ${\textstyle {\tfrac {1}{\sqrt {1-z^{2}}}}}$ , as a binomial series, and integrating term by term (using the integral definition as above). The series for arctangent can similarly be derived by expanding its derivative ${\textstyle {\frac {1}{1+z^{2}}}}$ in a geometric series, and applying the integral definition above (see Leibniz series).

${\displaystyle {\begin{aligned}\arcsin(z)&=z+\left({\frac {1}{2}}\right){\frac {z^{3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {z^{5}}{5}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {z^{7}}{7}}+\cdots \\[5pt]&=\sum _{n=0}^{\infty }{\frac {(2n-1)!!}{(2n)!!}}{\frac {z^{2n+1}}{2n+1}}\\[5pt]&=\sum _{n=0}^{\infty }{\frac {(2n)!}{(2^{n}n!)^{2}}}{\frac {z^{2n+1}}{2n+1}}\,;\qquad |z|\leq 1\end{aligned}}}$
${\displaystyle \arctan(z)=z-{\frac {z^{3}}{3}}+{\frac {z^{5}}{5}}-{\frac {z^{7}}{7}}+\cdots =\sum _{n=0}^{\infty }{\frac {(-1)^{n}z^{2n+1}}{2n+1}}\,;\qquad |z|\leq 1\qquad z\neq i,-i}$
Series for the other inverse trigonometric functions can be given in terms of these according to the relationships given above. For example, ${\displaystyle \arccos(x)=\pi /2-\arcsin(x)}$ , ${\displaystyle \operatorname {arccsc}(x)=\arcsin(1/x)}$ , and so on. Another series is given by:

${\displaystyle 2\left(\arcsin \left({\frac {x}{2}}\right)\right)^{2}=\sum _{n=1}^{\infty }{\frac {x^{2n}}{n^{2}{\binom {2n}{n}}}}.}$
Leonhard Euler found a series for the arctangent that converges more quickly than its Taylor series:

${\displaystyle \arctan(z)={\frac {z}{1+z^{2}}}\sum _{n=0}^{\infty }\prod _{k=1}^{n}{\frac {2kz^{2}}{(2k+1)(1+z^{2})}}.}$
(The term in the sum for n = 0 is the empty product, so is 1.)

Alternatively, this can be expressed as

${\displaystyle \arctan(z)=\sum _{n=0}^{\infty }{\frac {2^{2n}(n!)^{2}}{(2n+1)!}}{\frac {z^{2n+1}}{(1+z^{2})^{n+1}}}.}$
Another series for the arctangent function is given by

${\displaystyle \arctan(z)=i\sum _{n=1}^{\infty }{\frac {1}{2n-1}}\left({\frac {1}{(1+2i/z)^{2n-1}}}-{\frac {1}{(1-2i/z)^{2n-1}}}\right),}$
where ${\displaystyle i={\sqrt {-1}}}$ is the imaginary unit.


£#h5#£Continued fractions for arctangent£#/h5#£
Two alternatives to the power series for arctangent are these generalized continued fractions:

${\displaystyle \arctan(z)={\frac {z}{1+{\cfrac {(1z)^{2}}{3-1z^{2}+{\cfrac {(3z)^{2}}{5-3z^{2}+{\cfrac {(5z)^{2}}{7-5z^{2}+{\cfrac {(7z)^{2}}{9-7z^{2}+\ddots }}}}}}}}}}={\frac {z}{1+{\cfrac {(1z)^{2}}{3+{\cfrac {(2z)^{2}}{5+{\cfrac {(3z)^{2}}{7+{\cfrac {(4z)^{2}}{9+\ddots }}}}}}}}}}}$
The second of these is valid in the cut complex plane. There are two cuts, from −i to the point at infinity, going down the imaginary axis, and from i to the point at infinity, going up the same axis. It works best for real numbers running from −1 to 1. The partial denominators are the odd natural numbers, and the partial numerators (after the first) are just (nz)2, with each perfect square appearing once. The first was developed by Leonhard Euler; the second by Carl Friedrich Gauss utilizing the Gaussian hypergeometric series.


£#h5#£Indefinite integrals of inverse trigonometric functions£#/h5#£
For real and complex values of z:

${\displaystyle {\begin{aligned}\int \arcsin(z)\,dz&{}=z\,\arcsin(z)+{\sqrt {1-z^{2}}}+C\\\int \arccos(z)\,dz&{}=z\,\arccos(z)-{\sqrt {1-z^{2}}}+C\\\int \arctan(z)\,dz&{}=z\,\arctan(z)-{\frac {1}{2}}\ln \left(1+z^{2}\right)+C\\\int \operatorname {arccot}(z)\,dz&{}=z\,\operatorname {arccot}(z)+{\frac {1}{2}}\ln \left(1+z^{2}\right)+C\\\int \operatorname {arcsec}(z)\,dz&{}=z\,\operatorname {arcsec}(z)-\ln \left[z\left(1+{\sqrt {\frac {z^{2}-1}{z^{2}}}}\right)\right]+C\\\int \operatorname {arccsc}(z)\,dz&{}=z\,\operatorname {arccsc}(z)+\ln \left[z\left(1+{\sqrt {\frac {z^{2}-1}{z^{2}}}}\right)\right]+C\end{aligned}}}$
For real x ≥ 1:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\ln \left(x+{\sqrt {x^{2}-1}}\right)+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\ln \left(x+{\sqrt {x^{2}-1}}\right)+C\end{aligned}}}$
For all real x not between -1 and 1:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\operatorname {sgn}(x)\ln \left|x+{\sqrt {x^{2}-1}}\right|+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\operatorname {sgn}(x)\ln \left|x+{\sqrt {x^{2}-1}}\right|+C\end{aligned}}}$
The absolute value is necessary to compensate for both negative and positive values of the arcsecant and arccosecant functions. The signum function is also necessary due to the absolute values in the derivatives of the two functions, which create two different solutions for positive and negative values of x. These can be further simplified using the logarithmic definitions of the inverse hyperbolic functions:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\operatorname {arcosh} (|x|)+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\operatorname {arcosh} (|x|)+C\\\end{aligned}}}$
The absolute value in the argument of the arcosh function creates a negative half of its graph, making it identical to the signum logarithmic function shown above.

All of these antiderivatives can be derived using integration by parts and the simple derivative forms shown above.


£#h5#£Example£#/h5#£
Using ${\displaystyle \int u\,dv=uv-\int v\,du}$ (i.e. integration by parts), set

${\displaystyle {\begin{aligned}u&=\arcsin(x)&dv&=dx\\du&={\frac {dx}{\sqrt {1-x^{2}}}}&v&=x\end{aligned}}}$
Then

${\displaystyle \int \arcsin(x)\,dx=x\arcsin(x)-\int {\frac {x}{\sqrt {1-x^{2}}}}\,dx,}$
which by the simple substitution ${\displaystyle w=1-x^{2},\ dw=-2x\,dx}$ yields the final result:

${\displaystyle \int \arcsin(x)\,dx=x\arcsin(x)+{\sqrt {1-x^{2}}}+C}$

£#h5#£Extension to complex plane£#/h5#£
Since the inverse trigonometric functions are analytic functions, they can be extended from the real line to the complex plane. This results in functions with multiple sheets and branch points. One possible way of defining the extension is:

${\displaystyle \arctan(z)=\int _{0}^{z}{\frac {dx}{1+x^{2}}}\quad z\neq -i,+i}$
where the part of the imaginary axis which does not lie strictly between the branch points (−i and +i) is the branch cut between the principal sheet and other sheets. The path of the integral must not cross a branch cut. For z not on a branch cut, a straight line path from 0 to z is such a path. For z on a branch cut, the path must approach from Re[x] > 0 for the upper branch cut and from Re[x] < 0 for the lower branch cut.

The arcsine function may then be defined as:

${\displaystyle \arcsin(z)=\arctan \left({\frac {z}{\sqrt {1-z^{2}}}}\right)\quad z\neq -1,+1}$
where (the square-root function has its cut along the negative real axis and) the part of the real axis which does not lie strictly between −1 and +1 is the branch cut between the principal sheet of arcsin and other sheets;

${\displaystyle \arccos(z)={\frac {\pi }{2}}-\arcsin(z)\quad z\neq -1,+1}$
which has the same cut as arcsin;

${\displaystyle \operatorname {arccot}(z)={\frac {\pi }{2}}-\arctan(z)\quad z\neq -i,i}$
which has the same cut as arctan;

${\displaystyle \operatorname {arcsec}(z)=\arccos \left({\frac {1}{z}}\right)\quad z\neq -1,0,+1}$
where the part of the real axis between −1 and +1 inclusive is the cut between the principal sheet of arcsec and other sheets;

${\displaystyle \operatorname {arccsc}(z)=\arcsin \left({\frac {1}{z}}\right)\quad z\neq -1,0,+1}$
which has the same cut as arcsec.


£#h5#£Logarithmic forms£#/h5#£
These functions may also be expressed using complex logarithms. This extends their domains to the complex plane in a natural fashion. The following identities for principal values of the functions hold everywhere that they are defined, even on their branch cuts.

${\displaystyle {\begin{aligned}\arcsin(z)&{}=-i\ln \left({\sqrt {1-z^{2}}}+iz\right)=i\ln \left({\sqrt {1-z^{2}}}-iz\right)&{}=\operatorname {arccsc} \left({\frac {1}{z}}\right)\\[10pt]\arccos(z)&{}=-i\ln \left(i{\sqrt {1-z^{2}}}+z\right)={\frac {\pi }{2}}-\arcsin(z)&{}=\operatorname {arcsec} \left({\frac {1}{z}}\right)\\[10pt]\arctan(z)&{}=-{\frac {i}{2}}\ln \left({\frac {i-z}{i+z}}\right)=-{\frac {i}{2}}\ln \left({\frac {1+iz}{1-iz}}\right)&{}=\operatorname {arccot} \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arccot}(z)&{}=-{\frac {i}{2}}\ln \left({\frac {z+i}{z-i}}\right)=-{\frac {i}{2}}\ln \left({\frac {iz-1}{iz+1}}\right)&{}=\arctan \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arcsec}(z)&{}=-i\ln \left(i{\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {1}{z}}\right)={\frac {\pi }{2}}-\operatorname {arccsc}(z)&{}=\arccos \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arccsc}(z)&{}=-i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {i}{z}}\right)=i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}-{\frac {i}{z}}\right)&{}=\arcsin \left({\frac {1}{z}}\right)\end{aligned}}}$

£#h5#£Generalization£#/h5#£
Because all of the inverse trigonometric functions output an angle of a right triangle, they can be generalized by using Euler's formula to form a right triangle in the complex plane. Algebraically, this gives us:

${\displaystyle ce^{\theta i}=c\cos(\theta )+ci\sin(\theta )}$
or

${\displaystyle ce^{\theta i}=a+bi}$
where ${\displaystyle a}$ is the adjacent side, ${\displaystyle b}$ is the opposite side, and ${\displaystyle c}$ is the hypotenuse. From here, we can solve for ${\displaystyle \theta }$ .

${\displaystyle {\begin{aligned}e^{\ln(c)+\theta i}&=a+bi\\\ln c+\theta i&=\ln(a+bi)\\\theta &=\operatorname {Im} \left(\ln(a+bi)\right)\end{aligned}}}$
or

${\displaystyle \theta =-i\ln \left({\frac {a+bi}{c}}\right)=i\ln \left({\frac {c}{a+bi}}\right)}$
Simply taking the imaginary part works for any real-valued ${\displaystyle a}$ and ${\displaystyle b}$ , but if ${\displaystyle a}$ or ${\displaystyle b}$ is complex-valued, we have to use the final equation so that the real part of the result isn't excluded. Since the length of the hypotenuse doesn't change the angle, ignoring the real part of ${\displaystyle \ln(a+bi)}$ also removes ${\displaystyle c}$ from the equation. In the final equation, we see that the angle of the triangle in the complex plane can be found by inputting the lengths of each side. By setting one of the three sides equal to 1 and one of the remaining sides equal to our input ${\displaystyle z}$ , we obtain a formula for one of the inverse trig functions, for a total of six equations. Because the inverse trig functions require only one input, we must put the final side of the triangle in terms of the other two using the Pythagorean Theorem relation

${\displaystyle a^{2}+b^{2}=c^{2}}$
The table below shows the values of a, b, and c for each of the inverse trig functions and the equivalent expressions for ${\displaystyle \theta }$ that result from plugging the values into the equations above and simplifying.

${\displaystyle {\begin{aligned}&a&&b&&c&&\theta &&\theta _{\text{simplified}}&&\theta _{a,b\in \mathbb {R} }\\\arcsin(z)\ \ &{\sqrt {1-z^{2}}}&&z&&1&&-i\ln \left({\frac {{\sqrt {1-z^{2}}}+zi}{1}}\right)&&=-i\ln \left({\sqrt {1-z^{2}}}+zi\right)&&\operatorname {Im} \left(\ln \left({\sqrt {1-z^{2}}}+zi\right)\right)\\\arccos(z)\ \ &z&&{\sqrt {1-z^{2}}}&&1&&-i\ln \left({\frac {z+i{\sqrt {1-z^{2}}}}{1}}\right)&&=-i\ln \left(z+{\sqrt {z^{2}-1}}\right)&&\operatorname {Im} \left(\ln \left(z+{\sqrt {z^{2}-1}}\right)\right)\\\arctan(z)\ \ &1&&z&&{\sqrt {1+z^{2}}}&&i\ln \left({\frac {\sqrt {1+z^{2}}}{1+zi}}\right)&&={\frac {i}{2}}\ln \left({\frac {i+z}{i-z}}\right)&&\operatorname {Im} \left(\ln \left(1+zi\right)\right)\\\operatorname {arccot}(z)\ \ &z&&1&&{\sqrt {z^{2}+1}}&&i\ln \left({\frac {\sqrt {z^{2}+1}}{z+i}}\right)&&={\frac {i}{2}}\ln \left({\frac {z-i}{z+i}}\right)&&\operatorname {Im} \left(\ln \left(z+i\right)\right)\\\operatorname {arcsec}(z)\ \ &1&&{\sqrt {z^{2}-1}}&&z&&-i\ln \left({\frac {1+i{\sqrt {z^{2}-1}}}{z}}\right)&&=-i\ln \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z^{2}}}-1}}\right)&&\operatorname {Im} \left(\ln \left(1+{\sqrt {1-z^{2}}}\right)\right)\\\operatorname {arccsc}(z)\ \ &{\sqrt {z^{2}-1}}&&1&&z&&-i\ln \left({\frac {{\sqrt {z^{2}-1}}+i}{z}}\right)&&=-i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {i}{z}}\right)&&\operatorname {Im} \left(\ln \left({\sqrt {z^{2}-1}}+i\right)\right)\\\end{aligned}}}$
In this sense, all of the inverse trig functions can be thought of as specific cases of the complex-valued log function. Since this definition works for any complex-valued ${\displaystyle z}$ , this definition allows for hyperbolic angles as outputs and can be used to further define the inverse hyperbolic functions. Elementary proofs of the relations may also proceed via expansion to exponential forms of the trigonometric functions.


£#h5#£Example proof£#/h5#£
${\displaystyle {\begin{aligned}\sin(\phi )&=z\\\phi &=\arcsin(z)\end{aligned}}}$
Using the exponential definition of sine, and letting ${\displaystyle \xi =e^{\phi i},}$

${\displaystyle {\begin{aligned}z&={\frac {e^{\phi i}-e^{-\phi i}}{2i}}\\[10mu]2iz&=\xi -{\frac {1}{\xi }}\\[5mu]0&=\xi ^{2}-2i\xi z-1\\[5mu]\xi &=iz\pm {\sqrt {1-z^{2}}}\\[5mu]\phi &=-i\ln \left(iz\pm {\sqrt {1-z^{2}}}\right)\end{aligned}}}$
(the positive branch is chosen)

${\displaystyle \phi =\arcsin(z)=-i\ln \left(iz+{\sqrt {1-z^{2}}}\right)}$

£#h5#£Applications£#/h5#£
£#h5#£Finding the angle of a right triangle£#/h5#£
Inverse trigonometric functions are useful when trying to determine the remaining two angles of a right triangle when the lengths of the sides of the triangle are known. Recalling the right-triangle definitions of sine and cosine, it follows that

${\displaystyle \theta =\arcsin \left({\frac {\text{opposite}}{\text{hypotenuse}}}\right)=\arccos \left({\frac {\text{adjacent}}{\text{hypotenuse}}}\right).}$
Often, the hypotenuse is unknown and would need to be calculated before using arcsine or arccosine using the Pythagorean Theorem: ${\displaystyle a^{2}+b^{2}=h^{2}}$ where ${\displaystyle h}$ is the length of the hypotenuse. Arctangent comes in handy in this situation, as the length of the hypotenuse is not needed.

${\displaystyle \theta =\arctan \left({\frac {\text{opposite}}{\text{adjacent}}}\right)\,.}$
For example, suppose a roof drops 8 feet as it runs out 20 feet. The roof makes an angle θ with the horizontal, where θ may be computed as follows:

${\displaystyle \theta =\arctan \left({\frac {\text{opposite}}{\text{adjacent}}}\right)=\arctan \left({\frac {\text{rise}}{\text{run}}}\right)=\arctan \left({\frac {8}{20}}\right)\approx 21.8^{\circ }\,.}$

£#h5#£In computer science and engineering£#/h5#£
£#h5#£Two-argument variant of arctangent£#/h5#£

The two-argument atan2 function computes the arctangent of y / x given y and x, but with a range of (−π, π]. In other words, atan2(y, x) is the angle between the positive x-axis of a plane and the point (x, y) on it, with positive sign for counter-clockwise angles (upper half-plane, y > 0), and negative sign for clockwise angles (lower half-plane, y < 0). It was first introduced in many computer programming languages, but it is now also common in other fields of science and engineering.

In terms of the standard arctan function, that is with range of (−π/2, π/2), it can be expressed as follows:

${\displaystyle \operatorname {atan2} (y,x)={\begin{cases}\arctan \left({\frac {y}{x}}\right)&\quad x>0\\\arctan \left({\frac {y}{x}}\right)+\pi &\quad y\geq 0,\;x<0\\\arctan \left({\frac {y}{x}}\right)-\pi &\quad y<0,\;x<0\\{\frac {\pi }{2}}&\quad y>0,\;x=0\\-{\frac {\pi }{2}}&\quad y<0,\;x=0\\{\text{undefined}}&\quad y=0,\;x=0\end{cases}}}$
It also equals the principal value of the argument of the complex number x + iy.

This limited version of the function above may also be defined using the tangent half-angle formulae as follows:

${\displaystyle \operatorname {atan2} (y,x)=2\arctan \left({\frac {y}{{\sqrt {x^{2}+y^{2}}}+x}}\right)}$
provided that either x > 0 or y ≠ 0. However this fails if given x ≤ 0 and y = 0 so the expression is unsuitable for computational use.

The above argument order (y, x) seems to be the most common, and in particular is used in ISO standards such as the C programming language, but a few authors may use the opposite convention (x, y) so some caution is warranted. These variations are detailed at atan2.


£#h5#£Arctangent function with location parameter£#/h5#£
In many applications the solution ${\displaystyle y}$ of the equation ${\displaystyle x=\tan(y)}$ is to come as close as possible to a given value ${\displaystyle -\infty <\eta <\infty }$ . The adequate solution is produced by the parameter modified arctangent function

${\displaystyle y=\arctan _{\eta }(x):=\arctan(x)+\pi \,\operatorname {rni} \left({\frac {\eta -\arctan(x)}{\pi }}\right)\,.}$
The function ${\displaystyle \operatorname {rni} }$ rounds to the nearest integer.


£#h5#£Numerical accuracy£#/h5#£
For angles near 0 and π, arccosine is ill-conditioned and will thus calculate the angle with reduced accuracy in a computer implementation (due to the limited number of digits). Similarly, arcsine is inaccurate for angles near −π/2 and π/2.


£#h5#£See also£#/h5#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Abramowitz, Milton; Stegun, Irene A., eds. (1972). Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables. New York: Dover Publications. ISBN 978-0-486-61272-0.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Inverse Tangent". MathWorld.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Trigonometric Functions £#/li#££#/ul#£




£#h3#£Arccot£#/h3#£

In mathematics, the inverse trigonometric functions (occasionally also called arcus functions, antitrigonometric functions or cyclometric functions) are the inverse functions of the trigonometric functions (with suitably restricted domains). Specifically, they are the inverses of the sine, cosine, tangent, cotangent, secant, and cosecant functions, and are used to obtain an angle from any of the angle's trigonometric ratios. Inverse trigonometric functions are widely used in engineering, navigation, physics, and geometry.


£#h5#£Notation£#/h5#£
Several notations for the inverse trigonometric functions exist. The most common convention is to name inverse trigonometric functions using an arc- prefix: arcsin(x), arccos(x), arctan(x), etc. (This convention is used throughout this article.) This notation arises from the following geometric relationships: when measuring in radians, an angle of θ radians will correspond to an arc whose length is rθ, where r is the radius of the circle. Thus in the unit circle, "the arc whose cosine is x" is the same as "the angle whose cosine is x", because the length of the arc of the circle in radii is the same as the measurement of the angle in radians. In computer programming languages, the inverse trigonometric functions are often called by the abbreviated forms asin, acos, atan.

The notations sin−1(x), cos−1(x), tan−1(x), etc., as introduced by John Herschel in 1813, are often used as well in English-language sources, much more than the also established sin[−1](x), cos[−1](x), tan[−1](x),—conventions consistent with the notation of an inverse function, that is useful e.g. to define the multivalued version of each inverse trigonometric function: e.g. ${\displaystyle \tan ^{-1}(x)=\{\arctan(x)+\pi k\mid k\in \mathbb {Z} \}}$ . However, this might appear to conflict logically with the common semantics for expressions such as sin2(x) (although only sin2 x, without parentheses, is the really common one), which refer to numeric power rather than function composition, and therefore may result in confusion between multiplicative inverse or reciprocal and compositional inverse. The confusion is somewhat mitigated by the fact that each of the reciprocal trigonometric functions has its own name—for example, (cos(x))−1 = sec(x). Nevertheless, certain authors advise against using it for its ambiguity. Another precarious convention used by a tiny number of authors is to use an uppercase first letter, along with a −1 superscript: Sin−1(x), Cos−1(x), Tan−1(x), etc. Although its intention is to avoid confusion with the multiplicative inverse, which should be represented by sin−1(x), cos−1(x), etc., or, better, by sin−1 x, cos−1 x, etc., it in turn creates yet another major source of ambiguity: especially in light of the fact that many popular high-level programming languages (e.g. Wolfram's Mathematica, and University of Sydney's MAGMA) use those very same capitalised representations for the standard trig functions; whereas others (Python (ie SymPy and NumPy), Matlab, MAPLE etc) use lower-case.

Hence, since 2009, the ISO 80000-2 standard has specified solely the "arc" prefix for the inverse functions.


£#h5#£Basic concepts£#/h5#£
£#h5#£Principal values£#/h5#£
Since none of the six trigonometric functions are one-to-one, they must be restricted in order to have inverse functions. Therefore, the result ranges of the inverse functions are proper (i.e. strict) subsets of the domains of the original functions.

For example, using function in the sense of multivalued functions, just as the square root function ${\displaystyle y={\sqrt {x}}}$ could be defined from ${\displaystyle y^{2}=x,}$ the function ${\displaystyle y=\arcsin(x)}$ is defined so that ${\displaystyle \sin(y)=x.}$ For a given real number ${\displaystyle x,}$ with ${\displaystyle -1\leq x\leq 1,}$ there are multiple (in fact, countably infinitely many) numbers ${\displaystyle y}$ such that ${\displaystyle \sin(y)=x}$ ; for example, ${\displaystyle \sin(0)=0,}$ but also ${\displaystyle \sin(\pi )=0,}$ ${\displaystyle \sin(2\pi )=0,}$ etc. When only one value is desired, the function may be restricted to its principal branch. With this restriction, for each ${\displaystyle x}$ in the domain, the expression ${\displaystyle \arcsin(x)}$ will evaluate only to a single value, called its principal value. These properties apply to all the inverse trigonometric functions.

The principal inverses are listed in the following table.

Note: Some authors define the range of arcsecant to be ${\textstyle (0\leq y<{\frac {\pi }{2}}{\text{ or }}\pi \leq y<{\frac {3\pi }{2}})}$ , because the tangent function is nonnegative on this domain. This makes some computations more consistent. For example, using this range, ${\displaystyle \tan(\operatorname {arcsec}(x))={\sqrt {x^{2}-1}},}$ whereas with the range ${\textstyle (0\leq y<{\frac {\pi }{2}}{\text{ or }}{\frac {\pi }{2}}<y\leq \pi )}$ , we would have to write ${\displaystyle \tan(\operatorname {arcsec}(x))=\pm {\sqrt {x^{2}-1}},}$ since tangent is nonnegative on ${\textstyle 0\leq y<{\frac {\pi }{2}},}$ but nonpositive on ${\textstyle {\frac {\pi }{2}}<y\leq \pi .}$ For a similar reason, the same authors define the range of arccosecant to be ${\textstyle (-\pi <y\leq -{\frac {\pi }{2}}}$ or ${\textstyle 0<y\leq {\frac {\pi }{2}}).}$

If ${\displaystyle x}$ is allowed to be a complex number, then the range of ${\displaystyle y}$ applies only to its real part.

The table below displays names and domains of the inverse trigonometric functions along with the range of their usual principal values in radians.

The symbol ${\displaystyle \mathbb {R} =(-\infty ,\infty )}$ denotes the set of all real numbers and ${\displaystyle \mathbb {Z} =\{\ldots ,\,-2,\,-1,\,0,\,1,\,2,\,\ldots \}}$ denotes the set of all integers. The set of all integer multiples of ${\displaystyle \pi }$ is denoted by

The symbol ${\displaystyle \,\setminus \,}$ denotes set subtraction so that, for instance, ${\displaystyle \mathbb {R} \setminus (-1,1)=(-\infty ,-1]\cup [1,\infty )}$ is the set of points in ${\displaystyle \mathbb {R} }$ (that is, real numbers) that are not in the interval ${\displaystyle (-1,1).}$

The Minkowski sum notation ${\textstyle \pi \mathbb {Z} +(0,\pi )}$ and ${\displaystyle \pi \mathbb {Z} +{\bigl (}{-{\tfrac {\pi }{2}}},{\tfrac {\pi }{2}}{\bigr )}}$ that is used above to concisely write the domains of ${\displaystyle \cot ,\csc ,\tan ,{\text{ and }}\sec }$ is now explained.

Domain of cotangent ${\displaystyle \cot }$ and cosecant ${\displaystyle \csc }$ : The domains of ${\displaystyle \,\cot \,}$ and ${\displaystyle \,\csc \,}$ are the same. They are the set of all angles ${\displaystyle \theta }$ at which ${\displaystyle \sin \theta \neq 0,}$ i.e. all real numbers that are not of the form ${\displaystyle \pi n}$ for some integer ${\displaystyle n,}$

Domain of tangent ${\displaystyle \tan }$ and secant ${\displaystyle \sec }$ : The domains of ${\displaystyle \,\tan \,}$ and ${\displaystyle \,\sec \,}$ are the same. They are the set of all angles ${\displaystyle \theta }$ at which ${\displaystyle \cos \theta \neq 0,}$


£#h5#£Solutions to elementary trigonometric equations£#/h5#£
Each of the trigonometric functions is periodic in the real part of its argument, running through all its values twice in each interval of ${\displaystyle 2\pi }$ :

£#ul#££#li#£Sine and cosecant begin their period at ${\textstyle 2\pi k-{\frac {\pi }{2}}}$ (where ${\displaystyle k}$ is an integer), finish it at ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ and then reverse themselves over ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ to ${\displaystyle 2\pi k+{\frac {3\pi }{2}}.}$ £#/li#£ £#li#£Cosine and secant begin their period at ${\displaystyle 2\pi k,}$ finish it at ${\displaystyle 2\pi k+\pi .}$ and then reverse themselves over ${\displaystyle 2\pi k+\pi }$ to ${\displaystyle 2\pi k+2\pi .}$ £#/li#£ £#li#£Tangent begins its period at ${\textstyle 2\pi k-{\frac {\pi }{2}},}$ finishes it at ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ and then repeats it (forward) over ${\textstyle 2\pi k+{\frac {\pi }{2}}}$ to ${\textstyle 2\pi k+{\frac {3\pi }{2}}.}$ £#/li#£ £#li#£Cotangent begins its period at ${\displaystyle 2\pi k,}$ finishes it at ${\displaystyle 2\pi k+\pi ,}$ and then repeats it (forward) over ${\displaystyle 2\pi k+\pi }$ to ${\displaystyle 2\pi k+2\pi .}$ £#/li#££#/ul#£
This periodicity is reflected in the general inverses, where ${\displaystyle k}$ is some integer.

The following table shows how inverse trigonometric functions may be used to solve equalities involving the six standard trigonometric functions. It is assumed that the given values ${\displaystyle \theta ,}$ ${\displaystyle r,}$ ${\displaystyle s,}$ ${\displaystyle x,}$ and ${\displaystyle y}$ all lie within appropriate ranges so that the relevant expressions below are well-defined. Note that "for some ${\displaystyle k\in \mathbb {Z} }$ " is just another way of saying "for some integer ${\displaystyle k.}$ "

The symbol ${\displaystyle \,\iff \,}$ is logical equality. The expression "LHS ${\displaystyle \,\iff \,}$ RHS" indicates that either (a) the left hand side (i.e. LHS) and right hand side (i.e. RHS) are both true, or else (b) the left hand side and right hand side are both false; there is no option (c) (e.g. it is not possible for the LHS statement to be true and also simultaneously for the RHS statement to false), because otherwise "LHS ${\displaystyle \,\iff \,}$ RHS" would not have been written (see this footnote for an example illustrating this concept).

For example, if ${\displaystyle \cos \theta =-1}$ then ${\displaystyle \theta =\pi +2\pi k=-\pi +2\pi (1+k)}$ for some ${\displaystyle k\in \mathbb {Z} .}$ While if ${\displaystyle \sin \theta =\pm 1}$ then ${\textstyle \theta ={\frac {\pi }{2}}+\pi k=-{\frac {\pi }{2}}+\pi (k+1)}$ for some ${\displaystyle k\in \mathbb {Z} ,}$ where ${\displaystyle k}$ will be even if ${\displaystyle \sin \theta =1}$ and it will be odd if ${\displaystyle \sin \theta =-1.}$ The equations ${\displaystyle \sec \theta =-1}$ and ${\displaystyle \csc \theta =\pm 1}$ have the same solutions as ${\displaystyle \cos \theta =-1}$ and ${\displaystyle \sin \theta =\pm 1,}$ respectively. In all equations above except for those just solved (i.e. except for ${\displaystyle \sin }$ / ${\displaystyle \csc \theta =\pm 1}$ and ${\displaystyle \cos }$ / ${\displaystyle \sec \theta =-1}$ ), the integer ${\displaystyle k}$ in the solution's formula is uniquely determined by ${\displaystyle \theta }$ (for fixed ${\displaystyle r,s,x,}$ and ${\displaystyle y}$ ).

Detailed example and explanation of the "plus or minus" symbol ${\displaystyle \pm }$
The solutions to ${\displaystyle \cos \theta =x}$ and ${\displaystyle \sec \theta =x}$ involve the "plus or minus" symbol ${\displaystyle \,\pm ,\,}$ whose meaning is now clarified. Only the solution to ${\displaystyle \cos \theta =x}$ will be discussed since the discussion for ${\displaystyle \sec \theta =x}$ is the same. We are given ${\displaystyle x}$ between ${\displaystyle -1\leq x\leq 1}$ and we know that there is an angle ${\displaystyle \theta }$ in some give interval that satisfies ${\displaystyle \cos \theta =x.}$ We want to find this ${\displaystyle \theta .}$ The formula for the solution involves:

If ${\displaystyle \,\arccos x=0\,}$ (which only happens when ${\displaystyle x=1}$ ) then ${\displaystyle \,+\arccos x=0\,}$ and ${\displaystyle \,-\arccos x=0\,}$ so either way, ${\displaystyle \,\pm \arccos x\,}$ can only be equal to ${\displaystyle 0.}$ But if ${\displaystyle \,\arccos x\neq 0,\,}$ which will now be assumed, then the solution to ${\displaystyle \cos \theta =x,}$ which is written above as is shorthand for the following statement:
Either £#ul#££#li#£ ${\displaystyle \,\theta =\arccos x+2\pi k\,}$ for some integer ${\displaystyle k,}$
or else£#/li#£ £#li#£ ${\displaystyle \,\theta =-\arccos x+2\pi k\,}$ for some integer ${\displaystyle k.}$ £#/li#££#/ul#£
Because ${\displaystyle \,\arccos x\neq 0\,}$ and ${\displaystyle \,0<\arccos x\leq \pi \,}$ exactly one of these two equalities can hold. Additional information about ${\displaystyle \theta }$ is needed to determine which one holds. For example, suppose that ${\displaystyle x=0}$ and that all that is known about ${\displaystyle \theta }$ is that ${\displaystyle \,-\pi \leq \theta \leq \pi \,}$ (and nothing more is known). Then

and moreover, in this particular case ${\displaystyle k=0}$ (for both the ${\displaystyle \,+\,}$ case and the ${\displaystyle \,-\,}$ case) and so consequently, This means that ${\displaystyle \theta }$ could be either ${\displaystyle \,\pi /2\,}$ or ${\displaystyle \,-\pi /2.}$ Without additional information it is not possible to determine which of these values ${\displaystyle \theta }$ has. An example of some additional information that could determine the value of ${\displaystyle \theta }$ would be knowing that the angle is above the ${\displaystyle x}$ -axis(in which case ${\displaystyle \theta =\pi /2}$ ) or alternatively, knowing that it is below the ${\displaystyle x}$ -axis(in which case ${\displaystyle \theta =-\pi /2}$ ).
Transforming equations

The equations above can be transformed by using the reflection and shift identities:

These formulas imply, in particular, that the following hold:

where swapping ${\displaystyle \sin \leftrightarrow \csc ,}$ swapping ${\displaystyle \cos \leftrightarrow \sec ,}$ and swapping ${\displaystyle \tan \leftrightarrow \cot }$ gives the analogous equations for ${\displaystyle \csc ,\sec ,{\text{ and }}\cot ,}$ respectively.
So for example, by using the equality ${\textstyle \sin \left({\frac {\pi }{2}}-\theta \right)=\cos \theta ,}$ the equation ${\displaystyle \cos \theta =x}$ can be transformed into ${\textstyle \sin \left({\frac {\pi }{2}}-\theta \right)=x,}$ which allows for the solution to the equation ${\displaystyle \;\sin \varphi =x\;}$ (where ${\textstyle \varphi :={\frac {\pi }{2}}-\theta }$ ) to be used; that solution being: ${\displaystyle \varphi =(-1)^{k}\arcsin(x)+\pi k\;{\text{ for some }}k\in \mathbb {Z} ,}$ which becomes:

where using the fact that ${\displaystyle (-1)^{k}=(-1)^{-k}}$ and substituting ${\displaystyle h:=-k}$ proves that another solution to ${\displaystyle \;\cos \theta =x\;}$ is: The substitution ${\displaystyle \;\arcsin x={\frac {\pi }{2}}-\arccos x\;}$ may be used express the right hand side of the above formula in terms of ${\displaystyle \;\arccos x\;}$ instead of ${\displaystyle \;\arcsin x.\;}$
£#h5#£Equal identical trigonometric functions£#/h5#£
The table below shows how two angles ${\displaystyle \theta }$ and ${\displaystyle \varphi }$ must be related if their values under a given trigonometric function are equal or negatives of each other.


£#h5#£Relationships between trigonometric functions and inverse trigonometric functions£#/h5#£
Trigonometric functions of inverse trigonometric functions are tabulated below. A quick way to derive them is by considering the geometry of a right-angled triangle, with one side of length 1 and another side of length ${\displaystyle x,}$ then applying the Pythagorean theorem and definitions of the trigonometric ratios. Purely algebraic derivations are longer. It is worth noting that for arcsecant and arccosecant, the diagram assumes that ${\displaystyle x}$ is positive, and thus the result has to be corrected through the use of absolute values and the signum (sgn) operation.


£#h5#£Relationships among the inverse trigonometric functions£#/h5#£
Complementary angles:

${\displaystyle {\begin{aligned}\arccos(x)&={\frac {\pi }{2}}-\arcsin(x)\\[0.5em]\operatorname {arccot}(x)&={\frac {\pi }{2}}-\arctan(x)\\[0.5em]\operatorname {arccsc}(x)&={\frac {\pi }{2}}-\operatorname {arcsec}(x)\end{aligned}}}$
Negative arguments:

${\displaystyle {\begin{aligned}\arcsin(-x)&=-\arcsin(x)\\\arccos(-x)&=\pi -\arccos(x)\\\arctan(-x)&=-\arctan(x)\\\operatorname {arccot}(-x)&=\pi -\operatorname {arccot}(x)\\\operatorname {arcsec}(-x)&=\pi -\operatorname {arcsec}(x)\\\operatorname {arccsc}(-x)&=-\operatorname {arccsc}(x)\end{aligned}}}$
Reciprocal arguments:

${\displaystyle {\begin{aligned}\arccos \left({\frac {1}{x}}\right)&=\operatorname {arcsec}(x)\\[0.3em]\arcsin \left({\frac {1}{x}}\right)&=\operatorname {arccsc}(x)\\[0.3em]\arctan \left({\frac {1}{x}}\right)&={\frac {\pi }{2}}-\arctan(x)=\operatorname {arccot}(x)\,,{\text{ if }}x>0\\[0.3em]\arctan \left({\frac {1}{x}}\right)&=-{\frac {\pi }{2}}-\arctan(x)=\operatorname {arccot}(x)-\pi \,,{\text{ if }}x<0\\[0.3em]\operatorname {arccot} \left({\frac {1}{x}}\right)&={\frac {\pi }{2}}-\operatorname {arccot}(x)=\arctan(x)\,,{\text{ if }}x>0\\[0.3em]\operatorname {arccot} \left({\frac {1}{x}}\right)&={\frac {3\pi }{2}}-\operatorname {arccot}(x)=\pi +\arctan(x)\,,{\text{ if }}x<0\\[0.3em]\operatorname {arcsec} \left({\frac {1}{x}}\right)&=\arccos(x)\\[0.3em]\operatorname {arccsc} \left({\frac {1}{x}}\right)&=\arcsin(x)\end{aligned}}}$
Useful identities if one only has a fragment of a sine table:

${\displaystyle {\begin{aligned}\arccos(x)&=\arcsin \left({\sqrt {1-x^{2}}}\right)\,,{\text{ if }}0\leq x\leq 1{\text{ , from which you get }}\\\arccos &\left({\frac {1-x^{2}}{1+x^{2}}}\right)=\arcsin \left({\frac {2x}{1+x^{2}}}\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin &\left({\sqrt {1-x^{2}}}\right)={\frac {\pi }{2}}-\operatorname {sgn}(x)\arcsin(x)\\\arccos(x)&={\frac {1}{2}}\arccos \left(2x^{2}-1\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin(x)&={\frac {1}{2}}\arccos \left(1-2x^{2}\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin(x)&=\arctan \left({\frac {x}{\sqrt {1-x^{2}}}}\right)\\\arccos(x)&=\arctan \left({\frac {\sqrt {1-x^{2}}}{x}}\right)\\\arctan(x)&=\arcsin \left({\frac {x}{\sqrt {1+x^{2}}}}\right)\\\operatorname {arccot}(x)&=\arccos \left({\frac {x}{\sqrt {1+x^{2}}}}\right)\end{aligned}}}$
Whenever the square root of a complex number is used here, we choose the root with the positive real part (or positive imaginary part if the square was negative real).

A useful form that follows directly from the table above is

${\displaystyle \arctan \left(x\right)=\arccos \left({\sqrt {\frac {1}{1+x^{2}}}}\right)\,,{\text{ if }}x\geq 0}$ .
It is obtained by recognizing that ${\displaystyle \cos \left(\arctan \left(x\right)\right)={\sqrt {\frac {1}{1+x^{2}}}}=\cos \left(\arccos \left({\sqrt {\frac {1}{1+x^{2}}}}\right)\right)}$ .

From the half-angle formula, ${\displaystyle \tan \left({\tfrac {\theta }{2}}\right)={\tfrac {\sin(\theta )}{1+\cos(\theta )}}}$ , we get:

${\displaystyle {\begin{aligned}\arcsin(x)&=2\arctan \left({\frac {x}{1+{\sqrt {1-x^{2}}}}}\right)\\[0.5em]\arccos(x)&=2\arctan \left({\frac {\sqrt {1-x^{2}}}{1+x}}\right)\,,{\text{ if }}-1<x\leq 1\\[0.5em]\arctan(x)&=2\arctan \left({\frac {x}{1+{\sqrt {1+x^{2}}}}}\right)\end{aligned}}}$

£#h5#£Arctangent addition formula£#/h5#£
${\displaystyle \arctan(u)\pm \arctan(v)=\arctan \left({\frac {u\pm v}{1\mp uv}}\right){\pmod {\pi }}\,,\quad uv\neq 1\,.}$
This is derived from the tangent addition formula

${\displaystyle \tan(\alpha \pm \beta )={\frac {\tan(\alpha )\pm \tan(\beta )}{1\mp \tan(\alpha )\tan(\beta )}}\,,}$
by letting

${\displaystyle \alpha =\arctan(u)\,,\quad \beta =\arctan(v)\,.}$

£#h5#£In calculus£#/h5#£
£#h5#£Derivatives of inverse trigonometric functions£#/h5#£
The derivatives for complex values of z are as follows:

${\displaystyle {\begin{aligned}{\frac {d}{dz}}\arcsin(z)&{}={\frac {1}{\sqrt {1-z^{2}}}}\;;&z&{}\neq -1,+1\\{\frac {d}{dz}}\arccos(z)&{}=-{\frac {1}{\sqrt {1-z^{2}}}}\;;&z&{}\neq -1,+1\\{\frac {d}{dz}}\arctan(z)&{}={\frac {1}{1+z^{2}}}\;;&z&{}\neq -i,+i\\{\frac {d}{dz}}\operatorname {arccot}(z)&{}=-{\frac {1}{1+z^{2}}}\;;&z&{}\neq -i,+i\\{\frac {d}{dz}}\operatorname {arcsec}(z)&{}={\frac {1}{z^{2}{\sqrt {1-{\frac {1}{z^{2}}}}}}}\;;&z&{}\neq -1,0,+1\\{\frac {d}{dz}}\operatorname {arccsc}(z)&{}=-{\frac {1}{z^{2}{\sqrt {1-{\frac {1}{z^{2}}}}}}}\;;&z&{}\neq -1,0,+1\end{aligned}}}$
Only for real values of x:

${\displaystyle {\begin{aligned}{\frac {d}{dx}}\operatorname {arcsec}(x)&{}={\frac {1}{|x|{\sqrt {x^{2}-1}}}}\;;&|x|>1\\{\frac {d}{dx}}\operatorname {arccsc}(x)&{}=-{\frac {1}{|x|{\sqrt {x^{2}-1}}}}\;;&|x|>1\end{aligned}}}$
For a sample derivation: if ${\displaystyle \theta =\arcsin(x)}$ , we get:

${\displaystyle {\frac {d\arcsin(x)}{dx}}={\frac {d\theta }{d\sin(\theta )}}={\frac {d\theta }{\cos(\theta )\,d\theta }}={\frac {1}{\cos(\theta )}}={\frac {1}{\sqrt {1-\sin ^{2}(\theta )}}}={\frac {1}{\sqrt {1-x^{2}}}}}$

£#h5#£Expression as definite integrals£#/h5#£
Integrating the derivative and fixing the value at one point gives an expression for the inverse trigonometric function as a definite integral:

${\displaystyle {\begin{aligned}\arcsin(x)&{}=\int _{0}^{x}{\frac {1}{\sqrt {1-z^{2}}}}\,dz\;,&|x|&{}\leq 1\\\arccos(x)&{}=\int _{x}^{1}{\frac {1}{\sqrt {1-z^{2}}}}\,dz\;,&|x|&{}\leq 1\\\arctan(x)&{}=\int _{0}^{x}{\frac {1}{z^{2}+1}}\,dz\;,\\\operatorname {arccot}(x)&{}=\int _{x}^{\infty }{\frac {1}{z^{2}+1}}\,dz\;,\\\operatorname {arcsec}(x)&{}=\int _{1}^{x}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz=\pi +\int _{-x}^{-1}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz\;,&x&{}\geq 1\\\operatorname {arccsc}(x)&{}=\int _{x}^{\infty }{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz=\int _{-\infty }^{-x}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz\;,&x&{}\geq 1\\\end{aligned}}}$
When x equals 1, the integrals with limited domains are improper integrals, but still well-defined.


£#h5#£Infinite series£#/h5#£
Similar to the sine and cosine functions, the inverse trigonometric functions can also be calculated using power series, as follows. For arcsine, the series can be derived by expanding its derivative, ${\textstyle {\tfrac {1}{\sqrt {1-z^{2}}}}}$ , as a binomial series, and integrating term by term (using the integral definition as above). The series for arctangent can similarly be derived by expanding its derivative ${\textstyle {\frac {1}{1+z^{2}}}}$ in a geometric series, and applying the integral definition above (see Leibniz series).

${\displaystyle {\begin{aligned}\arcsin(z)&=z+\left({\frac {1}{2}}\right){\frac {z^{3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {z^{5}}{5}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {z^{7}}{7}}+\cdots \\[5pt]&=\sum _{n=0}^{\infty }{\frac {(2n-1)!!}{(2n)!!}}{\frac {z^{2n+1}}{2n+1}}\\[5pt]&=\sum _{n=0}^{\infty }{\frac {(2n)!}{(2^{n}n!)^{2}}}{\frac {z^{2n+1}}{2n+1}}\,;\qquad |z|\leq 1\end{aligned}}}$
${\displaystyle \arctan(z)=z-{\frac {z^{3}}{3}}+{\frac {z^{5}}{5}}-{\frac {z^{7}}{7}}+\cdots =\sum _{n=0}^{\infty }{\frac {(-1)^{n}z^{2n+1}}{2n+1}}\,;\qquad |z|\leq 1\qquad z\neq i,-i}$
Series for the other inverse trigonometric functions can be given in terms of these according to the relationships given above. For example, ${\displaystyle \arccos(x)=\pi /2-\arcsin(x)}$ , ${\displaystyle \operatorname {arccsc}(x)=\arcsin(1/x)}$ , and so on. Another series is given by:

${\displaystyle 2\left(\arcsin \left({\frac {x}{2}}\right)\right)^{2}=\sum _{n=1}^{\infty }{\frac {x^{2n}}{n^{2}{\binom {2n}{n}}}}.}$
Leonhard Euler found a series for the arctangent that converges more quickly than its Taylor series:

${\displaystyle \arctan(z)={\frac {z}{1+z^{2}}}\sum _{n=0}^{\infty }\prod _{k=1}^{n}{\frac {2kz^{2}}{(2k+1)(1+z^{2})}}.}$
(The term in the sum for n = 0 is the empty product, so is 1.)

Alternatively, this can be expressed as

${\displaystyle \arctan(z)=\sum _{n=0}^{\infty }{\frac {2^{2n}(n!)^{2}}{(2n+1)!}}{\frac {z^{2n+1}}{(1+z^{2})^{n+1}}}.}$
Another series for the arctangent function is given by

${\displaystyle \arctan(z)=i\sum _{n=1}^{\infty }{\frac {1}{2n-1}}\left({\frac {1}{(1+2i/z)^{2n-1}}}-{\frac {1}{(1-2i/z)^{2n-1}}}\right),}$
where ${\displaystyle i={\sqrt {-1}}}$ is the imaginary unit.


£#h5#£Continued fractions for arctangent£#/h5#£
Two alternatives to the power series for arctangent are these generalized continued fractions:

${\displaystyle \arctan(z)={\frac {z}{1+{\cfrac {(1z)^{2}}{3-1z^{2}+{\cfrac {(3z)^{2}}{5-3z^{2}+{\cfrac {(5z)^{2}}{7-5z^{2}+{\cfrac {(7z)^{2}}{9-7z^{2}+\ddots }}}}}}}}}}={\frac {z}{1+{\cfrac {(1z)^{2}}{3+{\cfrac {(2z)^{2}}{5+{\cfrac {(3z)^{2}}{7+{\cfrac {(4z)^{2}}{9+\ddots }}}}}}}}}}}$
The second of these is valid in the cut complex plane. There are two cuts, from −i to the point at infinity, going down the imaginary axis, and from i to the point at infinity, going up the same axis. It works best for real numbers running from −1 to 1. The partial denominators are the odd natural numbers, and the partial numerators (after the first) are just (nz)2, with each perfect square appearing once. The first was developed by Leonhard Euler; the second by Carl Friedrich Gauss utilizing the Gaussian hypergeometric series.


£#h5#£Indefinite integrals of inverse trigonometric functions£#/h5#£
For real and complex values of z:

${\displaystyle {\begin{aligned}\int \arcsin(z)\,dz&{}=z\,\arcsin(z)+{\sqrt {1-z^{2}}}+C\\\int \arccos(z)\,dz&{}=z\,\arccos(z)-{\sqrt {1-z^{2}}}+C\\\int \arctan(z)\,dz&{}=z\,\arctan(z)-{\frac {1}{2}}\ln \left(1+z^{2}\right)+C\\\int \operatorname {arccot}(z)\,dz&{}=z\,\operatorname {arccot}(z)+{\frac {1}{2}}\ln \left(1+z^{2}\right)+C\\\int \operatorname {arcsec}(z)\,dz&{}=z\,\operatorname {arcsec}(z)-\ln \left[z\left(1+{\sqrt {\frac {z^{2}-1}{z^{2}}}}\right)\right]+C\\\int \operatorname {arccsc}(z)\,dz&{}=z\,\operatorname {arccsc}(z)+\ln \left[z\left(1+{\sqrt {\frac {z^{2}-1}{z^{2}}}}\right)\right]+C\end{aligned}}}$
For real x ≥ 1:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\ln \left(x+{\sqrt {x^{2}-1}}\right)+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\ln \left(x+{\sqrt {x^{2}-1}}\right)+C\end{aligned}}}$
For all real x not between -1 and 1:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\operatorname {sgn}(x)\ln \left|x+{\sqrt {x^{2}-1}}\right|+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\operatorname {sgn}(x)\ln \left|x+{\sqrt {x^{2}-1}}\right|+C\end{aligned}}}$
The absolute value is necessary to compensate for both negative and positive values of the arcsecant and arccosecant functions. The signum function is also necessary due to the absolute values in the derivatives of the two functions, which create two different solutions for positive and negative values of x. These can be further simplified using the logarithmic definitions of the inverse hyperbolic functions:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\operatorname {arcosh} (|x|)+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\operatorname {arcosh} (|x|)+C\\\end{aligned}}}$
The absolute value in the argument of the arcosh function creates a negative half of its graph, making it identical to the signum logarithmic function shown above.

All of these antiderivatives can be derived using integration by parts and the simple derivative forms shown above.


£#h5#£Example£#/h5#£
Using ${\displaystyle \int u\,dv=uv-\int v\,du}$ (i.e. integration by parts), set

${\displaystyle {\begin{aligned}u&=\arcsin(x)&dv&=dx\\du&={\frac {dx}{\sqrt {1-x^{2}}}}&v&=x\end{aligned}}}$
Then

${\displaystyle \int \arcsin(x)\,dx=x\arcsin(x)-\int {\frac {x}{\sqrt {1-x^{2}}}}\,dx,}$
which by the simple substitution ${\displaystyle w=1-x^{2},\ dw=-2x\,dx}$ yields the final result:

${\displaystyle \int \arcsin(x)\,dx=x\arcsin(x)+{\sqrt {1-x^{2}}}+C}$

£#h5#£Extension to complex plane£#/h5#£
Since the inverse trigonometric functions are analytic functions, they can be extended from the real line to the complex plane. This results in functions with multiple sheets and branch points. One possible way of defining the extension is:

${\displaystyle \arctan(z)=\int _{0}^{z}{\frac {dx}{1+x^{2}}}\quad z\neq -i,+i}$
where the part of the imaginary axis which does not lie strictly between the branch points (−i and +i) is the branch cut between the principal sheet and other sheets. The path of the integral must not cross a branch cut. For z not on a branch cut, a straight line path from 0 to z is such a path. For z on a branch cut, the path must approach from Re[x] > 0 for the upper branch cut and from Re[x] < 0 for the lower branch cut.

The arcsine function may then be defined as:

${\displaystyle \arcsin(z)=\arctan \left({\frac {z}{\sqrt {1-z^{2}}}}\right)\quad z\neq -1,+1}$
where (the square-root function has its cut along the negative real axis and) the part of the real axis which does not lie strictly between −1 and +1 is the branch cut between the principal sheet of arcsin and other sheets;

${\displaystyle \arccos(z)={\frac {\pi }{2}}-\arcsin(z)\quad z\neq -1,+1}$
which has the same cut as arcsin;

${\displaystyle \operatorname {arccot}(z)={\frac {\pi }{2}}-\arctan(z)\quad z\neq -i,i}$
which has the same cut as arctan;

${\displaystyle \operatorname {arcsec}(z)=\arccos \left({\frac {1}{z}}\right)\quad z\neq -1,0,+1}$
where the part of the real axis between −1 and +1 inclusive is the cut between the principal sheet of arcsec and other sheets;

${\displaystyle \operatorname {arccsc}(z)=\arcsin \left({\frac {1}{z}}\right)\quad z\neq -1,0,+1}$
which has the same cut as arcsec.


£#h5#£Logarithmic forms£#/h5#£
These functions may also be expressed using complex logarithms. This extends their domains to the complex plane in a natural fashion. The following identities for principal values of the functions hold everywhere that they are defined, even on their branch cuts.

${\displaystyle {\begin{aligned}\arcsin(z)&{}=-i\ln \left({\sqrt {1-z^{2}}}+iz\right)=i\ln \left({\sqrt {1-z^{2}}}-iz\right)&{}=\operatorname {arccsc} \left({\frac {1}{z}}\right)\\[10pt]\arccos(z)&{}=-i\ln \left(i{\sqrt {1-z^{2}}}+z\right)={\frac {\pi }{2}}-\arcsin(z)&{}=\operatorname {arcsec} \left({\frac {1}{z}}\right)\\[10pt]\arctan(z)&{}=-{\frac {i}{2}}\ln \left({\frac {i-z}{i+z}}\right)=-{\frac {i}{2}}\ln \left({\frac {1+iz}{1-iz}}\right)&{}=\operatorname {arccot} \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arccot}(z)&{}=-{\frac {i}{2}}\ln \left({\frac {z+i}{z-i}}\right)=-{\frac {i}{2}}\ln \left({\frac {iz-1}{iz+1}}\right)&{}=\arctan \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arcsec}(z)&{}=-i\ln \left(i{\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {1}{z}}\right)={\frac {\pi }{2}}-\operatorname {arccsc}(z)&{}=\arccos \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arccsc}(z)&{}=-i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {i}{z}}\right)=i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}-{\frac {i}{z}}\right)&{}=\arcsin \left({\frac {1}{z}}\right)\end{aligned}}}$

£#h5#£Generalization£#/h5#£
Because all of the inverse trigonometric functions output an angle of a right triangle, they can be generalized by using Euler's formula to form a right triangle in the complex plane. Algebraically, this gives us:

${\displaystyle ce^{\theta i}=c\cos(\theta )+ci\sin(\theta )}$
or

${\displaystyle ce^{\theta i}=a+bi}$
where ${\displaystyle a}$ is the adjacent side, ${\displaystyle b}$ is the opposite side, and ${\displaystyle c}$ is the hypotenuse. From here, we can solve for ${\displaystyle \theta }$ .

${\displaystyle {\begin{aligned}e^{\ln(c)+\theta i}&=a+bi\\\ln c+\theta i&=\ln(a+bi)\\\theta &=\operatorname {Im} \left(\ln(a+bi)\right)\end{aligned}}}$
or

${\displaystyle \theta =-i\ln \left({\frac {a+bi}{c}}\right)=i\ln \left({\frac {c}{a+bi}}\right)}$
Simply taking the imaginary part works for any real-valued ${\displaystyle a}$ and ${\displaystyle b}$ , but if ${\displaystyle a}$ or ${\displaystyle b}$ is complex-valued, we have to use the final equation so that the real part of the result isn't excluded. Since the length of the hypotenuse doesn't change the angle, ignoring the real part of ${\displaystyle \ln(a+bi)}$ also removes ${\displaystyle c}$ from the equation. In the final equation, we see that the angle of the triangle in the complex plane can be found by inputting the lengths of each side. By setting one of the three sides equal to 1 and one of the remaining sides equal to our input ${\displaystyle z}$ , we obtain a formula for one of the inverse trig functions, for a total of six equations. Because the inverse trig functions require only one input, we must put the final side of the triangle in terms of the other two using the Pythagorean Theorem relation

${\displaystyle a^{2}+b^{2}=c^{2}}$
The table below shows the values of a, b, and c for each of the inverse trig functions and the equivalent expressions for ${\displaystyle \theta }$ that result from plugging the values into the equations above and simplifying.

${\displaystyle {\begin{aligned}&a&&b&&c&&\theta &&\theta _{\text{simplified}}&&\theta _{a,b\in \mathbb {R} }\\\arcsin(z)\ \ &{\sqrt {1-z^{2}}}&&z&&1&&-i\ln \left({\frac {{\sqrt {1-z^{2}}}+zi}{1}}\right)&&=-i\ln \left({\sqrt {1-z^{2}}}+zi\right)&&\operatorname {Im} \left(\ln \left({\sqrt {1-z^{2}}}+zi\right)\right)\\\arccos(z)\ \ &z&&{\sqrt {1-z^{2}}}&&1&&-i\ln \left({\frac {z+i{\sqrt {1-z^{2}}}}{1}}\right)&&=-i\ln \left(z+{\sqrt {z^{2}-1}}\right)&&\operatorname {Im} \left(\ln \left(z+{\sqrt {z^{2}-1}}\right)\right)\\\arctan(z)\ \ &1&&z&&{\sqrt {1+z^{2}}}&&i\ln \left({\frac {\sqrt {1+z^{2}}}{1+zi}}\right)&&={\frac {i}{2}}\ln \left({\frac {i+z}{i-z}}\right)&&\operatorname {Im} \left(\ln \left(1+zi\right)\right)\\\operatorname {arccot}(z)\ \ &z&&1&&{\sqrt {z^{2}+1}}&&i\ln \left({\frac {\sqrt {z^{2}+1}}{z+i}}\right)&&={\frac {i}{2}}\ln \left({\frac {z-i}{z+i}}\right)&&\operatorname {Im} \left(\ln \left(z+i\right)\right)\\\operatorname {arcsec}(z)\ \ &1&&{\sqrt {z^{2}-1}}&&z&&-i\ln \left({\frac {1+i{\sqrt {z^{2}-1}}}{z}}\right)&&=-i\ln \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z^{2}}}-1}}\right)&&\operatorname {Im} \left(\ln \left(1+{\sqrt {1-z^{2}}}\right)\right)\\\operatorname {arccsc}(z)\ \ &{\sqrt {z^{2}-1}}&&1&&z&&-i\ln \left({\frac {{\sqrt {z^{2}-1}}+i}{z}}\right)&&=-i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {i}{z}}\right)&&\operatorname {Im} \left(\ln \left({\sqrt {z^{2}-1}}+i\right)\right)\\\end{aligned}}}$
In this sense, all of the inverse trig functions can be thought of as specific cases of the complex-valued log function. Since this definition works for any complex-valued ${\displaystyle z}$ , this definition allows for hyperbolic angles as outputs and can be used to further define the inverse hyperbolic functions. Elementary proofs of the relations may also proceed via expansion to exponential forms of the trigonometric functions.


£#h5#£Example proof£#/h5#£
${\displaystyle {\begin{aligned}\sin(\phi )&=z\\\phi &=\arcsin(z)\end{aligned}}}$
Using the exponential definition of sine, and letting ${\displaystyle \xi =e^{\phi i},}$

${\displaystyle {\begin{aligned}z&={\frac {e^{\phi i}-e^{-\phi i}}{2i}}\\[10mu]2iz&=\xi -{\frac {1}{\xi }}\\[5mu]0&=\xi ^{2}-2i\xi z-1\\[5mu]\xi &=iz\pm {\sqrt {1-z^{2}}}\\[5mu]\phi &=-i\ln \left(iz\pm {\sqrt {1-z^{2}}}\right)\end{aligned}}}$
(the positive branch is chosen)

${\displaystyle \phi =\arcsin(z)=-i\ln \left(iz+{\sqrt {1-z^{2}}}\right)}$

£#h5#£Applications£#/h5#£
£#h5#£Finding the angle of a right triangle£#/h5#£
Inverse trigonometric functions are useful when trying to determine the remaining two angles of a right triangle when the lengths of the sides of the triangle are known. Recalling the right-triangle definitions of sine and cosine, it follows that

${\displaystyle \theta =\arcsin \left({\frac {\text{opposite}}{\text{hypotenuse}}}\right)=\arccos \left({\frac {\text{adjacent}}{\text{hypotenuse}}}\right).}$
Often, the hypotenuse is unknown and would need to be calculated before using arcsine or arccosine using the Pythagorean Theorem: ${\displaystyle a^{2}+b^{2}=h^{2}}$ where ${\displaystyle h}$ is the length of the hypotenuse. Arctangent comes in handy in this situation, as the length of the hypotenuse is not needed.

${\displaystyle \theta =\arctan \left({\frac {\text{opposite}}{\text{adjacent}}}\right)\,.}$
For example, suppose a roof drops 8 feet as it runs out 20 feet. The roof makes an angle θ with the horizontal, where θ may be computed as follows:

${\displaystyle \theta =\arctan \left({\frac {\text{opposite}}{\text{adjacent}}}\right)=\arctan \left({\frac {\text{rise}}{\text{run}}}\right)=\arctan \left({\frac {8}{20}}\right)\approx 21.8^{\circ }\,.}$

£#h5#£In computer science and engineering£#/h5#£
£#h5#£Two-argument variant of arctangent£#/h5#£

The two-argument atan2 function computes the arctangent of y / x given y and x, but with a range of (−π, π]. In other words, atan2(y, x) is the angle between the positive x-axis of a plane and the point (x, y) on it, with positive sign for counter-clockwise angles (upper half-plane, y > 0), and negative sign for clockwise angles (lower half-plane, y < 0). It was first introduced in many computer programming languages, but it is now also common in other fields of science and engineering.

In terms of the standard arctan function, that is with range of (−π/2, π/2), it can be expressed as follows:

${\displaystyle \operatorname {atan2} (y,x)={\begin{cases}\arctan \left({\frac {y}{x}}\right)&\quad x>0\\\arctan \left({\frac {y}{x}}\right)+\pi &\quad y\geq 0,\;x<0\\\arctan \left({\frac {y}{x}}\right)-\pi &\quad y<0,\;x<0\\{\frac {\pi }{2}}&\quad y>0,\;x=0\\-{\frac {\pi }{2}}&\quad y<0,\;x=0\\{\text{undefined}}&\quad y=0,\;x=0\end{cases}}}$
It also equals the principal value of the argument of the complex number x + iy.

This limited version of the function above may also be defined using the tangent half-angle formulae as follows:

${\displaystyle \operatorname {atan2} (y,x)=2\arctan \left({\frac {y}{{\sqrt {x^{2}+y^{2}}}+x}}\right)}$
provided that either x > 0 or y ≠ 0. However this fails if given x ≤ 0 and y = 0 so the expression is unsuitable for computational use.

The above argument order (y, x) seems to be the most common, and in particular is used in ISO standards such as the C programming language, but a few authors may use the opposite convention (x, y) so some caution is warranted. These variations are detailed at atan2.


£#h5#£Arctangent function with location parameter£#/h5#£
In many applications the solution ${\displaystyle y}$ of the equation ${\displaystyle x=\tan(y)}$ is to come as close as possible to a given value ${\displaystyle -\infty <\eta <\infty }$ . The adequate solution is produced by the parameter modified arctangent function

${\displaystyle y=\arctan _{\eta }(x):=\arctan(x)+\pi \,\operatorname {rni} \left({\frac {\eta -\arctan(x)}{\pi }}\right)\,.}$
The function ${\displaystyle \operatorname {rni} }$ rounds to the nearest integer.


£#h5#£Numerical accuracy£#/h5#£
For angles near 0 and π, arccosine is ill-conditioned and will thus calculate the angle with reduced accuracy in a computer implementation (due to the limited number of digits). Similarly, arcsine is inaccurate for angles near −π/2 and π/2.


£#h5#£See also£#/h5#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Abramowitz, Milton; Stegun, Irene A., eds. (1972). Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables. New York: Dover Publications. ISBN 978-0-486-61272-0.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Inverse Tangent". MathWorld.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Trigonometric Functions £#/li#££#/ul#£




£#h3#£Arccotangent£#/h3#£

In mathematics, the inverse trigonometric functions (occasionally also called arcus functions, antitrigonometric functions or cyclometric functions) are the inverse functions of the trigonometric functions (with suitably restricted domains). Specifically, they are the inverses of the sine, cosine, tangent, cotangent, secant, and cosecant functions, and are used to obtain an angle from any of the angle's trigonometric ratios. Inverse trigonometric functions are widely used in engineering, navigation, physics, and geometry.


£#h5#£Notation£#/h5#£
Several notations for the inverse trigonometric functions exist. The most common convention is to name inverse trigonometric functions using an arc- prefix: arcsin(x), arccos(x), arctan(x), etc. (This convention is used throughout this article.) This notation arises from the following geometric relationships: when measuring in radians, an angle of θ radians will correspond to an arc whose length is rθ, where r is the radius of the circle. Thus in the unit circle, "the arc whose cosine is x" is the same as "the angle whose cosine is x", because the length of the arc of the circle in radii is the same as the measurement of the angle in radians. In computer programming languages, the inverse trigonometric functions are often called by the abbreviated forms asin, acos, atan.

The notations sin−1(x), cos−1(x), tan−1(x), etc., as introduced by John Herschel in 1813, are often used as well in English-language sources, much more than the also established sin[−1](x), cos[−1](x), tan[−1](x),—conventions consistent with the notation of an inverse function, that is useful e.g. to define the multivalued version of each inverse trigonometric function: e.g. ${\displaystyle \tan ^{-1}(x)=\{\arctan(x)+\pi k\mid k\in \mathbb {Z} \}}$ . However, this might appear to conflict logically with the common semantics for expressions such as sin2(x) (although only sin2 x, without parentheses, is the really common one), which refer to numeric power rather than function composition, and therefore may result in confusion between multiplicative inverse or reciprocal and compositional inverse. The confusion is somewhat mitigated by the fact that each of the reciprocal trigonometric functions has its own name—for example, (cos(x))−1 = sec(x). Nevertheless, certain authors advise against using it for its ambiguity. Another precarious convention used by a tiny number of authors is to use an uppercase first letter, along with a −1 superscript: Sin−1(x), Cos−1(x), Tan−1(x), etc. Although its intention is to avoid confusion with the multiplicative inverse, which should be represented by sin−1(x), cos−1(x), etc., or, better, by sin−1 x, cos−1 x, etc., it in turn creates yet another major source of ambiguity: especially in light of the fact that many popular high-level programming languages (e.g. Wolfram's Mathematica, and University of Sydney's MAGMA) use those very same capitalised representations for the standard trig functions; whereas others (Python (ie SymPy and NumPy), Matlab, MAPLE etc) use lower-case.

Hence, since 2009, the ISO 80000-2 standard has specified solely the "arc" prefix for the inverse functions.


£#h5#£Basic concepts£#/h5#£
£#h5#£Principal values£#/h5#£
Since none of the six trigonometric functions are one-to-one, they must be restricted in order to have inverse functions. Therefore, the result ranges of the inverse functions are proper (i.e. strict) subsets of the domains of the original functions.

For example, using function in the sense of multivalued functions, just as the square root function ${\displaystyle y={\sqrt {x}}}$ could be defined from ${\displaystyle y^{2}=x,}$ the function ${\displaystyle y=\arcsin(x)}$ is defined so that ${\displaystyle \sin(y)=x.}$ For a given real number ${\displaystyle x,}$ with ${\displaystyle -1\leq x\leq 1,}$ there are multiple (in fact, countably infinitely many) numbers ${\displaystyle y}$ such that ${\displaystyle \sin(y)=x}$ ; for example, ${\displaystyle \sin(0)=0,}$ but also ${\displaystyle \sin(\pi )=0,}$ ${\displaystyle \sin(2\pi )=0,}$ etc. When only one value is desired, the function may be restricted to its principal branch. With this restriction, for each ${\displaystyle x}$ in the domain, the expression ${\displaystyle \arcsin(x)}$ will evaluate only to a single value, called its principal value. These properties apply to all the inverse trigonometric functions.

The principal inverses are listed in the following table.

Note: Some authors define the range of arcsecant to be ${\textstyle (0\leq y<{\frac {\pi }{2}}{\text{ or }}\pi \leq y<{\frac {3\pi }{2}})}$ , because the tangent function is nonnegative on this domain. This makes some computations more consistent. For example, using this range, ${\displaystyle \tan(\operatorname {arcsec}(x))={\sqrt {x^{2}-1}},}$ whereas with the range ${\textstyle (0\leq y<{\frac {\pi }{2}}{\text{ or }}{\frac {\pi }{2}}<y\leq \pi )}$ , we would have to write ${\displaystyle \tan(\operatorname {arcsec}(x))=\pm {\sqrt {x^{2}-1}},}$ since tangent is nonnegative on ${\textstyle 0\leq y<{\frac {\pi }{2}},}$ but nonpositive on ${\textstyle {\frac {\pi }{2}}<y\leq \pi .}$ For a similar reason, the same authors define the range of arccosecant to be ${\textstyle (-\pi <y\leq -{\frac {\pi }{2}}}$ or ${\textstyle 0<y\leq {\frac {\pi }{2}}).}$

If ${\displaystyle x}$ is allowed to be a complex number, then the range of ${\displaystyle y}$ applies only to its real part.

The table below displays names and domains of the inverse trigonometric functions along with the range of their usual principal values in radians.

The symbol ${\displaystyle \mathbb {R} =(-\infty ,\infty )}$ denotes the set of all real numbers and ${\displaystyle \mathbb {Z} =\{\ldots ,\,-2,\,-1,\,0,\,1,\,2,\,\ldots \}}$ denotes the set of all integers. The set of all integer multiples of ${\displaystyle \pi }$ is denoted by

The symbol ${\displaystyle \,\setminus \,}$ denotes set subtraction so that, for instance, ${\displaystyle \mathbb {R} \setminus (-1,1)=(-\infty ,-1]\cup [1,\infty )}$ is the set of points in ${\displaystyle \mathbb {R} }$ (that is, real numbers) that are not in the interval ${\displaystyle (-1,1).}$

The Minkowski sum notation ${\textstyle \pi \mathbb {Z} +(0,\pi )}$ and ${\displaystyle \pi \mathbb {Z} +{\bigl (}{-{\tfrac {\pi }{2}}},{\tfrac {\pi }{2}}{\bigr )}}$ that is used above to concisely write the domains of ${\displaystyle \cot ,\csc ,\tan ,{\text{ and }}\sec }$ is now explained.

Domain of cotangent ${\displaystyle \cot }$ and cosecant ${\displaystyle \csc }$ : The domains of ${\displaystyle \,\cot \,}$ and ${\displaystyle \,\csc \,}$ are the same. They are the set of all angles ${\displaystyle \theta }$ at which ${\displaystyle \sin \theta \neq 0,}$ i.e. all real numbers that are not of the form ${\displaystyle \pi n}$ for some integer ${\displaystyle n,}$

Domain of tangent ${\displaystyle \tan }$ and secant ${\displaystyle \sec }$ : The domains of ${\displaystyle \,\tan \,}$ and ${\displaystyle \,\sec \,}$ are the same. They are the set of all angles ${\displaystyle \theta }$ at which ${\displaystyle \cos \theta \neq 0,}$


£#h5#£Solutions to elementary trigonometric equations£#/h5#£
Each of the trigonometric functions is periodic in the real part of its argument, running through all its values twice in each interval of ${\displaystyle 2\pi }$ :

£#ul#££#li#£Sine and cosecant begin their period at ${\textstyle 2\pi k-{\frac {\pi }{2}}}$ (where ${\displaystyle k}$ is an integer), finish it at ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ and then reverse themselves over ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ to ${\displaystyle 2\pi k+{\frac {3\pi }{2}}.}$ £#/li#£ £#li#£Cosine and secant begin their period at ${\displaystyle 2\pi k,}$ finish it at ${\displaystyle 2\pi k+\pi .}$ and then reverse themselves over ${\displaystyle 2\pi k+\pi }$ to ${\displaystyle 2\pi k+2\pi .}$ £#/li#£ £#li#£Tangent begins its period at ${\textstyle 2\pi k-{\frac {\pi }{2}},}$ finishes it at ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ and then repeats it (forward) over ${\textstyle 2\pi k+{\frac {\pi }{2}}}$ to ${\textstyle 2\pi k+{\frac {3\pi }{2}}.}$ £#/li#£ £#li#£Cotangent begins its period at ${\displaystyle 2\pi k,}$ finishes it at ${\displaystyle 2\pi k+\pi ,}$ and then repeats it (forward) over ${\displaystyle 2\pi k+\pi }$ to ${\displaystyle 2\pi k+2\pi .}$ £#/li#££#/ul#£
This periodicity is reflected in the general inverses, where ${\displaystyle k}$ is some integer.

The following table shows how inverse trigonometric functions may be used to solve equalities involving the six standard trigonometric functions. It is assumed that the given values ${\displaystyle \theta ,}$ ${\displaystyle r,}$ ${\displaystyle s,}$ ${\displaystyle x,}$ and ${\displaystyle y}$ all lie within appropriate ranges so that the relevant expressions below are well-defined. Note that "for some ${\displaystyle k\in \mathbb {Z} }$ " is just another way of saying "for some integer ${\displaystyle k.}$ "

The symbol ${\displaystyle \,\iff \,}$ is logical equality. The expression "LHS ${\displaystyle \,\iff \,}$ RHS" indicates that either (a) the left hand side (i.e. LHS) and right hand side (i.e. RHS) are both true, or else (b) the left hand side and right hand side are both false; there is no option (c) (e.g. it is not possible for the LHS statement to be true and also simultaneously for the RHS statement to false), because otherwise "LHS ${\displaystyle \,\iff \,}$ RHS" would not have been written (see this footnote for an example illustrating this concept).

For example, if ${\displaystyle \cos \theta =-1}$ then ${\displaystyle \theta =\pi +2\pi k=-\pi +2\pi (1+k)}$ for some ${\displaystyle k\in \mathbb {Z} .}$ While if ${\displaystyle \sin \theta =\pm 1}$ then ${\textstyle \theta ={\frac {\pi }{2}}+\pi k=-{\frac {\pi }{2}}+\pi (k+1)}$ for some ${\displaystyle k\in \mathbb {Z} ,}$ where ${\displaystyle k}$ will be even if ${\displaystyle \sin \theta =1}$ and it will be odd if ${\displaystyle \sin \theta =-1.}$ The equations ${\displaystyle \sec \theta =-1}$ and ${\displaystyle \csc \theta =\pm 1}$ have the same solutions as ${\displaystyle \cos \theta =-1}$ and ${\displaystyle \sin \theta =\pm 1,}$ respectively. In all equations above except for those just solved (i.e. except for ${\displaystyle \sin }$ / ${\displaystyle \csc \theta =\pm 1}$ and ${\displaystyle \cos }$ / ${\displaystyle \sec \theta =-1}$ ), the integer ${\displaystyle k}$ in the solution's formula is uniquely determined by ${\displaystyle \theta }$ (for fixed ${\displaystyle r,s,x,}$ and ${\displaystyle y}$ ).

Detailed example and explanation of the "plus or minus" symbol ${\displaystyle \pm }$
The solutions to ${\displaystyle \cos \theta =x}$ and ${\displaystyle \sec \theta =x}$ involve the "plus or minus" symbol ${\displaystyle \,\pm ,\,}$ whose meaning is now clarified. Only the solution to ${\displaystyle \cos \theta =x}$ will be discussed since the discussion for ${\displaystyle \sec \theta =x}$ is the same. We are given ${\displaystyle x}$ between ${\displaystyle -1\leq x\leq 1}$ and we know that there is an angle ${\displaystyle \theta }$ in some give interval that satisfies ${\displaystyle \cos \theta =x.}$ We want to find this ${\displaystyle \theta .}$ The formula for the solution involves:

If ${\displaystyle \,\arccos x=0\,}$ (which only happens when ${\displaystyle x=1}$ ) then ${\displaystyle \,+\arccos x=0\,}$ and ${\displaystyle \,-\arccos x=0\,}$ so either way, ${\displaystyle \,\pm \arccos x\,}$ can only be equal to ${\displaystyle 0.}$ But if ${\displaystyle \,\arccos x\neq 0,\,}$ which will now be assumed, then the solution to ${\displaystyle \cos \theta =x,}$ which is written above as is shorthand for the following statement:
Either £#ul#££#li#£ ${\displaystyle \,\theta =\arccos x+2\pi k\,}$ for some integer ${\displaystyle k,}$
or else£#/li#£ £#li#£ ${\displaystyle \,\theta =-\arccos x+2\pi k\,}$ for some integer ${\displaystyle k.}$ £#/li#££#/ul#£
Because ${\displaystyle \,\arccos x\neq 0\,}$ and ${\displaystyle \,0<\arccos x\leq \pi \,}$ exactly one of these two equalities can hold. Additional information about ${\displaystyle \theta }$ is needed to determine which one holds. For example, suppose that ${\displaystyle x=0}$ and that all that is known about ${\displaystyle \theta }$ is that ${\displaystyle \,-\pi \leq \theta \leq \pi \,}$ (and nothing more is known). Then

and moreover, in this particular case ${\displaystyle k=0}$ (for both the ${\displaystyle \,+\,}$ case and the ${\displaystyle \,-\,}$ case) and so consequently, This means that ${\displaystyle \theta }$ could be either ${\displaystyle \,\pi /2\,}$ or ${\displaystyle \,-\pi /2.}$ Without additional information it is not possible to determine which of these values ${\displaystyle \theta }$ has. An example of some additional information that could determine the value of ${\displaystyle \theta }$ would be knowing that the angle is above the ${\displaystyle x}$ -axis(in which case ${\displaystyle \theta =\pi /2}$ ) or alternatively, knowing that it is below the ${\displaystyle x}$ -axis(in which case ${\displaystyle \theta =-\pi /2}$ ).
Transforming equations

The equations above can be transformed by using the reflection and shift identities:

These formulas imply, in particular, that the following hold:

where swapping ${\displaystyle \sin \leftrightarrow \csc ,}$ swapping ${\displaystyle \cos \leftrightarrow \sec ,}$ and swapping ${\displaystyle \tan \leftrightarrow \cot }$ gives the analogous equations for ${\displaystyle \csc ,\sec ,{\text{ and }}\cot ,}$ respectively.
So for example, by using the equality ${\textstyle \sin \left({\frac {\pi }{2}}-\theta \right)=\cos \theta ,}$ the equation ${\displaystyle \cos \theta =x}$ can be transformed into ${\textstyle \sin \left({\frac {\pi }{2}}-\theta \right)=x,}$ which allows for the solution to the equation ${\displaystyle \;\sin \varphi =x\;}$ (where ${\textstyle \varphi :={\frac {\pi }{2}}-\theta }$ ) to be used; that solution being: ${\displaystyle \varphi =(-1)^{k}\arcsin(x)+\pi k\;{\text{ for some }}k\in \mathbb {Z} ,}$ which becomes:

where using the fact that ${\displaystyle (-1)^{k}=(-1)^{-k}}$ and substituting ${\displaystyle h:=-k}$ proves that another solution to ${\displaystyle \;\cos \theta =x\;}$ is: The substitution ${\displaystyle \;\arcsin x={\frac {\pi }{2}}-\arccos x\;}$ may be used express the right hand side of the above formula in terms of ${\displaystyle \;\arccos x\;}$ instead of ${\displaystyle \;\arcsin x.\;}$
£#h5#£Equal identical trigonometric functions£#/h5#£
The table below shows how two angles ${\displaystyle \theta }$ and ${\displaystyle \varphi }$ must be related if their values under a given trigonometric function are equal or negatives of each other.


£#h5#£Relationships between trigonometric functions and inverse trigonometric functions£#/h5#£
Trigonometric functions of inverse trigonometric functions are tabulated below. A quick way to derive them is by considering the geometry of a right-angled triangle, with one side of length 1 and another side of length ${\displaystyle x,}$ then applying the Pythagorean theorem and definitions of the trigonometric ratios. Purely algebraic derivations are longer. It is worth noting that for arcsecant and arccosecant, the diagram assumes that ${\displaystyle x}$ is positive, and thus the result has to be corrected through the use of absolute values and the signum (sgn) operation.


£#h5#£Relationships among the inverse trigonometric functions£#/h5#£
Complementary angles:

${\displaystyle {\begin{aligned}\arccos(x)&={\frac {\pi }{2}}-\arcsin(x)\\[0.5em]\operatorname {arccot}(x)&={\frac {\pi }{2}}-\arctan(x)\\[0.5em]\operatorname {arccsc}(x)&={\frac {\pi }{2}}-\operatorname {arcsec}(x)\end{aligned}}}$
Negative arguments:

${\displaystyle {\begin{aligned}\arcsin(-x)&=-\arcsin(x)\\\arccos(-x)&=\pi -\arccos(x)\\\arctan(-x)&=-\arctan(x)\\\operatorname {arccot}(-x)&=\pi -\operatorname {arccot}(x)\\\operatorname {arcsec}(-x)&=\pi -\operatorname {arcsec}(x)\\\operatorname {arccsc}(-x)&=-\operatorname {arccsc}(x)\end{aligned}}}$
Reciprocal arguments:

${\displaystyle {\begin{aligned}\arccos \left({\frac {1}{x}}\right)&=\operatorname {arcsec}(x)\\[0.3em]\arcsin \left({\frac {1}{x}}\right)&=\operatorname {arccsc}(x)\\[0.3em]\arctan \left({\frac {1}{x}}\right)&={\frac {\pi }{2}}-\arctan(x)=\operatorname {arccot}(x)\,,{\text{ if }}x>0\\[0.3em]\arctan \left({\frac {1}{x}}\right)&=-{\frac {\pi }{2}}-\arctan(x)=\operatorname {arccot}(x)-\pi \,,{\text{ if }}x<0\\[0.3em]\operatorname {arccot} \left({\frac {1}{x}}\right)&={\frac {\pi }{2}}-\operatorname {arccot}(x)=\arctan(x)\,,{\text{ if }}x>0\\[0.3em]\operatorname {arccot} \left({\frac {1}{x}}\right)&={\frac {3\pi }{2}}-\operatorname {arccot}(x)=\pi +\arctan(x)\,,{\text{ if }}x<0\\[0.3em]\operatorname {arcsec} \left({\frac {1}{x}}\right)&=\arccos(x)\\[0.3em]\operatorname {arccsc} \left({\frac {1}{x}}\right)&=\arcsin(x)\end{aligned}}}$
Useful identities if one only has a fragment of a sine table:

${\displaystyle {\begin{aligned}\arccos(x)&=\arcsin \left({\sqrt {1-x^{2}}}\right)\,,{\text{ if }}0\leq x\leq 1{\text{ , from which you get }}\\\arccos &\left({\frac {1-x^{2}}{1+x^{2}}}\right)=\arcsin \left({\frac {2x}{1+x^{2}}}\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin &\left({\sqrt {1-x^{2}}}\right)={\frac {\pi }{2}}-\operatorname {sgn}(x)\arcsin(x)\\\arccos(x)&={\frac {1}{2}}\arccos \left(2x^{2}-1\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin(x)&={\frac {1}{2}}\arccos \left(1-2x^{2}\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin(x)&=\arctan \left({\frac {x}{\sqrt {1-x^{2}}}}\right)\\\arccos(x)&=\arctan \left({\frac {\sqrt {1-x^{2}}}{x}}\right)\\\arctan(x)&=\arcsin \left({\frac {x}{\sqrt {1+x^{2}}}}\right)\\\operatorname {arccot}(x)&=\arccos \left({\frac {x}{\sqrt {1+x^{2}}}}\right)\end{aligned}}}$
Whenever the square root of a complex number is used here, we choose the root with the positive real part (or positive imaginary part if the square was negative real).

A useful form that follows directly from the table above is

${\displaystyle \arctan \left(x\right)=\arccos \left({\sqrt {\frac {1}{1+x^{2}}}}\right)\,,{\text{ if }}x\geq 0}$ .
It is obtained by recognizing that ${\displaystyle \cos \left(\arctan \left(x\right)\right)={\sqrt {\frac {1}{1+x^{2}}}}=\cos \left(\arccos \left({\sqrt {\frac {1}{1+x^{2}}}}\right)\right)}$ .

From the half-angle formula, ${\displaystyle \tan \left({\tfrac {\theta }{2}}\right)={\tfrac {\sin(\theta )}{1+\cos(\theta )}}}$ , we get:

${\displaystyle {\begin{aligned}\arcsin(x)&=2\arctan \left({\frac {x}{1+{\sqrt {1-x^{2}}}}}\right)\\[0.5em]\arccos(x)&=2\arctan \left({\frac {\sqrt {1-x^{2}}}{1+x}}\right)\,,{\text{ if }}-1<x\leq 1\\[0.5em]\arctan(x)&=2\arctan \left({\frac {x}{1+{\sqrt {1+x^{2}}}}}\right)\end{aligned}}}$

£#h5#£Arctangent addition formula£#/h5#£
${\displaystyle \arctan(u)\pm \arctan(v)=\arctan \left({\frac {u\pm v}{1\mp uv}}\right){\pmod {\pi }}\,,\quad uv\neq 1\,.}$
This is derived from the tangent addition formula

${\displaystyle \tan(\alpha \pm \beta )={\frac {\tan(\alpha )\pm \tan(\beta )}{1\mp \tan(\alpha )\tan(\beta )}}\,,}$
by letting

${\displaystyle \alpha =\arctan(u)\,,\quad \beta =\arctan(v)\,.}$

£#h5#£In calculus£#/h5#£
£#h5#£Derivatives of inverse trigonometric functions£#/h5#£
The derivatives for complex values of z are as follows:

${\displaystyle {\begin{aligned}{\frac {d}{dz}}\arcsin(z)&{}={\frac {1}{\sqrt {1-z^{2}}}}\;;&z&{}\neq -1,+1\\{\frac {d}{dz}}\arccos(z)&{}=-{\frac {1}{\sqrt {1-z^{2}}}}\;;&z&{}\neq -1,+1\\{\frac {d}{dz}}\arctan(z)&{}={\frac {1}{1+z^{2}}}\;;&z&{}\neq -i,+i\\{\frac {d}{dz}}\operatorname {arccot}(z)&{}=-{\frac {1}{1+z^{2}}}\;;&z&{}\neq -i,+i\\{\frac {d}{dz}}\operatorname {arcsec}(z)&{}={\frac {1}{z^{2}{\sqrt {1-{\frac {1}{z^{2}}}}}}}\;;&z&{}\neq -1,0,+1\\{\frac {d}{dz}}\operatorname {arccsc}(z)&{}=-{\frac {1}{z^{2}{\sqrt {1-{\frac {1}{z^{2}}}}}}}\;;&z&{}\neq -1,0,+1\end{aligned}}}$
Only for real values of x:

${\displaystyle {\begin{aligned}{\frac {d}{dx}}\operatorname {arcsec}(x)&{}={\frac {1}{|x|{\sqrt {x^{2}-1}}}}\;;&|x|>1\\{\frac {d}{dx}}\operatorname {arccsc}(x)&{}=-{\frac {1}{|x|{\sqrt {x^{2}-1}}}}\;;&|x|>1\end{aligned}}}$
For a sample derivation: if ${\displaystyle \theta =\arcsin(x)}$ , we get:

${\displaystyle {\frac {d\arcsin(x)}{dx}}={\frac {d\theta }{d\sin(\theta )}}={\frac {d\theta }{\cos(\theta )\,d\theta }}={\frac {1}{\cos(\theta )}}={\frac {1}{\sqrt {1-\sin ^{2}(\theta )}}}={\frac {1}{\sqrt {1-x^{2}}}}}$

£#h5#£Expression as definite integrals£#/h5#£
Integrating the derivative and fixing the value at one point gives an expression for the inverse trigonometric function as a definite integral:

${\displaystyle {\begin{aligned}\arcsin(x)&{}=\int _{0}^{x}{\frac {1}{\sqrt {1-z^{2}}}}\,dz\;,&|x|&{}\leq 1\\\arccos(x)&{}=\int _{x}^{1}{\frac {1}{\sqrt {1-z^{2}}}}\,dz\;,&|x|&{}\leq 1\\\arctan(x)&{}=\int _{0}^{x}{\frac {1}{z^{2}+1}}\,dz\;,\\\operatorname {arccot}(x)&{}=\int _{x}^{\infty }{\frac {1}{z^{2}+1}}\,dz\;,\\\operatorname {arcsec}(x)&{}=\int _{1}^{x}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz=\pi +\int _{-x}^{-1}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz\;,&x&{}\geq 1\\\operatorname {arccsc}(x)&{}=\int _{x}^{\infty }{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz=\int _{-\infty }^{-x}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz\;,&x&{}\geq 1\\\end{aligned}}}$
When x equals 1, the integrals with limited domains are improper integrals, but still well-defined.


£#h5#£Infinite series£#/h5#£
Similar to the sine and cosine functions, the inverse trigonometric functions can also be calculated using power series, as follows. For arcsine, the series can be derived by expanding its derivative, ${\textstyle {\tfrac {1}{\sqrt {1-z^{2}}}}}$ , as a binomial series, and integrating term by term (using the integral definition as above). The series for arctangent can similarly be derived by expanding its derivative ${\textstyle {\frac {1}{1+z^{2}}}}$ in a geometric series, and applying the integral definition above (see Leibniz series).

${\displaystyle {\begin{aligned}\arcsin(z)&=z+\left({\frac {1}{2}}\right){\frac {z^{3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {z^{5}}{5}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {z^{7}}{7}}+\cdots \\[5pt]&=\sum _{n=0}^{\infty }{\frac {(2n-1)!!}{(2n)!!}}{\frac {z^{2n+1}}{2n+1}}\\[5pt]&=\sum _{n=0}^{\infty }{\frac {(2n)!}{(2^{n}n!)^{2}}}{\frac {z^{2n+1}}{2n+1}}\,;\qquad |z|\leq 1\end{aligned}}}$
${\displaystyle \arctan(z)=z-{\frac {z^{3}}{3}}+{\frac {z^{5}}{5}}-{\frac {z^{7}}{7}}+\cdots =\sum _{n=0}^{\infty }{\frac {(-1)^{n}z^{2n+1}}{2n+1}}\,;\qquad |z|\leq 1\qquad z\neq i,-i}$
Series for the other inverse trigonometric functions can be given in terms of these according to the relationships given above. For example, ${\displaystyle \arccos(x)=\pi /2-\arcsin(x)}$ , ${\displaystyle \operatorname {arccsc}(x)=\arcsin(1/x)}$ , and so on. Another series is given by:

${\displaystyle 2\left(\arcsin \left({\frac {x}{2}}\right)\right)^{2}=\sum _{n=1}^{\infty }{\frac {x^{2n}}{n^{2}{\binom {2n}{n}}}}.}$
Leonhard Euler found a series for the arctangent that converges more quickly than its Taylor series:

${\displaystyle \arctan(z)={\frac {z}{1+z^{2}}}\sum _{n=0}^{\infty }\prod _{k=1}^{n}{\frac {2kz^{2}}{(2k+1)(1+z^{2})}}.}$
(The term in the sum for n = 0 is the empty product, so is 1.)

Alternatively, this can be expressed as

${\displaystyle \arctan(z)=\sum _{n=0}^{\infty }{\frac {2^{2n}(n!)^{2}}{(2n+1)!}}{\frac {z^{2n+1}}{(1+z^{2})^{n+1}}}.}$
Another series for the arctangent function is given by

${\displaystyle \arctan(z)=i\sum _{n=1}^{\infty }{\frac {1}{2n-1}}\left({\frac {1}{(1+2i/z)^{2n-1}}}-{\frac {1}{(1-2i/z)^{2n-1}}}\right),}$
where ${\displaystyle i={\sqrt {-1}}}$ is the imaginary unit.


£#h5#£Continued fractions for arctangent£#/h5#£
Two alternatives to the power series for arctangent are these generalized continued fractions:

${\displaystyle \arctan(z)={\frac {z}{1+{\cfrac {(1z)^{2}}{3-1z^{2}+{\cfrac {(3z)^{2}}{5-3z^{2}+{\cfrac {(5z)^{2}}{7-5z^{2}+{\cfrac {(7z)^{2}}{9-7z^{2}+\ddots }}}}}}}}}}={\frac {z}{1+{\cfrac {(1z)^{2}}{3+{\cfrac {(2z)^{2}}{5+{\cfrac {(3z)^{2}}{7+{\cfrac {(4z)^{2}}{9+\ddots }}}}}}}}}}}$
The second of these is valid in the cut complex plane. There are two cuts, from −i to the point at infinity, going down the imaginary axis, and from i to the point at infinity, going up the same axis. It works best for real numbers running from −1 to 1. The partial denominators are the odd natural numbers, and the partial numerators (after the first) are just (nz)2, with each perfect square appearing once. The first was developed by Leonhard Euler; the second by Carl Friedrich Gauss utilizing the Gaussian hypergeometric series.


£#h5#£Indefinite integrals of inverse trigonometric functions£#/h5#£
For real and complex values of z:

${\displaystyle {\begin{aligned}\int \arcsin(z)\,dz&{}=z\,\arcsin(z)+{\sqrt {1-z^{2}}}+C\\\int \arccos(z)\,dz&{}=z\,\arccos(z)-{\sqrt {1-z^{2}}}+C\\\int \arctan(z)\,dz&{}=z\,\arctan(z)-{\frac {1}{2}}\ln \left(1+z^{2}\right)+C\\\int \operatorname {arccot}(z)\,dz&{}=z\,\operatorname {arccot}(z)+{\frac {1}{2}}\ln \left(1+z^{2}\right)+C\\\int \operatorname {arcsec}(z)\,dz&{}=z\,\operatorname {arcsec}(z)-\ln \left[z\left(1+{\sqrt {\frac {z^{2}-1}{z^{2}}}}\right)\right]+C\\\int \operatorname {arccsc}(z)\,dz&{}=z\,\operatorname {arccsc}(z)+\ln \left[z\left(1+{\sqrt {\frac {z^{2}-1}{z^{2}}}}\right)\right]+C\end{aligned}}}$
For real x ≥ 1:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\ln \left(x+{\sqrt {x^{2}-1}}\right)+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\ln \left(x+{\sqrt {x^{2}-1}}\right)+C\end{aligned}}}$
For all real x not between -1 and 1:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\operatorname {sgn}(x)\ln \left|x+{\sqrt {x^{2}-1}}\right|+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\operatorname {sgn}(x)\ln \left|x+{\sqrt {x^{2}-1}}\right|+C\end{aligned}}}$
The absolute value is necessary to compensate for both negative and positive values of the arcsecant and arccosecant functions. The signum function is also necessary due to the absolute values in the derivatives of the two functions, which create two different solutions for positive and negative values of x. These can be further simplified using the logarithmic definitions of the inverse hyperbolic functions:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\operatorname {arcosh} (|x|)+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\operatorname {arcosh} (|x|)+C\\\end{aligned}}}$
The absolute value in the argument of the arcosh function creates a negative half of its graph, making it identical to the signum logarithmic function shown above.

All of these antiderivatives can be derived using integration by parts and the simple derivative forms shown above.


£#h5#£Example£#/h5#£
Using ${\displaystyle \int u\,dv=uv-\int v\,du}$ (i.e. integration by parts), set

${\displaystyle {\begin{aligned}u&=\arcsin(x)&dv&=dx\\du&={\frac {dx}{\sqrt {1-x^{2}}}}&v&=x\end{aligned}}}$
Then

${\displaystyle \int \arcsin(x)\,dx=x\arcsin(x)-\int {\frac {x}{\sqrt {1-x^{2}}}}\,dx,}$
which by the simple substitution ${\displaystyle w=1-x^{2},\ dw=-2x\,dx}$ yields the final result:

${\displaystyle \int \arcsin(x)\,dx=x\arcsin(x)+{\sqrt {1-x^{2}}}+C}$

£#h5#£Extension to complex plane£#/h5#£
Since the inverse trigonometric functions are analytic functions, they can be extended from the real line to the complex plane. This results in functions with multiple sheets and branch points. One possible way of defining the extension is:

${\displaystyle \arctan(z)=\int _{0}^{z}{\frac {dx}{1+x^{2}}}\quad z\neq -i,+i}$
where the part of the imaginary axis which does not lie strictly between the branch points (−i and +i) is the branch cut between the principal sheet and other sheets. The path of the integral must not cross a branch cut. For z not on a branch cut, a straight line path from 0 to z is such a path. For z on a branch cut, the path must approach from Re[x] > 0 for the upper branch cut and from Re[x] < 0 for the lower branch cut.

The arcsine function may then be defined as:

${\displaystyle \arcsin(z)=\arctan \left({\frac {z}{\sqrt {1-z^{2}}}}\right)\quad z\neq -1,+1}$
where (the square-root function has its cut along the negative real axis and) the part of the real axis which does not lie strictly between −1 and +1 is the branch cut between the principal sheet of arcsin and other sheets;

${\displaystyle \arccos(z)={\frac {\pi }{2}}-\arcsin(z)\quad z\neq -1,+1}$
which has the same cut as arcsin;

${\displaystyle \operatorname {arccot}(z)={\frac {\pi }{2}}-\arctan(z)\quad z\neq -i,i}$
which has the same cut as arctan;

${\displaystyle \operatorname {arcsec}(z)=\arccos \left({\frac {1}{z}}\right)\quad z\neq -1,0,+1}$
where the part of the real axis between −1 and +1 inclusive is the cut between the principal sheet of arcsec and other sheets;

${\displaystyle \operatorname {arccsc}(z)=\arcsin \left({\frac {1}{z}}\right)\quad z\neq -1,0,+1}$
which has the same cut as arcsec.


£#h5#£Logarithmic forms£#/h5#£
These functions may also be expressed using complex logarithms. This extends their domains to the complex plane in a natural fashion. The following identities for principal values of the functions hold everywhere that they are defined, even on their branch cuts.

${\displaystyle {\begin{aligned}\arcsin(z)&{}=-i\ln \left({\sqrt {1-z^{2}}}+iz\right)=i\ln \left({\sqrt {1-z^{2}}}-iz\right)&{}=\operatorname {arccsc} \left({\frac {1}{z}}\right)\\[10pt]\arccos(z)&{}=-i\ln \left(i{\sqrt {1-z^{2}}}+z\right)={\frac {\pi }{2}}-\arcsin(z)&{}=\operatorname {arcsec} \left({\frac {1}{z}}\right)\\[10pt]\arctan(z)&{}=-{\frac {i}{2}}\ln \left({\frac {i-z}{i+z}}\right)=-{\frac {i}{2}}\ln \left({\frac {1+iz}{1-iz}}\right)&{}=\operatorname {arccot} \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arccot}(z)&{}=-{\frac {i}{2}}\ln \left({\frac {z+i}{z-i}}\right)=-{\frac {i}{2}}\ln \left({\frac {iz-1}{iz+1}}\right)&{}=\arctan \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arcsec}(z)&{}=-i\ln \left(i{\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {1}{z}}\right)={\frac {\pi }{2}}-\operatorname {arccsc}(z)&{}=\arccos \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arccsc}(z)&{}=-i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {i}{z}}\right)=i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}-{\frac {i}{z}}\right)&{}=\arcsin \left({\frac {1}{z}}\right)\end{aligned}}}$

£#h5#£Generalization£#/h5#£
Because all of the inverse trigonometric functions output an angle of a right triangle, they can be generalized by using Euler's formula to form a right triangle in the complex plane. Algebraically, this gives us:

${\displaystyle ce^{\theta i}=c\cos(\theta )+ci\sin(\theta )}$
or

${\displaystyle ce^{\theta i}=a+bi}$
where ${\displaystyle a}$ is the adjacent side, ${\displaystyle b}$ is the opposite side, and ${\displaystyle c}$ is the hypotenuse. From here, we can solve for ${\displaystyle \theta }$ .

${\displaystyle {\begin{aligned}e^{\ln(c)+\theta i}&=a+bi\\\ln c+\theta i&=\ln(a+bi)\\\theta &=\operatorname {Im} \left(\ln(a+bi)\right)\end{aligned}}}$
or

${\displaystyle \theta =-i\ln \left({\frac {a+bi}{c}}\right)=i\ln \left({\frac {c}{a+bi}}\right)}$
Simply taking the imaginary part works for any real-valued ${\displaystyle a}$ and ${\displaystyle b}$ , but if ${\displaystyle a}$ or ${\displaystyle b}$ is complex-valued, we have to use the final equation so that the real part of the result isn't excluded. Since the length of the hypotenuse doesn't change the angle, ignoring the real part of ${\displaystyle \ln(a+bi)}$ also removes ${\displaystyle c}$ from the equation. In the final equation, we see that the angle of the triangle in the complex plane can be found by inputting the lengths of each side. By setting one of the three sides equal to 1 and one of the remaining sides equal to our input ${\displaystyle z}$ , we obtain a formula for one of the inverse trig functions, for a total of six equations. Because the inverse trig functions require only one input, we must put the final side of the triangle in terms of the other two using the Pythagorean Theorem relation

${\displaystyle a^{2}+b^{2}=c^{2}}$
The table below shows the values of a, b, and c for each of the inverse trig functions and the equivalent expressions for ${\displaystyle \theta }$ that result from plugging the values into the equations above and simplifying.

${\displaystyle {\begin{aligned}&a&&b&&c&&\theta &&\theta _{\text{simplified}}&&\theta _{a,b\in \mathbb {R} }\\\arcsin(z)\ \ &{\sqrt {1-z^{2}}}&&z&&1&&-i\ln \left({\frac {{\sqrt {1-z^{2}}}+zi}{1}}\right)&&=-i\ln \left({\sqrt {1-z^{2}}}+zi\right)&&\operatorname {Im} \left(\ln \left({\sqrt {1-z^{2}}}+zi\right)\right)\\\arccos(z)\ \ &z&&{\sqrt {1-z^{2}}}&&1&&-i\ln \left({\frac {z+i{\sqrt {1-z^{2}}}}{1}}\right)&&=-i\ln \left(z+{\sqrt {z^{2}-1}}\right)&&\operatorname {Im} \left(\ln \left(z+{\sqrt {z^{2}-1}}\right)\right)\\\arctan(z)\ \ &1&&z&&{\sqrt {1+z^{2}}}&&i\ln \left({\frac {\sqrt {1+z^{2}}}{1+zi}}\right)&&={\frac {i}{2}}\ln \left({\frac {i+z}{i-z}}\right)&&\operatorname {Im} \left(\ln \left(1+zi\right)\right)\\\operatorname {arccot}(z)\ \ &z&&1&&{\sqrt {z^{2}+1}}&&i\ln \left({\frac {\sqrt {z^{2}+1}}{z+i}}\right)&&={\frac {i}{2}}\ln \left({\frac {z-i}{z+i}}\right)&&\operatorname {Im} \left(\ln \left(z+i\right)\right)\\\operatorname {arcsec}(z)\ \ &1&&{\sqrt {z^{2}-1}}&&z&&-i\ln \left({\frac {1+i{\sqrt {z^{2}-1}}}{z}}\right)&&=-i\ln \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z^{2}}}-1}}\right)&&\operatorname {Im} \left(\ln \left(1+{\sqrt {1-z^{2}}}\right)\right)\\\operatorname {arccsc}(z)\ \ &{\sqrt {z^{2}-1}}&&1&&z&&-i\ln \left({\frac {{\sqrt {z^{2}-1}}+i}{z}}\right)&&=-i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {i}{z}}\right)&&\operatorname {Im} \left(\ln \left({\sqrt {z^{2}-1}}+i\right)\right)\\\end{aligned}}}$
In this sense, all of the inverse trig functions can be thought of as specific cases of the complex-valued log function. Since this definition works for any complex-valued ${\displaystyle z}$ , this definition allows for hyperbolic angles as outputs and can be used to further define the inverse hyperbolic functions. Elementary proofs of the relations may also proceed via expansion to exponential forms of the trigonometric functions.


£#h5#£Example proof£#/h5#£
${\displaystyle {\begin{aligned}\sin(\phi )&=z\\\phi &=\arcsin(z)\end{aligned}}}$
Using the exponential definition of sine, and letting ${\displaystyle \xi =e^{\phi i},}$

${\displaystyle {\begin{aligned}z&={\frac {e^{\phi i}-e^{-\phi i}}{2i}}\\[10mu]2iz&=\xi -{\frac {1}{\xi }}\\[5mu]0&=\xi ^{2}-2i\xi z-1\\[5mu]\xi &=iz\pm {\sqrt {1-z^{2}}}\\[5mu]\phi &=-i\ln \left(iz\pm {\sqrt {1-z^{2}}}\right)\end{aligned}}}$
(the positive branch is chosen)

${\displaystyle \phi =\arcsin(z)=-i\ln \left(iz+{\sqrt {1-z^{2}}}\right)}$

£#h5#£Applications£#/h5#£
£#h5#£Finding the angle of a right triangle£#/h5#£
Inverse trigonometric functions are useful when trying to determine the remaining two angles of a right triangle when the lengths of the sides of the triangle are known. Recalling the right-triangle definitions of sine and cosine, it follows that

${\displaystyle \theta =\arcsin \left({\frac {\text{opposite}}{\text{hypotenuse}}}\right)=\arccos \left({\frac {\text{adjacent}}{\text{hypotenuse}}}\right).}$
Often, the hypotenuse is unknown and would need to be calculated before using arcsine or arccosine using the Pythagorean Theorem: ${\displaystyle a^{2}+b^{2}=h^{2}}$ where ${\displaystyle h}$ is the length of the hypotenuse. Arctangent comes in handy in this situation, as the length of the hypotenuse is not needed.

${\displaystyle \theta =\arctan \left({\frac {\text{opposite}}{\text{adjacent}}}\right)\,.}$
For example, suppose a roof drops 8 feet as it runs out 20 feet. The roof makes an angle θ with the horizontal, where θ may be computed as follows:

${\displaystyle \theta =\arctan \left({\frac {\text{opposite}}{\text{adjacent}}}\right)=\arctan \left({\frac {\text{rise}}{\text{run}}}\right)=\arctan \left({\frac {8}{20}}\right)\approx 21.8^{\circ }\,.}$

£#h5#£In computer science and engineering£#/h5#£
£#h5#£Two-argument variant of arctangent£#/h5#£

The two-argument atan2 function computes the arctangent of y / x given y and x, but with a range of (−π, π]. In other words, atan2(y, x) is the angle between the positive x-axis of a plane and the point (x, y) on it, with positive sign for counter-clockwise angles (upper half-plane, y > 0), and negative sign for clockwise angles (lower half-plane, y < 0). It was first introduced in many computer programming languages, but it is now also common in other fields of science and engineering.

In terms of the standard arctan function, that is with range of (−π/2, π/2), it can be expressed as follows:

${\displaystyle \operatorname {atan2} (y,x)={\begin{cases}\arctan \left({\frac {y}{x}}\right)&\quad x>0\\\arctan \left({\frac {y}{x}}\right)+\pi &\quad y\geq 0,\;x<0\\\arctan \left({\frac {y}{x}}\right)-\pi &\quad y<0,\;x<0\\{\frac {\pi }{2}}&\quad y>0,\;x=0\\-{\frac {\pi }{2}}&\quad y<0,\;x=0\\{\text{undefined}}&\quad y=0,\;x=0\end{cases}}}$
It also equals the principal value of the argument of the complex number x + iy.

This limited version of the function above may also be defined using the tangent half-angle formulae as follows:

${\displaystyle \operatorname {atan2} (y,x)=2\arctan \left({\frac {y}{{\sqrt {x^{2}+y^{2}}}+x}}\right)}$
provided that either x > 0 or y ≠ 0. However this fails if given x ≤ 0 and y = 0 so the expression is unsuitable for computational use.

The above argument order (y, x) seems to be the most common, and in particular is used in ISO standards such as the C programming language, but a few authors may use the opposite convention (x, y) so some caution is warranted. These variations are detailed at atan2.


£#h5#£Arctangent function with location parameter£#/h5#£
In many applications the solution ${\displaystyle y}$ of the equation ${\displaystyle x=\tan(y)}$ is to come as close as possible to a given value ${\displaystyle -\infty <\eta <\infty }$ . The adequate solution is produced by the parameter modified arctangent function

${\displaystyle y=\arctan _{\eta }(x):=\arctan(x)+\pi \,\operatorname {rni} \left({\frac {\eta -\arctan(x)}{\pi }}\right)\,.}$
The function ${\displaystyle \operatorname {rni} }$ rounds to the nearest integer.


£#h5#£Numerical accuracy£#/h5#£
For angles near 0 and π, arccosine is ill-conditioned and will thus calculate the angle with reduced accuracy in a computer implementation (due to the limited number of digits). Similarly, arcsine is inaccurate for angles near −π/2 and π/2.


£#h5#£See also£#/h5#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Abramowitz, Milton; Stegun, Irene A., eds. (1972). Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables. New York: Dover Publications. ISBN 978-0-486-61272-0.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Inverse Tangent". MathWorld.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Trigonometric Functions £#/li#££#/ul#£




£#h3#£Arccoth£#/h3#£


In mathematics, the inverse hyperbolic functions are the inverse functions of the hyperbolic functions.

For a given value of a hyperbolic function, the corresponding inverse hyperbolic function provides the corresponding hyperbolic angle. The size of the hyperbolic angle is equal to the area of the corresponding hyperbolic sector of the hyperbola xy = 1, or twice the area of the corresponding sector of the unit hyperbola x2 − y2 = 1, just as a circular angle is twice the area of the circular sector of the unit circle. Some authors have called inverse hyperbolic functions "area functions" to realize the hyperbolic angles.

Hyperbolic functions occur in the calculations of angles and distances in hyperbolic geometry. It also occurs in the solutions of many linear differential equations (such as the equation defining a catenary), cubic equations, and Laplace's equation in Cartesian coordinates. Laplace's equations are important in many areas of physics, including electromagnetic theory, heat transfer, fluid dynamics, and special relativity.


£#h5#£Notation£#/h5#£
The ISO 80000-2 standard abbreviations consist of ar- followed by the abbreviation of the corresponding hyperbolic function (e.g., arsinh, arcosh). The prefix arc- followed by the corresponding hyperbolic function (e.g., arcsinh, arccosh) is also commonly seen, by analogy with the nomenclature for inverse trigonometric functions. These are misnomers, since the prefix arc is the abbreviation for arcus, while the prefix ar stands for area; the hyperbolic functions are not directly related to arcs.

Other authors prefer to use the notation argsinh, argcosh, argtanh, and so on, where the prefix arg is the abbreviation of the Latin argumentum. In computer science, this is often shortened to asinh.

The notation sinh−1(x), cosh−1(x), etc., is also used, despite the fact that care must be taken to avoid misinterpretations of the superscript −1 as a power, as opposed to a shorthand to denote the inverse function (e.g., cosh−1(x) versus cosh(x)−1).


£#h5#£Definitions in terms of logarithms£#/h5#£
Since the hyperbolic functions are rational functions of ex whose numerator and denominator are of degree at most two, these functions may be solved in terms of ex, by using the quadratic formula; then, taking the natural logarithm gives the following expressions for the inverse hyperbolic functions.

For complex arguments, the inverse hyperbolic functions, the square root and the logarithm are multi-valued functions, and the equalities of the next subsections may be viewed as equalities of multi-valued functions.

For all inverse hyperbolic functions (save the inverse hyperbolic cotangent and the inverse hyperbolic cosecant), the domain of the real function is connected.


£#h5#£Inverse hyperbolic sine£#/h5#£
Inverse hyperbolic sine (a.k.a. area hyperbolic sine) (Latin: Area sinus hyperbolicus):

${\displaystyle \operatorname {arsinh} x=\ln \left(x+{\sqrt {x^{2}+1}}\right)}$
The domain is the whole real line.


£#h5#£Inverse hyperbolic cosine£#/h5#£
Inverse hyperbolic cosine (a.k.a. area hyperbolic cosine) (Latin: Area cosinus hyperbolicus):

${\displaystyle \operatorname {arcosh} x=\ln \left(x+{\sqrt {x^{2}-1}}\right)}$
The domain is the closed interval [1, +∞ ).


£#h5#£Inverse hyperbolic tangent£#/h5#£
Inverse hyperbolic tangent (a.k.a. area hyperbolic tangent) (Latin: Area tangens hyperbolicus):

${\displaystyle \operatorname {artanh} x={\frac {1}{2}}\ln \left({\frac {1+x}{1-x}}\right)}$
The domain is the open interval (−1, 1).


£#h5#£Inverse hyperbolic cotangent£#/h5#£
Inverse hyperbolic cotangent (a.k.a., area hyperbolic cotangent) (Latin: Area cotangens hyperbolicus):

${\displaystyle \operatorname {arcoth} x={\frac {1}{2}}\ln \left({\frac {x+1}{x-1}}\right)}$
The domain is the union of the open intervals (−∞, −1) and (1, +∞).


£#h5#£Inverse hyperbolic secant£#/h5#£
Inverse hyperbolic secant (a.k.a., area hyperbolic secant) (Latin: Area secans hyperbolicus):

${\displaystyle \operatorname {arsech} x=\ln \left({\frac {1}{x}}+{\sqrt {{\frac {1}{x^{2}}}-1}}\right)=\ln \left({\frac {1+{\sqrt {1-x^{2}}}}{x}}\right)}$
The domain is the semi-open interval (0, 1].


£#h5#£Inverse hyperbolic cosecant£#/h5#£
Inverse hyperbolic cosecant (a.k.a., area hyperbolic cosecant) (Latin: Area cosecans hyperbolicus):

${\displaystyle \operatorname {arcsch} x=\ln \left({\frac {1}{x}}+{\sqrt {{\frac {1}{x^{2}}}+1}}\right)}$
The domain is the real line with 0 removed.


£#h5#£Addition formulae£#/h5#£
${\displaystyle \operatorname {arsinh} u\pm \operatorname {arsinh} v=\operatorname {arsinh} \left(u{\sqrt {1+v^{2}}}\pm v{\sqrt {1+u^{2}}}\right)}$
${\displaystyle \operatorname {arcosh} u\pm \operatorname {arcosh} v=\operatorname {arcosh} \left(uv\pm {\sqrt {(u^{2}-1)(v^{2}-1)}}\right)}$
${\displaystyle \operatorname {artanh} u\pm \operatorname {artanh} v=\operatorname {artanh} \left({\frac {u\pm v}{1\pm uv}}\right)}$
${\displaystyle \operatorname {arcoth} u\pm \operatorname {arcoth} v=\operatorname {arcoth} \left({\frac {1\pm uv}{u\pm v}}\right)}$
${\displaystyle {\begin{aligned}\operatorname {arsinh} u+\operatorname {arcosh} v&=\operatorname {arsinh} \left(uv+{\sqrt {(1+u^{2})(v^{2}-1)}}\right)\\&=\operatorname {arcosh} \left(v{\sqrt {1+u^{2}}}+u{\sqrt {v^{2}-1}}\right)\end{aligned}}}$

£#h5#£Other identities£#/h5#£
${\displaystyle {\begin{aligned}2\operatorname {arcosh} x&=\operatorname {arcosh} (2x^{2}-1)&\quad {\hbox{ for }}x\geq 1\\4\operatorname {arcosh} x&=\operatorname {arcosh} (8x^{4}-8x^{2}+1)&\quad {\hbox{ for }}x\geq 1\\2\operatorname {arsinh} x&=\operatorname {arcosh} (2x^{2}+1)&\quad {\hbox{ for }}x\geq 0\\4\operatorname {arsinh} x&=\operatorname {arcosh} (8x^{4}+8x^{2}+1)&\quad {\hbox{ for }}x\geq 0\end{aligned}}}$
${\displaystyle \ln(x)=\operatorname {arcosh} \left({\frac {x^{2}+1}{2x}}\right)=\operatorname {arsinh} \left({\frac {x^{2}-1}{2x}}\right)=\operatorname {artanh} \left({\frac {x^{2}-1}{x^{2}+1}}\right)}$

£#h5#£Composition of hyperbolic and inverse hyperbolic functions£#/h5#£
${\displaystyle {\begin{aligned}&\sinh(\operatorname {arcosh} x)={\sqrt {x^{2}-1}}\quad {\text{for}}\quad |x|>1\\&\sinh(\operatorname {artanh} x)={\frac {x}{\sqrt {1-x^{2}}}}\quad {\text{for}}\quad -1<x<1\\&\cosh(\operatorname {arsinh} x)={\sqrt {1+x^{2}}}\\&\cosh(\operatorname {artanh} x)={\frac {1}{\sqrt {1-x^{2}}}}\quad {\text{for}}\quad -1<x<1\\&\tanh(\operatorname {arsinh} x)={\frac {x}{\sqrt {1+x^{2}}}}\\&\tanh(\operatorname {arcosh} x)={\frac {\sqrt {x^{2}-1}}{x}}\quad {\text{for}}\quad |x|>1\end{aligned}}}$

£#h5#£Composition of inverse hyperbolic and trigonometric functions£#/h5#£
${\displaystyle \operatorname {arsinh} \left(\tan \alpha \right)=\operatorname {artanh} \left(\sin \alpha \right)=\ln \left({\frac {1+\sin \alpha }{\cos \alpha }}\right)=\pm \operatorname {arcosh} \left({\frac {1}{\cos \alpha }}\right)}$
${\displaystyle \ln \left(\left|\tan \alpha \right|\right)=-\operatorname {artanh} \left(\cos 2\alpha \right)}$

£#h5#£Conversions£#/h5#£
${\displaystyle \ln x=\operatorname {artanh} \left({\frac {x^{2}-1}{x^{2}+1}}\right)=\operatorname {arsinh} \left({\frac {x^{2}-1}{2x}}\right)=\pm \operatorname {arcosh} \left({\frac {x^{2}+1}{2x}}\right)}$
${\displaystyle \operatorname {artanh} x=\operatorname {arsinh} \left({\frac {x}{\sqrt {1-x^{2}}}}\right)=\pm \operatorname {arcosh} \left({\frac {1}{\sqrt {1-x^{2}}}}\right)}$
${\displaystyle \operatorname {arsinh} x=\operatorname {artanh} \left({\frac {x}{\sqrt {1+x^{2}}}}\right)=\pm \operatorname {arcosh} \left({\sqrt {1+x^{2}}}\right)}$
${\displaystyle \operatorname {arcosh} x=\left|\operatorname {arsinh} \left({\sqrt {x^{2}-1}}\right)\right|=\left|\operatorname {artanh} \left({\frac {\sqrt {x^{2}-1}}{x}}\right)\right|}$

£#h5#£Derivatives£#/h5#£
${\displaystyle {\begin{aligned}{\frac {d}{dx}}\operatorname {arsinh} x&{}={\frac {1}{\sqrt {x^{2}+1}}},{\text{ for all real }}x\\{\frac {d}{dx}}\operatorname {arcosh} x&{}={\frac {1}{\sqrt {x^{2}-1}}},{\text{ for all real }}x>1\\{\frac {d}{dx}}\operatorname {artanh} x&{}={\frac {1}{1-x^{2}}},{\text{ for all real }}|x|<1\\{\frac {d}{dx}}\operatorname {arcoth} x&{}={\frac {1}{1-x^{2}}},{\text{ for all real }}|x|>1\\{\frac {d}{dx}}\operatorname {arsech} x&{}={\frac {-1}{x{\sqrt {1-x^{2}}}}},{\text{ for all real }}x\in (0,1)\\{\frac {d}{dx}}\operatorname {arcsch} x&{}={\frac {-1}{|x|{\sqrt {1+x^{2}}}}},{\text{ for all real }}x{\text{, except }}0\\\end{aligned}}}$
For an example differentiation: let θ = arsinh x, so (where sinh2 θ = (sinh θ)2):

${\displaystyle {\frac {d\,\operatorname {arsinh} x}{dx}}={\frac {d\theta }{d\sinh \theta }}={\frac {1}{\cosh \theta }}={\frac {1}{\sqrt {1+\sinh ^{2}\theta }}}={\frac {1}{\sqrt {1+x^{2}}}}.}$

£#h5#£Series expansions£#/h5#£
Expansion series can be obtained for the above functions:

${\displaystyle {\begin{aligned}\operatorname {arsinh} x&=x-\left({\frac {1}{2}}\right){\frac {x^{3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{5}}{5}}-\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{7}}{7}}\pm \cdots \\&=\sum _{n=0}^{\infty }\left({\frac {(-1)^{n}(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{2n+1}}{2n+1}},\qquad \left|x\right|<1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcosh} x&=\ln(2x)-\left(\left({\frac {1}{2}}\right){\frac {x^{-2}}{2}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{-4}}{4}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{-6}}{6}}+\cdots \right)\\&=\ln(2x)-\sum _{n=1}^{\infty }\left({\frac {(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{-2n}}{2n}},\qquad \left|x\right|>1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {artanh} x&=x+{\frac {x^{3}}{3}}+{\frac {x^{5}}{5}}+{\frac {x^{7}}{7}}+\cdots \\&=\sum _{n=0}^{\infty }{\frac {x^{2n+1}}{2n+1}},\qquad \left|x\right|<1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcsch} x=\operatorname {arsinh} {\frac {1}{x}}&=x^{-1}-\left({\frac {1}{2}}\right){\frac {x^{-3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{-5}}{5}}-\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{-7}}{7}}\pm \cdots \\&=\sum _{n=0}^{\infty }\left({\frac {(-1)^{n}(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{-(2n+1)}}{2n+1}},\qquad \left|x\right|>1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arsech} x=\operatorname {arcosh} {\frac {1}{x}}&=\ln {\frac {2}{x}}-\left(\left({\frac {1}{2}}\right){\frac {x^{2}}{2}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{4}}{4}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{6}}{6}}+\cdots \right)\\&=\ln {\frac {2}{x}}-\sum _{n=1}^{\infty }\left({\frac {(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{2n}}{2n}},\qquad 0<x\leq 1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcoth} x=\operatorname {artanh} {\frac {1}{x}}&=x^{-1}+{\frac {x^{-3}}{3}}+{\frac {x^{-5}}{5}}+{\frac {x^{-7}}{7}}+\cdots \\&=\sum _{n=0}^{\infty }{\frac {x^{-(2n+1)}}{2n+1}},\qquad \left|x\right|>1\end{aligned}}}$
Asymptotic expansion for the arsinh x is given by

${\displaystyle \operatorname {arsinh} x=\ln(2x)+\sum \limits _{n=1}^{\infty }{\left({-1}\right)^{n-1}{\frac {\left({2n-1}\right)!!}{2n\left({2n}\right)!!}}}{\frac {1}{x^{2n}}}}$



£#h5#£Principal values in the complex plane£#/h5#£
As functions of a complex variable, inverse hyperbolic functions are multivalued functions that are analytic, except at a finite number of points. For such a function, it is common to define a principal value, which is a single valued analytic function which coincides with one specific branch of the multivalued function, over a domain consisting of the complex plane in which a finite number of arcs (usually half lines or line segments) have been removed. These arcs are called branch cuts. For specifying the branch, that is, defining which value of the multivalued function is considered at each point, one generally define it at a particular point, and deduce the value everywhere in the domain of definition of the principal value by analytic continuation. When possible, it is better to define the principal value directly—without referring to analytic continuation.

For example, for the square root, the principal value is defined as the square root that has a positive real part. This defines a single valued analytic function, which is defined everywhere, except for non-positive real values of the variables (where the two square roots have a zero real part). This principal value of the square root function is denoted ${\displaystyle {\sqrt {x}}}$ in what follows. Similarly, the principal value of the logarithm, denoted ${\displaystyle \operatorname {Log} }$ in what follows, is defined as the value for which the imaginary part has the smallest absolute value. It is defined everywhere except for non-positive real values of the variable, for which two different values of the logarithm reach the minimum.

For all inverse hyperbolic functions, the principal value may be defined in terms of principal values of the square root and the logarithm function. However, in some cases, the formulas of § Definitions in terms of logarithms do not give a correct principal value, as giving a domain of definition which is too small and, in one case non-connected.


£#h5#£Principal value of the inverse hyperbolic sine£#/h5#£
The principal value of the inverse hyperbolic sine is given by

${\displaystyle \operatorname {arsinh} z=\operatorname {Log} (z+{\sqrt {z^{2}+1}}\,)\,.}$
The argument of the square root is a non-positive real number, if and only if z belongs to one of the intervals [i, +i∞) and (−i∞, −i] of the imaginary axis. If the argument of the logarithm is real, then it is positive. Thus this formula defines a principal value for arsinh, with branch cuts [i, +i∞) and (−i∞, −i]. This is optimal, as the branch cuts must connect the singular points i and −i to the infinity.


£#h5#£Principal value of the inverse hyperbolic cosine£#/h5#£
The formula for the inverse hyperbolic cosine given in § Inverse hyperbolic cosine is not convenient, since similar to the principal values of the logarithm and the square root, the principal value of arcosh would not be defined for imaginary z. Thus the square root has to be factorized, leading to

${\displaystyle \operatorname {arcosh} z=\operatorname {Log} (z+{\sqrt {z+1}}{\sqrt {z-1}}\,)\,.}$
The principal values of the square roots are both defined, except if z belongs to the real interval (−∞, 1]. If the argument of the logarithm is real, then z is real and has the same sign. Thus, the above formula defines a principal value of arcosh outside the real interval (−∞, 1], which is thus the unique branch cut.


£#h5#£Principal values of the inverse hyperbolic tangent and cotangent£#/h5#£
The formulas given in § Definitions in terms of logarithms suggests

${\displaystyle {\begin{aligned}\operatorname {artanh} z&={\frac {1}{2}}\operatorname {Log} \left({\frac {1+z}{1-z}}\right)\\\operatorname {arcoth} z&={\frac {1}{2}}\operatorname {Log} \left({\frac {z+1}{z-1}}\right)\end{aligned}}}$
for the definition of the principal values of the inverse hyperbolic tangent and cotangent. In these formulas, the argument of the logarithm is real if and only if z is real. For artanh, this argument is in the real interval (−∞, 0], if z belongs either to (−∞, −1] or to [1, ∞). For arcoth, the argument of the logarithm is in (−∞, 0], if and only if z belongs to the real interval [−1, 1].

Therefore, these formulas define convenient principal values, for which the branch cuts are (−∞, −1] and [1, ∞) for the inverse hyperbolic tangent, and [−1, 1] for the inverse hyperbolic cotangent.

In view of a better numerical evaluation near the branch cuts, some authors use the following definitions of the principal values, although the second one introduces a removable singularity at z = 0. The two definitions of ${\displaystyle \operatorname {artanh} }$ differ for real values of ${\displaystyle z}$ with ${\displaystyle z>1}$ . The ones of ${\displaystyle \operatorname {arcoth} }$ differ for real values of ${\displaystyle z}$ with ${\displaystyle z\in [0,1)}$ .

${\displaystyle {\begin{aligned}\operatorname {artanh} z&={\tfrac {1}{2}}\operatorname {Log} \left({1+z}\right)-{\tfrac {1}{2}}\operatorname {Log} \left({1-z}\right)\\\operatorname {arcoth} z&={\tfrac {1}{2}}\operatorname {Log} \left({1+{\frac {1}{z}}}\right)-{\tfrac {1}{2}}\operatorname {Log} \left({1-{\frac {1}{z}}}\right)\end{aligned}}}$

£#h5#£Principal value of the inverse hyperbolic cosecant£#/h5#£
For the inverse hyperbolic cosecant, the principal value is defined as

${\displaystyle \operatorname {arcsch} z=\operatorname {Log} \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z^{2}}}+1}}\,\right)}$ .
It is defined when the arguments of the logarithm and the square root are not non-positive real numbers. The principal value of the square root is thus defined outside the interval [−i, i] of the imaginary line. If the argument of the logarithm is real, then z is a non-zero real number, and this implies that the argument of the logarithm is positive.

Thus, the principal value is defined by the above formula outside the branch cut, consisting of the interval [−i, i] of the imaginary line.

For z = 0, there is a singular point that is included in the branch cut.


£#h5#£Principal value of the inverse hyperbolic secant£#/h5#£
Here, as in the case of the inverse hyperbolic cosine, we have to factorize the square root. This gives the principal value

${\displaystyle \operatorname {arsech} z=\operatorname {Log} \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z}}+1}}\,{\sqrt {{\frac {1}{z}}-1}}\right).}$
If the argument of a square root is real, then z is real, and it follows that both principal values of square roots are defined, except if z is real and belongs to one of the intervals (−∞, 0] and [1, +∞). If the argument of the logarithm is real and negative, then z is also real and negative. It follows that the principal value of arsech is well defined, by the above formula outside two branch cuts, the real intervals (−∞, 0] and [1, +∞).

For z = 0, there is a singular point that is included in one of the branch cuts.


£#h5#£Graphical representation£#/h5#£
In the following graphical representation of the principal values of the inverse hyperbolic functions, the branch cuts appear as discontinuities of the color. The fact that the whole branch cuts appear as discontinuities, shows that these principal values may not be extended into analytic functions defined over larger domains. In other words, the above defined branch cuts are minimal.


£#h5#£See also£#/h5#£ £#ul#££#li#£Complex logarithm£#/li#£ £#li#£Hyperbolic secant distribution£#/li#£ £#li#£ISO 80000-2£#/li#£ £#li#£List of integrals of inverse hyperbolic functions£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£Bibliography£#/h5#£ £#ul#££#li#£Herbert Busemann and Paul J. Kelly (1953) Projective Geometry and Projective Metrics, page 207, Academic Press.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Inverse hyperbolic functions", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Hyperbolic Functions £#/li#££#/ul#£




£#h3#£Arccsc£#/h3#£

In mathematics, the inverse trigonometric functions (occasionally also called arcus functions, antitrigonometric functions or cyclometric functions) are the inverse functions of the trigonometric functions (with suitably restricted domains). Specifically, they are the inverses of the sine, cosine, tangent, cotangent, secant, and cosecant functions, and are used to obtain an angle from any of the angle's trigonometric ratios. Inverse trigonometric functions are widely used in engineering, navigation, physics, and geometry.


£#h5#£Notation£#/h5#£
Several notations for the inverse trigonometric functions exist. The most common convention is to name inverse trigonometric functions using an arc- prefix: arcsin(x), arccos(x), arctan(x), etc. (This convention is used throughout this article.) This notation arises from the following geometric relationships: when measuring in radians, an angle of θ radians will correspond to an arc whose length is rθ, where r is the radius of the circle. Thus in the unit circle, "the arc whose cosine is x" is the same as "the angle whose cosine is x", because the length of the arc of the circle in radii is the same as the measurement of the angle in radians. In computer programming languages, the inverse trigonometric functions are often called by the abbreviated forms asin, acos, atan.

The notations sin−1(x), cos−1(x), tan−1(x), etc., as introduced by John Herschel in 1813, are often used as well in English-language sources, much more than the also established sin[−1](x), cos[−1](x), tan[−1](x),—conventions consistent with the notation of an inverse function, that is useful e.g. to define the multivalued version of each inverse trigonometric function: e.g. ${\displaystyle \tan ^{-1}(x)=\{\arctan(x)+\pi k\mid k\in \mathbb {Z} \}}$ . However, this might appear to conflict logically with the common semantics for expressions such as sin2(x) (although only sin2 x, without parentheses, is the really common one), which refer to numeric power rather than function composition, and therefore may result in confusion between multiplicative inverse or reciprocal and compositional inverse. The confusion is somewhat mitigated by the fact that each of the reciprocal trigonometric functions has its own name—for example, (cos(x))−1 = sec(x). Nevertheless, certain authors advise against using it for its ambiguity. Another precarious convention used by a tiny number of authors is to use an uppercase first letter, along with a −1 superscript: Sin−1(x), Cos−1(x), Tan−1(x), etc. Although its intention is to avoid confusion with the multiplicative inverse, which should be represented by sin−1(x), cos−1(x), etc., or, better, by sin−1 x, cos−1 x, etc., it in turn creates yet another major source of ambiguity: especially in light of the fact that many popular high-level programming languages (e.g. Wolfram's Mathematica, and University of Sydney's MAGMA) use those very same capitalised representations for the standard trig functions; whereas others (Python (ie SymPy and NumPy), Matlab, MAPLE etc) use lower-case.

Hence, since 2009, the ISO 80000-2 standard has specified solely the "arc" prefix for the inverse functions.


£#h5#£Basic concepts£#/h5#£
£#h5#£Principal values£#/h5#£
Since none of the six trigonometric functions are one-to-one, they must be restricted in order to have inverse functions. Therefore, the result ranges of the inverse functions are proper (i.e. strict) subsets of the domains of the original functions.

For example, using function in the sense of multivalued functions, just as the square root function ${\displaystyle y={\sqrt {x}}}$ could be defined from ${\displaystyle y^{2}=x,}$ the function ${\displaystyle y=\arcsin(x)}$ is defined so that ${\displaystyle \sin(y)=x.}$ For a given real number ${\displaystyle x,}$ with ${\displaystyle -1\leq x\leq 1,}$ there are multiple (in fact, countably infinitely many) numbers ${\displaystyle y}$ such that ${\displaystyle \sin(y)=x}$ ; for example, ${\displaystyle \sin(0)=0,}$ but also ${\displaystyle \sin(\pi )=0,}$ ${\displaystyle \sin(2\pi )=0,}$ etc. When only one value is desired, the function may be restricted to its principal branch. With this restriction, for each ${\displaystyle x}$ in the domain, the expression ${\displaystyle \arcsin(x)}$ will evaluate only to a single value, called its principal value. These properties apply to all the inverse trigonometric functions.

The principal inverses are listed in the following table.

Note: Some authors define the range of arcsecant to be ${\textstyle (0\leq y<{\frac {\pi }{2}}{\text{ or }}\pi \leq y<{\frac {3\pi }{2}})}$ , because the tangent function is nonnegative on this domain. This makes some computations more consistent. For example, using this range, ${\displaystyle \tan(\operatorname {arcsec}(x))={\sqrt {x^{2}-1}},}$ whereas with the range ${\textstyle (0\leq y<{\frac {\pi }{2}}{\text{ or }}{\frac {\pi }{2}}<y\leq \pi )}$ , we would have to write ${\displaystyle \tan(\operatorname {arcsec}(x))=\pm {\sqrt {x^{2}-1}},}$ since tangent is nonnegative on ${\textstyle 0\leq y<{\frac {\pi }{2}},}$ but nonpositive on ${\textstyle {\frac {\pi }{2}}<y\leq \pi .}$ For a similar reason, the same authors define the range of arccosecant to be ${\textstyle (-\pi <y\leq -{\frac {\pi }{2}}}$ or ${\textstyle 0<y\leq {\frac {\pi }{2}}).}$

If ${\displaystyle x}$ is allowed to be a complex number, then the range of ${\displaystyle y}$ applies only to its real part.

The table below displays names and domains of the inverse trigonometric functions along with the range of their usual principal values in radians.

The symbol ${\displaystyle \mathbb {R} =(-\infty ,\infty )}$ denotes the set of all real numbers and ${\displaystyle \mathbb {Z} =\{\ldots ,\,-2,\,-1,\,0,\,1,\,2,\,\ldots \}}$ denotes the set of all integers. The set of all integer multiples of ${\displaystyle \pi }$ is denoted by

The symbol ${\displaystyle \,\setminus \,}$ denotes set subtraction so that, for instance, ${\displaystyle \mathbb {R} \setminus (-1,1)=(-\infty ,-1]\cup [1,\infty )}$ is the set of points in ${\displaystyle \mathbb {R} }$ (that is, real numbers) that are not in the interval ${\displaystyle (-1,1).}$

The Minkowski sum notation ${\textstyle \pi \mathbb {Z} +(0,\pi )}$ and ${\displaystyle \pi \mathbb {Z} +{\bigl (}{-{\tfrac {\pi }{2}}},{\tfrac {\pi }{2}}{\bigr )}}$ that is used above to concisely write the domains of ${\displaystyle \cot ,\csc ,\tan ,{\text{ and }}\sec }$ is now explained.

Domain of cotangent ${\displaystyle \cot }$ and cosecant ${\displaystyle \csc }$ : The domains of ${\displaystyle \,\cot \,}$ and ${\displaystyle \,\csc \,}$ are the same. They are the set of all angles ${\displaystyle \theta }$ at which ${\displaystyle \sin \theta \neq 0,}$ i.e. all real numbers that are not of the form ${\displaystyle \pi n}$ for some integer ${\displaystyle n,}$

Domain of tangent ${\displaystyle \tan }$ and secant ${\displaystyle \sec }$ : The domains of ${\displaystyle \,\tan \,}$ and ${\displaystyle \,\sec \,}$ are the same. They are the set of all angles ${\displaystyle \theta }$ at which ${\displaystyle \cos \theta \neq 0,}$


£#h5#£Solutions to elementary trigonometric equations£#/h5#£
Each of the trigonometric functions is periodic in the real part of its argument, running through all its values twice in each interval of ${\displaystyle 2\pi }$ :

£#ul#££#li#£Sine and cosecant begin their period at ${\textstyle 2\pi k-{\frac {\pi }{2}}}$ (where ${\displaystyle k}$ is an integer), finish it at ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ and then reverse themselves over ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ to ${\displaystyle 2\pi k+{\frac {3\pi }{2}}.}$ £#/li#£ £#li#£Cosine and secant begin their period at ${\displaystyle 2\pi k,}$ finish it at ${\displaystyle 2\pi k+\pi .}$ and then reverse themselves over ${\displaystyle 2\pi k+\pi }$ to ${\displaystyle 2\pi k+2\pi .}$ £#/li#£ £#li#£Tangent begins its period at ${\textstyle 2\pi k-{\frac {\pi }{2}},}$ finishes it at ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ and then repeats it (forward) over ${\textstyle 2\pi k+{\frac {\pi }{2}}}$ to ${\textstyle 2\pi k+{\frac {3\pi }{2}}.}$ £#/li#£ £#li#£Cotangent begins its period at ${\displaystyle 2\pi k,}$ finishes it at ${\displaystyle 2\pi k+\pi ,}$ and then repeats it (forward) over ${\displaystyle 2\pi k+\pi }$ to ${\displaystyle 2\pi k+2\pi .}$ £#/li#££#/ul#£
This periodicity is reflected in the general inverses, where ${\displaystyle k}$ is some integer.

The following table shows how inverse trigonometric functions may be used to solve equalities involving the six standard trigonometric functions. It is assumed that the given values ${\displaystyle \theta ,}$ ${\displaystyle r,}$ ${\displaystyle s,}$ ${\displaystyle x,}$ and ${\displaystyle y}$ all lie within appropriate ranges so that the relevant expressions below are well-defined. Note that "for some ${\displaystyle k\in \mathbb {Z} }$ " is just another way of saying "for some integer ${\displaystyle k.}$ "

The symbol ${\displaystyle \,\iff \,}$ is logical equality. The expression "LHS ${\displaystyle \,\iff \,}$ RHS" indicates that either (a) the left hand side (i.e. LHS) and right hand side (i.e. RHS) are both true, or else (b) the left hand side and right hand side are both false; there is no option (c) (e.g. it is not possible for the LHS statement to be true and also simultaneously for the RHS statement to false), because otherwise "LHS ${\displaystyle \,\iff \,}$ RHS" would not have been written (see this footnote for an example illustrating this concept).

For example, if ${\displaystyle \cos \theta =-1}$ then ${\displaystyle \theta =\pi +2\pi k=-\pi +2\pi (1+k)}$ for some ${\displaystyle k\in \mathbb {Z} .}$ While if ${\displaystyle \sin \theta =\pm 1}$ then ${\textstyle \theta ={\frac {\pi }{2}}+\pi k=-{\frac {\pi }{2}}+\pi (k+1)}$ for some ${\displaystyle k\in \mathbb {Z} ,}$ where ${\displaystyle k}$ will be even if ${\displaystyle \sin \theta =1}$ and it will be odd if ${\displaystyle \sin \theta =-1.}$ The equations ${\displaystyle \sec \theta =-1}$ and ${\displaystyle \csc \theta =\pm 1}$ have the same solutions as ${\displaystyle \cos \theta =-1}$ and ${\displaystyle \sin \theta =\pm 1,}$ respectively. In all equations above except for those just solved (i.e. except for ${\displaystyle \sin }$ / ${\displaystyle \csc \theta =\pm 1}$ and ${\displaystyle \cos }$ / ${\displaystyle \sec \theta =-1}$ ), the integer ${\displaystyle k}$ in the solution's formula is uniquely determined by ${\displaystyle \theta }$ (for fixed ${\displaystyle r,s,x,}$ and ${\displaystyle y}$ ).

Detailed example and explanation of the "plus or minus" symbol ${\displaystyle \pm }$
The solutions to ${\displaystyle \cos \theta =x}$ and ${\displaystyle \sec \theta =x}$ involve the "plus or minus" symbol ${\displaystyle \,\pm ,\,}$ whose meaning is now clarified. Only the solution to ${\displaystyle \cos \theta =x}$ will be discussed since the discussion for ${\displaystyle \sec \theta =x}$ is the same. We are given ${\displaystyle x}$ between ${\displaystyle -1\leq x\leq 1}$ and we know that there is an angle ${\displaystyle \theta }$ in some give interval that satisfies ${\displaystyle \cos \theta =x.}$ We want to find this ${\displaystyle \theta .}$ The formula for the solution involves:

If ${\displaystyle \,\arccos x=0\,}$ (which only happens when ${\displaystyle x=1}$ ) then ${\displaystyle \,+\arccos x=0\,}$ and ${\displaystyle \,-\arccos x=0\,}$ so either way, ${\displaystyle \,\pm \arccos x\,}$ can only be equal to ${\displaystyle 0.}$ But if ${\displaystyle \,\arccos x\neq 0,\,}$ which will now be assumed, then the solution to ${\displaystyle \cos \theta =x,}$ which is written above as is shorthand for the following statement:
Either £#ul#££#li#£ ${\displaystyle \,\theta =\arccos x+2\pi k\,}$ for some integer ${\displaystyle k,}$
or else£#/li#£ £#li#£ ${\displaystyle \,\theta =-\arccos x+2\pi k\,}$ for some integer ${\displaystyle k.}$ £#/li#££#/ul#£
Because ${\displaystyle \,\arccos x\neq 0\,}$ and ${\displaystyle \,0<\arccos x\leq \pi \,}$ exactly one of these two equalities can hold. Additional information about ${\displaystyle \theta }$ is needed to determine which one holds. For example, suppose that ${\displaystyle x=0}$ and that all that is known about ${\displaystyle \theta }$ is that ${\displaystyle \,-\pi \leq \theta \leq \pi \,}$ (and nothing more is known). Then

and moreover, in this particular case ${\displaystyle k=0}$ (for both the ${\displaystyle \,+\,}$ case and the ${\displaystyle \,-\,}$ case) and so consequently, This means that ${\displaystyle \theta }$ could be either ${\displaystyle \,\pi /2\,}$ or ${\displaystyle \,-\pi /2.}$ Without additional information it is not possible to determine which of these values ${\displaystyle \theta }$ has. An example of some additional information that could determine the value of ${\displaystyle \theta }$ would be knowing that the angle is above the ${\displaystyle x}$ -axis(in which case ${\displaystyle \theta =\pi /2}$ ) or alternatively, knowing that it is below the ${\displaystyle x}$ -axis(in which case ${\displaystyle \theta =-\pi /2}$ ).
Transforming equations

The equations above can be transformed by using the reflection and shift identities:

These formulas imply, in particular, that the following hold:

where swapping ${\displaystyle \sin \leftrightarrow \csc ,}$ swapping ${\displaystyle \cos \leftrightarrow \sec ,}$ and swapping ${\displaystyle \tan \leftrightarrow \cot }$ gives the analogous equations for ${\displaystyle \csc ,\sec ,{\text{ and }}\cot ,}$ respectively.
So for example, by using the equality ${\textstyle \sin \left({\frac {\pi }{2}}-\theta \right)=\cos \theta ,}$ the equation ${\displaystyle \cos \theta =x}$ can be transformed into ${\textstyle \sin \left({\frac {\pi }{2}}-\theta \right)=x,}$ which allows for the solution to the equation ${\displaystyle \;\sin \varphi =x\;}$ (where ${\textstyle \varphi :={\frac {\pi }{2}}-\theta }$ ) to be used; that solution being: ${\displaystyle \varphi =(-1)^{k}\arcsin(x)+\pi k\;{\text{ for some }}k\in \mathbb {Z} ,}$ which becomes:

where using the fact that ${\displaystyle (-1)^{k}=(-1)^{-k}}$ and substituting ${\displaystyle h:=-k}$ proves that another solution to ${\displaystyle \;\cos \theta =x\;}$ is: The substitution ${\displaystyle \;\arcsin x={\frac {\pi }{2}}-\arccos x\;}$ may be used express the right hand side of the above formula in terms of ${\displaystyle \;\arccos x\;}$ instead of ${\displaystyle \;\arcsin x.\;}$
£#h5#£Equal identical trigonometric functions£#/h5#£
The table below shows how two angles ${\displaystyle \theta }$ and ${\displaystyle \varphi }$ must be related if their values under a given trigonometric function are equal or negatives of each other.


£#h5#£Relationships between trigonometric functions and inverse trigonometric functions£#/h5#£
Trigonometric functions of inverse trigonometric functions are tabulated below. A quick way to derive them is by considering the geometry of a right-angled triangle, with one side of length 1 and another side of length ${\displaystyle x,}$ then applying the Pythagorean theorem and definitions of the trigonometric ratios. Purely algebraic derivations are longer. It is worth noting that for arcsecant and arccosecant, the diagram assumes that ${\displaystyle x}$ is positive, and thus the result has to be corrected through the use of absolute values and the signum (sgn) operation.


£#h5#£Relationships among the inverse trigonometric functions£#/h5#£
Complementary angles:

${\displaystyle {\begin{aligned}\arccos(x)&={\frac {\pi }{2}}-\arcsin(x)\\[0.5em]\operatorname {arccot}(x)&={\frac {\pi }{2}}-\arctan(x)\\[0.5em]\operatorname {arccsc}(x)&={\frac {\pi }{2}}-\operatorname {arcsec}(x)\end{aligned}}}$
Negative arguments:

${\displaystyle {\begin{aligned}\arcsin(-x)&=-\arcsin(x)\\\arccos(-x)&=\pi -\arccos(x)\\\arctan(-x)&=-\arctan(x)\\\operatorname {arccot}(-x)&=\pi -\operatorname {arccot}(x)\\\operatorname {arcsec}(-x)&=\pi -\operatorname {arcsec}(x)\\\operatorname {arccsc}(-x)&=-\operatorname {arccsc}(x)\end{aligned}}}$
Reciprocal arguments:

${\displaystyle {\begin{aligned}\arccos \left({\frac {1}{x}}\right)&=\operatorname {arcsec}(x)\\[0.3em]\arcsin \left({\frac {1}{x}}\right)&=\operatorname {arccsc}(x)\\[0.3em]\arctan \left({\frac {1}{x}}\right)&={\frac {\pi }{2}}-\arctan(x)=\operatorname {arccot}(x)\,,{\text{ if }}x>0\\[0.3em]\arctan \left({\frac {1}{x}}\right)&=-{\frac {\pi }{2}}-\arctan(x)=\operatorname {arccot}(x)-\pi \,,{\text{ if }}x<0\\[0.3em]\operatorname {arccot} \left({\frac {1}{x}}\right)&={\frac {\pi }{2}}-\operatorname {arccot}(x)=\arctan(x)\,,{\text{ if }}x>0\\[0.3em]\operatorname {arccot} \left({\frac {1}{x}}\right)&={\frac {3\pi }{2}}-\operatorname {arccot}(x)=\pi +\arctan(x)\,,{\text{ if }}x<0\\[0.3em]\operatorname {arcsec} \left({\frac {1}{x}}\right)&=\arccos(x)\\[0.3em]\operatorname {arccsc} \left({\frac {1}{x}}\right)&=\arcsin(x)\end{aligned}}}$
Useful identities if one only has a fragment of a sine table:

${\displaystyle {\begin{aligned}\arccos(x)&=\arcsin \left({\sqrt {1-x^{2}}}\right)\,,{\text{ if }}0\leq x\leq 1{\text{ , from which you get }}\\\arccos &\left({\frac {1-x^{2}}{1+x^{2}}}\right)=\arcsin \left({\frac {2x}{1+x^{2}}}\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin &\left({\sqrt {1-x^{2}}}\right)={\frac {\pi }{2}}-\operatorname {sgn}(x)\arcsin(x)\\\arccos(x)&={\frac {1}{2}}\arccos \left(2x^{2}-1\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin(x)&={\frac {1}{2}}\arccos \left(1-2x^{2}\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin(x)&=\arctan \left({\frac {x}{\sqrt {1-x^{2}}}}\right)\\\arccos(x)&=\arctan \left({\frac {\sqrt {1-x^{2}}}{x}}\right)\\\arctan(x)&=\arcsin \left({\frac {x}{\sqrt {1+x^{2}}}}\right)\\\operatorname {arccot}(x)&=\arccos \left({\frac {x}{\sqrt {1+x^{2}}}}\right)\end{aligned}}}$
Whenever the square root of a complex number is used here, we choose the root with the positive real part (or positive imaginary part if the square was negative real).

A useful form that follows directly from the table above is

${\displaystyle \arctan \left(x\right)=\arccos \left({\sqrt {\frac {1}{1+x^{2}}}}\right)\,,{\text{ if }}x\geq 0}$ .
It is obtained by recognizing that ${\displaystyle \cos \left(\arctan \left(x\right)\right)={\sqrt {\frac {1}{1+x^{2}}}}=\cos \left(\arccos \left({\sqrt {\frac {1}{1+x^{2}}}}\right)\right)}$ .

From the half-angle formula, ${\displaystyle \tan \left({\tfrac {\theta }{2}}\right)={\tfrac {\sin(\theta )}{1+\cos(\theta )}}}$ , we get:

${\displaystyle {\begin{aligned}\arcsin(x)&=2\arctan \left({\frac {x}{1+{\sqrt {1-x^{2}}}}}\right)\\[0.5em]\arccos(x)&=2\arctan \left({\frac {\sqrt {1-x^{2}}}{1+x}}\right)\,,{\text{ if }}-1<x\leq 1\\[0.5em]\arctan(x)&=2\arctan \left({\frac {x}{1+{\sqrt {1+x^{2}}}}}\right)\end{aligned}}}$

£#h5#£Arctangent addition formula£#/h5#£
${\displaystyle \arctan(u)\pm \arctan(v)=\arctan \left({\frac {u\pm v}{1\mp uv}}\right){\pmod {\pi }}\,,\quad uv\neq 1\,.}$
This is derived from the tangent addition formula

${\displaystyle \tan(\alpha \pm \beta )={\frac {\tan(\alpha )\pm \tan(\beta )}{1\mp \tan(\alpha )\tan(\beta )}}\,,}$
by letting

${\displaystyle \alpha =\arctan(u)\,,\quad \beta =\arctan(v)\,.}$

£#h5#£In calculus£#/h5#£
£#h5#£Derivatives of inverse trigonometric functions£#/h5#£
The derivatives for complex values of z are as follows:

${\displaystyle {\begin{aligned}{\frac {d}{dz}}\arcsin(z)&{}={\frac {1}{\sqrt {1-z^{2}}}}\;;&z&{}\neq -1,+1\\{\frac {d}{dz}}\arccos(z)&{}=-{\frac {1}{\sqrt {1-z^{2}}}}\;;&z&{}\neq -1,+1\\{\frac {d}{dz}}\arctan(z)&{}={\frac {1}{1+z^{2}}}\;;&z&{}\neq -i,+i\\{\frac {d}{dz}}\operatorname {arccot}(z)&{}=-{\frac {1}{1+z^{2}}}\;;&z&{}\neq -i,+i\\{\frac {d}{dz}}\operatorname {arcsec}(z)&{}={\frac {1}{z^{2}{\sqrt {1-{\frac {1}{z^{2}}}}}}}\;;&z&{}\neq -1,0,+1\\{\frac {d}{dz}}\operatorname {arccsc}(z)&{}=-{\frac {1}{z^{2}{\sqrt {1-{\frac {1}{z^{2}}}}}}}\;;&z&{}\neq -1,0,+1\end{aligned}}}$
Only for real values of x:

${\displaystyle {\begin{aligned}{\frac {d}{dx}}\operatorname {arcsec}(x)&{}={\frac {1}{|x|{\sqrt {x^{2}-1}}}}\;;&|x|>1\\{\frac {d}{dx}}\operatorname {arccsc}(x)&{}=-{\frac {1}{|x|{\sqrt {x^{2}-1}}}}\;;&|x|>1\end{aligned}}}$
For a sample derivation: if ${\displaystyle \theta =\arcsin(x)}$ , we get:

${\displaystyle {\frac {d\arcsin(x)}{dx}}={\frac {d\theta }{d\sin(\theta )}}={\frac {d\theta }{\cos(\theta )\,d\theta }}={\frac {1}{\cos(\theta )}}={\frac {1}{\sqrt {1-\sin ^{2}(\theta )}}}={\frac {1}{\sqrt {1-x^{2}}}}}$

£#h5#£Expression as definite integrals£#/h5#£
Integrating the derivative and fixing the value at one point gives an expression for the inverse trigonometric function as a definite integral:

${\displaystyle {\begin{aligned}\arcsin(x)&{}=\int _{0}^{x}{\frac {1}{\sqrt {1-z^{2}}}}\,dz\;,&|x|&{}\leq 1\\\arccos(x)&{}=\int _{x}^{1}{\frac {1}{\sqrt {1-z^{2}}}}\,dz\;,&|x|&{}\leq 1\\\arctan(x)&{}=\int _{0}^{x}{\frac {1}{z^{2}+1}}\,dz\;,\\\operatorname {arccot}(x)&{}=\int _{x}^{\infty }{\frac {1}{z^{2}+1}}\,dz\;,\\\operatorname {arcsec}(x)&{}=\int _{1}^{x}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz=\pi +\int _{-x}^{-1}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz\;,&x&{}\geq 1\\\operatorname {arccsc}(x)&{}=\int _{x}^{\infty }{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz=\int _{-\infty }^{-x}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz\;,&x&{}\geq 1\\\end{aligned}}}$
When x equals 1, the integrals with limited domains are improper integrals, but still well-defined.


£#h5#£Infinite series£#/h5#£
Similar to the sine and cosine functions, the inverse trigonometric functions can also be calculated using power series, as follows. For arcsine, the series can be derived by expanding its derivative, ${\textstyle {\tfrac {1}{\sqrt {1-z^{2}}}}}$ , as a binomial series, and integrating term by term (using the integral definition as above). The series for arctangent can similarly be derived by expanding its derivative ${\textstyle {\frac {1}{1+z^{2}}}}$ in a geometric series, and applying the integral definition above (see Leibniz series).

${\displaystyle {\begin{aligned}\arcsin(z)&=z+\left({\frac {1}{2}}\right){\frac {z^{3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {z^{5}}{5}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {z^{7}}{7}}+\cdots \\[5pt]&=\sum _{n=0}^{\infty }{\frac {(2n-1)!!}{(2n)!!}}{\frac {z^{2n+1}}{2n+1}}\\[5pt]&=\sum _{n=0}^{\infty }{\frac {(2n)!}{(2^{n}n!)^{2}}}{\frac {z^{2n+1}}{2n+1}}\,;\qquad |z|\leq 1\end{aligned}}}$
${\displaystyle \arctan(z)=z-{\frac {z^{3}}{3}}+{\frac {z^{5}}{5}}-{\frac {z^{7}}{7}}+\cdots =\sum _{n=0}^{\infty }{\frac {(-1)^{n}z^{2n+1}}{2n+1}}\,;\qquad |z|\leq 1\qquad z\neq i,-i}$
Series for the other inverse trigonometric functions can be given in terms of these according to the relationships given above. For example, ${\displaystyle \arccos(x)=\pi /2-\arcsin(x)}$ , ${\displaystyle \operatorname {arccsc}(x)=\arcsin(1/x)}$ , and so on. Another series is given by:

${\displaystyle 2\left(\arcsin \left({\frac {x}{2}}\right)\right)^{2}=\sum _{n=1}^{\infty }{\frac {x^{2n}}{n^{2}{\binom {2n}{n}}}}.}$
Leonhard Euler found a series for the arctangent that converges more quickly than its Taylor series:

${\displaystyle \arctan(z)={\frac {z}{1+z^{2}}}\sum _{n=0}^{\infty }\prod _{k=1}^{n}{\frac {2kz^{2}}{(2k+1)(1+z^{2})}}.}$
(The term in the sum for n = 0 is the empty product, so is 1.)

Alternatively, this can be expressed as

${\displaystyle \arctan(z)=\sum _{n=0}^{\infty }{\frac {2^{2n}(n!)^{2}}{(2n+1)!}}{\frac {z^{2n+1}}{(1+z^{2})^{n+1}}}.}$
Another series for the arctangent function is given by

${\displaystyle \arctan(z)=i\sum _{n=1}^{\infty }{\frac {1}{2n-1}}\left({\frac {1}{(1+2i/z)^{2n-1}}}-{\frac {1}{(1-2i/z)^{2n-1}}}\right),}$
where ${\displaystyle i={\sqrt {-1}}}$ is the imaginary unit.


£#h5#£Continued fractions for arctangent£#/h5#£
Two alternatives to the power series for arctangent are these generalized continued fractions:

${\displaystyle \arctan(z)={\frac {z}{1+{\cfrac {(1z)^{2}}{3-1z^{2}+{\cfrac {(3z)^{2}}{5-3z^{2}+{\cfrac {(5z)^{2}}{7-5z^{2}+{\cfrac {(7z)^{2}}{9-7z^{2}+\ddots }}}}}}}}}}={\frac {z}{1+{\cfrac {(1z)^{2}}{3+{\cfrac {(2z)^{2}}{5+{\cfrac {(3z)^{2}}{7+{\cfrac {(4z)^{2}}{9+\ddots }}}}}}}}}}}$
The second of these is valid in the cut complex plane. There are two cuts, from −i to the point at infinity, going down the imaginary axis, and from i to the point at infinity, going up the same axis. It works best for real numbers running from −1 to 1. The partial denominators are the odd natural numbers, and the partial numerators (after the first) are just (nz)2, with each perfect square appearing once. The first was developed by Leonhard Euler; the second by Carl Friedrich Gauss utilizing the Gaussian hypergeometric series.


£#h5#£Indefinite integrals of inverse trigonometric functions£#/h5#£
For real and complex values of z:

${\displaystyle {\begin{aligned}\int \arcsin(z)\,dz&{}=z\,\arcsin(z)+{\sqrt {1-z^{2}}}+C\\\int \arccos(z)\,dz&{}=z\,\arccos(z)-{\sqrt {1-z^{2}}}+C\\\int \arctan(z)\,dz&{}=z\,\arctan(z)-{\frac {1}{2}}\ln \left(1+z^{2}\right)+C\\\int \operatorname {arccot}(z)\,dz&{}=z\,\operatorname {arccot}(z)+{\frac {1}{2}}\ln \left(1+z^{2}\right)+C\\\int \operatorname {arcsec}(z)\,dz&{}=z\,\operatorname {arcsec}(z)-\ln \left[z\left(1+{\sqrt {\frac {z^{2}-1}{z^{2}}}}\right)\right]+C\\\int \operatorname {arccsc}(z)\,dz&{}=z\,\operatorname {arccsc}(z)+\ln \left[z\left(1+{\sqrt {\frac {z^{2}-1}{z^{2}}}}\right)\right]+C\end{aligned}}}$
For real x ≥ 1:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\ln \left(x+{\sqrt {x^{2}-1}}\right)+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\ln \left(x+{\sqrt {x^{2}-1}}\right)+C\end{aligned}}}$
For all real x not between -1 and 1:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\operatorname {sgn}(x)\ln \left|x+{\sqrt {x^{2}-1}}\right|+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\operatorname {sgn}(x)\ln \left|x+{\sqrt {x^{2}-1}}\right|+C\end{aligned}}}$
The absolute value is necessary to compensate for both negative and positive values of the arcsecant and arccosecant functions. The signum function is also necessary due to the absolute values in the derivatives of the two functions, which create two different solutions for positive and negative values of x. These can be further simplified using the logarithmic definitions of the inverse hyperbolic functions:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\operatorname {arcosh} (|x|)+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\operatorname {arcosh} (|x|)+C\\\end{aligned}}}$
The absolute value in the argument of the arcosh function creates a negative half of its graph, making it identical to the signum logarithmic function shown above.

All of these antiderivatives can be derived using integration by parts and the simple derivative forms shown above.


£#h5#£Example£#/h5#£
Using ${\displaystyle \int u\,dv=uv-\int v\,du}$ (i.e. integration by parts), set

${\displaystyle {\begin{aligned}u&=\arcsin(x)&dv&=dx\\du&={\frac {dx}{\sqrt {1-x^{2}}}}&v&=x\end{aligned}}}$
Then

${\displaystyle \int \arcsin(x)\,dx=x\arcsin(x)-\int {\frac {x}{\sqrt {1-x^{2}}}}\,dx,}$
which by the simple substitution ${\displaystyle w=1-x^{2},\ dw=-2x\,dx}$ yields the final result:

${\displaystyle \int \arcsin(x)\,dx=x\arcsin(x)+{\sqrt {1-x^{2}}}+C}$

£#h5#£Extension to complex plane£#/h5#£
Since the inverse trigonometric functions are analytic functions, they can be extended from the real line to the complex plane. This results in functions with multiple sheets and branch points. One possible way of defining the extension is:

${\displaystyle \arctan(z)=\int _{0}^{z}{\frac {dx}{1+x^{2}}}\quad z\neq -i,+i}$
where the part of the imaginary axis which does not lie strictly between the branch points (−i and +i) is the branch cut between the principal sheet and other sheets. The path of the integral must not cross a branch cut. For z not on a branch cut, a straight line path from 0 to z is such a path. For z on a branch cut, the path must approach from Re[x] > 0 for the upper branch cut and from Re[x] < 0 for the lower branch cut.

The arcsine function may then be defined as:

${\displaystyle \arcsin(z)=\arctan \left({\frac {z}{\sqrt {1-z^{2}}}}\right)\quad z\neq -1,+1}$
where (the square-root function has its cut along the negative real axis and) the part of the real axis which does not lie strictly between −1 and +1 is the branch cut between the principal sheet of arcsin and other sheets;

${\displaystyle \arccos(z)={\frac {\pi }{2}}-\arcsin(z)\quad z\neq -1,+1}$
which has the same cut as arcsin;

${\displaystyle \operatorname {arccot}(z)={\frac {\pi }{2}}-\arctan(z)\quad z\neq -i,i}$
which has the same cut as arctan;

${\displaystyle \operatorname {arcsec}(z)=\arccos \left({\frac {1}{z}}\right)\quad z\neq -1,0,+1}$
where the part of the real axis between −1 and +1 inclusive is the cut between the principal sheet of arcsec and other sheets;

${\displaystyle \operatorname {arccsc}(z)=\arcsin \left({\frac {1}{z}}\right)\quad z\neq -1,0,+1}$
which has the same cut as arcsec.


£#h5#£Logarithmic forms£#/h5#£
These functions may also be expressed using complex logarithms. This extends their domains to the complex plane in a natural fashion. The following identities for principal values of the functions hold everywhere that they are defined, even on their branch cuts.

${\displaystyle {\begin{aligned}\arcsin(z)&{}=-i\ln \left({\sqrt {1-z^{2}}}+iz\right)=i\ln \left({\sqrt {1-z^{2}}}-iz\right)&{}=\operatorname {arccsc} \left({\frac {1}{z}}\right)\\[10pt]\arccos(z)&{}=-i\ln \left(i{\sqrt {1-z^{2}}}+z\right)={\frac {\pi }{2}}-\arcsin(z)&{}=\operatorname {arcsec} \left({\frac {1}{z}}\right)\\[10pt]\arctan(z)&{}=-{\frac {i}{2}}\ln \left({\frac {i-z}{i+z}}\right)=-{\frac {i}{2}}\ln \left({\frac {1+iz}{1-iz}}\right)&{}=\operatorname {arccot} \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arccot}(z)&{}=-{\frac {i}{2}}\ln \left({\frac {z+i}{z-i}}\right)=-{\frac {i}{2}}\ln \left({\frac {iz-1}{iz+1}}\right)&{}=\arctan \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arcsec}(z)&{}=-i\ln \left(i{\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {1}{z}}\right)={\frac {\pi }{2}}-\operatorname {arccsc}(z)&{}=\arccos \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arccsc}(z)&{}=-i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {i}{z}}\right)=i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}-{\frac {i}{z}}\right)&{}=\arcsin \left({\frac {1}{z}}\right)\end{aligned}}}$

£#h5#£Generalization£#/h5#£
Because all of the inverse trigonometric functions output an angle of a right triangle, they can be generalized by using Euler's formula to form a right triangle in the complex plane. Algebraically, this gives us:

${\displaystyle ce^{\theta i}=c\cos(\theta )+ci\sin(\theta )}$
or

${\displaystyle ce^{\theta i}=a+bi}$
where ${\displaystyle a}$ is the adjacent side, ${\displaystyle b}$ is the opposite side, and ${\displaystyle c}$ is the hypotenuse. From here, we can solve for ${\displaystyle \theta }$ .

${\displaystyle {\begin{aligned}e^{\ln(c)+\theta i}&=a+bi\\\ln c+\theta i&=\ln(a+bi)\\\theta &=\operatorname {Im} \left(\ln(a+bi)\right)\end{aligned}}}$
or

${\displaystyle \theta =-i\ln \left({\frac {a+bi}{c}}\right)=i\ln \left({\frac {c}{a+bi}}\right)}$
Simply taking the imaginary part works for any real-valued ${\displaystyle a}$ and ${\displaystyle b}$ , but if ${\displaystyle a}$ or ${\displaystyle b}$ is complex-valued, we have to use the final equation so that the real part of the result isn't excluded. Since the length of the hypotenuse doesn't change the angle, ignoring the real part of ${\displaystyle \ln(a+bi)}$ also removes ${\displaystyle c}$ from the equation. In the final equation, we see that the angle of the triangle in the complex plane can be found by inputting the lengths of each side. By setting one of the three sides equal to 1 and one of the remaining sides equal to our input ${\displaystyle z}$ , we obtain a formula for one of the inverse trig functions, for a total of six equations. Because the inverse trig functions require only one input, we must put the final side of the triangle in terms of the other two using the Pythagorean Theorem relation

${\displaystyle a^{2}+b^{2}=c^{2}}$
The table below shows the values of a, b, and c for each of the inverse trig functions and the equivalent expressions for ${\displaystyle \theta }$ that result from plugging the values into the equations above and simplifying.

${\displaystyle {\begin{aligned}&a&&b&&c&&\theta &&\theta _{\text{simplified}}&&\theta _{a,b\in \mathbb {R} }\\\arcsin(z)\ \ &{\sqrt {1-z^{2}}}&&z&&1&&-i\ln \left({\frac {{\sqrt {1-z^{2}}}+zi}{1}}\right)&&=-i\ln \left({\sqrt {1-z^{2}}}+zi\right)&&\operatorname {Im} \left(\ln \left({\sqrt {1-z^{2}}}+zi\right)\right)\\\arccos(z)\ \ &z&&{\sqrt {1-z^{2}}}&&1&&-i\ln \left({\frac {z+i{\sqrt {1-z^{2}}}}{1}}\right)&&=-i\ln \left(z+{\sqrt {z^{2}-1}}\right)&&\operatorname {Im} \left(\ln \left(z+{\sqrt {z^{2}-1}}\right)\right)\\\arctan(z)\ \ &1&&z&&{\sqrt {1+z^{2}}}&&i\ln \left({\frac {\sqrt {1+z^{2}}}{1+zi}}\right)&&={\frac {i}{2}}\ln \left({\frac {i+z}{i-z}}\right)&&\operatorname {Im} \left(\ln \left(1+zi\right)\right)\\\operatorname {arccot}(z)\ \ &z&&1&&{\sqrt {z^{2}+1}}&&i\ln \left({\frac {\sqrt {z^{2}+1}}{z+i}}\right)&&={\frac {i}{2}}\ln \left({\frac {z-i}{z+i}}\right)&&\operatorname {Im} \left(\ln \left(z+i\right)\right)\\\operatorname {arcsec}(z)\ \ &1&&{\sqrt {z^{2}-1}}&&z&&-i\ln \left({\frac {1+i{\sqrt {z^{2}-1}}}{z}}\right)&&=-i\ln \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z^{2}}}-1}}\right)&&\operatorname {Im} \left(\ln \left(1+{\sqrt {1-z^{2}}}\right)\right)\\\operatorname {arccsc}(z)\ \ &{\sqrt {z^{2}-1}}&&1&&z&&-i\ln \left({\frac {{\sqrt {z^{2}-1}}+i}{z}}\right)&&=-i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {i}{z}}\right)&&\operatorname {Im} \left(\ln \left({\sqrt {z^{2}-1}}+i\right)\right)\\\end{aligned}}}$
In this sense, all of the inverse trig functions can be thought of as specific cases of the complex-valued log function. Since this definition works for any complex-valued ${\displaystyle z}$ , this definition allows for hyperbolic angles as outputs and can be used to further define the inverse hyperbolic functions. Elementary proofs of the relations may also proceed via expansion to exponential forms of the trigonometric functions.


£#h5#£Example proof£#/h5#£
${\displaystyle {\begin{aligned}\sin(\phi )&=z\\\phi &=\arcsin(z)\end{aligned}}}$
Using the exponential definition of sine, and letting ${\displaystyle \xi =e^{\phi i},}$

${\displaystyle {\begin{aligned}z&={\frac {e^{\phi i}-e^{-\phi i}}{2i}}\\[10mu]2iz&=\xi -{\frac {1}{\xi }}\\[5mu]0&=\xi ^{2}-2i\xi z-1\\[5mu]\xi &=iz\pm {\sqrt {1-z^{2}}}\\[5mu]\phi &=-i\ln \left(iz\pm {\sqrt {1-z^{2}}}\right)\end{aligned}}}$
(the positive branch is chosen)

${\displaystyle \phi =\arcsin(z)=-i\ln \left(iz+{\sqrt {1-z^{2}}}\right)}$

£#h5#£Applications£#/h5#£
£#h5#£Finding the angle of a right triangle£#/h5#£
Inverse trigonometric functions are useful when trying to determine the remaining two angles of a right triangle when the lengths of the sides of the triangle are known. Recalling the right-triangle definitions of sine and cosine, it follows that

${\displaystyle \theta =\arcsin \left({\frac {\text{opposite}}{\text{hypotenuse}}}\right)=\arccos \left({\frac {\text{adjacent}}{\text{hypotenuse}}}\right).}$
Often, the hypotenuse is unknown and would need to be calculated before using arcsine or arccosine using the Pythagorean Theorem: ${\displaystyle a^{2}+b^{2}=h^{2}}$ where ${\displaystyle h}$ is the length of the hypotenuse. Arctangent comes in handy in this situation, as the length of the hypotenuse is not needed.

${\displaystyle \theta =\arctan \left({\frac {\text{opposite}}{\text{adjacent}}}\right)\,.}$
For example, suppose a roof drops 8 feet as it runs out 20 feet. The roof makes an angle θ with the horizontal, where θ may be computed as follows:

${\displaystyle \theta =\arctan \left({\frac {\text{opposite}}{\text{adjacent}}}\right)=\arctan \left({\frac {\text{rise}}{\text{run}}}\right)=\arctan \left({\frac {8}{20}}\right)\approx 21.8^{\circ }\,.}$

£#h5#£In computer science and engineering£#/h5#£
£#h5#£Two-argument variant of arctangent£#/h5#£

The two-argument atan2 function computes the arctangent of y / x given y and x, but with a range of (−π, π]. In other words, atan2(y, x) is the angle between the positive x-axis of a plane and the point (x, y) on it, with positive sign for counter-clockwise angles (upper half-plane, y > 0), and negative sign for clockwise angles (lower half-plane, y < 0). It was first introduced in many computer programming languages, but it is now also common in other fields of science and engineering.

In terms of the standard arctan function, that is with range of (−π/2, π/2), it can be expressed as follows:

${\displaystyle \operatorname {atan2} (y,x)={\begin{cases}\arctan \left({\frac {y}{x}}\right)&\quad x>0\\\arctan \left({\frac {y}{x}}\right)+\pi &\quad y\geq 0,\;x<0\\\arctan \left({\frac {y}{x}}\right)-\pi &\quad y<0,\;x<0\\{\frac {\pi }{2}}&\quad y>0,\;x=0\\-{\frac {\pi }{2}}&\quad y<0,\;x=0\\{\text{undefined}}&\quad y=0,\;x=0\end{cases}}}$
It also equals the principal value of the argument of the complex number x + iy.

This limited version of the function above may also be defined using the tangent half-angle formulae as follows:

${\displaystyle \operatorname {atan2} (y,x)=2\arctan \left({\frac {y}{{\sqrt {x^{2}+y^{2}}}+x}}\right)}$
provided that either x > 0 or y ≠ 0. However this fails if given x ≤ 0 and y = 0 so the expression is unsuitable for computational use.

The above argument order (y, x) seems to be the most common, and in particular is used in ISO standards such as the C programming language, but a few authors may use the opposite convention (x, y) so some caution is warranted. These variations are detailed at atan2.


£#h5#£Arctangent function with location parameter£#/h5#£
In many applications the solution ${\displaystyle y}$ of the equation ${\displaystyle x=\tan(y)}$ is to come as close as possible to a given value ${\displaystyle -\infty <\eta <\infty }$ . The adequate solution is produced by the parameter modified arctangent function

${\displaystyle y=\arctan _{\eta }(x):=\arctan(x)+\pi \,\operatorname {rni} \left({\frac {\eta -\arctan(x)}{\pi }}\right)\,.}$
The function ${\displaystyle \operatorname {rni} }$ rounds to the nearest integer.


£#h5#£Numerical accuracy£#/h5#£
For angles near 0 and π, arccosine is ill-conditioned and will thus calculate the angle with reduced accuracy in a computer implementation (due to the limited number of digits). Similarly, arcsine is inaccurate for angles near −π/2 and π/2.


£#h5#£See also£#/h5#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Abramowitz, Milton; Stegun, Irene A., eds. (1972). Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables. New York: Dover Publications. ISBN 978-0-486-61272-0.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Inverse Tangent". MathWorld.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Trigonometric Functions £#/li#££#/ul#£




£#h3#£Arccsch£#/h3#£


In mathematics, the inverse hyperbolic functions are the inverse functions of the hyperbolic functions.

For a given value of a hyperbolic function, the corresponding inverse hyperbolic function provides the corresponding hyperbolic angle. The size of the hyperbolic angle is equal to the area of the corresponding hyperbolic sector of the hyperbola xy = 1, or twice the area of the corresponding sector of the unit hyperbola x2 − y2 = 1, just as a circular angle is twice the area of the circular sector of the unit circle. Some authors have called inverse hyperbolic functions "area functions" to realize the hyperbolic angles.

Hyperbolic functions occur in the calculations of angles and distances in hyperbolic geometry. It also occurs in the solutions of many linear differential equations (such as the equation defining a catenary), cubic equations, and Laplace's equation in Cartesian coordinates. Laplace's equations are important in many areas of physics, including electromagnetic theory, heat transfer, fluid dynamics, and special relativity.


£#h5#£Notation£#/h5#£
The ISO 80000-2 standard abbreviations consist of ar- followed by the abbreviation of the corresponding hyperbolic function (e.g., arsinh, arcosh). The prefix arc- followed by the corresponding hyperbolic function (e.g., arcsinh, arccosh) is also commonly seen, by analogy with the nomenclature for inverse trigonometric functions. These are misnomers, since the prefix arc is the abbreviation for arcus, while the prefix ar stands for area; the hyperbolic functions are not directly related to arcs.

Other authors prefer to use the notation argsinh, argcosh, argtanh, and so on, where the prefix arg is the abbreviation of the Latin argumentum. In computer science, this is often shortened to asinh.

The notation sinh−1(x), cosh−1(x), etc., is also used, despite the fact that care must be taken to avoid misinterpretations of the superscript −1 as a power, as opposed to a shorthand to denote the inverse function (e.g., cosh−1(x) versus cosh(x)−1).


£#h5#£Definitions in terms of logarithms£#/h5#£
Since the hyperbolic functions are rational functions of ex whose numerator and denominator are of degree at most two, these functions may be solved in terms of ex, by using the quadratic formula; then, taking the natural logarithm gives the following expressions for the inverse hyperbolic functions.

For complex arguments, the inverse hyperbolic functions, the square root and the logarithm are multi-valued functions, and the equalities of the next subsections may be viewed as equalities of multi-valued functions.

For all inverse hyperbolic functions (save the inverse hyperbolic cotangent and the inverse hyperbolic cosecant), the domain of the real function is connected.


£#h5#£Inverse hyperbolic sine£#/h5#£
Inverse hyperbolic sine (a.k.a. area hyperbolic sine) (Latin: Area sinus hyperbolicus):

${\displaystyle \operatorname {arsinh} x=\ln \left(x+{\sqrt {x^{2}+1}}\right)}$
The domain is the whole real line.


£#h5#£Inverse hyperbolic cosine£#/h5#£
Inverse hyperbolic cosine (a.k.a. area hyperbolic cosine) (Latin: Area cosinus hyperbolicus):

${\displaystyle \operatorname {arcosh} x=\ln \left(x+{\sqrt {x^{2}-1}}\right)}$
The domain is the closed interval [1, +∞ ).


£#h5#£Inverse hyperbolic tangent£#/h5#£
Inverse hyperbolic tangent (a.k.a. area hyperbolic tangent) (Latin: Area tangens hyperbolicus):

${\displaystyle \operatorname {artanh} x={\frac {1}{2}}\ln \left({\frac {1+x}{1-x}}\right)}$
The domain is the open interval (−1, 1).


£#h5#£Inverse hyperbolic cotangent£#/h5#£
Inverse hyperbolic cotangent (a.k.a., area hyperbolic cotangent) (Latin: Area cotangens hyperbolicus):

${\displaystyle \operatorname {arcoth} x={\frac {1}{2}}\ln \left({\frac {x+1}{x-1}}\right)}$
The domain is the union of the open intervals (−∞, −1) and (1, +∞).


£#h5#£Inverse hyperbolic secant£#/h5#£
Inverse hyperbolic secant (a.k.a., area hyperbolic secant) (Latin: Area secans hyperbolicus):

${\displaystyle \operatorname {arsech} x=\ln \left({\frac {1}{x}}+{\sqrt {{\frac {1}{x^{2}}}-1}}\right)=\ln \left({\frac {1+{\sqrt {1-x^{2}}}}{x}}\right)}$
The domain is the semi-open interval (0, 1].


£#h5#£Inverse hyperbolic cosecant£#/h5#£
Inverse hyperbolic cosecant (a.k.a., area hyperbolic cosecant) (Latin: Area cosecans hyperbolicus):

${\displaystyle \operatorname {arcsch} x=\ln \left({\frac {1}{x}}+{\sqrt {{\frac {1}{x^{2}}}+1}}\right)}$
The domain is the real line with 0 removed.


£#h5#£Addition formulae£#/h5#£
${\displaystyle \operatorname {arsinh} u\pm \operatorname {arsinh} v=\operatorname {arsinh} \left(u{\sqrt {1+v^{2}}}\pm v{\sqrt {1+u^{2}}}\right)}$
${\displaystyle \operatorname {arcosh} u\pm \operatorname {arcosh} v=\operatorname {arcosh} \left(uv\pm {\sqrt {(u^{2}-1)(v^{2}-1)}}\right)}$
${\displaystyle \operatorname {artanh} u\pm \operatorname {artanh} v=\operatorname {artanh} \left({\frac {u\pm v}{1\pm uv}}\right)}$
${\displaystyle \operatorname {arcoth} u\pm \operatorname {arcoth} v=\operatorname {arcoth} \left({\frac {1\pm uv}{u\pm v}}\right)}$
${\displaystyle {\begin{aligned}\operatorname {arsinh} u+\operatorname {arcosh} v&=\operatorname {arsinh} \left(uv+{\sqrt {(1+u^{2})(v^{2}-1)}}\right)\\&=\operatorname {arcosh} \left(v{\sqrt {1+u^{2}}}+u{\sqrt {v^{2}-1}}\right)\end{aligned}}}$

£#h5#£Other identities£#/h5#£
${\displaystyle {\begin{aligned}2\operatorname {arcosh} x&=\operatorname {arcosh} (2x^{2}-1)&\quad {\hbox{ for }}x\geq 1\\4\operatorname {arcosh} x&=\operatorname {arcosh} (8x^{4}-8x^{2}+1)&\quad {\hbox{ for }}x\geq 1\\2\operatorname {arsinh} x&=\operatorname {arcosh} (2x^{2}+1)&\quad {\hbox{ for }}x\geq 0\\4\operatorname {arsinh} x&=\operatorname {arcosh} (8x^{4}+8x^{2}+1)&\quad {\hbox{ for }}x\geq 0\end{aligned}}}$
${\displaystyle \ln(x)=\operatorname {arcosh} \left({\frac {x^{2}+1}{2x}}\right)=\operatorname {arsinh} \left({\frac {x^{2}-1}{2x}}\right)=\operatorname {artanh} \left({\frac {x^{2}-1}{x^{2}+1}}\right)}$

£#h5#£Composition of hyperbolic and inverse hyperbolic functions£#/h5#£
${\displaystyle {\begin{aligned}&\sinh(\operatorname {arcosh} x)={\sqrt {x^{2}-1}}\quad {\text{for}}\quad |x|>1\\&\sinh(\operatorname {artanh} x)={\frac {x}{\sqrt {1-x^{2}}}}\quad {\text{for}}\quad -1<x<1\\&\cosh(\operatorname {arsinh} x)={\sqrt {1+x^{2}}}\\&\cosh(\operatorname {artanh} x)={\frac {1}{\sqrt {1-x^{2}}}}\quad {\text{for}}\quad -1<x<1\\&\tanh(\operatorname {arsinh} x)={\frac {x}{\sqrt {1+x^{2}}}}\\&\tanh(\operatorname {arcosh} x)={\frac {\sqrt {x^{2}-1}}{x}}\quad {\text{for}}\quad |x|>1\end{aligned}}}$

£#h5#£Composition of inverse hyperbolic and trigonometric functions£#/h5#£
${\displaystyle \operatorname {arsinh} \left(\tan \alpha \right)=\operatorname {artanh} \left(\sin \alpha \right)=\ln \left({\frac {1+\sin \alpha }{\cos \alpha }}\right)=\pm \operatorname {arcosh} \left({\frac {1}{\cos \alpha }}\right)}$
${\displaystyle \ln \left(\left|\tan \alpha \right|\right)=-\operatorname {artanh} \left(\cos 2\alpha \right)}$

£#h5#£Conversions£#/h5#£
${\displaystyle \ln x=\operatorname {artanh} \left({\frac {x^{2}-1}{x^{2}+1}}\right)=\operatorname {arsinh} \left({\frac {x^{2}-1}{2x}}\right)=\pm \operatorname {arcosh} \left({\frac {x^{2}+1}{2x}}\right)}$
${\displaystyle \operatorname {artanh} x=\operatorname {arsinh} \left({\frac {x}{\sqrt {1-x^{2}}}}\right)=\pm \operatorname {arcosh} \left({\frac {1}{\sqrt {1-x^{2}}}}\right)}$
${\displaystyle \operatorname {arsinh} x=\operatorname {artanh} \left({\frac {x}{\sqrt {1+x^{2}}}}\right)=\pm \operatorname {arcosh} \left({\sqrt {1+x^{2}}}\right)}$
${\displaystyle \operatorname {arcosh} x=\left|\operatorname {arsinh} \left({\sqrt {x^{2}-1}}\right)\right|=\left|\operatorname {artanh} \left({\frac {\sqrt {x^{2}-1}}{x}}\right)\right|}$

£#h5#£Derivatives£#/h5#£
${\displaystyle {\begin{aligned}{\frac {d}{dx}}\operatorname {arsinh} x&{}={\frac {1}{\sqrt {x^{2}+1}}},{\text{ for all real }}x\\{\frac {d}{dx}}\operatorname {arcosh} x&{}={\frac {1}{\sqrt {x^{2}-1}}},{\text{ for all real }}x>1\\{\frac {d}{dx}}\operatorname {artanh} x&{}={\frac {1}{1-x^{2}}},{\text{ for all real }}|x|<1\\{\frac {d}{dx}}\operatorname {arcoth} x&{}={\frac {1}{1-x^{2}}},{\text{ for all real }}|x|>1\\{\frac {d}{dx}}\operatorname {arsech} x&{}={\frac {-1}{x{\sqrt {1-x^{2}}}}},{\text{ for all real }}x\in (0,1)\\{\frac {d}{dx}}\operatorname {arcsch} x&{}={\frac {-1}{|x|{\sqrt {1+x^{2}}}}},{\text{ for all real }}x{\text{, except }}0\\\end{aligned}}}$
For an example differentiation: let θ = arsinh x, so (where sinh2 θ = (sinh θ)2):

${\displaystyle {\frac {d\,\operatorname {arsinh} x}{dx}}={\frac {d\theta }{d\sinh \theta }}={\frac {1}{\cosh \theta }}={\frac {1}{\sqrt {1+\sinh ^{2}\theta }}}={\frac {1}{\sqrt {1+x^{2}}}}.}$

£#h5#£Series expansions£#/h5#£
Expansion series can be obtained for the above functions:

${\displaystyle {\begin{aligned}\operatorname {arsinh} x&=x-\left({\frac {1}{2}}\right){\frac {x^{3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{5}}{5}}-\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{7}}{7}}\pm \cdots \\&=\sum _{n=0}^{\infty }\left({\frac {(-1)^{n}(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{2n+1}}{2n+1}},\qquad \left|x\right|<1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcosh} x&=\ln(2x)-\left(\left({\frac {1}{2}}\right){\frac {x^{-2}}{2}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{-4}}{4}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{-6}}{6}}+\cdots \right)\\&=\ln(2x)-\sum _{n=1}^{\infty }\left({\frac {(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{-2n}}{2n}},\qquad \left|x\right|>1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {artanh} x&=x+{\frac {x^{3}}{3}}+{\frac {x^{5}}{5}}+{\frac {x^{7}}{7}}+\cdots \\&=\sum _{n=0}^{\infty }{\frac {x^{2n+1}}{2n+1}},\qquad \left|x\right|<1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcsch} x=\operatorname {arsinh} {\frac {1}{x}}&=x^{-1}-\left({\frac {1}{2}}\right){\frac {x^{-3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{-5}}{5}}-\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{-7}}{7}}\pm \cdots \\&=\sum _{n=0}^{\infty }\left({\frac {(-1)^{n}(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{-(2n+1)}}{2n+1}},\qquad \left|x\right|>1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arsech} x=\operatorname {arcosh} {\frac {1}{x}}&=\ln {\frac {2}{x}}-\left(\left({\frac {1}{2}}\right){\frac {x^{2}}{2}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{4}}{4}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{6}}{6}}+\cdots \right)\\&=\ln {\frac {2}{x}}-\sum _{n=1}^{\infty }\left({\frac {(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{2n}}{2n}},\qquad 0<x\leq 1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcoth} x=\operatorname {artanh} {\frac {1}{x}}&=x^{-1}+{\frac {x^{-3}}{3}}+{\frac {x^{-5}}{5}}+{\frac {x^{-7}}{7}}+\cdots \\&=\sum _{n=0}^{\infty }{\frac {x^{-(2n+1)}}{2n+1}},\qquad \left|x\right|>1\end{aligned}}}$
Asymptotic expansion for the arsinh x is given by

${\displaystyle \operatorname {arsinh} x=\ln(2x)+\sum \limits _{n=1}^{\infty }{\left({-1}\right)^{n-1}{\frac {\left({2n-1}\right)!!}{2n\left({2n}\right)!!}}}{\frac {1}{x^{2n}}}}$



£#h5#£Principal values in the complex plane£#/h5#£
As functions of a complex variable, inverse hyperbolic functions are multivalued functions that are analytic, except at a finite number of points. For such a function, it is common to define a principal value, which is a single valued analytic function which coincides with one specific branch of the multivalued function, over a domain consisting of the complex plane in which a finite number of arcs (usually half lines or line segments) have been removed. These arcs are called branch cuts. For specifying the branch, that is, defining which value of the multivalued function is considered at each point, one generally define it at a particular point, and deduce the value everywhere in the domain of definition of the principal value by analytic continuation. When possible, it is better to define the principal value directly—without referring to analytic continuation.

For example, for the square root, the principal value is defined as the square root that has a positive real part. This defines a single valued analytic function, which is defined everywhere, except for non-positive real values of the variables (where the two square roots have a zero real part). This principal value of the square root function is denoted ${\displaystyle {\sqrt {x}}}$ in what follows. Similarly, the principal value of the logarithm, denoted ${\displaystyle \operatorname {Log} }$ in what follows, is defined as the value for which the imaginary part has the smallest absolute value. It is defined everywhere except for non-positive real values of the variable, for which two different values of the logarithm reach the minimum.

For all inverse hyperbolic functions, the principal value may be defined in terms of principal values of the square root and the logarithm function. However, in some cases, the formulas of § Definitions in terms of logarithms do not give a correct principal value, as giving a domain of definition which is too small and, in one case non-connected.


£#h5#£Principal value of the inverse hyperbolic sine£#/h5#£
The principal value of the inverse hyperbolic sine is given by

${\displaystyle \operatorname {arsinh} z=\operatorname {Log} (z+{\sqrt {z^{2}+1}}\,)\,.}$
The argument of the square root is a non-positive real number, if and only if z belongs to one of the intervals [i, +i∞) and (−i∞, −i] of the imaginary axis. If the argument of the logarithm is real, then it is positive. Thus this formula defines a principal value for arsinh, with branch cuts [i, +i∞) and (−i∞, −i]. This is optimal, as the branch cuts must connect the singular points i and −i to the infinity.


£#h5#£Principal value of the inverse hyperbolic cosine£#/h5#£
The formula for the inverse hyperbolic cosine given in § Inverse hyperbolic cosine is not convenient, since similar to the principal values of the logarithm and the square root, the principal value of arcosh would not be defined for imaginary z. Thus the square root has to be factorized, leading to

${\displaystyle \operatorname {arcosh} z=\operatorname {Log} (z+{\sqrt {z+1}}{\sqrt {z-1}}\,)\,.}$
The principal values of the square roots are both defined, except if z belongs to the real interval (−∞, 1]. If the argument of the logarithm is real, then z is real and has the same sign. Thus, the above formula defines a principal value of arcosh outside the real interval (−∞, 1], which is thus the unique branch cut.


£#h5#£Principal values of the inverse hyperbolic tangent and cotangent£#/h5#£
The formulas given in § Definitions in terms of logarithms suggests

${\displaystyle {\begin{aligned}\operatorname {artanh} z&={\frac {1}{2}}\operatorname {Log} \left({\frac {1+z}{1-z}}\right)\\\operatorname {arcoth} z&={\frac {1}{2}}\operatorname {Log} \left({\frac {z+1}{z-1}}\right)\end{aligned}}}$
for the definition of the principal values of the inverse hyperbolic tangent and cotangent. In these formulas, the argument of the logarithm is real if and only if z is real. For artanh, this argument is in the real interval (−∞, 0], if z belongs either to (−∞, −1] or to [1, ∞). For arcoth, the argument of the logarithm is in (−∞, 0], if and only if z belongs to the real interval [−1, 1].

Therefore, these formulas define convenient principal values, for which the branch cuts are (−∞, −1] and [1, ∞) for the inverse hyperbolic tangent, and [−1, 1] for the inverse hyperbolic cotangent.

In view of a better numerical evaluation near the branch cuts, some authors use the following definitions of the principal values, although the second one introduces a removable singularity at z = 0. The two definitions of ${\displaystyle \operatorname {artanh} }$ differ for real values of ${\displaystyle z}$ with ${\displaystyle z>1}$ . The ones of ${\displaystyle \operatorname {arcoth} }$ differ for real values of ${\displaystyle z}$ with ${\displaystyle z\in [0,1)}$ .

${\displaystyle {\begin{aligned}\operatorname {artanh} z&={\tfrac {1}{2}}\operatorname {Log} \left({1+z}\right)-{\tfrac {1}{2}}\operatorname {Log} \left({1-z}\right)\\\operatorname {arcoth} z&={\tfrac {1}{2}}\operatorname {Log} \left({1+{\frac {1}{z}}}\right)-{\tfrac {1}{2}}\operatorname {Log} \left({1-{\frac {1}{z}}}\right)\end{aligned}}}$

£#h5#£Principal value of the inverse hyperbolic cosecant£#/h5#£
For the inverse hyperbolic cosecant, the principal value is defined as

${\displaystyle \operatorname {arcsch} z=\operatorname {Log} \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z^{2}}}+1}}\,\right)}$ .
It is defined when the arguments of the logarithm and the square root are not non-positive real numbers. The principal value of the square root is thus defined outside the interval [−i, i] of the imaginary line. If the argument of the logarithm is real, then z is a non-zero real number, and this implies that the argument of the logarithm is positive.

Thus, the principal value is defined by the above formula outside the branch cut, consisting of the interval [−i, i] of the imaginary line.

For z = 0, there is a singular point that is included in the branch cut.


£#h5#£Principal value of the inverse hyperbolic secant£#/h5#£
Here, as in the case of the inverse hyperbolic cosine, we have to factorize the square root. This gives the principal value

${\displaystyle \operatorname {arsech} z=\operatorname {Log} \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z}}+1}}\,{\sqrt {{\frac {1}{z}}-1}}\right).}$
If the argument of a square root is real, then z is real, and it follows that both principal values of square roots are defined, except if z is real and belongs to one of the intervals (−∞, 0] and [1, +∞). If the argument of the logarithm is real and negative, then z is also real and negative. It follows that the principal value of arsech is well defined, by the above formula outside two branch cuts, the real intervals (−∞, 0] and [1, +∞).

For z = 0, there is a singular point that is included in one of the branch cuts.


£#h5#£Graphical representation£#/h5#£
In the following graphical representation of the principal values of the inverse hyperbolic functions, the branch cuts appear as discontinuities of the color. The fact that the whole branch cuts appear as discontinuities, shows that these principal values may not be extended into analytic functions defined over larger domains. In other words, the above defined branch cuts are minimal.


£#h5#£See also£#/h5#£ £#ul#££#li#£Complex logarithm£#/li#£ £#li#£Hyperbolic secant distribution£#/li#£ £#li#£ISO 80000-2£#/li#£ £#li#£List of integrals of inverse hyperbolic functions£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£Bibliography£#/h5#£ £#ul#££#li#£Herbert Busemann and Paul J. Kelly (1953) Projective Geometry and Projective Metrics, page 207, Academic Press.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Inverse hyperbolic functions", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Hyperbolic Functions £#/li#££#/ul#£




£#h3#£Arcctg£#/h3#£

In mathematics, the inverse trigonometric functions (occasionally also called arcus functions, antitrigonometric functions or cyclometric functions) are the inverse functions of the trigonometric functions (with suitably restricted domains). Specifically, they are the inverses of the sine, cosine, tangent, cotangent, secant, and cosecant functions, and are used to obtain an angle from any of the angle's trigonometric ratios. Inverse trigonometric functions are widely used in engineering, navigation, physics, and geometry.


£#h5#£Notation£#/h5#£
Several notations for the inverse trigonometric functions exist. The most common convention is to name inverse trigonometric functions using an arc- prefix: arcsin(x), arccos(x), arctan(x), etc. (This convention is used throughout this article.) This notation arises from the following geometric relationships: when measuring in radians, an angle of θ radians will correspond to an arc whose length is rθ, where r is the radius of the circle. Thus in the unit circle, "the arc whose cosine is x" is the same as "the angle whose cosine is x", because the length of the arc of the circle in radii is the same as the measurement of the angle in radians. In computer programming languages, the inverse trigonometric functions are often called by the abbreviated forms asin, acos, atan.

The notations sin−1(x), cos−1(x), tan−1(x), etc., as introduced by John Herschel in 1813, are often used as well in English-language sources, much more than the also established sin[−1](x), cos[−1](x), tan[−1](x),—conventions consistent with the notation of an inverse function, that is useful e.g. to define the multivalued version of each inverse trigonometric function: e.g. ${\displaystyle \tan ^{-1}(x)=\{\arctan(x)+\pi k\mid k\in \mathbb {Z} \}}$ . However, this might appear to conflict logically with the common semantics for expressions such as sin2(x) (although only sin2 x, without parentheses, is the really common one), which refer to numeric power rather than function composition, and therefore may result in confusion between multiplicative inverse or reciprocal and compositional inverse. The confusion is somewhat mitigated by the fact that each of the reciprocal trigonometric functions has its own name—for example, (cos(x))−1 = sec(x). Nevertheless, certain authors advise against using it for its ambiguity. Another precarious convention used by a tiny number of authors is to use an uppercase first letter, along with a −1 superscript: Sin−1(x), Cos−1(x), Tan−1(x), etc. Although its intention is to avoid confusion with the multiplicative inverse, which should be represented by sin−1(x), cos−1(x), etc., or, better, by sin−1 x, cos−1 x, etc., it in turn creates yet another major source of ambiguity: especially in light of the fact that many popular high-level programming languages (e.g. Wolfram's Mathematica, and University of Sydney's MAGMA) use those very same capitalised representations for the standard trig functions; whereas others (Python (ie SymPy and NumPy), Matlab, MAPLE etc) use lower-case.

Hence, since 2009, the ISO 80000-2 standard has specified solely the "arc" prefix for the inverse functions.


£#h5#£Basic concepts£#/h5#£
£#h5#£Principal values£#/h5#£
Since none of the six trigonometric functions are one-to-one, they must be restricted in order to have inverse functions. Therefore, the result ranges of the inverse functions are proper (i.e. strict) subsets of the domains of the original functions.

For example, using function in the sense of multivalued functions, just as the square root function ${\displaystyle y={\sqrt {x}}}$ could be defined from ${\displaystyle y^{2}=x,}$ the function ${\displaystyle y=\arcsin(x)}$ is defined so that ${\displaystyle \sin(y)=x.}$ For a given real number ${\displaystyle x,}$ with ${\displaystyle -1\leq x\leq 1,}$ there are multiple (in fact, countably infinitely many) numbers ${\displaystyle y}$ such that ${\displaystyle \sin(y)=x}$ ; for example, ${\displaystyle \sin(0)=0,}$ but also ${\displaystyle \sin(\pi )=0,}$ ${\displaystyle \sin(2\pi )=0,}$ etc. When only one value is desired, the function may be restricted to its principal branch. With this restriction, for each ${\displaystyle x}$ in the domain, the expression ${\displaystyle \arcsin(x)}$ will evaluate only to a single value, called its principal value. These properties apply to all the inverse trigonometric functions.

The principal inverses are listed in the following table.

Note: Some authors define the range of arcsecant to be ${\textstyle (0\leq y<{\frac {\pi }{2}}{\text{ or }}\pi \leq y<{\frac {3\pi }{2}})}$ , because the tangent function is nonnegative on this domain. This makes some computations more consistent. For example, using this range, ${\displaystyle \tan(\operatorname {arcsec}(x))={\sqrt {x^{2}-1}},}$ whereas with the range ${\textstyle (0\leq y<{\frac {\pi }{2}}{\text{ or }}{\frac {\pi }{2}}<y\leq \pi )}$ , we would have to write ${\displaystyle \tan(\operatorname {arcsec}(x))=\pm {\sqrt {x^{2}-1}},}$ since tangent is nonnegative on ${\textstyle 0\leq y<{\frac {\pi }{2}},}$ but nonpositive on ${\textstyle {\frac {\pi }{2}}<y\leq \pi .}$ For a similar reason, the same authors define the range of arccosecant to be ${\textstyle (-\pi <y\leq -{\frac {\pi }{2}}}$ or ${\textstyle 0<y\leq {\frac {\pi }{2}}).}$

If ${\displaystyle x}$ is allowed to be a complex number, then the range of ${\displaystyle y}$ applies only to its real part.

The table below displays names and domains of the inverse trigonometric functions along with the range of their usual principal values in radians.

The symbol ${\displaystyle \mathbb {R} =(-\infty ,\infty )}$ denotes the set of all real numbers and ${\displaystyle \mathbb {Z} =\{\ldots ,\,-2,\,-1,\,0,\,1,\,2,\,\ldots \}}$ denotes the set of all integers. The set of all integer multiples of ${\displaystyle \pi }$ is denoted by

The symbol ${\displaystyle \,\setminus \,}$ denotes set subtraction so that, for instance, ${\displaystyle \mathbb {R} \setminus (-1,1)=(-\infty ,-1]\cup [1,\infty )}$ is the set of points in ${\displaystyle \mathbb {R} }$ (that is, real numbers) that are not in the interval ${\displaystyle (-1,1).}$

The Minkowski sum notation ${\textstyle \pi \mathbb {Z} +(0,\pi )}$ and ${\displaystyle \pi \mathbb {Z} +{\bigl (}{-{\tfrac {\pi }{2}}},{\tfrac {\pi }{2}}{\bigr )}}$ that is used above to concisely write the domains of ${\displaystyle \cot ,\csc ,\tan ,{\text{ and }}\sec }$ is now explained.

Domain of cotangent ${\displaystyle \cot }$ and cosecant ${\displaystyle \csc }$ : The domains of ${\displaystyle \,\cot \,}$ and ${\displaystyle \,\csc \,}$ are the same. They are the set of all angles ${\displaystyle \theta }$ at which ${\displaystyle \sin \theta \neq 0,}$ i.e. all real numbers that are not of the form ${\displaystyle \pi n}$ for some integer ${\displaystyle n,}$

Domain of tangent ${\displaystyle \tan }$ and secant ${\displaystyle \sec }$ : The domains of ${\displaystyle \,\tan \,}$ and ${\displaystyle \,\sec \,}$ are the same. They are the set of all angles ${\displaystyle \theta }$ at which ${\displaystyle \cos \theta \neq 0,}$


£#h5#£Solutions to elementary trigonometric equations£#/h5#£
Each of the trigonometric functions is periodic in the real part of its argument, running through all its values twice in each interval of ${\displaystyle 2\pi }$ :

£#ul#££#li#£Sine and cosecant begin their period at ${\textstyle 2\pi k-{\frac {\pi }{2}}}$ (where ${\displaystyle k}$ is an integer), finish it at ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ and then reverse themselves over ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ to ${\displaystyle 2\pi k+{\frac {3\pi }{2}}.}$ £#/li#£ £#li#£Cosine and secant begin their period at ${\displaystyle 2\pi k,}$ finish it at ${\displaystyle 2\pi k+\pi .}$ and then reverse themselves over ${\displaystyle 2\pi k+\pi }$ to ${\displaystyle 2\pi k+2\pi .}$ £#/li#£ £#li#£Tangent begins its period at ${\textstyle 2\pi k-{\frac {\pi }{2}},}$ finishes it at ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ and then repeats it (forward) over ${\textstyle 2\pi k+{\frac {\pi }{2}}}$ to ${\textstyle 2\pi k+{\frac {3\pi }{2}}.}$ £#/li#£ £#li#£Cotangent begins its period at ${\displaystyle 2\pi k,}$ finishes it at ${\displaystyle 2\pi k+\pi ,}$ and then repeats it (forward) over ${\displaystyle 2\pi k+\pi }$ to ${\displaystyle 2\pi k+2\pi .}$ £#/li#££#/ul#£
This periodicity is reflected in the general inverses, where ${\displaystyle k}$ is some integer.

The following table shows how inverse trigonometric functions may be used to solve equalities involving the six standard trigonometric functions. It is assumed that the given values ${\displaystyle \theta ,}$ ${\displaystyle r,}$ ${\displaystyle s,}$ ${\displaystyle x,}$ and ${\displaystyle y}$ all lie within appropriate ranges so that the relevant expressions below are well-defined. Note that "for some ${\displaystyle k\in \mathbb {Z} }$ " is just another way of saying "for some integer ${\displaystyle k.}$ "

The symbol ${\displaystyle \,\iff \,}$ is logical equality. The expression "LHS ${\displaystyle \,\iff \,}$ RHS" indicates that either (a) the left hand side (i.e. LHS) and right hand side (i.e. RHS) are both true, or else (b) the left hand side and right hand side are both false; there is no option (c) (e.g. it is not possible for the LHS statement to be true and also simultaneously for the RHS statement to false), because otherwise "LHS ${\displaystyle \,\iff \,}$ RHS" would not have been written (see this footnote for an example illustrating this concept).

For example, if ${\displaystyle \cos \theta =-1}$ then ${\displaystyle \theta =\pi +2\pi k=-\pi +2\pi (1+k)}$ for some ${\displaystyle k\in \mathbb {Z} .}$ While if ${\displaystyle \sin \theta =\pm 1}$ then ${\textstyle \theta ={\frac {\pi }{2}}+\pi k=-{\frac {\pi }{2}}+\pi (k+1)}$ for some ${\displaystyle k\in \mathbb {Z} ,}$ where ${\displaystyle k}$ will be even if ${\displaystyle \sin \theta =1}$ and it will be odd if ${\displaystyle \sin \theta =-1.}$ The equations ${\displaystyle \sec \theta =-1}$ and ${\displaystyle \csc \theta =\pm 1}$ have the same solutions as ${\displaystyle \cos \theta =-1}$ and ${\displaystyle \sin \theta =\pm 1,}$ respectively. In all equations above except for those just solved (i.e. except for ${\displaystyle \sin }$ / ${\displaystyle \csc \theta =\pm 1}$ and ${\displaystyle \cos }$ / ${\displaystyle \sec \theta =-1}$ ), the integer ${\displaystyle k}$ in the solution's formula is uniquely determined by ${\displaystyle \theta }$ (for fixed ${\displaystyle r,s,x,}$ and ${\displaystyle y}$ ).

Detailed example and explanation of the "plus or minus" symbol ${\displaystyle \pm }$
The solutions to ${\displaystyle \cos \theta =x}$ and ${\displaystyle \sec \theta =x}$ involve the "plus or minus" symbol ${\displaystyle \,\pm ,\,}$ whose meaning is now clarified. Only the solution to ${\displaystyle \cos \theta =x}$ will be discussed since the discussion for ${\displaystyle \sec \theta =x}$ is the same. We are given ${\displaystyle x}$ between ${\displaystyle -1\leq x\leq 1}$ and we know that there is an angle ${\displaystyle \theta }$ in some give interval that satisfies ${\displaystyle \cos \theta =x.}$ We want to find this ${\displaystyle \theta .}$ The formula for the solution involves:

If ${\displaystyle \,\arccos x=0\,}$ (which only happens when ${\displaystyle x=1}$ ) then ${\displaystyle \,+\arccos x=0\,}$ and ${\displaystyle \,-\arccos x=0\,}$ so either way, ${\displaystyle \,\pm \arccos x\,}$ can only be equal to ${\displaystyle 0.}$ But if ${\displaystyle \,\arccos x\neq 0,\,}$ which will now be assumed, then the solution to ${\displaystyle \cos \theta =x,}$ which is written above as is shorthand for the following statement:
Either £#ul#££#li#£ ${\displaystyle \,\theta =\arccos x+2\pi k\,}$ for some integer ${\displaystyle k,}$
or else£#/li#£ £#li#£ ${\displaystyle \,\theta =-\arccos x+2\pi k\,}$ for some integer ${\displaystyle k.}$ £#/li#££#/ul#£
Because ${\displaystyle \,\arccos x\neq 0\,}$ and ${\displaystyle \,0<\arccos x\leq \pi \,}$ exactly one of these two equalities can hold. Additional information about ${\displaystyle \theta }$ is needed to determine which one holds. For example, suppose that ${\displaystyle x=0}$ and that all that is known about ${\displaystyle \theta }$ is that ${\displaystyle \,-\pi \leq \theta \leq \pi \,}$ (and nothing more is known). Then

and moreover, in this particular case ${\displaystyle k=0}$ (for both the ${\displaystyle \,+\,}$ case and the ${\displaystyle \,-\,}$ case) and so consequently, This means that ${\displaystyle \theta }$ could be either ${\displaystyle \,\pi /2\,}$ or ${\displaystyle \,-\pi /2.}$ Without additional information it is not possible to determine which of these values ${\displaystyle \theta }$ has. An example of some additional information that could determine the value of ${\displaystyle \theta }$ would be knowing that the angle is above the ${\displaystyle x}$ -axis(in which case ${\displaystyle \theta =\pi /2}$ ) or alternatively, knowing that it is below the ${\displaystyle x}$ -axis(in which case ${\displaystyle \theta =-\pi /2}$ ).
Transforming equations

The equations above can be transformed by using the reflection and shift identities:

These formulas imply, in particular, that the following hold:

where swapping ${\displaystyle \sin \leftrightarrow \csc ,}$ swapping ${\displaystyle \cos \leftrightarrow \sec ,}$ and swapping ${\displaystyle \tan \leftrightarrow \cot }$ gives the analogous equations for ${\displaystyle \csc ,\sec ,{\text{ and }}\cot ,}$ respectively.
So for example, by using the equality ${\textstyle \sin \left({\frac {\pi }{2}}-\theta \right)=\cos \theta ,}$ the equation ${\displaystyle \cos \theta =x}$ can be transformed into ${\textstyle \sin \left({\frac {\pi }{2}}-\theta \right)=x,}$ which allows for the solution to the equation ${\displaystyle \;\sin \varphi =x\;}$ (where ${\textstyle \varphi :={\frac {\pi }{2}}-\theta }$ ) to be used; that solution being: ${\displaystyle \varphi =(-1)^{k}\arcsin(x)+\pi k\;{\text{ for some }}k\in \mathbb {Z} ,}$ which becomes:

where using the fact that ${\displaystyle (-1)^{k}=(-1)^{-k}}$ and substituting ${\displaystyle h:=-k}$ proves that another solution to ${\displaystyle \;\cos \theta =x\;}$ is: The substitution ${\displaystyle \;\arcsin x={\frac {\pi }{2}}-\arccos x\;}$ may be used express the right hand side of the above formula in terms of ${\displaystyle \;\arccos x\;}$ instead of ${\displaystyle \;\arcsin x.\;}$
£#h5#£Equal identical trigonometric functions£#/h5#£
The table below shows how two angles ${\displaystyle \theta }$ and ${\displaystyle \varphi }$ must be related if their values under a given trigonometric function are equal or negatives of each other.


£#h5#£Relationships between trigonometric functions and inverse trigonometric functions£#/h5#£
Trigonometric functions of inverse trigonometric functions are tabulated below. A quick way to derive them is by considering the geometry of a right-angled triangle, with one side of length 1 and another side of length ${\displaystyle x,}$ then applying the Pythagorean theorem and definitions of the trigonometric ratios. Purely algebraic derivations are longer. It is worth noting that for arcsecant and arccosecant, the diagram assumes that ${\displaystyle x}$ is positive, and thus the result has to be corrected through the use of absolute values and the signum (sgn) operation.


£#h5#£Relationships among the inverse trigonometric functions£#/h5#£
Complementary angles:

${\displaystyle {\begin{aligned}\arccos(x)&={\frac {\pi }{2}}-\arcsin(x)\\[0.5em]\operatorname {arccot}(x)&={\frac {\pi }{2}}-\arctan(x)\\[0.5em]\operatorname {arccsc}(x)&={\frac {\pi }{2}}-\operatorname {arcsec}(x)\end{aligned}}}$
Negative arguments:

${\displaystyle {\begin{aligned}\arcsin(-x)&=-\arcsin(x)\\\arccos(-x)&=\pi -\arccos(x)\\\arctan(-x)&=-\arctan(x)\\\operatorname {arccot}(-x)&=\pi -\operatorname {arccot}(x)\\\operatorname {arcsec}(-x)&=\pi -\operatorname {arcsec}(x)\\\operatorname {arccsc}(-x)&=-\operatorname {arccsc}(x)\end{aligned}}}$
Reciprocal arguments:

${\displaystyle {\begin{aligned}\arccos \left({\frac {1}{x}}\right)&=\operatorname {arcsec}(x)\\[0.3em]\arcsin \left({\frac {1}{x}}\right)&=\operatorname {arccsc}(x)\\[0.3em]\arctan \left({\frac {1}{x}}\right)&={\frac {\pi }{2}}-\arctan(x)=\operatorname {arccot}(x)\,,{\text{ if }}x>0\\[0.3em]\arctan \left({\frac {1}{x}}\right)&=-{\frac {\pi }{2}}-\arctan(x)=\operatorname {arccot}(x)-\pi \,,{\text{ if }}x<0\\[0.3em]\operatorname {arccot} \left({\frac {1}{x}}\right)&={\frac {\pi }{2}}-\operatorname {arccot}(x)=\arctan(x)\,,{\text{ if }}x>0\\[0.3em]\operatorname {arccot} \left({\frac {1}{x}}\right)&={\frac {3\pi }{2}}-\operatorname {arccot}(x)=\pi +\arctan(x)\,,{\text{ if }}x<0\\[0.3em]\operatorname {arcsec} \left({\frac {1}{x}}\right)&=\arccos(x)\\[0.3em]\operatorname {arccsc} \left({\frac {1}{x}}\right)&=\arcsin(x)\end{aligned}}}$
Useful identities if one only has a fragment of a sine table:

${\displaystyle {\begin{aligned}\arccos(x)&=\arcsin \left({\sqrt {1-x^{2}}}\right)\,,{\text{ if }}0\leq x\leq 1{\text{ , from which you get }}\\\arccos &\left({\frac {1-x^{2}}{1+x^{2}}}\right)=\arcsin \left({\frac {2x}{1+x^{2}}}\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin &\left({\sqrt {1-x^{2}}}\right)={\frac {\pi }{2}}-\operatorname {sgn}(x)\arcsin(x)\\\arccos(x)&={\frac {1}{2}}\arccos \left(2x^{2}-1\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin(x)&={\frac {1}{2}}\arccos \left(1-2x^{2}\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin(x)&=\arctan \left({\frac {x}{\sqrt {1-x^{2}}}}\right)\\\arccos(x)&=\arctan \left({\frac {\sqrt {1-x^{2}}}{x}}\right)\\\arctan(x)&=\arcsin \left({\frac {x}{\sqrt {1+x^{2}}}}\right)\\\operatorname {arccot}(x)&=\arccos \left({\frac {x}{\sqrt {1+x^{2}}}}\right)\end{aligned}}}$
Whenever the square root of a complex number is used here, we choose the root with the positive real part (or positive imaginary part if the square was negative real).

A useful form that follows directly from the table above is

${\displaystyle \arctan \left(x\right)=\arccos \left({\sqrt {\frac {1}{1+x^{2}}}}\right)\,,{\text{ if }}x\geq 0}$ .
It is obtained by recognizing that ${\displaystyle \cos \left(\arctan \left(x\right)\right)={\sqrt {\frac {1}{1+x^{2}}}}=\cos \left(\arccos \left({\sqrt {\frac {1}{1+x^{2}}}}\right)\right)}$ .

From the half-angle formula, ${\displaystyle \tan \left({\tfrac {\theta }{2}}\right)={\tfrac {\sin(\theta )}{1+\cos(\theta )}}}$ , we get:

${\displaystyle {\begin{aligned}\arcsin(x)&=2\arctan \left({\frac {x}{1+{\sqrt {1-x^{2}}}}}\right)\\[0.5em]\arccos(x)&=2\arctan \left({\frac {\sqrt {1-x^{2}}}{1+x}}\right)\,,{\text{ if }}-1<x\leq 1\\[0.5em]\arctan(x)&=2\arctan \left({\frac {x}{1+{\sqrt {1+x^{2}}}}}\right)\end{aligned}}}$

£#h5#£Arctangent addition formula£#/h5#£
${\displaystyle \arctan(u)\pm \arctan(v)=\arctan \left({\frac {u\pm v}{1\mp uv}}\right){\pmod {\pi }}\,,\quad uv\neq 1\,.}$
This is derived from the tangent addition formula

${\displaystyle \tan(\alpha \pm \beta )={\frac {\tan(\alpha )\pm \tan(\beta )}{1\mp \tan(\alpha )\tan(\beta )}}\,,}$
by letting

${\displaystyle \alpha =\arctan(u)\,,\quad \beta =\arctan(v)\,.}$

£#h5#£In calculus£#/h5#£
£#h5#£Derivatives of inverse trigonometric functions£#/h5#£
The derivatives for complex values of z are as follows:

${\displaystyle {\begin{aligned}{\frac {d}{dz}}\arcsin(z)&{}={\frac {1}{\sqrt {1-z^{2}}}}\;;&z&{}\neq -1,+1\\{\frac {d}{dz}}\arccos(z)&{}=-{\frac {1}{\sqrt {1-z^{2}}}}\;;&z&{}\neq -1,+1\\{\frac {d}{dz}}\arctan(z)&{}={\frac {1}{1+z^{2}}}\;;&z&{}\neq -i,+i\\{\frac {d}{dz}}\operatorname {arccot}(z)&{}=-{\frac {1}{1+z^{2}}}\;;&z&{}\neq -i,+i\\{\frac {d}{dz}}\operatorname {arcsec}(z)&{}={\frac {1}{z^{2}{\sqrt {1-{\frac {1}{z^{2}}}}}}}\;;&z&{}\neq -1,0,+1\\{\frac {d}{dz}}\operatorname {arccsc}(z)&{}=-{\frac {1}{z^{2}{\sqrt {1-{\frac {1}{z^{2}}}}}}}\;;&z&{}\neq -1,0,+1\end{aligned}}}$
Only for real values of x:

${\displaystyle {\begin{aligned}{\frac {d}{dx}}\operatorname {arcsec}(x)&{}={\frac {1}{|x|{\sqrt {x^{2}-1}}}}\;;&|x|>1\\{\frac {d}{dx}}\operatorname {arccsc}(x)&{}=-{\frac {1}{|x|{\sqrt {x^{2}-1}}}}\;;&|x|>1\end{aligned}}}$
For a sample derivation: if ${\displaystyle \theta =\arcsin(x)}$ , we get:

${\displaystyle {\frac {d\arcsin(x)}{dx}}={\frac {d\theta }{d\sin(\theta )}}={\frac {d\theta }{\cos(\theta )\,d\theta }}={\frac {1}{\cos(\theta )}}={\frac {1}{\sqrt {1-\sin ^{2}(\theta )}}}={\frac {1}{\sqrt {1-x^{2}}}}}$

£#h5#£Expression as definite integrals£#/h5#£
Integrating the derivative and fixing the value at one point gives an expression for the inverse trigonometric function as a definite integral:

${\displaystyle {\begin{aligned}\arcsin(x)&{}=\int _{0}^{x}{\frac {1}{\sqrt {1-z^{2}}}}\,dz\;,&|x|&{}\leq 1\\\arccos(x)&{}=\int _{x}^{1}{\frac {1}{\sqrt {1-z^{2}}}}\,dz\;,&|x|&{}\leq 1\\\arctan(x)&{}=\int _{0}^{x}{\frac {1}{z^{2}+1}}\,dz\;,\\\operatorname {arccot}(x)&{}=\int _{x}^{\infty }{\frac {1}{z^{2}+1}}\,dz\;,\\\operatorname {arcsec}(x)&{}=\int _{1}^{x}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz=\pi +\int _{-x}^{-1}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz\;,&x&{}\geq 1\\\operatorname {arccsc}(x)&{}=\int _{x}^{\infty }{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz=\int _{-\infty }^{-x}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz\;,&x&{}\geq 1\\\end{aligned}}}$
When x equals 1, the integrals with limited domains are improper integrals, but still well-defined.


£#h5#£Infinite series£#/h5#£
Similar to the sine and cosine functions, the inverse trigonometric functions can also be calculated using power series, as follows. For arcsine, the series can be derived by expanding its derivative, ${\textstyle {\tfrac {1}{\sqrt {1-z^{2}}}}}$ , as a binomial series, and integrating term by term (using the integral definition as above). The series for arctangent can similarly be derived by expanding its derivative ${\textstyle {\frac {1}{1+z^{2}}}}$ in a geometric series, and applying the integral definition above (see Leibniz series).

${\displaystyle {\begin{aligned}\arcsin(z)&=z+\left({\frac {1}{2}}\right){\frac {z^{3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {z^{5}}{5}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {z^{7}}{7}}+\cdots \\[5pt]&=\sum _{n=0}^{\infty }{\frac {(2n-1)!!}{(2n)!!}}{\frac {z^{2n+1}}{2n+1}}\\[5pt]&=\sum _{n=0}^{\infty }{\frac {(2n)!}{(2^{n}n!)^{2}}}{\frac {z^{2n+1}}{2n+1}}\,;\qquad |z|\leq 1\end{aligned}}}$
${\displaystyle \arctan(z)=z-{\frac {z^{3}}{3}}+{\frac {z^{5}}{5}}-{\frac {z^{7}}{7}}+\cdots =\sum _{n=0}^{\infty }{\frac {(-1)^{n}z^{2n+1}}{2n+1}}\,;\qquad |z|\leq 1\qquad z\neq i,-i}$
Series for the other inverse trigonometric functions can be given in terms of these according to the relationships given above. For example, ${\displaystyle \arccos(x)=\pi /2-\arcsin(x)}$ , ${\displaystyle \operatorname {arccsc}(x)=\arcsin(1/x)}$ , and so on. Another series is given by:

${\displaystyle 2\left(\arcsin \left({\frac {x}{2}}\right)\right)^{2}=\sum _{n=1}^{\infty }{\frac {x^{2n}}{n^{2}{\binom {2n}{n}}}}.}$
Leonhard Euler found a series for the arctangent that converges more quickly than its Taylor series:

${\displaystyle \arctan(z)={\frac {z}{1+z^{2}}}\sum _{n=0}^{\infty }\prod _{k=1}^{n}{\frac {2kz^{2}}{(2k+1)(1+z^{2})}}.}$
(The term in the sum for n = 0 is the empty product, so is 1.)

Alternatively, this can be expressed as

${\displaystyle \arctan(z)=\sum _{n=0}^{\infty }{\frac {2^{2n}(n!)^{2}}{(2n+1)!}}{\frac {z^{2n+1}}{(1+z^{2})^{n+1}}}.}$
Another series for the arctangent function is given by

${\displaystyle \arctan(z)=i\sum _{n=1}^{\infty }{\frac {1}{2n-1}}\left({\frac {1}{(1+2i/z)^{2n-1}}}-{\frac {1}{(1-2i/z)^{2n-1}}}\right),}$
where ${\displaystyle i={\sqrt {-1}}}$ is the imaginary unit.


£#h5#£Continued fractions for arctangent£#/h5#£
Two alternatives to the power series for arctangent are these generalized continued fractions:

${\displaystyle \arctan(z)={\frac {z}{1+{\cfrac {(1z)^{2}}{3-1z^{2}+{\cfrac {(3z)^{2}}{5-3z^{2}+{\cfrac {(5z)^{2}}{7-5z^{2}+{\cfrac {(7z)^{2}}{9-7z^{2}+\ddots }}}}}}}}}}={\frac {z}{1+{\cfrac {(1z)^{2}}{3+{\cfrac {(2z)^{2}}{5+{\cfrac {(3z)^{2}}{7+{\cfrac {(4z)^{2}}{9+\ddots }}}}}}}}}}}$
The second of these is valid in the cut complex plane. There are two cuts, from −i to the point at infinity, going down the imaginary axis, and from i to the point at infinity, going up the same axis. It works best for real numbers running from −1 to 1. The partial denominators are the odd natural numbers, and the partial numerators (after the first) are just (nz)2, with each perfect square appearing once. The first was developed by Leonhard Euler; the second by Carl Friedrich Gauss utilizing the Gaussian hypergeometric series.


£#h5#£Indefinite integrals of inverse trigonometric functions£#/h5#£
For real and complex values of z:

${\displaystyle {\begin{aligned}\int \arcsin(z)\,dz&{}=z\,\arcsin(z)+{\sqrt {1-z^{2}}}+C\\\int \arccos(z)\,dz&{}=z\,\arccos(z)-{\sqrt {1-z^{2}}}+C\\\int \arctan(z)\,dz&{}=z\,\arctan(z)-{\frac {1}{2}}\ln \left(1+z^{2}\right)+C\\\int \operatorname {arccot}(z)\,dz&{}=z\,\operatorname {arccot}(z)+{\frac {1}{2}}\ln \left(1+z^{2}\right)+C\\\int \operatorname {arcsec}(z)\,dz&{}=z\,\operatorname {arcsec}(z)-\ln \left[z\left(1+{\sqrt {\frac {z^{2}-1}{z^{2}}}}\right)\right]+C\\\int \operatorname {arccsc}(z)\,dz&{}=z\,\operatorname {arccsc}(z)+\ln \left[z\left(1+{\sqrt {\frac {z^{2}-1}{z^{2}}}}\right)\right]+C\end{aligned}}}$
For real x ≥ 1:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\ln \left(x+{\sqrt {x^{2}-1}}\right)+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\ln \left(x+{\sqrt {x^{2}-1}}\right)+C\end{aligned}}}$
For all real x not between -1 and 1:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\operatorname {sgn}(x)\ln \left|x+{\sqrt {x^{2}-1}}\right|+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\operatorname {sgn}(x)\ln \left|x+{\sqrt {x^{2}-1}}\right|+C\end{aligned}}}$
The absolute value is necessary to compensate for both negative and positive values of the arcsecant and arccosecant functions. The signum function is also necessary due to the absolute values in the derivatives of the two functions, which create two different solutions for positive and negative values of x. These can be further simplified using the logarithmic definitions of the inverse hyperbolic functions:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\operatorname {arcosh} (|x|)+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\operatorname {arcosh} (|x|)+C\\\end{aligned}}}$
The absolute value in the argument of the arcosh function creates a negative half of its graph, making it identical to the signum logarithmic function shown above.

All of these antiderivatives can be derived using integration by parts and the simple derivative forms shown above.


£#h5#£Example£#/h5#£
Using ${\displaystyle \int u\,dv=uv-\int v\,du}$ (i.e. integration by parts), set

${\displaystyle {\begin{aligned}u&=\arcsin(x)&dv&=dx\\du&={\frac {dx}{\sqrt {1-x^{2}}}}&v&=x\end{aligned}}}$
Then

${\displaystyle \int \arcsin(x)\,dx=x\arcsin(x)-\int {\frac {x}{\sqrt {1-x^{2}}}}\,dx,}$
which by the simple substitution ${\displaystyle w=1-x^{2},\ dw=-2x\,dx}$ yields the final result:

${\displaystyle \int \arcsin(x)\,dx=x\arcsin(x)+{\sqrt {1-x^{2}}}+C}$

£#h5#£Extension to complex plane£#/h5#£
Since the inverse trigonometric functions are analytic functions, they can be extended from the real line to the complex plane. This results in functions with multiple sheets and branch points. One possible way of defining the extension is:

${\displaystyle \arctan(z)=\int _{0}^{z}{\frac {dx}{1+x^{2}}}\quad z\neq -i,+i}$
where the part of the imaginary axis which does not lie strictly between the branch points (−i and +i) is the branch cut between the principal sheet and other sheets. The path of the integral must not cross a branch cut. For z not on a branch cut, a straight line path from 0 to z is such a path. For z on a branch cut, the path must approach from Re[x] > 0 for the upper branch cut and from Re[x] < 0 for the lower branch cut.

The arcsine function may then be defined as:

${\displaystyle \arcsin(z)=\arctan \left({\frac {z}{\sqrt {1-z^{2}}}}\right)\quad z\neq -1,+1}$
where (the square-root function has its cut along the negative real axis and) the part of the real axis which does not lie strictly between −1 and +1 is the branch cut between the principal sheet of arcsin and other sheets;

${\displaystyle \arccos(z)={\frac {\pi }{2}}-\arcsin(z)\quad z\neq -1,+1}$
which has the same cut as arcsin;

${\displaystyle \operatorname {arccot}(z)={\frac {\pi }{2}}-\arctan(z)\quad z\neq -i,i}$
which has the same cut as arctan;

${\displaystyle \operatorname {arcsec}(z)=\arccos \left({\frac {1}{z}}\right)\quad z\neq -1,0,+1}$
where the part of the real axis between −1 and +1 inclusive is the cut between the principal sheet of arcsec and other sheets;

${\displaystyle \operatorname {arccsc}(z)=\arcsin \left({\frac {1}{z}}\right)\quad z\neq -1,0,+1}$
which has the same cut as arcsec.


£#h5#£Logarithmic forms£#/h5#£
These functions may also be expressed using complex logarithms. This extends their domains to the complex plane in a natural fashion. The following identities for principal values of the functions hold everywhere that they are defined, even on their branch cuts.

${\displaystyle {\begin{aligned}\arcsin(z)&{}=-i\ln \left({\sqrt {1-z^{2}}}+iz\right)=i\ln \left({\sqrt {1-z^{2}}}-iz\right)&{}=\operatorname {arccsc} \left({\frac {1}{z}}\right)\\[10pt]\arccos(z)&{}=-i\ln \left(i{\sqrt {1-z^{2}}}+z\right)={\frac {\pi }{2}}-\arcsin(z)&{}=\operatorname {arcsec} \left({\frac {1}{z}}\right)\\[10pt]\arctan(z)&{}=-{\frac {i}{2}}\ln \left({\frac {i-z}{i+z}}\right)=-{\frac {i}{2}}\ln \left({\frac {1+iz}{1-iz}}\right)&{}=\operatorname {arccot} \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arccot}(z)&{}=-{\frac {i}{2}}\ln \left({\frac {z+i}{z-i}}\right)=-{\frac {i}{2}}\ln \left({\frac {iz-1}{iz+1}}\right)&{}=\arctan \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arcsec}(z)&{}=-i\ln \left(i{\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {1}{z}}\right)={\frac {\pi }{2}}-\operatorname {arccsc}(z)&{}=\arccos \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arccsc}(z)&{}=-i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {i}{z}}\right)=i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}-{\frac {i}{z}}\right)&{}=\arcsin \left({\frac {1}{z}}\right)\end{aligned}}}$

£#h5#£Generalization£#/h5#£
Because all of the inverse trigonometric functions output an angle of a right triangle, they can be generalized by using Euler's formula to form a right triangle in the complex plane. Algebraically, this gives us:

${\displaystyle ce^{\theta i}=c\cos(\theta )+ci\sin(\theta )}$
or

${\displaystyle ce^{\theta i}=a+bi}$
where ${\displaystyle a}$ is the adjacent side, ${\displaystyle b}$ is the opposite side, and ${\displaystyle c}$ is the hypotenuse. From here, we can solve for ${\displaystyle \theta }$ .

${\displaystyle {\begin{aligned}e^{\ln(c)+\theta i}&=a+bi\\\ln c+\theta i&=\ln(a+bi)\\\theta &=\operatorname {Im} \left(\ln(a+bi)\right)\end{aligned}}}$
or

${\displaystyle \theta =-i\ln \left({\frac {a+bi}{c}}\right)=i\ln \left({\frac {c}{a+bi}}\right)}$
Simply taking the imaginary part works for any real-valued ${\displaystyle a}$ and ${\displaystyle b}$ , but if ${\displaystyle a}$ or ${\displaystyle b}$ is complex-valued, we have to use the final equation so that the real part of the result isn't excluded. Since the length of the hypotenuse doesn't change the angle, ignoring the real part of ${\displaystyle \ln(a+bi)}$ also removes ${\displaystyle c}$ from the equation. In the final equation, we see that the angle of the triangle in the complex plane can be found by inputting the lengths of each side. By setting one of the three sides equal to 1 and one of the remaining sides equal to our input ${\displaystyle z}$ , we obtain a formula for one of the inverse trig functions, for a total of six equations. Because the inverse trig functions require only one input, we must put the final side of the triangle in terms of the other two using the Pythagorean Theorem relation

${\displaystyle a^{2}+b^{2}=c^{2}}$
The table below shows the values of a, b, and c for each of the inverse trig functions and the equivalent expressions for ${\displaystyle \theta }$ that result from plugging the values into the equations above and simplifying.

${\displaystyle {\begin{aligned}&a&&b&&c&&\theta &&\theta _{\text{simplified}}&&\theta _{a,b\in \mathbb {R} }\\\arcsin(z)\ \ &{\sqrt {1-z^{2}}}&&z&&1&&-i\ln \left({\frac {{\sqrt {1-z^{2}}}+zi}{1}}\right)&&=-i\ln \left({\sqrt {1-z^{2}}}+zi\right)&&\operatorname {Im} \left(\ln \left({\sqrt {1-z^{2}}}+zi\right)\right)\\\arccos(z)\ \ &z&&{\sqrt {1-z^{2}}}&&1&&-i\ln \left({\frac {z+i{\sqrt {1-z^{2}}}}{1}}\right)&&=-i\ln \left(z+{\sqrt {z^{2}-1}}\right)&&\operatorname {Im} \left(\ln \left(z+{\sqrt {z^{2}-1}}\right)\right)\\\arctan(z)\ \ &1&&z&&{\sqrt {1+z^{2}}}&&i\ln \left({\frac {\sqrt {1+z^{2}}}{1+zi}}\right)&&={\frac {i}{2}}\ln \left({\frac {i+z}{i-z}}\right)&&\operatorname {Im} \left(\ln \left(1+zi\right)\right)\\\operatorname {arccot}(z)\ \ &z&&1&&{\sqrt {z^{2}+1}}&&i\ln \left({\frac {\sqrt {z^{2}+1}}{z+i}}\right)&&={\frac {i}{2}}\ln \left({\frac {z-i}{z+i}}\right)&&\operatorname {Im} \left(\ln \left(z+i\right)\right)\\\operatorname {arcsec}(z)\ \ &1&&{\sqrt {z^{2}-1}}&&z&&-i\ln \left({\frac {1+i{\sqrt {z^{2}-1}}}{z}}\right)&&=-i\ln \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z^{2}}}-1}}\right)&&\operatorname {Im} \left(\ln \left(1+{\sqrt {1-z^{2}}}\right)\right)\\\operatorname {arccsc}(z)\ \ &{\sqrt {z^{2}-1}}&&1&&z&&-i\ln \left({\frac {{\sqrt {z^{2}-1}}+i}{z}}\right)&&=-i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {i}{z}}\right)&&\operatorname {Im} \left(\ln \left({\sqrt {z^{2}-1}}+i\right)\right)\\\end{aligned}}}$
In this sense, all of the inverse trig functions can be thought of as specific cases of the complex-valued log function. Since this definition works for any complex-valued ${\displaystyle z}$ , this definition allows for hyperbolic angles as outputs and can be used to further define the inverse hyperbolic functions. Elementary proofs of the relations may also proceed via expansion to exponential forms of the trigonometric functions.


£#h5#£Example proof£#/h5#£
${\displaystyle {\begin{aligned}\sin(\phi )&=z\\\phi &=\arcsin(z)\end{aligned}}}$
Using the exponential definition of sine, and letting ${\displaystyle \xi =e^{\phi i},}$

${\displaystyle {\begin{aligned}z&={\frac {e^{\phi i}-e^{-\phi i}}{2i}}\\[10mu]2iz&=\xi -{\frac {1}{\xi }}\\[5mu]0&=\xi ^{2}-2i\xi z-1\\[5mu]\xi &=iz\pm {\sqrt {1-z^{2}}}\\[5mu]\phi &=-i\ln \left(iz\pm {\sqrt {1-z^{2}}}\right)\end{aligned}}}$
(the positive branch is chosen)

${\displaystyle \phi =\arcsin(z)=-i\ln \left(iz+{\sqrt {1-z^{2}}}\right)}$

£#h5#£Applications£#/h5#£
£#h5#£Finding the angle of a right triangle£#/h5#£
Inverse trigonometric functions are useful when trying to determine the remaining two angles of a right triangle when the lengths of the sides of the triangle are known. Recalling the right-triangle definitions of sine and cosine, it follows that

${\displaystyle \theta =\arcsin \left({\frac {\text{opposite}}{\text{hypotenuse}}}\right)=\arccos \left({\frac {\text{adjacent}}{\text{hypotenuse}}}\right).}$
Often, the hypotenuse is unknown and would need to be calculated before using arcsine or arccosine using the Pythagorean Theorem: ${\displaystyle a^{2}+b^{2}=h^{2}}$ where ${\displaystyle h}$ is the length of the hypotenuse. Arctangent comes in handy in this situation, as the length of the hypotenuse is not needed.

${\displaystyle \theta =\arctan \left({\frac {\text{opposite}}{\text{adjacent}}}\right)\,.}$
For example, suppose a roof drops 8 feet as it runs out 20 feet. The roof makes an angle θ with the horizontal, where θ may be computed as follows:

${\displaystyle \theta =\arctan \left({\frac {\text{opposite}}{\text{adjacent}}}\right)=\arctan \left({\frac {\text{rise}}{\text{run}}}\right)=\arctan \left({\frac {8}{20}}\right)\approx 21.8^{\circ }\,.}$

£#h5#£In computer science and engineering£#/h5#£
£#h5#£Two-argument variant of arctangent£#/h5#£

The two-argument atan2 function computes the arctangent of y / x given y and x, but with a range of (−π, π]. In other words, atan2(y, x) is the angle between the positive x-axis of a plane and the point (x, y) on it, with positive sign for counter-clockwise angles (upper half-plane, y > 0), and negative sign for clockwise angles (lower half-plane, y < 0). It was first introduced in many computer programming languages, but it is now also common in other fields of science and engineering.

In terms of the standard arctan function, that is with range of (−π/2, π/2), it can be expressed as follows:

${\displaystyle \operatorname {atan2} (y,x)={\begin{cases}\arctan \left({\frac {y}{x}}\right)&\quad x>0\\\arctan \left({\frac {y}{x}}\right)+\pi &\quad y\geq 0,\;x<0\\\arctan \left({\frac {y}{x}}\right)-\pi &\quad y<0,\;x<0\\{\frac {\pi }{2}}&\quad y>0,\;x=0\\-{\frac {\pi }{2}}&\quad y<0,\;x=0\\{\text{undefined}}&\quad y=0,\;x=0\end{cases}}}$
It also equals the principal value of the argument of the complex number x + iy.

This limited version of the function above may also be defined using the tangent half-angle formulae as follows:

${\displaystyle \operatorname {atan2} (y,x)=2\arctan \left({\frac {y}{{\sqrt {x^{2}+y^{2}}}+x}}\right)}$
provided that either x > 0 or y ≠ 0. However this fails if given x ≤ 0 and y = 0 so the expression is unsuitable for computational use.

The above argument order (y, x) seems to be the most common, and in particular is used in ISO standards such as the C programming language, but a few authors may use the opposite convention (x, y) so some caution is warranted. These variations are detailed at atan2.


£#h5#£Arctangent function with location parameter£#/h5#£
In many applications the solution ${\displaystyle y}$ of the equation ${\displaystyle x=\tan(y)}$ is to come as close as possible to a given value ${\displaystyle -\infty <\eta <\infty }$ . The adequate solution is produced by the parameter modified arctangent function

${\displaystyle y=\arctan _{\eta }(x):=\arctan(x)+\pi \,\operatorname {rni} \left({\frac {\eta -\arctan(x)}{\pi }}\right)\,.}$
The function ${\displaystyle \operatorname {rni} }$ rounds to the nearest integer.


£#h5#£Numerical accuracy£#/h5#£
For angles near 0 and π, arccosine is ill-conditioned and will thus calculate the angle with reduced accuracy in a computer implementation (due to the limited number of digits). Similarly, arcsine is inaccurate for angles near −π/2 and π/2.


£#h5#£See also£#/h5#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Abramowitz, Milton; Stegun, Irene A., eds. (1972). Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables. New York: Dover Publications. ISBN 978-0-486-61272-0.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Inverse Tangent". MathWorld.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Trigonometric Functions £#/li#££#/ul#£




£#h3#£Archimedes' Axiom£#/h3#£

In abstract algebra and analysis, the Archimedean property, named after the ancient Greek mathematician Archimedes of Syracuse, is a property held by some algebraic structures, such as ordered or normed groups, and fields. The property, typically construed, states that given two positive numbers x and y, there is an integer n such that nx > y. It also means that the set of natural numbers is not bounded above. Roughly speaking, it is the property of having no infinitely large or infinitely small elements. It was Otto Stolz who gave the axiom of Archimedes its name because it appears as Axiom V of Archimedes’ On the Sphere and Cylinder.

The notion arose from the theory of magnitudes of Ancient Greece; it still plays an important role in modern mathematics such as David Hilbert's axioms for geometry, and the theories of ordered groups, ordered fields, and local fields.

An algebraic structure in which any two non-zero elements are comparable, in the sense that neither of them is infinitesimal with respect to the other, is said to be Archimedean. A structure which has a pair of non-zero elements, one of which is infinitesimal with respect to the other, is said to be non-Archimedean. For example, a linearly ordered group that is Archimedean is an Archimedean group.

This can be made precise in various contexts with slightly different formulations. For example, in the context of ordered fields, one has the axiom of Archimedes which formulates this property, where the field of real numbers is Archimedean, but that of rational functions in real coefficients is not.


£#h5#£History and origin of the name of the Archimedean property£#/h5#£
The concept was named by Otto Stolz (in the 1880s) after the ancient Greek geometer and physicist Archimedes of Syracuse.

The Archimedean property appears in Book V of Euclid's Elements as Definition 4:

Magnitudes are said to have a ratio to one another which can, when multiplied, exceed one another.

Because Archimedes credited it to Eudoxus of Cnidus it is also known as the "Theorem of Eudoxus" or the Eudoxus axiom.

Archimedes used infinitesimals in heuristic arguments, although he denied that those were finished mathematical proofs.


£#h5#£Definition for linearly ordered groups£#/h5#£
Let x and y be positive elements of a linearly ordered group G. Then x is infinitesimal with respect to y (or equivalently, y is infinite with respect to x) if, for every natural number n, the multiple nx is less than y, that is, the following inequality holds:

This definition can be extended to the entire group by taking absolute values.

The group G is Archimedean if there is no pair (x, y) such that x is infinitesimal with respect to y.

Additionally, if K is an algebraic structure with a unit (1) — for example, a ring — a similar definition applies to K. If x is infinitesimal with respect to 1, then x is an infinitesimal element. Likewise, if y is infinite with respect to 1, then y is an infinite element. The algebraic structure K is Archimedean if it has no infinite elements and no infinitesimal elements.


£#h5#£Ordered fields£#/h5#£
Ordered fields have some additional properties:

£#ul#££#li#£The rational numbers are embedded in any ordered field. That is, any ordered field has characteristic zero.£#/li#£ £#li#£If x is infinitesimal, then 1/x is infinite, and vice versa. Therefore, to verify that a field is Archimedean it is enough to check only that there are no infinitesimal elements, or to check that there are no infinite elements.£#/li#£ £#li#£If x is infinitesimal and r is a rational number, then rx is also infinitesimal. As a result, given a general element c, the three numbers c/2, c, and 2c are either all infinitesimal or all non-infinitesimal.£#/li#££#/ul#£
In this setting, an ordered field K is Archimedean precisely when the following statement, called the axiom of Archimedes, holds:

"Let x be any element of K. Then there exists a natural number n such that n > x."
Alternatively one can use the following characterization:


£#h5#£Definition for normed fields£#/h5#£
The qualifier "Archimedean" is also formulated in the theory of rank one valued fields and normed spaces over rank one valued fields as follows. Let F be a field endowed with an absolute value function, i.e., a function which associates the real number 0 with the field element 0 and associates a positive real number ${\displaystyle |x|}$ with each non-zero x ∈ F and satisfies ${\displaystyle |xy|=|x||y|}$ and ${\displaystyle |x+y|\leq |x|+|y|}$ . Then, F is said to be Archimedean if for any non-zero x ∈ F there exists a natural number n such that

Similarly, a normed space is Archimedean if a sum of n terms, each equal to a non-zero vector x, has norm greater than one for sufficiently large n. A field with an absolute value or a normed space is either Archimedean or satisfies the stronger condition, referred to as the ultrametric triangle inequality,

respectively. A field or normed space satisfying the ultrametric triangle inequality is called non-Archimedean.
The concept of a non-Archimedean normed linear space was introduced by A. F. Monna.


£#h5#£Examples and non-examples£#/h5#£
£#h5#£Archimedean property of the real numbers£#/h5#£
The field of the rational numbers can be assigned one of a number of absolute value functions, including the trivial function ${\displaystyle |x|=1,}$ when x ≠ 0, the more usual ${\textstyle |x|={\sqrt {x^{2}}}}$ , and the p-adic absolute value functions. By Ostrowski's theorem, every non-trivial absolute value on the rational numbers is equivalent to either the usual absolute value or some p-adic absolute value. The rational field is not complete with respect to non-trivial absolute values; with respect to the trivial absolute value, the rational field is a discrete topological space, so complete. The completion with respect to the usual absolute value (from the order) is the field of real numbers. By this construction the field of real numbers is Archimedean both as an ordered field and as a normed field. On the other hand, the completions with respect to the other non-trivial absolute values give the fields of p-adic numbers, where p is a prime integer number (see below); since the p-adic absolute values satisfy the ultrametric property, then the p-adic number fields are non-Archimedean as normed fields (they cannot be made into ordered fields).

In the axiomatic theory of real numbers, the non-existence of nonzero infinitesimal real numbers is implied by the least upper bound property as follows. Denote by Z the set consisting of all positive infinitesimals. This set is bounded above by 1. Now assume for a contradiction that Z is nonempty. Then it has a least upper bound c, which is also positive, so c/2 < c < 2c. Since c is an upper bound of Z and 2c is strictly larger than c, 2c is not a positive infinitesimal. That is, there is some natural number n for which 1/n < 2c. On the other hand, c/2 is a positive infinitesimal, since by the definition of least upper bound there must be an infinitesimal x between c/2 and c, and if 1/k < c/2 ≤ x then x is not infinitesimal. But 1/(4n) < c/2, so c/2 is not infinitesimal, and this is a contradiction. This means that Z is empty after all: there are no positive, infinitesimal real numbers.

The Archimedean property of real numbers holds also in constructive analysis, even though the least upper bound property may fail in that context.


£#h5#£Non-Archimedean ordered field£#/h5#£
For an example of an ordered field that is not Archimedean, take the field of rational functions with real coefficients. (A rational function is any function that can be expressed as one polynomial divided by another polynomial; we will assume in what follows that this has been done in such a way that the leading coefficient of the denominator is positive.) To make this an ordered field, one must assign an ordering compatible with the addition and multiplication operations. Now f > g if and only if f − g > 0, so we only have to say which rational functions are considered positive. Call the function positive if the leading coefficient of the numerator is positive. (One must check that this ordering is well defined and compatible with addition and multiplication.) By this definition, the rational function 1/x is positive but less than the rational function 1. In fact, if n is any natural number, then n(1/x) = n/x is positive but still less than 1, no matter how big n is. Therefore, 1/x is an infinitesimal in this field.

This example generalizes to other coefficients. Taking rational functions with rational instead of real coefficients produces a countable non-Archimedean ordered field. Taking the coefficients to be the rational functions in a different variable, say y, produces an example with a different order type.


£#h5#£Non-Archimedean valued fields£#/h5#£
The field of the rational numbers endowed with the p-adic metric and the p-adic number fields which are the completions, do not have the Archimedean property as fields with absolute values. All Archimedean valued fields are isometrically isomorphic to a subfield of the complex numbers with a power of the usual absolute value.


£#h5#£Equivalent definitions of Archimedean ordered field£#/h5#£
Every linearly ordered field K contains (an isomorphic copy of) the rationals as an ordered subfield, namely the subfield generated by the multiplicative unit 1 of K, which in turn contains the integers as an ordered subgroup, which contains the natural numbers as an ordered monoid. The embedding of the rationals then gives a way of speaking about the rationals, integers, and natural numbers in K. The following are equivalent characterizations of Archimedean fields in terms of these substructures.

£#li#£The natural numbers are cofinal in K. That is, every element of K is less than some natural number. (This is not the case when there exist infinite elements.) Thus an Archimedean field is one whose natural numbers grow without bound.£#/li#£ £#li#£Zero is the infimum in K of the set {1/2, 1/3, 1/4, ...}. (If K contained a positive infinitesimal it would be a lower bound for the set whence zero would not be the greatest lower bound.)£#/li#£ £#li#£The set of elements of K between the positive and negative rationals is non-open. This is because the set consists of all the infinitesimals, which is just the set {0} when there are no nonzero infinitesimals, and otherwise is open, there being neither a least nor greatest nonzero infinitesimal. Observe that in both cases, the set of infinitesimals is closed. In the latter case, (i) every infinitesimal is less than every positive rational, (ii) there is neither a greatest infinitesimal nor a least positive rational, and (iii) there is nothing else in between. Consequently, any non-Archimedean ordered field is both incomplete and disconnected.£#/li#£ £#li#£For any x in K the set of integers greater than x has a least element. (If x were a negative infinite quantity every integer would be greater than it.)£#/li#£ £#li#£Every nonempty open interval of K contains a rational. (If x is a positive infinitesimal, the open interval (x, 2x) contains infinitely many infinitesimals but not a single rational.)£#/li#£ £#li#£The rationals are dense in K with respect to both sup and inf. (That is, every element of K is the sup of some set of rationals, and the inf of some other set of rationals.) Thus an Archimedean field is any dense ordered extension of the rationals, in the sense of any ordered field that densely embeds its rational elements.£#/li#£

£#h5#£See also£#/h5#£ £#ul#££#li#£0.999... – Alternative decimal expansion of 1£#/li#£ £#li#£Archimedean ordered vector space – A binary relation on a vector space£#/li#£ £#li#£Construction of the real numbers – Axiomatic definitions of the real numbers£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Boyer, C. B. and Merzbach, U. C. A History of Mathematics, 2nd ed. New York: Wiley, pp. 89 and 129, 1991.£#/li#££#li#£Itô, K. (Ed.). §155B and 155D in Encyclopedic Dictionary of Mathematics, 2nd ed., Vol. 2. Cambridge, MA: MIT Press, p. 611, 1986.£#/li#££#li#£Stolz, O. "Zur Geometrie der Alten, insbesondere über ein Axiom des Archimedes." Math. Ann. 22, 504-520, 1883.£#/li#££#li#£Stolz, O. "Über das Axiom des Archimedes." Math. Ann. 39, 107-112, 1891.£#/li#££#li#£Veronese, G. "Il continuo rettilineo e l'assioma cinque d'Archimede." Atti della Reale Accademia dei Lincei Ser. 4, No. 6, 603-624, 1890.£#/li#££#li#£ Boyer, C. B. and Merzbach, U. C. A History of Mathematics, 2nd ed. New York: Wiley, pp. 89 and 129, 1991. £#/li#££#li#£ Itô, K. (Ed.). §155B and 155D in Encyclopedic Dictionary of Mathematics, 2nd ed., Vol. 2. Cambridge, MA: MIT Press, p. 611, 1986. £#/li#££#li#£ Stolz, O. "Zur Geometrie der Alten, insbesondere über ein Axiom des Archimedes." Math. Ann. 22, 504-520, 1883. £#/li#££#li#£ Stolz, O. "Über das Axiom des Archimedes." Math. Ann. 39, 107-112, 1891. £#/li#££#li#£ Veronese, G. "Il continuo rettilineo e l'assioma cinque d'Archimede." Atti della Reale Accademia dei Lincei Ser. 4, No. 6, 603-624, 1890. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Geometry > Line Geometry > Incidence £#/li#££#li#£ Number Theory > Arithmetic > Fractions £#/li#££#li#£ Calculus and Analysis > Inequalities £#/li#££#/ul#£




£#h3#£Archimedes' Lemma£#/h3#£

Hilbert's axioms are a set of 20 assumptions proposed by David Hilbert in 1899 in his book Grundlagen der Geometrie (tr. The Foundations of Geometry) as the foundation for a modern treatment of Euclidean geometry. Other well-known modern axiomatizations of Euclidean geometry are those of Alfred Tarski and of George Birkhoff.


£#h5#£The axioms£#/h5#£
Hilbert's axiom system is constructed with six primitive notions: three primitive terms:

£#ul#££#li#£point;£#/li#£ £#li#£line;£#/li#£ £#li#£plane;£#/li#££#/ul#£
and three primitive relations:

£#ul#££#li#£Betweenness, a ternary relation linking points;£#/li#£ £#li#£Lies on (Containment), three binary relations, one linking points and straight lines, one linking points and planes, and one linking straight lines and planes;£#/li#£ £#li#£Congruence, two binary relations, one linking line segments and one linking angles, each denoted by an infix ≅.£#/li#££#/ul#£
Line segments, angles, and triangles may each be defined in terms of points and straight lines, using the relations of betweenness and containment. All points, straight lines, and planes in the following axioms are distinct unless otherwise stated.


£#h5#£I. Incidence£#/h5#£
£#li#£For every two points A and B there exists a line a that contains them both. We write AB = a or BA = a. Instead of "contains", we may also employ other forms of expression; for example, we may say "A lies upon a", "A is a point of a", "a goes through A and through B", "a joins A to B", etc. If A lies upon a and at the same time upon another line b, we make use also of the expression: "The lines a and b have the point A in common", etc.£#/li#£ £#li#£For every two points there exists no more than one line that contains them both; consequently, if AB = a and AC = a, where B ≠ C, then also BC = a.£#/li#£ £#li#£There exist at least two points on a line. There exist at least three points that do not lie on the same line.£#/li#£ £#li#£For every three points A, B, C not situated on the same line there exists a plane α that contains all of them. For every plane there exists a point which lies on it. We write ABC = α. We employ also the expressions: "A, B, C lie in α"; "A, B, C are points of α", etc.£#/li#£ £#li#£For every three points A, B, C which do not lie in the same line, there exists no more than one plane that contains them all.£#/li#£ £#li#£If two points A, B of a line a lie in a plane α, then every point of a lies in α. In this case we say: "The line a lies in the plane α", etc.£#/li#£ £#li#£If two planes α, β have a point A in common, then they have at least a second point B in common.£#/li#£ £#li#£There exist at least four points not lying in a plane.£#/li#£

£#h5#£II. Order£#/h5#£
£#li#£If a point B lies between points A and C, B is also between C and A, and there exists a line containing the distinct points A, B, C.£#/li#£ £#li#£If A and C are two points, then there exists at least one point B on the line AC such that C lies between A and B.£#/li#£ £#li#£Of any three points situated on a line, there is no more than one which lies between the other two.£#/li#£ £#li#£Pasch's Axiom: Let A, B, C be three points not lying in the same line and let a be a line lying in the plane ABC and not passing through any of the points A, B, C. Then, if the line a passes through a point of the segment AB, it will also pass through either a point of the segment BC or a point of the segment AC.£#/li#£

£#h5#£III. Congruence£#/h5#£
£#li#£If A, B are two points on a line a, and if A′ is a point upon the same or another line a′, then, upon a given side of A′ on the straight line a′, we can always find a point B′ so that the segment AB is congruent to the segment A′B′. We indicate this relation by writing AB ≅ A′B′. Every segment is congruent to itself; that is, we always have AB ≅ AB.
We can state the above axiom briefly by saying that every segment can be laid off upon a given side of a given point of a given straight line in at least one way.£#/li#£ £#li#£If a segment AB is congruent to the segment A′B′ and also to the segment A″B″, then the segment A′B′ is congruent to the segment A″B″; that is, if AB ≅ A′B′ and AB ≅ A″B″, then A′B′ ≅ A″B″.£#/li#£ £#li#£Let AB and BC be two segments of a line a which have no points in common aside from the point B, and, furthermore, let A′B′ and B′C′ be two segments of the same or of another line a′ having, likewise, no point other than B′ in common. Then, if AB ≅ A′B′ and BC ≅ B′C′, we have AC ≅ A′C′.£#/li#£ £#li#£Let an angle ∠ (h,k) be given in the plane α and let a line a′ be given in a plane α′. Suppose also that, in the plane α′, a definite side of the straight line a′ be assigned. Denote by h′ a ray of the straight line a′ emanating from a point O′ of this line. Then in the plane α′ there is one and only one ray k′ such that the angle ∠ (h, k), or ∠ (k, h), is congruent to the angle ∠ (h′, k′) and at the same time all interior points of the angle ∠ (h′, k′) lie upon the given side of a′. We express this relation by means of the notation ∠ (h, k) ≅ ∠ (h′, k′).£#/li#£ £#li#£If the angle ∠ (h, k) is congruent to the angle ∠ (h′, k′) and to the angle ∠ (h″, k″), then the angle ∠ (h′, k′) is congruent to the angle ∠ (h″, k″); that is to say, if ∠ (h, k) ≅ ∠ (h′, k′) and ∠ (h, k) ≅ ∠ (h″, k″), then ∠ (h′, k′) ≅ ∠ (h″, k″).£#/li#£ £#li#£If, in the two triangles ABC and A′B′C′ the congruences AB ≅ A′B′, AC ≅ A′C′, ∠BAC ≅ ∠B′A′C′ hold, then the congruence ∠ABC ≅ ∠A′B′C′ holds (and, by a change of notation, it follows that ∠ACB ≅ ∠A′C′B′ also holds).£#/li#£

£#h5#£IV. Parallels£#/h5#£
£#li#£Euclid's Axiom: Let a be any line and A a point not on it. Then there is at most one line in the plane, determined by a and A, that passes through A and does not intersect a.£#/li#£

£#h5#£V. Continuity£#/h5#£
£#li#£Axiom of Archimedes: If AB and CD are any segments then there exists a number n such that n segments CD constructed contiguously from A, along the ray from A through B, will pass beyond the point B.£#/li#£ £#li#£Axiom of line completeness: An extension (An extended line from a line that already exists, usually used in geometry) of a set of points on a line with its order and congruence relations that would preserve the relations existing among the original elements as well as the fundamental properties of line order and congruence that follows from Axioms I-III and from V-1 is impossible.£#/li#£

£#h5#£Hilbert's discarded axiom£#/h5#£
Hilbert (1899) included a 21st axiom that read as follows:

II.4. Any four points A, B, C, D of a line can always be labeled so that B shall lie between A and C and also between A and D, and, furthermore, that C shall lie between A and D and also between B and D.
E.H. Moore and R.L. Moore independently proved that this axiom is redundant, and the former published this result in an article appearing in the Transactions of the American Mathematical Society in 1902.

Before this, the axiom now listed as II.4. was numbered II.5.


£#h5#£Editions and translations of Grundlagen der Geometrie£#/h5#£
The original monograph, based on his own lectures, was organized and written by Hilbert for a memorial address given in 1899. This was quickly followed by a French translation, in which Hilbert added V.2, the Completeness Axiom. An English translation, authorized by Hilbert, was made by E.J. Townsend and copyrighted in 1902. This translation incorporated the changes made in the French translation and so is considered to be a translation of the 2nd edition. Hilbert continued to make changes in the text and several editions appeared in German. The 7th edition was the last to appear in Hilbert's lifetime. In the Preface of this edition Hilbert wrote:

"The present Seventh Edition of my book Foundations of Geometry brings considerable improvements and additions to the previous edition, partly from my subsequent lectures on this subject and partly from improvements made in the meantime by other writers. The main text of the book has been revised accordingly."
New editions followed the 7th, but the main text was essentially not revised. The modifications in these editions occur in the appendices and in supplements. The changes in the text were large when compared to the original and a new English translation was commissioned by Open Court Publishers, who had published the Townsend translation. So, the 2nd English Edition was translated by Leo Unger from the 10th German edition in 1971. This translation incorporates several revisions and enlargements of the later German editions by Paul Bernays.

The Unger translation differs from the Townsend translation with respect to the axioms in the following ways:

£#ul#££#li#£Old axiom II.4 is renamed as Theorem 5 and moved.£#/li#£ £#li#£Old axiom II.5 (Pasch's Axiom) is renumbered as II.4.£#/li#£ £#li#£V.2, the Axiom of Line Completeness, replaced:£#/li#££#/ul#£
Axiom of completeness. To a system of points, straight lines, and planes, it is impossible to add other elements in such a manner that the system thus generalized shall form a new geometry obeying all of the five groups of axioms. In other words, the elements of geometry form a system which is not susceptible of extension, if we regard the five groups of axioms as valid.
£#ul#££#li#£The old axiom V.2 is now Theorem 32.£#/li#££#/ul#£
The last two modifications are due to P. Bernays.

Other changes of note are:

£#ul#££#li#£The term straight line used by Townsend has been replaced by line throughout.£#/li#£ £#li#£The Axioms of Incidence were called Axioms of Connection by Townsend.£#/li#££#/ul#£
£#h5#£Application£#/h5#£
These axioms axiomatize Euclidean solid geometry. Removing five axioms mentioning "plane" in an essential way, namely I.4–8, and modifying III.4 and IV.1 to omit mention of planes, yields an axiomatization of Euclidean plane geometry.

Hilbert's axioms, unlike Tarski's axioms, do not constitute a first-order theory because the axioms V.1–2 cannot be expressed in first-order logic.

The value of Hilbert's Grundlagen was more methodological than substantive or pedagogical. Other major contributions to the axiomatics of geometry were those of Moritz Pasch, Mario Pieri, Oswald Veblen, Edward Vermilye Huntington, Gilbert Robinson, and Henry George Forder. The value of the Grundlagen is its pioneering approach to metamathematical questions, including the use of models to prove axioms independent; and the need to prove the consistency and completeness of an axiom system.

Mathematics in the twentieth century evolved into a network of axiomatic formal systems. This was, in considerable part, influenced by the example Hilbert set in the Grundlagen. A 2003 effort (Meikle and Fleuriot) to formalize the Grundlagen with a computer, though, found that some of Hilbert's proofs appear to rely on diagrams and geometric intuition, and as such revealed some potential ambiguities and omissions in his definitions.


£#h5#£See also£#/h5#£ £#ul#££#li#£Euclidean space£#/li#£ £#li#£Foundations of geometry£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Howard Eves, 1997 (1958). Foundations and Fundamental Concepts of Mathematics. Dover. Chpt. 4.2 covers the Hilbert axioms for plane geometry.£#/li#£ £#li#£Ivor Grattan-Guinness, 2000. In Search of Mathematical Roots. Princeton University Press.£#/li#£ £#li#£David Hilbert, 1980 (1899). The Foundations of Geometry, 2nd ed. Chicago: Open Court.£#/li#£ £#li#£Laura I. Meikle and Jacques D. Fleuriot (2003), Formalizing Hilbert's Grundlagen in Isabelle/Isar, Theorem Proving in Higher Order Logics, Lecture Notes in Computer Science, Volume 2758/2003, 319-334, doi:10.1007/10930755_21£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Hilbert system of axioms", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#£ £#li#£"Hilbert's Axioms" at the UMBC Math Department£#/li#£ £#li#£"Hilbert's Axioms" at Mathworld£#/li#£ £#li#£ Foundations of Geometry public domain audiobook at LibriVox£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Geometry > Line Geometry > Incidence £#/li#££#li#£ Number Theory > Arithmetic > Fractions £#/li#££#li#£ Calculus and Analysis > Inequalities £#/li#££#/ul#£




£#h3#£Archimedes' Postulate£#/h3#£

Hilbert's axioms are a set of 20 assumptions proposed by David Hilbert in 1899 in his book Grundlagen der Geometrie (tr. The Foundations of Geometry) as the foundation for a modern treatment of Euclidean geometry. Other well-known modern axiomatizations of Euclidean geometry are those of Alfred Tarski and of George Birkhoff.


£#h5#£The axioms£#/h5#£
Hilbert's axiom system is constructed with six primitive notions: three primitive terms:

£#ul#££#li#£point;£#/li#£ £#li#£line;£#/li#£ £#li#£plane;£#/li#££#/ul#£
and three primitive relations:

£#ul#££#li#£Betweenness, a ternary relation linking points;£#/li#£ £#li#£Lies on (Containment), three binary relations, one linking points and straight lines, one linking points and planes, and one linking straight lines and planes;£#/li#£ £#li#£Congruence, two binary relations, one linking line segments and one linking angles, each denoted by an infix ≅.£#/li#££#/ul#£
Line segments, angles, and triangles may each be defined in terms of points and straight lines, using the relations of betweenness and containment. All points, straight lines, and planes in the following axioms are distinct unless otherwise stated.


£#h5#£I. Incidence£#/h5#£
£#li#£For every two points A and B there exists a line a that contains them both. We write AB = a or BA = a. Instead of "contains", we may also employ other forms of expression; for example, we may say "A lies upon a", "A is a point of a", "a goes through A and through B", "a joins A to B", etc. If A lies upon a and at the same time upon another line b, we make use also of the expression: "The lines a and b have the point A in common", etc.£#/li#£ £#li#£For every two points there exists no more than one line that contains them both; consequently, if AB = a and AC = a, where B ≠ C, then also BC = a.£#/li#£ £#li#£There exist at least two points on a line. There exist at least three points that do not lie on the same line.£#/li#£ £#li#£For every three points A, B, C not situated on the same line there exists a plane α that contains all of them. For every plane there exists a point which lies on it. We write ABC = α. We employ also the expressions: "A, B, C lie in α"; "A, B, C are points of α", etc.£#/li#£ £#li#£For every three points A, B, C which do not lie in the same line, there exists no more than one plane that contains them all.£#/li#£ £#li#£If two points A, B of a line a lie in a plane α, then every point of a lies in α. In this case we say: "The line a lies in the plane α", etc.£#/li#£ £#li#£If two planes α, β have a point A in common, then they have at least a second point B in common.£#/li#£ £#li#£There exist at least four points not lying in a plane.£#/li#£

£#h5#£II. Order£#/h5#£
£#li#£If a point B lies between points A and C, B is also between C and A, and there exists a line containing the distinct points A, B, C.£#/li#£ £#li#£If A and C are two points, then there exists at least one point B on the line AC such that C lies between A and B.£#/li#£ £#li#£Of any three points situated on a line, there is no more than one which lies between the other two.£#/li#£ £#li#£Pasch's Axiom: Let A, B, C be three points not lying in the same line and let a be a line lying in the plane ABC and not passing through any of the points A, B, C. Then, if the line a passes through a point of the segment AB, it will also pass through either a point of the segment BC or a point of the segment AC.£#/li#£

£#h5#£III. Congruence£#/h5#£
£#li#£If A, B are two points on a line a, and if A′ is a point upon the same or another line a′, then, upon a given side of A′ on the straight line a′, we can always find a point B′ so that the segment AB is congruent to the segment A′B′. We indicate this relation by writing AB ≅ A′B′. Every segment is congruent to itself; that is, we always have AB ≅ AB.
We can state the above axiom briefly by saying that every segment can be laid off upon a given side of a given point of a given straight line in at least one way.£#/li#£ £#li#£If a segment AB is congruent to the segment A′B′ and also to the segment A″B″, then the segment A′B′ is congruent to the segment A″B″; that is, if AB ≅ A′B′ and AB ≅ A″B″, then A′B′ ≅ A″B″.£#/li#£ £#li#£Let AB and BC be two segments of a line a which have no points in common aside from the point B, and, furthermore, let A′B′ and B′C′ be two segments of the same or of another line a′ having, likewise, no point other than B′ in common. Then, if AB ≅ A′B′ and BC ≅ B′C′, we have AC ≅ A′C′.£#/li#£ £#li#£Let an angle ∠ (h,k) be given in the plane α and let a line a′ be given in a plane α′. Suppose also that, in the plane α′, a definite side of the straight line a′ be assigned. Denote by h′ a ray of the straight line a′ emanating from a point O′ of this line. Then in the plane α′ there is one and only one ray k′ such that the angle ∠ (h, k), or ∠ (k, h), is congruent to the angle ∠ (h′, k′) and at the same time all interior points of the angle ∠ (h′, k′) lie upon the given side of a′. We express this relation by means of the notation ∠ (h, k) ≅ ∠ (h′, k′).£#/li#£ £#li#£If the angle ∠ (h, k) is congruent to the angle ∠ (h′, k′) and to the angle ∠ (h″, k″), then the angle ∠ (h′, k′) is congruent to the angle ∠ (h″, k″); that is to say, if ∠ (h, k) ≅ ∠ (h′, k′) and ∠ (h, k) ≅ ∠ (h″, k″), then ∠ (h′, k′) ≅ ∠ (h″, k″).£#/li#£ £#li#£If, in the two triangles ABC and A′B′C′ the congruences AB ≅ A′B′, AC ≅ A′C′, ∠BAC ≅ ∠B′A′C′ hold, then the congruence ∠ABC ≅ ∠A′B′C′ holds (and, by a change of notation, it follows that ∠ACB ≅ ∠A′C′B′ also holds).£#/li#£

£#h5#£IV. Parallels£#/h5#£
£#li#£Euclid's Axiom: Let a be any line and A a point not on it. Then there is at most one line in the plane, determined by a and A, that passes through A and does not intersect a.£#/li#£

£#h5#£V. Continuity£#/h5#£
£#li#£Axiom of Archimedes: If AB and CD are any segments then there exists a number n such that n segments CD constructed contiguously from A, along the ray from A through B, will pass beyond the point B.£#/li#£ £#li#£Axiom of line completeness: An extension (An extended line from a line that already exists, usually used in geometry) of a set of points on a line with its order and congruence relations that would preserve the relations existing among the original elements as well as the fundamental properties of line order and congruence that follows from Axioms I-III and from V-1 is impossible.£#/li#£

£#h5#£Hilbert's discarded axiom£#/h5#£
Hilbert (1899) included a 21st axiom that read as follows:

II.4. Any four points A, B, C, D of a line can always be labeled so that B shall lie between A and C and also between A and D, and, furthermore, that C shall lie between A and D and also between B and D.
E.H. Moore and R.L. Moore independently proved that this axiom is redundant, and the former published this result in an article appearing in the Transactions of the American Mathematical Society in 1902.

Before this, the axiom now listed as II.4. was numbered II.5.


£#h5#£Editions and translations of Grundlagen der Geometrie£#/h5#£
The original monograph, based on his own lectures, was organized and written by Hilbert for a memorial address given in 1899. This was quickly followed by a French translation, in which Hilbert added V.2, the Completeness Axiom. An English translation, authorized by Hilbert, was made by E.J. Townsend and copyrighted in 1902. This translation incorporated the changes made in the French translation and so is considered to be a translation of the 2nd edition. Hilbert continued to make changes in the text and several editions appeared in German. The 7th edition was the last to appear in Hilbert's lifetime. In the Preface of this edition Hilbert wrote:

"The present Seventh Edition of my book Foundations of Geometry brings considerable improvements and additions to the previous edition, partly from my subsequent lectures on this subject and partly from improvements made in the meantime by other writers. The main text of the book has been revised accordingly."
New editions followed the 7th, but the main text was essentially not revised. The modifications in these editions occur in the appendices and in supplements. The changes in the text were large when compared to the original and a new English translation was commissioned by Open Court Publishers, who had published the Townsend translation. So, the 2nd English Edition was translated by Leo Unger from the 10th German edition in 1971. This translation incorporates several revisions and enlargements of the later German editions by Paul Bernays.

The Unger translation differs from the Townsend translation with respect to the axioms in the following ways:

£#ul#££#li#£Old axiom II.4 is renamed as Theorem 5 and moved.£#/li#£ £#li#£Old axiom II.5 (Pasch's Axiom) is renumbered as II.4.£#/li#£ £#li#£V.2, the Axiom of Line Completeness, replaced:£#/li#££#/ul#£
Axiom of completeness. To a system of points, straight lines, and planes, it is impossible to add other elements in such a manner that the system thus generalized shall form a new geometry obeying all of the five groups of axioms. In other words, the elements of geometry form a system which is not susceptible of extension, if we regard the five groups of axioms as valid.
£#ul#££#li#£The old axiom V.2 is now Theorem 32.£#/li#££#/ul#£
The last two modifications are due to P. Bernays.

Other changes of note are:

£#ul#££#li#£The term straight line used by Townsend has been replaced by line throughout.£#/li#£ £#li#£The Axioms of Incidence were called Axioms of Connection by Townsend.£#/li#££#/ul#£
£#h5#£Application£#/h5#£
These axioms axiomatize Euclidean solid geometry. Removing five axioms mentioning "plane" in an essential way, namely I.4–8, and modifying III.4 and IV.1 to omit mention of planes, yields an axiomatization of Euclidean plane geometry.

Hilbert's axioms, unlike Tarski's axioms, do not constitute a first-order theory because the axioms V.1–2 cannot be expressed in first-order logic.

The value of Hilbert's Grundlagen was more methodological than substantive or pedagogical. Other major contributions to the axiomatics of geometry were those of Moritz Pasch, Mario Pieri, Oswald Veblen, Edward Vermilye Huntington, Gilbert Robinson, and Henry George Forder. The value of the Grundlagen is its pioneering approach to metamathematical questions, including the use of models to prove axioms independent; and the need to prove the consistency and completeness of an axiom system.

Mathematics in the twentieth century evolved into a network of axiomatic formal systems. This was, in considerable part, influenced by the example Hilbert set in the Grundlagen. A 2003 effort (Meikle and Fleuriot) to formalize the Grundlagen with a computer, though, found that some of Hilbert's proofs appear to rely on diagrams and geometric intuition, and as such revealed some potential ambiguities and omissions in his definitions.


£#h5#£See also£#/h5#£ £#ul#££#li#£Euclidean space£#/li#£ £#li#£Foundations of geometry£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Howard Eves, 1997 (1958). Foundations and Fundamental Concepts of Mathematics. Dover. Chpt. 4.2 covers the Hilbert axioms for plane geometry.£#/li#£ £#li#£Ivor Grattan-Guinness, 2000. In Search of Mathematical Roots. Princeton University Press.£#/li#£ £#li#£David Hilbert, 1980 (1899). The Foundations of Geometry, 2nd ed. Chicago: Open Court.£#/li#£ £#li#£Laura I. Meikle and Jacques D. Fleuriot (2003), Formalizing Hilbert's Grundlagen in Isabelle/Isar, Theorem Proving in Higher Order Logics, Lecture Notes in Computer Science, Volume 2758/2003, 319-334, doi:10.1007/10930755_21£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Hilbert system of axioms", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#£ £#li#£"Hilbert's Axioms" at the UMBC Math Department£#/li#£ £#li#£"Hilbert's Axioms" at Mathworld£#/li#£ £#li#£ Foundations of Geometry public domain audiobook at LibriVox£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Geometry > Line Geometry > Incidence £#/li#££#li#£ Number Theory > Arithmetic > Fractions £#/li#££#li#£ Calculus and Analysis > Inequalities £#/li#££#/ul#£




£#h3#£Arc Length£#/h3#£

Arc length is the distance between two points along a section of a curve.

Determining the length of an irregular arc segment by approximating the arc segment as connected (straight) line segments is also called rectification of a curve. If the rectification of a curve results in a finite number (so the curve has a finite length), then the curve is said to be rectifiable.

If a curve can be parameterized as an injective and continuously differentiable function (i.e., the derivative is a continuous function) ${\displaystyle f\colon [a,b]\to \mathbb {R} ^{n}}$ , then the curve is rectifiable (i.e., it has a finite length).

The advent of infinitesimal calculus led to a general formula that provides closed-form solutions in some cases.


£#h5#£General approach£#/h5#£
A curve in the plane can be approximated by connecting a finite number of points on the curve using (straight) line segments to create a polygonal path. Since it is straightforward to calculate the length of each linear segment (using the Pythagorean theorem in Euclidean space, for example), the total length of the approximation can be found by summation of the lengths of each linear segment; that approximation is known as the (cumulative) chordal distance.

If the curve is not already a polygonal path, then using a progressively larger number of line segments of smaller lengths will result in better curve length approximations. Such a curve length determination by approximating the curve as connected (straight) line segments is called rectification of a curve. The lengths of the successive approximations will not decrease and may keep increasing indefinitely, but for smooth curves they will tend to a finite limit as the lengths of the segments get arbitrarily small.

For some curves, there is a smallest number ${\displaystyle L}$ that is an upper bound on the length of all polygonal approximations (rectification). These curves are called rectifiable and the arc length is defined as the number ${\displaystyle L}$ .


£#h5#£Formula for a smooth curve£#/h5#£
Let ${\displaystyle f\colon [a,b]\to \mathbb {R} ^{n}}$ be an injective and continuously differentiable (i.e., the derivative is a continuous function) function. The length of the curve defined by ${\displaystyle f}$ can be defined as the limit of the sum of linear segment lengths for a regular partition of ${\displaystyle [a,b]}$ as the number of segments approaches infinity. This means

where ${\displaystyle t_{i}=a+i(b-a)/N=a+i\Delta t}$ with ${\displaystyle \Delta t={\frac {b-a}{N}}=t_{i}-t_{i-1}}$ for ${\displaystyle i=0,1,\dotsc ,N.}$ This definition is equivalent to the standard definition of arc length as an integral:
The last equality is proved by the following steps:

£#li#£The second fundamental theorem of calculus shows ${\displaystyle f(t_{i})-f(t_{i-1})=\int _{t_{i-1}}^{t_{i}}f'(t)\ dt=\Delta t\int _{0}^{1}f'(t_{i-1}+\theta (t_{i}-t_{i-1}))\ d\theta }$ where ${\displaystyle t=t_{i-1}+\theta (t_{i}-t_{i-1})}$ over ${\displaystyle \theta \in [0,1]}$ maps to ${\displaystyle [t_{i-1},t_{i}]}$ and ${\displaystyle dt=(t_{i}-t_{i-1})d\theta =\Delta td\theta }$ . In the below step, the following equivalent expression is used.£#/li#£ £#li#£The function ${\displaystyle \left|f'\right|}$ is a continuous function from a closed interval ${\displaystyle [a,b]}$ to the set of real numbers, thus it is uniformly continuous according to the Heine–Cantor theorem, so there is a positive real and monotonically non-decreasing function ${\displaystyle \delta (\varepsilon )}$ of positive real numbers ${\displaystyle \varepsilon }$ such that ${\displaystyle \Delta t<\delta (\varepsilon )}$ implies ${\displaystyle \left|\left|f'(t_{i-1}+\theta (t_{i}-t_{i-1}))\right|-\left|f'(t_{i})\right|\right|<\varepsilon }$ where ${\displaystyle \Delta t=t_{i}-t_{i-1}}$ and ${\displaystyle \theta \in [0,1]}$ . Let's consider the limit ${\displaystyle {\ce {N\to \infty }}}$ of the following formula,With the above step result, it becomesTerms are rearranged so that it becomeswhere in the leftmost side ${\displaystyle \left|f'(t_{i})\right|=\int _{0}^{1}\left|f'(t_{i})\right|d\theta }$ is used. By ${\displaystyle \left|\left|f'(t_{i-1}+\theta (t_{i}-t_{i-1}))\right|-\left|f'(t_{i})\right|\right|<\varepsilon }$ for ${\displaystyle N>(b-a)/\delta (\varepsilon )}$ so that ${\displaystyle \Delta t<\delta (\varepsilon )}$ , it becomeswith ${\displaystyle \left|f'(t_{i})\right|=\int _{0}^{1}\left|f'(t_{i})\right|d\theta }$ , ${\displaystyle \varepsilon N\Delta t=\varepsilon (b-a)}$ , and ${\displaystyle N>(b-a)/\delta (\varepsilon )}$ . In the limit ${\displaystyle N\to \infty ,}$ ${\displaystyle \delta (\varepsilon )\to 0}$ so ${\displaystyle \varepsilon \to 0}$ thus the left side of ${\displaystyle <}$ approaches to ${\displaystyle 0}$ . In other words, ${\displaystyle \sum _{i=1}^{N}\left|{\frac {f(t_{i})-f(t_{i-1})}{\Delta t}}\right|\Delta t=\sum _{i=1}^{N}\left|f'(t_{i})\right|\Delta t}$ in this limit, and the right side of this equality is just the Riemann integral of ${\displaystyle \left|f'(t)\right|}$ on ${\displaystyle [a,b].}$ This definition of arc length shows that the length of a curve represented by a continuously differentiable function ${\displaystyle f:[a,b]\to \mathbb {R} ^{n}}$ on ${\displaystyle [a,b]}$ is always finite, i.e., rectifiable.£#/li#£
The definition of arc length of a smooth curve as the integral of the norm of the derivative is equivalent to the definition

where the supremum is taken over all possible partitions ${\displaystyle a=t_{0}<t_{1}<\dots <t_{N-1}<t_{N}=b}$ of ${\displaystyle [a,b].}$ This definition as the supremum of the all possible partition sums is also valid if ${\displaystyle f}$ is merely continuous, not differentiable.
A curve can be parameterized in infinitely many ways. Let ${\displaystyle \varphi :[a,b]\to [c,d]}$ be any continuously differentiable bijection. Then ${\displaystyle g=f\circ \varphi ^{-1}:[c,d]\to \mathbb {R} ^{n}}$ is another continuously differentiable parameterization of the curve originally defined by ${\displaystyle f.}$ The arc length of the curve is the same regardless of the parameterization used to define the curve:


£#h5#£Finding arc lengths by integration£#/h5#£
If a planar curve in ${\displaystyle \mathbb {R} ^{2}}$ is defined by the equation ${\displaystyle y=f(x),}$ where ${\displaystyle f}$ is continuously differentiable, then it is simply a special case of a parametric equation where ${\displaystyle x=t}$ and ${\displaystyle y=f(t).}$ The Euclidean distance of each infinitesimal segment of the arc can be given by:

The arc length is then given by:

Curves with closed-form solutions for arc length include the catenary, circle, cycloid, logarithmic spiral, parabola, semicubical parabola and straight line. The lack of a closed form solution for the arc length of an elliptic and hyperbolic arc led to the development of the elliptic integrals.


£#h5#£Numerical integration£#/h5#£
In most cases, including even simple curves, there are no closed-form solutions for arc length and numerical integration is necessary. Numerical integration of the arc length integral is usually very efficient. For example, consider the problem of finding the length of a quarter of the unit circle by numerically integrating the arc length integral. The upper half of the unit circle can be parameterized as ${\displaystyle y={\sqrt {1-x^{2}}}.}$ The interval ${\displaystyle x\in \left[-{\sqrt {2}}/2,{\sqrt {2}}/2\right]}$ corresponds to a quarter of the circle. Since ${\textstyle dy/dx=-x/{\sqrt {1-x^{2}}}}$ and ${\displaystyle 1+(dy/dx)^{2}=1/\left(1-x^{2}\right),}$ the length of a quarter of the unit circle is

The 15-point Gauss–Kronrod rule estimate for this integral of 1.570796326808177 differs from the true length of

by 1.3×10−11 and the 16-point Gaussian quadrature rule estimate of 1.570796326794727 differs from the true length by only 1.7×10−13. This means it is possible to evaluate this integral to almost machine precision with only 16 integrand evaluations.
£#h5#£Curve on a surface£#/h5#£
Let ${\displaystyle \mathbf {x} (u,v)}$ be a surface mapping and let ${\displaystyle \mathbf {C} (t)=(u(t),v(t))}$ be a curve on this surface. The integrand of the arc length integral is ${\displaystyle \left|\left(\mathbf {x} \circ \mathbf {C} \right)'(t)\right|.}$ Evaluating the derivative requires the chain rule for vector fields:

The squared norm of this vector is

(where ${\displaystyle g_{ij}}$ is the first fundamental form coefficient), so the integrand of the arc length integral can be written as ${\displaystyle {\sqrt {g_{ab}\left(u^{a}\right)'\left(u^{b}\right)'\,}}}$ (where ${\displaystyle u^{1}=u}$ and ${\displaystyle u^{2}=v}$ ).
£#h5#£Other coordinate systems£#/h5#£
Let ${\displaystyle \mathbf {C} (t)=(r(t),\theta (t))}$ be a curve expressed in polar coordinates. The mapping that transforms from polar coordinates to rectangular coordinates is

The integrand of the arc length integral is ${\displaystyle \left|\left(\mathbf {x} \circ \mathbf {C} \right)'(t)\right|.}$ The chain rule for vector fields shows that ${\displaystyle D(\mathbf {x} \circ \mathbf {C} )=\mathbf {x} _{r}r'+\mathbf {x} _{\theta }\theta '.}$ So the squared integrand of the arc length integral is

So for a curve expressed in polar coordinates, the arc length is

Now let ${\displaystyle \mathbf {C} (t)=(r(t),\theta (t),\phi (t))}$ be a curve expressed in spherical coordinates where ${\displaystyle \theta }$ is the polar angle measured from the positive ${\displaystyle z}$ -axis and ${\displaystyle \phi }$ is the azimuthal angle. The mapping that transforms from spherical coordinates to rectangular coordinates is

Using the chain rule again shows that ${\displaystyle D(\mathbf {x} \circ \mathbf {C} )=\mathbf {x} _{r}r'+\mathbf {x} _{\theta }\theta '+\mathbf {x} _{\phi }\phi '.}$ All dot products ${\displaystyle \mathbf {x} _{i}\cdot \mathbf {x} _{j}}$ where ${\displaystyle i}$ and ${\displaystyle j}$ differ are zero, so the squared norm of this vector is

So for a curve expressed in spherical coordinates, the arc length is

A very similar calculation shows that the arc length of a curve expressed in cylindrical coordinates is


£#h5#£Simple cases£#/h5#£
£#h5#£Arcs of circles £#/h5#£
Arc lengths are denoted by s, since the Latin word for length (or size) is spatium.

In the following lines, ${\displaystyle r}$ represents the radius of a circle, ${\displaystyle d}$ is its diameter, ${\displaystyle C}$ is its circumference, ${\displaystyle s}$ is the length of an arc of the circle, and ${\displaystyle \theta }$ is the angle which the arc subtends at the centre of the circle. The distances ${\displaystyle r,d,C,}$ and ${\displaystyle s}$ are expressed in the same units.

£#ul#££#li#£ ${\displaystyle C=2\pi r,}$ which is the same as ${\displaystyle C=\pi d.}$ This equation is a definition of ${\displaystyle \pi .}$ £#/li#£ £#li#£If the arc is a semicircle, then ${\displaystyle s=\pi r.}$ £#/li#£ £#li#£For an arbitrary circular arc: £#ul#££#li#£If ${\displaystyle \theta }$ is in radians then ${\displaystyle s=r\theta .}$ This is a definition of the radian.£#/li#£ £#li#£If ${\displaystyle \theta }$ is in degrees, then ${\displaystyle s={\frac {\pi r\theta }{180^{\circ }}},}$ which is the same as ${\displaystyle s={\frac {C\theta }{360^{\circ }}}.}$ £#/li#£ £#li#£If ${\displaystyle \theta }$ is in grads (100 grads, or grades, or gradians are one right-angle), then ${\displaystyle s={\frac {\pi r\theta }{200{\text{ grad}}}},}$ which is the same as ${\displaystyle s={\frac {C\theta }{400{\text{ grad}}}}.}$ £#/li#£ £#li#£If ${\displaystyle \theta }$ is in turns (one turn is a complete rotation, or 360°, or 400 grads, or ${\displaystyle 2\pi }$ radians), then ${\displaystyle s=C\theta /1{\text{ turn}}}$ .£#/li#££#/ul#££#/li#££#/ul#£
£#h5#£Great circles on Earth£#/h5#£
Two units of length, the nautical mile and the metre (or kilometre), were originally defined so the lengths of arcs of great circles on the Earth's surface would be simply numerically related to the angles they subtend at its centre. The simple equation ${\displaystyle s=\theta }$ applies in the following circumstances:

£#ul#££#li#£if ${\displaystyle s}$ is in nautical miles, and ${\displaystyle \theta }$ is in arcminutes (1⁄60 degree), or£#/li#£ £#li#£if ${\displaystyle s}$ is in kilometres, and ${\displaystyle \theta }$ is in centigrades (1⁄100 grad).£#/li#££#/ul#£
The lengths of the distance units were chosen to make the circumference of the Earth equal 40000 kilometres, or 21600 nautical miles. Those are the numbers of the corresponding angle units in one complete turn.

Those definitions of the metre and the nautical mile have been superseded by more precise ones, but the original definitions are still accurate enough for conceptual purposes and some calculations. For example, they imply that one kilometre is exactly 0.54 nautical miles. Using official modern definitions, one nautical mile is exactly 1.852 kilometres, which implies that 1 kilometre is about 0.53995680 nautical miles. This modern ratio differs from the one calculated from the original definitions by less than one part in 10,000.


£#h5#£Other simple cases£#/h5#£ £#ul#££#li#£Archimedean spiral § Arc length£#/li#£ £#li#£Cycloid § Arc length£#/li#£ £#li#£Ellipse § Arc length£#/li#£ £#li#£Helix § Arc length£#/li#£ £#li#£Parabola § Arc length£#/li#£ £#li#£Sine and cosine § Arc length£#/li#£ £#li#£Triangle wave § Arc length£#/li#££#/ul#£
£#h5#£Historical methods£#/h5#£
£#h5#£Antiquity£#/h5#£
For much of the history of mathematics, even the greatest thinkers considered it impossible to compute the length of an irregular arc. Although Archimedes had pioneered a way of finding the area beneath a curve with his "method of exhaustion", few believed it was even possible for curves to have definite lengths, as do straight lines. The first ground was broken in this field, as it often has been in calculus, by approximation. People began to inscribe polygons within the curves and compute the length of the sides for a somewhat accurate measurement of the length. By using more segments, and by decreasing the length of each segment, they were able to obtain a more and more accurate approximation. In particular, by inscribing a polygon of many sides in a circle, they were able to find approximate values of π.


£#h5#£17th century£#/h5#£
In the 17th century, the method of exhaustion led to the rectification by geometrical methods of several transcendental curves: the logarithmic spiral by Evangelista Torricelli in 1645 (some sources say John Wallis in the 1650s), the cycloid by Christopher Wren in 1658, and the catenary by Gottfried Leibniz in 1691.

In 1659, Wallis credited William Neile's discovery of the first rectification of a nontrivial algebraic curve, the semicubical parabola. The accompanying figures appear on page 145. On page 91, William Neile is mentioned as Gulielmus Nelius.


£#h5#£Integral form£#/h5#£
Before the full formal development of calculus, the basis for the modern integral form for arc length was independently discovered by Hendrik van Heuraet and Pierre de Fermat.

In 1659 van Heuraet published a construction showing that the problem of determining arc length could be transformed into the problem of determining the area under a curve (i.e., an integral). As an example of his method, he determined the arc length of a semicubical parabola, which required finding the area under a parabola. In 1660, Fermat published a more general theory containing the same result in his De linearum curvarum cum lineis rectis comparatione dissertatio geometrica (Geometric dissertation on curved lines in comparison with straight lines).

Building on his previous work with tangents, Fermat used the curve

${\displaystyle y=x^{\frac {3}{2}}\,}$
whose tangent at x = a had a slope of

${\displaystyle {3 \over 2}a^{\frac {1}{2}}}$
so the tangent line would have the equation

${\displaystyle y={3 \over 2}a^{\frac {1}{2}}(x-a)+f(a).}$
Next, he increased a by a small amount to a + ε, making segment AC a relatively good approximation for the length of the curve from A to D. To find the length of the segment AC, he used the Pythagorean theorem:

${\displaystyle {\begin{aligned}AC^{2}&=AB^{2}+BC^{2}\\&=\varepsilon ^{2}+{9 \over 4}a\varepsilon ^{2}\\&=\varepsilon ^{2}\left(1+{9 \over 4}a\right)\end{aligned}}}$
which, when solved, yields

${\displaystyle AC=\varepsilon {\sqrt {1+{9 \over 4}a\,}}.}$
In order to approximate the length, Fermat would sum up a sequence of short segments.


£#h5#£Curves with infinite length£#/h5#£
As mentioned above, some curves are non-rectifiable. That is, there is no upper bound on the lengths of polygonal approximations; the length can be made arbitrarily large. Informally, such curves are said to have infinite length. There are continuous curves on which every arc (other than a single-point arc) has infinite length. An example of such a curve is the Koch curve. Another example of a curve with infinite length is the graph of the function defined by f(x) = x sin(1/x) for any open set with 0 as one of its delimiters and f(0) = 0. Sometimes the Hausdorff dimension and Hausdorff measure are used to quantify the size of such curves.


£#h5#£Generalization to (pseudo-)Riemannian manifolds£#/h5#£
Let ${\displaystyle M}$ be a (pseudo-)Riemannian manifold, ${\displaystyle \gamma :[0,1]\rightarrow M}$ a curve in ${\displaystyle M}$ and ${\displaystyle g}$ the (pseudo-) metric tensor.

The length of ${\displaystyle \gamma }$ is defined to be

where ${\displaystyle \gamma '(t)\in T_{\gamma (t)}M}$ is the tangent vector of ${\displaystyle \gamma }$ at ${\displaystyle t.}$ The sign in the square root is chosen once for a given curve, to ensure that the square root is a real number. The positive sign is chosen for spacelike curves; in a pseudo-Riemannian manifold, the negative sign may be chosen for timelike curves. Thus the length of a curve is a non-negative real number. Usually no curves are considered which are partly spacelike and partly timelike.
In theory of relativity, arc length of timelike curves (world lines) is the proper time elapsed along the world line, and arc length of a spacelike curve the proper distance along the curve.


£#h5#£See also£#/h5#£ £#ul#££#li#£Arc (geometry)£#/li#£ £#li#£Circumference£#/li#£ £#li#£Crofton formula£#/li#£ £#li#£Elliptic integral£#/li#£ £#li#£Geodesics£#/li#£ £#li#£Intrinsic equation£#/li#£ £#li#£Integral approximations£#/li#£ £#li#£Line integral£#/li#£ £#li#£Meridian arc£#/li#£ £#li#£Multivariable calculus£#/li#£ £#li#£Sinuosity£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£Sources£#/h5#£ £#ul#££#li#£Farouki, Rida T. (1999). "Curves from motion, motion from curves". In Laurent, P.-J.; Sablonniere, P.; Schumaker, L. L. (eds.). Curve and Surface Design: Saint-Malo 1999. Vanderbilt Univ. Press. pp. 63–90. ISBN 978-0-8265-1356-4.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Rectifiable curve", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#£ £#li#£The History of Curvature£#/li#£ £#li#£Weisstein, Eric W. "Arc Length". MathWorld.£#/li#£ £#li#£Arc Length by Ed Pegg Jr., The Wolfram Demonstrations Project, 2007.£#/li#£ £#li#£Calculus Study Guide – Arc Length (Rectification)£#/li#£ £#li#£Famous Curves Index The MacTutor History of Mathematics archive£#/li#£ £#li#£Arc Length Approximation by Chad Pierson, Josh Fritz, and Angela Sharp, The Wolfram Demonstrations Project.£#/li#£ £#li#£Length of a Curve Experiment Illustrates numerical solution of finding length of a curve.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Differential Geometry > Differential Geometry of Curves £#/li#££#li#£ Calculus and Analysis > Calculus > Multivariable Calculus £#/li#££#/ul#£




£#h3#£Arcsec£#/h3#£

Arcsec, ArcSec, ARCSEC, or arcsec may refer to:

£#ul#££#li#£Arcsecond, a unit of angular measurement£#/li#£ £#li#£Arcsecant, an inverse trigonometric function£#/li#££#/ul#£
£#h5#£See also£#/h5#£ £#ul#££#li#£sec−1 (disambiguation)£#/li#£ £#li#£asec (disambiguation)£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Trigonometric Functions £#/li#££#/ul#£




£#h3#£Arcsecant£#/h3#£

In mathematics, the inverse trigonometric functions (occasionally also called arcus functions, antitrigonometric functions or cyclometric functions) are the inverse functions of the trigonometric functions (with suitably restricted domains). Specifically, they are the inverses of the sine, cosine, tangent, cotangent, secant, and cosecant functions, and are used to obtain an angle from any of the angle's trigonometric ratios. Inverse trigonometric functions are widely used in engineering, navigation, physics, and geometry.


£#h5#£Notation£#/h5#£
Several notations for the inverse trigonometric functions exist. The most common convention is to name inverse trigonometric functions using an arc- prefix: arcsin(x), arccos(x), arctan(x), etc. (This convention is used throughout this article.) This notation arises from the following geometric relationships: when measuring in radians, an angle of θ radians will correspond to an arc whose length is rθ, where r is the radius of the circle. Thus in the unit circle, "the arc whose cosine is x" is the same as "the angle whose cosine is x", because the length of the arc of the circle in radii is the same as the measurement of the angle in radians. In computer programming languages, the inverse trigonometric functions are often called by the abbreviated forms asin, acos, atan.

The notations sin−1(x), cos−1(x), tan−1(x), etc., as introduced by John Herschel in 1813, are often used as well in English-language sources, much more than the also established sin[−1](x), cos[−1](x), tan[−1](x),—conventions consistent with the notation of an inverse function, that is useful e.g. to define the multivalued version of each inverse trigonometric function: e.g. ${\displaystyle \tan ^{-1}(x)=\{\arctan(x)+\pi k\mid k\in \mathbb {Z} \}}$ . However, this might appear to conflict logically with the common semantics for expressions such as sin2(x) (although only sin2 x, without parentheses, is the really common one), which refer to numeric power rather than function composition, and therefore may result in confusion between multiplicative inverse or reciprocal and compositional inverse. The confusion is somewhat mitigated by the fact that each of the reciprocal trigonometric functions has its own name—for example, (cos(x))−1 = sec(x). Nevertheless, certain authors advise against using it for its ambiguity. Another precarious convention used by a tiny number of authors is to use an uppercase first letter, along with a −1 superscript: Sin−1(x), Cos−1(x), Tan−1(x), etc. Although its intention is to avoid confusion with the multiplicative inverse, which should be represented by sin−1(x), cos−1(x), etc., or, better, by sin−1 x, cos−1 x, etc., it in turn creates yet another major source of ambiguity: especially in light of the fact that many popular high-level programming languages (e.g. Wolfram's Mathematica, and University of Sydney's MAGMA) use those very same capitalised representations for the standard trig functions; whereas others (Python (ie SymPy and NumPy), Matlab, MAPLE etc) use lower-case.

Hence, since 2009, the ISO 80000-2 standard has specified solely the "arc" prefix for the inverse functions.


£#h5#£Basic concepts£#/h5#£
£#h5#£Principal values£#/h5#£
Since none of the six trigonometric functions are one-to-one, they must be restricted in order to have inverse functions. Therefore, the result ranges of the inverse functions are proper (i.e. strict) subsets of the domains of the original functions.

For example, using function in the sense of multivalued functions, just as the square root function ${\displaystyle y={\sqrt {x}}}$ could be defined from ${\displaystyle y^{2}=x,}$ the function ${\displaystyle y=\arcsin(x)}$ is defined so that ${\displaystyle \sin(y)=x.}$ For a given real number ${\displaystyle x,}$ with ${\displaystyle -1\leq x\leq 1,}$ there are multiple (in fact, countably infinitely many) numbers ${\displaystyle y}$ such that ${\displaystyle \sin(y)=x}$ ; for example, ${\displaystyle \sin(0)=0,}$ but also ${\displaystyle \sin(\pi )=0,}$ ${\displaystyle \sin(2\pi )=0,}$ etc. When only one value is desired, the function may be restricted to its principal branch. With this restriction, for each ${\displaystyle x}$ in the domain, the expression ${\displaystyle \arcsin(x)}$ will evaluate only to a single value, called its principal value. These properties apply to all the inverse trigonometric functions.

The principal inverses are listed in the following table.

Note: Some authors define the range of arcsecant to be ${\textstyle (0\leq y<{\frac {\pi }{2}}{\text{ or }}\pi \leq y<{\frac {3\pi }{2}})}$ , because the tangent function is nonnegative on this domain. This makes some computations more consistent. For example, using this range, ${\displaystyle \tan(\operatorname {arcsec}(x))={\sqrt {x^{2}-1}},}$ whereas with the range ${\textstyle (0\leq y<{\frac {\pi }{2}}{\text{ or }}{\frac {\pi }{2}}<y\leq \pi )}$ , we would have to write ${\displaystyle \tan(\operatorname {arcsec}(x))=\pm {\sqrt {x^{2}-1}},}$ since tangent is nonnegative on ${\textstyle 0\leq y<{\frac {\pi }{2}},}$ but nonpositive on ${\textstyle {\frac {\pi }{2}}<y\leq \pi .}$ For a similar reason, the same authors define the range of arccosecant to be ${\textstyle (-\pi <y\leq -{\frac {\pi }{2}}}$ or ${\textstyle 0<y\leq {\frac {\pi }{2}}).}$

If ${\displaystyle x}$ is allowed to be a complex number, then the range of ${\displaystyle y}$ applies only to its real part.

The table below displays names and domains of the inverse trigonometric functions along with the range of their usual principal values in radians.

The symbol ${\displaystyle \mathbb {R} =(-\infty ,\infty )}$ denotes the set of all real numbers and ${\displaystyle \mathbb {Z} =\{\ldots ,\,-2,\,-1,\,0,\,1,\,2,\,\ldots \}}$ denotes the set of all integers. The set of all integer multiples of ${\displaystyle \pi }$ is denoted by

The symbol ${\displaystyle \,\setminus \,}$ denotes set subtraction so that, for instance, ${\displaystyle \mathbb {R} \setminus (-1,1)=(-\infty ,-1]\cup [1,\infty )}$ is the set of points in ${\displaystyle \mathbb {R} }$ (that is, real numbers) that are not in the interval ${\displaystyle (-1,1).}$

The Minkowski sum notation ${\textstyle \pi \mathbb {Z} +(0,\pi )}$ and ${\displaystyle \pi \mathbb {Z} +{\bigl (}{-{\tfrac {\pi }{2}}},{\tfrac {\pi }{2}}{\bigr )}}$ that is used above to concisely write the domains of ${\displaystyle \cot ,\csc ,\tan ,{\text{ and }}\sec }$ is now explained.

Domain of cotangent ${\displaystyle \cot }$ and cosecant ${\displaystyle \csc }$ : The domains of ${\displaystyle \,\cot \,}$ and ${\displaystyle \,\csc \,}$ are the same. They are the set of all angles ${\displaystyle \theta }$ at which ${\displaystyle \sin \theta \neq 0,}$ i.e. all real numbers that are not of the form ${\displaystyle \pi n}$ for some integer ${\displaystyle n,}$

Domain of tangent ${\displaystyle \tan }$ and secant ${\displaystyle \sec }$ : The domains of ${\displaystyle \,\tan \,}$ and ${\displaystyle \,\sec \,}$ are the same. They are the set of all angles ${\displaystyle \theta }$ at which ${\displaystyle \cos \theta \neq 0,}$


£#h5#£Solutions to elementary trigonometric equations£#/h5#£
Each of the trigonometric functions is periodic in the real part of its argument, running through all its values twice in each interval of ${\displaystyle 2\pi }$ :

£#ul#££#li#£Sine and cosecant begin their period at ${\textstyle 2\pi k-{\frac {\pi }{2}}}$ (where ${\displaystyle k}$ is an integer), finish it at ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ and then reverse themselves over ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ to ${\displaystyle 2\pi k+{\frac {3\pi }{2}}.}$ £#/li#£ £#li#£Cosine and secant begin their period at ${\displaystyle 2\pi k,}$ finish it at ${\displaystyle 2\pi k+\pi .}$ and then reverse themselves over ${\displaystyle 2\pi k+\pi }$ to ${\displaystyle 2\pi k+2\pi .}$ £#/li#£ £#li#£Tangent begins its period at ${\textstyle 2\pi k-{\frac {\pi }{2}},}$ finishes it at ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ and then repeats it (forward) over ${\textstyle 2\pi k+{\frac {\pi }{2}}}$ to ${\textstyle 2\pi k+{\frac {3\pi }{2}}.}$ £#/li#£ £#li#£Cotangent begins its period at ${\displaystyle 2\pi k,}$ finishes it at ${\displaystyle 2\pi k+\pi ,}$ and then repeats it (forward) over ${\displaystyle 2\pi k+\pi }$ to ${\displaystyle 2\pi k+2\pi .}$ £#/li#££#/ul#£
This periodicity is reflected in the general inverses, where ${\displaystyle k}$ is some integer.

The following table shows how inverse trigonometric functions may be used to solve equalities involving the six standard trigonometric functions. It is assumed that the given values ${\displaystyle \theta ,}$ ${\displaystyle r,}$ ${\displaystyle s,}$ ${\displaystyle x,}$ and ${\displaystyle y}$ all lie within appropriate ranges so that the relevant expressions below are well-defined. Note that "for some ${\displaystyle k\in \mathbb {Z} }$ " is just another way of saying "for some integer ${\displaystyle k.}$ "

The symbol ${\displaystyle \,\iff \,}$ is logical equality. The expression "LHS ${\displaystyle \,\iff \,}$ RHS" indicates that either (a) the left hand side (i.e. LHS) and right hand side (i.e. RHS) are both true, or else (b) the left hand side and right hand side are both false; there is no option (c) (e.g. it is not possible for the LHS statement to be true and also simultaneously for the RHS statement to false), because otherwise "LHS ${\displaystyle \,\iff \,}$ RHS" would not have been written (see this footnote for an example illustrating this concept).

For example, if ${\displaystyle \cos \theta =-1}$ then ${\displaystyle \theta =\pi +2\pi k=-\pi +2\pi (1+k)}$ for some ${\displaystyle k\in \mathbb {Z} .}$ While if ${\displaystyle \sin \theta =\pm 1}$ then ${\textstyle \theta ={\frac {\pi }{2}}+\pi k=-{\frac {\pi }{2}}+\pi (k+1)}$ for some ${\displaystyle k\in \mathbb {Z} ,}$ where ${\displaystyle k}$ will be even if ${\displaystyle \sin \theta =1}$ and it will be odd if ${\displaystyle \sin \theta =-1.}$ The equations ${\displaystyle \sec \theta =-1}$ and ${\displaystyle \csc \theta =\pm 1}$ have the same solutions as ${\displaystyle \cos \theta =-1}$ and ${\displaystyle \sin \theta =\pm 1,}$ respectively. In all equations above except for those just solved (i.e. except for ${\displaystyle \sin }$ / ${\displaystyle \csc \theta =\pm 1}$ and ${\displaystyle \cos }$ / ${\displaystyle \sec \theta =-1}$ ), the integer ${\displaystyle k}$ in the solution's formula is uniquely determined by ${\displaystyle \theta }$ (for fixed ${\displaystyle r,s,x,}$ and ${\displaystyle y}$ ).

Detailed example and explanation of the "plus or minus" symbol ${\displaystyle \pm }$
The solutions to ${\displaystyle \cos \theta =x}$ and ${\displaystyle \sec \theta =x}$ involve the "plus or minus" symbol ${\displaystyle \,\pm ,\,}$ whose meaning is now clarified. Only the solution to ${\displaystyle \cos \theta =x}$ will be discussed since the discussion for ${\displaystyle \sec \theta =x}$ is the same. We are given ${\displaystyle x}$ between ${\displaystyle -1\leq x\leq 1}$ and we know that there is an angle ${\displaystyle \theta }$ in some give interval that satisfies ${\displaystyle \cos \theta =x.}$ We want to find this ${\displaystyle \theta .}$ The formula for the solution involves:

If ${\displaystyle \,\arccos x=0\,}$ (which only happens when ${\displaystyle x=1}$ ) then ${\displaystyle \,+\arccos x=0\,}$ and ${\displaystyle \,-\arccos x=0\,}$ so either way, ${\displaystyle \,\pm \arccos x\,}$ can only be equal to ${\displaystyle 0.}$ But if ${\displaystyle \,\arccos x\neq 0,\,}$ which will now be assumed, then the solution to ${\displaystyle \cos \theta =x,}$ which is written above as is shorthand for the following statement:
Either £#ul#££#li#£ ${\displaystyle \,\theta =\arccos x+2\pi k\,}$ for some integer ${\displaystyle k,}$
or else£#/li#£ £#li#£ ${\displaystyle \,\theta =-\arccos x+2\pi k\,}$ for some integer ${\displaystyle k.}$ £#/li#££#/ul#£
Because ${\displaystyle \,\arccos x\neq 0\,}$ and ${\displaystyle \,0<\arccos x\leq \pi \,}$ exactly one of these two equalities can hold. Additional information about ${\displaystyle \theta }$ is needed to determine which one holds. For example, suppose that ${\displaystyle x=0}$ and that all that is known about ${\displaystyle \theta }$ is that ${\displaystyle \,-\pi \leq \theta \leq \pi \,}$ (and nothing more is known). Then

and moreover, in this particular case ${\displaystyle k=0}$ (for both the ${\displaystyle \,+\,}$ case and the ${\displaystyle \,-\,}$ case) and so consequently, This means that ${\displaystyle \theta }$ could be either ${\displaystyle \,\pi /2\,}$ or ${\displaystyle \,-\pi /2.}$ Without additional information it is not possible to determine which of these values ${\displaystyle \theta }$ has. An example of some additional information that could determine the value of ${\displaystyle \theta }$ would be knowing that the angle is above the ${\displaystyle x}$ -axis(in which case ${\displaystyle \theta =\pi /2}$ ) or alternatively, knowing that it is below the ${\displaystyle x}$ -axis(in which case ${\displaystyle \theta =-\pi /2}$ ).
Transforming equations

The equations above can be transformed by using the reflection and shift identities:

These formulas imply, in particular, that the following hold:

where swapping ${\displaystyle \sin \leftrightarrow \csc ,}$ swapping ${\displaystyle \cos \leftrightarrow \sec ,}$ and swapping ${\displaystyle \tan \leftrightarrow \cot }$ gives the analogous equations for ${\displaystyle \csc ,\sec ,{\text{ and }}\cot ,}$ respectively.
So for example, by using the equality ${\textstyle \sin \left({\frac {\pi }{2}}-\theta \right)=\cos \theta ,}$ the equation ${\displaystyle \cos \theta =x}$ can be transformed into ${\textstyle \sin \left({\frac {\pi }{2}}-\theta \right)=x,}$ which allows for the solution to the equation ${\displaystyle \;\sin \varphi =x\;}$ (where ${\textstyle \varphi :={\frac {\pi }{2}}-\theta }$ ) to be used; that solution being: ${\displaystyle \varphi =(-1)^{k}\arcsin(x)+\pi k\;{\text{ for some }}k\in \mathbb {Z} ,}$ which becomes:

where using the fact that ${\displaystyle (-1)^{k}=(-1)^{-k}}$ and substituting ${\displaystyle h:=-k}$ proves that another solution to ${\displaystyle \;\cos \theta =x\;}$ is: The substitution ${\displaystyle \;\arcsin x={\frac {\pi }{2}}-\arccos x\;}$ may be used express the right hand side of the above formula in terms of ${\displaystyle \;\arccos x\;}$ instead of ${\displaystyle \;\arcsin x.\;}$
£#h5#£Equal identical trigonometric functions£#/h5#£
The table below shows how two angles ${\displaystyle \theta }$ and ${\displaystyle \varphi }$ must be related if their values under a given trigonometric function are equal or negatives of each other.


£#h5#£Relationships between trigonometric functions and inverse trigonometric functions£#/h5#£
Trigonometric functions of inverse trigonometric functions are tabulated below. A quick way to derive them is by considering the geometry of a right-angled triangle, with one side of length 1 and another side of length ${\displaystyle x,}$ then applying the Pythagorean theorem and definitions of the trigonometric ratios. Purely algebraic derivations are longer. It is worth noting that for arcsecant and arccosecant, the diagram assumes that ${\displaystyle x}$ is positive, and thus the result has to be corrected through the use of absolute values and the signum (sgn) operation.


£#h5#£Relationships among the inverse trigonometric functions£#/h5#£
Complementary angles:

${\displaystyle {\begin{aligned}\arccos(x)&={\frac {\pi }{2}}-\arcsin(x)\\[0.5em]\operatorname {arccot}(x)&={\frac {\pi }{2}}-\arctan(x)\\[0.5em]\operatorname {arccsc}(x)&={\frac {\pi }{2}}-\operatorname {arcsec}(x)\end{aligned}}}$
Negative arguments:

${\displaystyle {\begin{aligned}\arcsin(-x)&=-\arcsin(x)\\\arccos(-x)&=\pi -\arccos(x)\\\arctan(-x)&=-\arctan(x)\\\operatorname {arccot}(-x)&=\pi -\operatorname {arccot}(x)\\\operatorname {arcsec}(-x)&=\pi -\operatorname {arcsec}(x)\\\operatorname {arccsc}(-x)&=-\operatorname {arccsc}(x)\end{aligned}}}$
Reciprocal arguments:

${\displaystyle {\begin{aligned}\arccos \left({\frac {1}{x}}\right)&=\operatorname {arcsec}(x)\\[0.3em]\arcsin \left({\frac {1}{x}}\right)&=\operatorname {arccsc}(x)\\[0.3em]\arctan \left({\frac {1}{x}}\right)&={\frac {\pi }{2}}-\arctan(x)=\operatorname {arccot}(x)\,,{\text{ if }}x>0\\[0.3em]\arctan \left({\frac {1}{x}}\right)&=-{\frac {\pi }{2}}-\arctan(x)=\operatorname {arccot}(x)-\pi \,,{\text{ if }}x<0\\[0.3em]\operatorname {arccot} \left({\frac {1}{x}}\right)&={\frac {\pi }{2}}-\operatorname {arccot}(x)=\arctan(x)\,,{\text{ if }}x>0\\[0.3em]\operatorname {arccot} \left({\frac {1}{x}}\right)&={\frac {3\pi }{2}}-\operatorname {arccot}(x)=\pi +\arctan(x)\,,{\text{ if }}x<0\\[0.3em]\operatorname {arcsec} \left({\frac {1}{x}}\right)&=\arccos(x)\\[0.3em]\operatorname {arccsc} \left({\frac {1}{x}}\right)&=\arcsin(x)\end{aligned}}}$
Useful identities if one only has a fragment of a sine table:

${\displaystyle {\begin{aligned}\arccos(x)&=\arcsin \left({\sqrt {1-x^{2}}}\right)\,,{\text{ if }}0\leq x\leq 1{\text{ , from which you get }}\\\arccos &\left({\frac {1-x^{2}}{1+x^{2}}}\right)=\arcsin \left({\frac {2x}{1+x^{2}}}\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin &\left({\sqrt {1-x^{2}}}\right)={\frac {\pi }{2}}-\operatorname {sgn}(x)\arcsin(x)\\\arccos(x)&={\frac {1}{2}}\arccos \left(2x^{2}-1\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin(x)&={\frac {1}{2}}\arccos \left(1-2x^{2}\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin(x)&=\arctan \left({\frac {x}{\sqrt {1-x^{2}}}}\right)\\\arccos(x)&=\arctan \left({\frac {\sqrt {1-x^{2}}}{x}}\right)\\\arctan(x)&=\arcsin \left({\frac {x}{\sqrt {1+x^{2}}}}\right)\\\operatorname {arccot}(x)&=\arccos \left({\frac {x}{\sqrt {1+x^{2}}}}\right)\end{aligned}}}$
Whenever the square root of a complex number is used here, we choose the root with the positive real part (or positive imaginary part if the square was negative real).

A useful form that follows directly from the table above is

${\displaystyle \arctan \left(x\right)=\arccos \left({\sqrt {\frac {1}{1+x^{2}}}}\right)\,,{\text{ if }}x\geq 0}$ .
It is obtained by recognizing that ${\displaystyle \cos \left(\arctan \left(x\right)\right)={\sqrt {\frac {1}{1+x^{2}}}}=\cos \left(\arccos \left({\sqrt {\frac {1}{1+x^{2}}}}\right)\right)}$ .

From the half-angle formula, ${\displaystyle \tan \left({\tfrac {\theta }{2}}\right)={\tfrac {\sin(\theta )}{1+\cos(\theta )}}}$ , we get:

${\displaystyle {\begin{aligned}\arcsin(x)&=2\arctan \left({\frac {x}{1+{\sqrt {1-x^{2}}}}}\right)\\[0.5em]\arccos(x)&=2\arctan \left({\frac {\sqrt {1-x^{2}}}{1+x}}\right)\,,{\text{ if }}-1<x\leq 1\\[0.5em]\arctan(x)&=2\arctan \left({\frac {x}{1+{\sqrt {1+x^{2}}}}}\right)\end{aligned}}}$

£#h5#£Arctangent addition formula£#/h5#£
${\displaystyle \arctan(u)\pm \arctan(v)=\arctan \left({\frac {u\pm v}{1\mp uv}}\right){\pmod {\pi }}\,,\quad uv\neq 1\,.}$
This is derived from the tangent addition formula

${\displaystyle \tan(\alpha \pm \beta )={\frac {\tan(\alpha )\pm \tan(\beta )}{1\mp \tan(\alpha )\tan(\beta )}}\,,}$
by letting

${\displaystyle \alpha =\arctan(u)\,,\quad \beta =\arctan(v)\,.}$

£#h5#£In calculus£#/h5#£
£#h5#£Derivatives of inverse trigonometric functions£#/h5#£
The derivatives for complex values of z are as follows:

${\displaystyle {\begin{aligned}{\frac {d}{dz}}\arcsin(z)&{}={\frac {1}{\sqrt {1-z^{2}}}}\;;&z&{}\neq -1,+1\\{\frac {d}{dz}}\arccos(z)&{}=-{\frac {1}{\sqrt {1-z^{2}}}}\;;&z&{}\neq -1,+1\\{\frac {d}{dz}}\arctan(z)&{}={\frac {1}{1+z^{2}}}\;;&z&{}\neq -i,+i\\{\frac {d}{dz}}\operatorname {arccot}(z)&{}=-{\frac {1}{1+z^{2}}}\;;&z&{}\neq -i,+i\\{\frac {d}{dz}}\operatorname {arcsec}(z)&{}={\frac {1}{z^{2}{\sqrt {1-{\frac {1}{z^{2}}}}}}}\;;&z&{}\neq -1,0,+1\\{\frac {d}{dz}}\operatorname {arccsc}(z)&{}=-{\frac {1}{z^{2}{\sqrt {1-{\frac {1}{z^{2}}}}}}}\;;&z&{}\neq -1,0,+1\end{aligned}}}$
Only for real values of x:

${\displaystyle {\begin{aligned}{\frac {d}{dx}}\operatorname {arcsec}(x)&{}={\frac {1}{|x|{\sqrt {x^{2}-1}}}}\;;&|x|>1\\{\frac {d}{dx}}\operatorname {arccsc}(x)&{}=-{\frac {1}{|x|{\sqrt {x^{2}-1}}}}\;;&|x|>1\end{aligned}}}$
For a sample derivation: if ${\displaystyle \theta =\arcsin(x)}$ , we get:

${\displaystyle {\frac {d\arcsin(x)}{dx}}={\frac {d\theta }{d\sin(\theta )}}={\frac {d\theta }{\cos(\theta )\,d\theta }}={\frac {1}{\cos(\theta )}}={\frac {1}{\sqrt {1-\sin ^{2}(\theta )}}}={\frac {1}{\sqrt {1-x^{2}}}}}$

£#h5#£Expression as definite integrals£#/h5#£
Integrating the derivative and fixing the value at one point gives an expression for the inverse trigonometric function as a definite integral:

${\displaystyle {\begin{aligned}\arcsin(x)&{}=\int _{0}^{x}{\frac {1}{\sqrt {1-z^{2}}}}\,dz\;,&|x|&{}\leq 1\\\arccos(x)&{}=\int _{x}^{1}{\frac {1}{\sqrt {1-z^{2}}}}\,dz\;,&|x|&{}\leq 1\\\arctan(x)&{}=\int _{0}^{x}{\frac {1}{z^{2}+1}}\,dz\;,\\\operatorname {arccot}(x)&{}=\int _{x}^{\infty }{\frac {1}{z^{2}+1}}\,dz\;,\\\operatorname {arcsec}(x)&{}=\int _{1}^{x}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz=\pi +\int _{-x}^{-1}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz\;,&x&{}\geq 1\\\operatorname {arccsc}(x)&{}=\int _{x}^{\infty }{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz=\int _{-\infty }^{-x}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz\;,&x&{}\geq 1\\\end{aligned}}}$
When x equals 1, the integrals with limited domains are improper integrals, but still well-defined.


£#h5#£Infinite series£#/h5#£
Similar to the sine and cosine functions, the inverse trigonometric functions can also be calculated using power series, as follows. For arcsine, the series can be derived by expanding its derivative, ${\textstyle {\tfrac {1}{\sqrt {1-z^{2}}}}}$ , as a binomial series, and integrating term by term (using the integral definition as above). The series for arctangent can similarly be derived by expanding its derivative ${\textstyle {\frac {1}{1+z^{2}}}}$ in a geometric series, and applying the integral definition above (see Leibniz series).

${\displaystyle {\begin{aligned}\arcsin(z)&=z+\left({\frac {1}{2}}\right){\frac {z^{3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {z^{5}}{5}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {z^{7}}{7}}+\cdots \\[5pt]&=\sum _{n=0}^{\infty }{\frac {(2n-1)!!}{(2n)!!}}{\frac {z^{2n+1}}{2n+1}}\\[5pt]&=\sum _{n=0}^{\infty }{\frac {(2n)!}{(2^{n}n!)^{2}}}{\frac {z^{2n+1}}{2n+1}}\,;\qquad |z|\leq 1\end{aligned}}}$
${\displaystyle \arctan(z)=z-{\frac {z^{3}}{3}}+{\frac {z^{5}}{5}}-{\frac {z^{7}}{7}}+\cdots =\sum _{n=0}^{\infty }{\frac {(-1)^{n}z^{2n+1}}{2n+1}}\,;\qquad |z|\leq 1\qquad z\neq i,-i}$
Series for the other inverse trigonometric functions can be given in terms of these according to the relationships given above. For example, ${\displaystyle \arccos(x)=\pi /2-\arcsin(x)}$ , ${\displaystyle \operatorname {arccsc}(x)=\arcsin(1/x)}$ , and so on. Another series is given by:

${\displaystyle 2\left(\arcsin \left({\frac {x}{2}}\right)\right)^{2}=\sum _{n=1}^{\infty }{\frac {x^{2n}}{n^{2}{\binom {2n}{n}}}}.}$
Leonhard Euler found a series for the arctangent that converges more quickly than its Taylor series:

${\displaystyle \arctan(z)={\frac {z}{1+z^{2}}}\sum _{n=0}^{\infty }\prod _{k=1}^{n}{\frac {2kz^{2}}{(2k+1)(1+z^{2})}}.}$
(The term in the sum for n = 0 is the empty product, so is 1.)

Alternatively, this can be expressed as

${\displaystyle \arctan(z)=\sum _{n=0}^{\infty }{\frac {2^{2n}(n!)^{2}}{(2n+1)!}}{\frac {z^{2n+1}}{(1+z^{2})^{n+1}}}.}$
Another series for the arctangent function is given by

${\displaystyle \arctan(z)=i\sum _{n=1}^{\infty }{\frac {1}{2n-1}}\left({\frac {1}{(1+2i/z)^{2n-1}}}-{\frac {1}{(1-2i/z)^{2n-1}}}\right),}$
where ${\displaystyle i={\sqrt {-1}}}$ is the imaginary unit.


£#h5#£Continued fractions for arctangent£#/h5#£
Two alternatives to the power series for arctangent are these generalized continued fractions:

${\displaystyle \arctan(z)={\frac {z}{1+{\cfrac {(1z)^{2}}{3-1z^{2}+{\cfrac {(3z)^{2}}{5-3z^{2}+{\cfrac {(5z)^{2}}{7-5z^{2}+{\cfrac {(7z)^{2}}{9-7z^{2}+\ddots }}}}}}}}}}={\frac {z}{1+{\cfrac {(1z)^{2}}{3+{\cfrac {(2z)^{2}}{5+{\cfrac {(3z)^{2}}{7+{\cfrac {(4z)^{2}}{9+\ddots }}}}}}}}}}}$
The second of these is valid in the cut complex plane. There are two cuts, from −i to the point at infinity, going down the imaginary axis, and from i to the point at infinity, going up the same axis. It works best for real numbers running from −1 to 1. The partial denominators are the odd natural numbers, and the partial numerators (after the first) are just (nz)2, with each perfect square appearing once. The first was developed by Leonhard Euler; the second by Carl Friedrich Gauss utilizing the Gaussian hypergeometric series.


£#h5#£Indefinite integrals of inverse trigonometric functions£#/h5#£
For real and complex values of z:

${\displaystyle {\begin{aligned}\int \arcsin(z)\,dz&{}=z\,\arcsin(z)+{\sqrt {1-z^{2}}}+C\\\int \arccos(z)\,dz&{}=z\,\arccos(z)-{\sqrt {1-z^{2}}}+C\\\int \arctan(z)\,dz&{}=z\,\arctan(z)-{\frac {1}{2}}\ln \left(1+z^{2}\right)+C\\\int \operatorname {arccot}(z)\,dz&{}=z\,\operatorname {arccot}(z)+{\frac {1}{2}}\ln \left(1+z^{2}\right)+C\\\int \operatorname {arcsec}(z)\,dz&{}=z\,\operatorname {arcsec}(z)-\ln \left[z\left(1+{\sqrt {\frac {z^{2}-1}{z^{2}}}}\right)\right]+C\\\int \operatorname {arccsc}(z)\,dz&{}=z\,\operatorname {arccsc}(z)+\ln \left[z\left(1+{\sqrt {\frac {z^{2}-1}{z^{2}}}}\right)\right]+C\end{aligned}}}$
For real x ≥ 1:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\ln \left(x+{\sqrt {x^{2}-1}}\right)+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\ln \left(x+{\sqrt {x^{2}-1}}\right)+C\end{aligned}}}$
For all real x not between -1 and 1:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\operatorname {sgn}(x)\ln \left|x+{\sqrt {x^{2}-1}}\right|+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\operatorname {sgn}(x)\ln \left|x+{\sqrt {x^{2}-1}}\right|+C\end{aligned}}}$
The absolute value is necessary to compensate for both negative and positive values of the arcsecant and arccosecant functions. The signum function is also necessary due to the absolute values in the derivatives of the two functions, which create two different solutions for positive and negative values of x. These can be further simplified using the logarithmic definitions of the inverse hyperbolic functions:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\operatorname {arcosh} (|x|)+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\operatorname {arcosh} (|x|)+C\\\end{aligned}}}$
The absolute value in the argument of the arcosh function creates a negative half of its graph, making it identical to the signum logarithmic function shown above.

All of these antiderivatives can be derived using integration by parts and the simple derivative forms shown above.


£#h5#£Example£#/h5#£
Using ${\displaystyle \int u\,dv=uv-\int v\,du}$ (i.e. integration by parts), set

${\displaystyle {\begin{aligned}u&=\arcsin(x)&dv&=dx\\du&={\frac {dx}{\sqrt {1-x^{2}}}}&v&=x\end{aligned}}}$
Then

${\displaystyle \int \arcsin(x)\,dx=x\arcsin(x)-\int {\frac {x}{\sqrt {1-x^{2}}}}\,dx,}$
which by the simple substitution ${\displaystyle w=1-x^{2},\ dw=-2x\,dx}$ yields the final result:

${\displaystyle \int \arcsin(x)\,dx=x\arcsin(x)+{\sqrt {1-x^{2}}}+C}$

£#h5#£Extension to complex plane£#/h5#£
Since the inverse trigonometric functions are analytic functions, they can be extended from the real line to the complex plane. This results in functions with multiple sheets and branch points. One possible way of defining the extension is:

${\displaystyle \arctan(z)=\int _{0}^{z}{\frac {dx}{1+x^{2}}}\quad z\neq -i,+i}$
where the part of the imaginary axis which does not lie strictly between the branch points (−i and +i) is the branch cut between the principal sheet and other sheets. The path of the integral must not cross a branch cut. For z not on a branch cut, a straight line path from 0 to z is such a path. For z on a branch cut, the path must approach from Re[x] > 0 for the upper branch cut and from Re[x] < 0 for the lower branch cut.

The arcsine function may then be defined as:

${\displaystyle \arcsin(z)=\arctan \left({\frac {z}{\sqrt {1-z^{2}}}}\right)\quad z\neq -1,+1}$
where (the square-root function has its cut along the negative real axis and) the part of the real axis which does not lie strictly between −1 and +1 is the branch cut between the principal sheet of arcsin and other sheets;

${\displaystyle \arccos(z)={\frac {\pi }{2}}-\arcsin(z)\quad z\neq -1,+1}$
which has the same cut as arcsin;

${\displaystyle \operatorname {arccot}(z)={\frac {\pi }{2}}-\arctan(z)\quad z\neq -i,i}$
which has the same cut as arctan;

${\displaystyle \operatorname {arcsec}(z)=\arccos \left({\frac {1}{z}}\right)\quad z\neq -1,0,+1}$
where the part of the real axis between −1 and +1 inclusive is the cut between the principal sheet of arcsec and other sheets;

${\displaystyle \operatorname {arccsc}(z)=\arcsin \left({\frac {1}{z}}\right)\quad z\neq -1,0,+1}$
which has the same cut as arcsec.


£#h5#£Logarithmic forms£#/h5#£
These functions may also be expressed using complex logarithms. This extends their domains to the complex plane in a natural fashion. The following identities for principal values of the functions hold everywhere that they are defined, even on their branch cuts.

${\displaystyle {\begin{aligned}\arcsin(z)&{}=-i\ln \left({\sqrt {1-z^{2}}}+iz\right)=i\ln \left({\sqrt {1-z^{2}}}-iz\right)&{}=\operatorname {arccsc} \left({\frac {1}{z}}\right)\\[10pt]\arccos(z)&{}=-i\ln \left(i{\sqrt {1-z^{2}}}+z\right)={\frac {\pi }{2}}-\arcsin(z)&{}=\operatorname {arcsec} \left({\frac {1}{z}}\right)\\[10pt]\arctan(z)&{}=-{\frac {i}{2}}\ln \left({\frac {i-z}{i+z}}\right)=-{\frac {i}{2}}\ln \left({\frac {1+iz}{1-iz}}\right)&{}=\operatorname {arccot} \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arccot}(z)&{}=-{\frac {i}{2}}\ln \left({\frac {z+i}{z-i}}\right)=-{\frac {i}{2}}\ln \left({\frac {iz-1}{iz+1}}\right)&{}=\arctan \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arcsec}(z)&{}=-i\ln \left(i{\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {1}{z}}\right)={\frac {\pi }{2}}-\operatorname {arccsc}(z)&{}=\arccos \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arccsc}(z)&{}=-i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {i}{z}}\right)=i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}-{\frac {i}{z}}\right)&{}=\arcsin \left({\frac {1}{z}}\right)\end{aligned}}}$

£#h5#£Generalization£#/h5#£
Because all of the inverse trigonometric functions output an angle of a right triangle, they can be generalized by using Euler's formula to form a right triangle in the complex plane. Algebraically, this gives us:

${\displaystyle ce^{\theta i}=c\cos(\theta )+ci\sin(\theta )}$
or

${\displaystyle ce^{\theta i}=a+bi}$
where ${\displaystyle a}$ is the adjacent side, ${\displaystyle b}$ is the opposite side, and ${\displaystyle c}$ is the hypotenuse. From here, we can solve for ${\displaystyle \theta }$ .

${\displaystyle {\begin{aligned}e^{\ln(c)+\theta i}&=a+bi\\\ln c+\theta i&=\ln(a+bi)\\\theta &=\operatorname {Im} \left(\ln(a+bi)\right)\end{aligned}}}$
or

${\displaystyle \theta =-i\ln \left({\frac {a+bi}{c}}\right)=i\ln \left({\frac {c}{a+bi}}\right)}$
Simply taking the imaginary part works for any real-valued ${\displaystyle a}$ and ${\displaystyle b}$ , but if ${\displaystyle a}$ or ${\displaystyle b}$ is complex-valued, we have to use the final equation so that the real part of the result isn't excluded. Since the length of the hypotenuse doesn't change the angle, ignoring the real part of ${\displaystyle \ln(a+bi)}$ also removes ${\displaystyle c}$ from the equation. In the final equation, we see that the angle of the triangle in the complex plane can be found by inputting the lengths of each side. By setting one of the three sides equal to 1 and one of the remaining sides equal to our input ${\displaystyle z}$ , we obtain a formula for one of the inverse trig functions, for a total of six equations. Because the inverse trig functions require only one input, we must put the final side of the triangle in terms of the other two using the Pythagorean Theorem relation

${\displaystyle a^{2}+b^{2}=c^{2}}$
The table below shows the values of a, b, and c for each of the inverse trig functions and the equivalent expressions for ${\displaystyle \theta }$ that result from plugging the values into the equations above and simplifying.

${\displaystyle {\begin{aligned}&a&&b&&c&&\theta &&\theta _{\text{simplified}}&&\theta _{a,b\in \mathbb {R} }\\\arcsin(z)\ \ &{\sqrt {1-z^{2}}}&&z&&1&&-i\ln \left({\frac {{\sqrt {1-z^{2}}}+zi}{1}}\right)&&=-i\ln \left({\sqrt {1-z^{2}}}+zi\right)&&\operatorname {Im} \left(\ln \left({\sqrt {1-z^{2}}}+zi\right)\right)\\\arccos(z)\ \ &z&&{\sqrt {1-z^{2}}}&&1&&-i\ln \left({\frac {z+i{\sqrt {1-z^{2}}}}{1}}\right)&&=-i\ln \left(z+{\sqrt {z^{2}-1}}\right)&&\operatorname {Im} \left(\ln \left(z+{\sqrt {z^{2}-1}}\right)\right)\\\arctan(z)\ \ &1&&z&&{\sqrt {1+z^{2}}}&&i\ln \left({\frac {\sqrt {1+z^{2}}}{1+zi}}\right)&&={\frac {i}{2}}\ln \left({\frac {i+z}{i-z}}\right)&&\operatorname {Im} \left(\ln \left(1+zi\right)\right)\\\operatorname {arccot}(z)\ \ &z&&1&&{\sqrt {z^{2}+1}}&&i\ln \left({\frac {\sqrt {z^{2}+1}}{z+i}}\right)&&={\frac {i}{2}}\ln \left({\frac {z-i}{z+i}}\right)&&\operatorname {Im} \left(\ln \left(z+i\right)\right)\\\operatorname {arcsec}(z)\ \ &1&&{\sqrt {z^{2}-1}}&&z&&-i\ln \left({\frac {1+i{\sqrt {z^{2}-1}}}{z}}\right)&&=-i\ln \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z^{2}}}-1}}\right)&&\operatorname {Im} \left(\ln \left(1+{\sqrt {1-z^{2}}}\right)\right)\\\operatorname {arccsc}(z)\ \ &{\sqrt {z^{2}-1}}&&1&&z&&-i\ln \left({\frac {{\sqrt {z^{2}-1}}+i}{z}}\right)&&=-i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {i}{z}}\right)&&\operatorname {Im} \left(\ln \left({\sqrt {z^{2}-1}}+i\right)\right)\\\end{aligned}}}$
In this sense, all of the inverse trig functions can be thought of as specific cases of the complex-valued log function. Since this definition works for any complex-valued ${\displaystyle z}$ , this definition allows for hyperbolic angles as outputs and can be used to further define the inverse hyperbolic functions. Elementary proofs of the relations may also proceed via expansion to exponential forms of the trigonometric functions.


£#h5#£Example proof£#/h5#£
${\displaystyle {\begin{aligned}\sin(\phi )&=z\\\phi &=\arcsin(z)\end{aligned}}}$
Using the exponential definition of sine, and letting ${\displaystyle \xi =e^{\phi i},}$

${\displaystyle {\begin{aligned}z&={\frac {e^{\phi i}-e^{-\phi i}}{2i}}\\[10mu]2iz&=\xi -{\frac {1}{\xi }}\\[5mu]0&=\xi ^{2}-2i\xi z-1\\[5mu]\xi &=iz\pm {\sqrt {1-z^{2}}}\\[5mu]\phi &=-i\ln \left(iz\pm {\sqrt {1-z^{2}}}\right)\end{aligned}}}$
(the positive branch is chosen)

${\displaystyle \phi =\arcsin(z)=-i\ln \left(iz+{\sqrt {1-z^{2}}}\right)}$

£#h5#£Applications£#/h5#£
£#h5#£Finding the angle of a right triangle£#/h5#£
Inverse trigonometric functions are useful when trying to determine the remaining two angles of a right triangle when the lengths of the sides of the triangle are known. Recalling the right-triangle definitions of sine and cosine, it follows that

${\displaystyle \theta =\arcsin \left({\frac {\text{opposite}}{\text{hypotenuse}}}\right)=\arccos \left({\frac {\text{adjacent}}{\text{hypotenuse}}}\right).}$
Often, the hypotenuse is unknown and would need to be calculated before using arcsine or arccosine using the Pythagorean Theorem: ${\displaystyle a^{2}+b^{2}=h^{2}}$ where ${\displaystyle h}$ is the length of the hypotenuse. Arctangent comes in handy in this situation, as the length of the hypotenuse is not needed.

${\displaystyle \theta =\arctan \left({\frac {\text{opposite}}{\text{adjacent}}}\right)\,.}$
For example, suppose a roof drops 8 feet as it runs out 20 feet. The roof makes an angle θ with the horizontal, where θ may be computed as follows:

${\displaystyle \theta =\arctan \left({\frac {\text{opposite}}{\text{adjacent}}}\right)=\arctan \left({\frac {\text{rise}}{\text{run}}}\right)=\arctan \left({\frac {8}{20}}\right)\approx 21.8^{\circ }\,.}$

£#h5#£In computer science and engineering£#/h5#£
£#h5#£Two-argument variant of arctangent£#/h5#£

The two-argument atan2 function computes the arctangent of y / x given y and x, but with a range of (−π, π]. In other words, atan2(y, x) is the angle between the positive x-axis of a plane and the point (x, y) on it, with positive sign for counter-clockwise angles (upper half-plane, y > 0), and negative sign for clockwise angles (lower half-plane, y < 0). It was first introduced in many computer programming languages, but it is now also common in other fields of science and engineering.

In terms of the standard arctan function, that is with range of (−π/2, π/2), it can be expressed as follows:

${\displaystyle \operatorname {atan2} (y,x)={\begin{cases}\arctan \left({\frac {y}{x}}\right)&\quad x>0\\\arctan \left({\frac {y}{x}}\right)+\pi &\quad y\geq 0,\;x<0\\\arctan \left({\frac {y}{x}}\right)-\pi &\quad y<0,\;x<0\\{\frac {\pi }{2}}&\quad y>0,\;x=0\\-{\frac {\pi }{2}}&\quad y<0,\;x=0\\{\text{undefined}}&\quad y=0,\;x=0\end{cases}}}$
It also equals the principal value of the argument of the complex number x + iy.

This limited version of the function above may also be defined using the tangent half-angle formulae as follows:

${\displaystyle \operatorname {atan2} (y,x)=2\arctan \left({\frac {y}{{\sqrt {x^{2}+y^{2}}}+x}}\right)}$
provided that either x > 0 or y ≠ 0. However this fails if given x ≤ 0 and y = 0 so the expression is unsuitable for computational use.

The above argument order (y, x) seems to be the most common, and in particular is used in ISO standards such as the C programming language, but a few authors may use the opposite convention (x, y) so some caution is warranted. These variations are detailed at atan2.


£#h5#£Arctangent function with location parameter£#/h5#£
In many applications the solution ${\displaystyle y}$ of the equation ${\displaystyle x=\tan(y)}$ is to come as close as possible to a given value ${\displaystyle -\infty <\eta <\infty }$ . The adequate solution is produced by the parameter modified arctangent function

${\displaystyle y=\arctan _{\eta }(x):=\arctan(x)+\pi \,\operatorname {rni} \left({\frac {\eta -\arctan(x)}{\pi }}\right)\,.}$
The function ${\displaystyle \operatorname {rni} }$ rounds to the nearest integer.


£#h5#£Numerical accuracy£#/h5#£
For angles near 0 and π, arccosine is ill-conditioned and will thus calculate the angle with reduced accuracy in a computer implementation (due to the limited number of digits). Similarly, arcsine is inaccurate for angles near −π/2 and π/2.


£#h5#£See also£#/h5#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Abramowitz, Milton; Stegun, Irene A., eds. (1972). Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables. New York: Dover Publications. ISBN 978-0-486-61272-0.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Inverse Tangent". MathWorld.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Trigonometric Functions £#/li#££#/ul#£




£#h3#£Arcsech£#/h3#£


In mathematics, the inverse hyperbolic functions are the inverse functions of the hyperbolic functions.

For a given value of a hyperbolic function, the corresponding inverse hyperbolic function provides the corresponding hyperbolic angle. The size of the hyperbolic angle is equal to the area of the corresponding hyperbolic sector of the hyperbola xy = 1, or twice the area of the corresponding sector of the unit hyperbola x2 − y2 = 1, just as a circular angle is twice the area of the circular sector of the unit circle. Some authors have called inverse hyperbolic functions "area functions" to realize the hyperbolic angles.

Hyperbolic functions occur in the calculations of angles and distances in hyperbolic geometry. It also occurs in the solutions of many linear differential equations (such as the equation defining a catenary), cubic equations, and Laplace's equation in Cartesian coordinates. Laplace's equations are important in many areas of physics, including electromagnetic theory, heat transfer, fluid dynamics, and special relativity.


£#h5#£Notation£#/h5#£
The ISO 80000-2 standard abbreviations consist of ar- followed by the abbreviation of the corresponding hyperbolic function (e.g., arsinh, arcosh). The prefix arc- followed by the corresponding hyperbolic function (e.g., arcsinh, arccosh) is also commonly seen, by analogy with the nomenclature for inverse trigonometric functions. These are misnomers, since the prefix arc is the abbreviation for arcus, while the prefix ar stands for area; the hyperbolic functions are not directly related to arcs.

Other authors prefer to use the notation argsinh, argcosh, argtanh, and so on, where the prefix arg is the abbreviation of the Latin argumentum. In computer science, this is often shortened to asinh.

The notation sinh−1(x), cosh−1(x), etc., is also used, despite the fact that care must be taken to avoid misinterpretations of the superscript −1 as a power, as opposed to a shorthand to denote the inverse function (e.g., cosh−1(x) versus cosh(x)−1).


£#h5#£Definitions in terms of logarithms£#/h5#£
Since the hyperbolic functions are rational functions of ex whose numerator and denominator are of degree at most two, these functions may be solved in terms of ex, by using the quadratic formula; then, taking the natural logarithm gives the following expressions for the inverse hyperbolic functions.

For complex arguments, the inverse hyperbolic functions, the square root and the logarithm are multi-valued functions, and the equalities of the next subsections may be viewed as equalities of multi-valued functions.

For all inverse hyperbolic functions (save the inverse hyperbolic cotangent and the inverse hyperbolic cosecant), the domain of the real function is connected.


£#h5#£Inverse hyperbolic sine£#/h5#£
Inverse hyperbolic sine (a.k.a. area hyperbolic sine) (Latin: Area sinus hyperbolicus):

${\displaystyle \operatorname {arsinh} x=\ln \left(x+{\sqrt {x^{2}+1}}\right)}$
The domain is the whole real line.


£#h5#£Inverse hyperbolic cosine£#/h5#£
Inverse hyperbolic cosine (a.k.a. area hyperbolic cosine) (Latin: Area cosinus hyperbolicus):

${\displaystyle \operatorname {arcosh} x=\ln \left(x+{\sqrt {x^{2}-1}}\right)}$
The domain is the closed interval [1, +∞ ).


£#h5#£Inverse hyperbolic tangent£#/h5#£
Inverse hyperbolic tangent (a.k.a. area hyperbolic tangent) (Latin: Area tangens hyperbolicus):

${\displaystyle \operatorname {artanh} x={\frac {1}{2}}\ln \left({\frac {1+x}{1-x}}\right)}$
The domain is the open interval (−1, 1).


£#h5#£Inverse hyperbolic cotangent£#/h5#£
Inverse hyperbolic cotangent (a.k.a., area hyperbolic cotangent) (Latin: Area cotangens hyperbolicus):

${\displaystyle \operatorname {arcoth} x={\frac {1}{2}}\ln \left({\frac {x+1}{x-1}}\right)}$
The domain is the union of the open intervals (−∞, −1) and (1, +∞).


£#h5#£Inverse hyperbolic secant£#/h5#£
Inverse hyperbolic secant (a.k.a., area hyperbolic secant) (Latin: Area secans hyperbolicus):

${\displaystyle \operatorname {arsech} x=\ln \left({\frac {1}{x}}+{\sqrt {{\frac {1}{x^{2}}}-1}}\right)=\ln \left({\frac {1+{\sqrt {1-x^{2}}}}{x}}\right)}$
The domain is the semi-open interval (0, 1].


£#h5#£Inverse hyperbolic cosecant£#/h5#£
Inverse hyperbolic cosecant (a.k.a., area hyperbolic cosecant) (Latin: Area cosecans hyperbolicus):

${\displaystyle \operatorname {arcsch} x=\ln \left({\frac {1}{x}}+{\sqrt {{\frac {1}{x^{2}}}+1}}\right)}$
The domain is the real line with 0 removed.


£#h5#£Addition formulae£#/h5#£
${\displaystyle \operatorname {arsinh} u\pm \operatorname {arsinh} v=\operatorname {arsinh} \left(u{\sqrt {1+v^{2}}}\pm v{\sqrt {1+u^{2}}}\right)}$
${\displaystyle \operatorname {arcosh} u\pm \operatorname {arcosh} v=\operatorname {arcosh} \left(uv\pm {\sqrt {(u^{2}-1)(v^{2}-1)}}\right)}$
${\displaystyle \operatorname {artanh} u\pm \operatorname {artanh} v=\operatorname {artanh} \left({\frac {u\pm v}{1\pm uv}}\right)}$
${\displaystyle \operatorname {arcoth} u\pm \operatorname {arcoth} v=\operatorname {arcoth} \left({\frac {1\pm uv}{u\pm v}}\right)}$
${\displaystyle {\begin{aligned}\operatorname {arsinh} u+\operatorname {arcosh} v&=\operatorname {arsinh} \left(uv+{\sqrt {(1+u^{2})(v^{2}-1)}}\right)\\&=\operatorname {arcosh} \left(v{\sqrt {1+u^{2}}}+u{\sqrt {v^{2}-1}}\right)\end{aligned}}}$

£#h5#£Other identities£#/h5#£
${\displaystyle {\begin{aligned}2\operatorname {arcosh} x&=\operatorname {arcosh} (2x^{2}-1)&\quad {\hbox{ for }}x\geq 1\\4\operatorname {arcosh} x&=\operatorname {arcosh} (8x^{4}-8x^{2}+1)&\quad {\hbox{ for }}x\geq 1\\2\operatorname {arsinh} x&=\operatorname {arcosh} (2x^{2}+1)&\quad {\hbox{ for }}x\geq 0\\4\operatorname {arsinh} x&=\operatorname {arcosh} (8x^{4}+8x^{2}+1)&\quad {\hbox{ for }}x\geq 0\end{aligned}}}$
${\displaystyle \ln(x)=\operatorname {arcosh} \left({\frac {x^{2}+1}{2x}}\right)=\operatorname {arsinh} \left({\frac {x^{2}-1}{2x}}\right)=\operatorname {artanh} \left({\frac {x^{2}-1}{x^{2}+1}}\right)}$

£#h5#£Composition of hyperbolic and inverse hyperbolic functions£#/h5#£
${\displaystyle {\begin{aligned}&\sinh(\operatorname {arcosh} x)={\sqrt {x^{2}-1}}\quad {\text{for}}\quad |x|>1\\&\sinh(\operatorname {artanh} x)={\frac {x}{\sqrt {1-x^{2}}}}\quad {\text{for}}\quad -1<x<1\\&\cosh(\operatorname {arsinh} x)={\sqrt {1+x^{2}}}\\&\cosh(\operatorname {artanh} x)={\frac {1}{\sqrt {1-x^{2}}}}\quad {\text{for}}\quad -1<x<1\\&\tanh(\operatorname {arsinh} x)={\frac {x}{\sqrt {1+x^{2}}}}\\&\tanh(\operatorname {arcosh} x)={\frac {\sqrt {x^{2}-1}}{x}}\quad {\text{for}}\quad |x|>1\end{aligned}}}$

£#h5#£Composition of inverse hyperbolic and trigonometric functions£#/h5#£
${\displaystyle \operatorname {arsinh} \left(\tan \alpha \right)=\operatorname {artanh} \left(\sin \alpha \right)=\ln \left({\frac {1+\sin \alpha }{\cos \alpha }}\right)=\pm \operatorname {arcosh} \left({\frac {1}{\cos \alpha }}\right)}$
${\displaystyle \ln \left(\left|\tan \alpha \right|\right)=-\operatorname {artanh} \left(\cos 2\alpha \right)}$

£#h5#£Conversions£#/h5#£
${\displaystyle \ln x=\operatorname {artanh} \left({\frac {x^{2}-1}{x^{2}+1}}\right)=\operatorname {arsinh} \left({\frac {x^{2}-1}{2x}}\right)=\pm \operatorname {arcosh} \left({\frac {x^{2}+1}{2x}}\right)}$
${\displaystyle \operatorname {artanh} x=\operatorname {arsinh} \left({\frac {x}{\sqrt {1-x^{2}}}}\right)=\pm \operatorname {arcosh} \left({\frac {1}{\sqrt {1-x^{2}}}}\right)}$
${\displaystyle \operatorname {arsinh} x=\operatorname {artanh} \left({\frac {x}{\sqrt {1+x^{2}}}}\right)=\pm \operatorname {arcosh} \left({\sqrt {1+x^{2}}}\right)}$
${\displaystyle \operatorname {arcosh} x=\left|\operatorname {arsinh} \left({\sqrt {x^{2}-1}}\right)\right|=\left|\operatorname {artanh} \left({\frac {\sqrt {x^{2}-1}}{x}}\right)\right|}$

£#h5#£Derivatives£#/h5#£
${\displaystyle {\begin{aligned}{\frac {d}{dx}}\operatorname {arsinh} x&{}={\frac {1}{\sqrt {x^{2}+1}}},{\text{ for all real }}x\\{\frac {d}{dx}}\operatorname {arcosh} x&{}={\frac {1}{\sqrt {x^{2}-1}}},{\text{ for all real }}x>1\\{\frac {d}{dx}}\operatorname {artanh} x&{}={\frac {1}{1-x^{2}}},{\text{ for all real }}|x|<1\\{\frac {d}{dx}}\operatorname {arcoth} x&{}={\frac {1}{1-x^{2}}},{\text{ for all real }}|x|>1\\{\frac {d}{dx}}\operatorname {arsech} x&{}={\frac {-1}{x{\sqrt {1-x^{2}}}}},{\text{ for all real }}x\in (0,1)\\{\frac {d}{dx}}\operatorname {arcsch} x&{}={\frac {-1}{|x|{\sqrt {1+x^{2}}}}},{\text{ for all real }}x{\text{, except }}0\\\end{aligned}}}$
For an example differentiation: let θ = arsinh x, so (where sinh2 θ = (sinh θ)2):

${\displaystyle {\frac {d\,\operatorname {arsinh} x}{dx}}={\frac {d\theta }{d\sinh \theta }}={\frac {1}{\cosh \theta }}={\frac {1}{\sqrt {1+\sinh ^{2}\theta }}}={\frac {1}{\sqrt {1+x^{2}}}}.}$

£#h5#£Series expansions£#/h5#£
Expansion series can be obtained for the above functions:

${\displaystyle {\begin{aligned}\operatorname {arsinh} x&=x-\left({\frac {1}{2}}\right){\frac {x^{3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{5}}{5}}-\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{7}}{7}}\pm \cdots \\&=\sum _{n=0}^{\infty }\left({\frac {(-1)^{n}(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{2n+1}}{2n+1}},\qquad \left|x\right|<1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcosh} x&=\ln(2x)-\left(\left({\frac {1}{2}}\right){\frac {x^{-2}}{2}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{-4}}{4}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{-6}}{6}}+\cdots \right)\\&=\ln(2x)-\sum _{n=1}^{\infty }\left({\frac {(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{-2n}}{2n}},\qquad \left|x\right|>1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {artanh} x&=x+{\frac {x^{3}}{3}}+{\frac {x^{5}}{5}}+{\frac {x^{7}}{7}}+\cdots \\&=\sum _{n=0}^{\infty }{\frac {x^{2n+1}}{2n+1}},\qquad \left|x\right|<1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcsch} x=\operatorname {arsinh} {\frac {1}{x}}&=x^{-1}-\left({\frac {1}{2}}\right){\frac {x^{-3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{-5}}{5}}-\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{-7}}{7}}\pm \cdots \\&=\sum _{n=0}^{\infty }\left({\frac {(-1)^{n}(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{-(2n+1)}}{2n+1}},\qquad \left|x\right|>1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arsech} x=\operatorname {arcosh} {\frac {1}{x}}&=\ln {\frac {2}{x}}-\left(\left({\frac {1}{2}}\right){\frac {x^{2}}{2}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{4}}{4}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{6}}{6}}+\cdots \right)\\&=\ln {\frac {2}{x}}-\sum _{n=1}^{\infty }\left({\frac {(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{2n}}{2n}},\qquad 0<x\leq 1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcoth} x=\operatorname {artanh} {\frac {1}{x}}&=x^{-1}+{\frac {x^{-3}}{3}}+{\frac {x^{-5}}{5}}+{\frac {x^{-7}}{7}}+\cdots \\&=\sum _{n=0}^{\infty }{\frac {x^{-(2n+1)}}{2n+1}},\qquad \left|x\right|>1\end{aligned}}}$
Asymptotic expansion for the arsinh x is given by

${\displaystyle \operatorname {arsinh} x=\ln(2x)+\sum \limits _{n=1}^{\infty }{\left({-1}\right)^{n-1}{\frac {\left({2n-1}\right)!!}{2n\left({2n}\right)!!}}}{\frac {1}{x^{2n}}}}$



£#h5#£Principal values in the complex plane£#/h5#£
As functions of a complex variable, inverse hyperbolic functions are multivalued functions that are analytic, except at a finite number of points. For such a function, it is common to define a principal value, which is a single valued analytic function which coincides with one specific branch of the multivalued function, over a domain consisting of the complex plane in which a finite number of arcs (usually half lines or line segments) have been removed. These arcs are called branch cuts. For specifying the branch, that is, defining which value of the multivalued function is considered at each point, one generally define it at a particular point, and deduce the value everywhere in the domain of definition of the principal value by analytic continuation. When possible, it is better to define the principal value directly—without referring to analytic continuation.

For example, for the square root, the principal value is defined as the square root that has a positive real part. This defines a single valued analytic function, which is defined everywhere, except for non-positive real values of the variables (where the two square roots have a zero real part). This principal value of the square root function is denoted ${\displaystyle {\sqrt {x}}}$ in what follows. Similarly, the principal value of the logarithm, denoted ${\displaystyle \operatorname {Log} }$ in what follows, is defined as the value for which the imaginary part has the smallest absolute value. It is defined everywhere except for non-positive real values of the variable, for which two different values of the logarithm reach the minimum.

For all inverse hyperbolic functions, the principal value may be defined in terms of principal values of the square root and the logarithm function. However, in some cases, the formulas of § Definitions in terms of logarithms do not give a correct principal value, as giving a domain of definition which is too small and, in one case non-connected.


£#h5#£Principal value of the inverse hyperbolic sine£#/h5#£
The principal value of the inverse hyperbolic sine is given by

${\displaystyle \operatorname {arsinh} z=\operatorname {Log} (z+{\sqrt {z^{2}+1}}\,)\,.}$
The argument of the square root is a non-positive real number, if and only if z belongs to one of the intervals [i, +i∞) and (−i∞, −i] of the imaginary axis. If the argument of the logarithm is real, then it is positive. Thus this formula defines a principal value for arsinh, with branch cuts [i, +i∞) and (−i∞, −i]. This is optimal, as the branch cuts must connect the singular points i and −i to the infinity.


£#h5#£Principal value of the inverse hyperbolic cosine£#/h5#£
The formula for the inverse hyperbolic cosine given in § Inverse hyperbolic cosine is not convenient, since similar to the principal values of the logarithm and the square root, the principal value of arcosh would not be defined for imaginary z. Thus the square root has to be factorized, leading to

${\displaystyle \operatorname {arcosh} z=\operatorname {Log} (z+{\sqrt {z+1}}{\sqrt {z-1}}\,)\,.}$
The principal values of the square roots are both defined, except if z belongs to the real interval (−∞, 1]. If the argument of the logarithm is real, then z is real and has the same sign. Thus, the above formula defines a principal value of arcosh outside the real interval (−∞, 1], which is thus the unique branch cut.


£#h5#£Principal values of the inverse hyperbolic tangent and cotangent£#/h5#£
The formulas given in § Definitions in terms of logarithms suggests

${\displaystyle {\begin{aligned}\operatorname {artanh} z&={\frac {1}{2}}\operatorname {Log} \left({\frac {1+z}{1-z}}\right)\\\operatorname {arcoth} z&={\frac {1}{2}}\operatorname {Log} \left({\frac {z+1}{z-1}}\right)\end{aligned}}}$
for the definition of the principal values of the inverse hyperbolic tangent and cotangent. In these formulas, the argument of the logarithm is real if and only if z is real. For artanh, this argument is in the real interval (−∞, 0], if z belongs either to (−∞, −1] or to [1, ∞). For arcoth, the argument of the logarithm is in (−∞, 0], if and only if z belongs to the real interval [−1, 1].

Therefore, these formulas define convenient principal values, for which the branch cuts are (−∞, −1] and [1, ∞) for the inverse hyperbolic tangent, and [−1, 1] for the inverse hyperbolic cotangent.

In view of a better numerical evaluation near the branch cuts, some authors use the following definitions of the principal values, although the second one introduces a removable singularity at z = 0. The two definitions of ${\displaystyle \operatorname {artanh} }$ differ for real values of ${\displaystyle z}$ with ${\displaystyle z>1}$ . The ones of ${\displaystyle \operatorname {arcoth} }$ differ for real values of ${\displaystyle z}$ with ${\displaystyle z\in [0,1)}$ .

${\displaystyle {\begin{aligned}\operatorname {artanh} z&={\tfrac {1}{2}}\operatorname {Log} \left({1+z}\right)-{\tfrac {1}{2}}\operatorname {Log} \left({1-z}\right)\\\operatorname {arcoth} z&={\tfrac {1}{2}}\operatorname {Log} \left({1+{\frac {1}{z}}}\right)-{\tfrac {1}{2}}\operatorname {Log} \left({1-{\frac {1}{z}}}\right)\end{aligned}}}$

£#h5#£Principal value of the inverse hyperbolic cosecant£#/h5#£
For the inverse hyperbolic cosecant, the principal value is defined as

${\displaystyle \operatorname {arcsch} z=\operatorname {Log} \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z^{2}}}+1}}\,\right)}$ .
It is defined when the arguments of the logarithm and the square root are not non-positive real numbers. The principal value of the square root is thus defined outside the interval [−i, i] of the imaginary line. If the argument of the logarithm is real, then z is a non-zero real number, and this implies that the argument of the logarithm is positive.

Thus, the principal value is defined by the above formula outside the branch cut, consisting of the interval [−i, i] of the imaginary line.

For z = 0, there is a singular point that is included in the branch cut.


£#h5#£Principal value of the inverse hyperbolic secant£#/h5#£
Here, as in the case of the inverse hyperbolic cosine, we have to factorize the square root. This gives the principal value

${\displaystyle \operatorname {arsech} z=\operatorname {Log} \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z}}+1}}\,{\sqrt {{\frac {1}{z}}-1}}\right).}$
If the argument of a square root is real, then z is real, and it follows that both principal values of square roots are defined, except if z is real and belongs to one of the intervals (−∞, 0] and [1, +∞). If the argument of the logarithm is real and negative, then z is also real and negative. It follows that the principal value of arsech is well defined, by the above formula outside two branch cuts, the real intervals (−∞, 0] and [1, +∞).

For z = 0, there is a singular point that is included in one of the branch cuts.


£#h5#£Graphical representation£#/h5#£
In the following graphical representation of the principal values of the inverse hyperbolic functions, the branch cuts appear as discontinuities of the color. The fact that the whole branch cuts appear as discontinuities, shows that these principal values may not be extended into analytic functions defined over larger domains. In other words, the above defined branch cuts are minimal.


£#h5#£See also£#/h5#£ £#ul#££#li#£Complex logarithm£#/li#£ £#li#£Hyperbolic secant distribution£#/li#£ £#li#£ISO 80000-2£#/li#£ £#li#£List of integrals of inverse hyperbolic functions£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£Bibliography£#/h5#£ £#ul#££#li#£Herbert Busemann and Paul J. Kelly (1953) Projective Geometry and Projective Metrics, page 207, Academic Press.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Inverse hyperbolic functions", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Hyperbolic Functions £#/li#££#/ul#£




£#h3#£Arcsin£#/h3#£

In mathematics, the inverse trigonometric functions (occasionally also called arcus functions, antitrigonometric functions or cyclometric functions) are the inverse functions of the trigonometric functions (with suitably restricted domains). Specifically, they are the inverses of the sine, cosine, tangent, cotangent, secant, and cosecant functions, and are used to obtain an angle from any of the angle's trigonometric ratios. Inverse trigonometric functions are widely used in engineering, navigation, physics, and geometry.


£#h5#£Notation£#/h5#£
Several notations for the inverse trigonometric functions exist. The most common convention is to name inverse trigonometric functions using an arc- prefix: arcsin(x), arccos(x), arctan(x), etc. (This convention is used throughout this article.) This notation arises from the following geometric relationships: when measuring in radians, an angle of θ radians will correspond to an arc whose length is rθ, where r is the radius of the circle. Thus in the unit circle, "the arc whose cosine is x" is the same as "the angle whose cosine is x", because the length of the arc of the circle in radii is the same as the measurement of the angle in radians. In computer programming languages, the inverse trigonometric functions are often called by the abbreviated forms asin, acos, atan.

The notations sin−1(x), cos−1(x), tan−1(x), etc., as introduced by John Herschel in 1813, are often used as well in English-language sources, much more than the also established sin[−1](x), cos[−1](x), tan[−1](x),—conventions consistent with the notation of an inverse function, that is useful e.g. to define the multivalued version of each inverse trigonometric function: e.g. ${\displaystyle \tan ^{-1}(x)=\{\arctan(x)+\pi k\mid k\in \mathbb {Z} \}}$ . However, this might appear to conflict logically with the common semantics for expressions such as sin2(x) (although only sin2 x, without parentheses, is the really common one), which refer to numeric power rather than function composition, and therefore may result in confusion between multiplicative inverse or reciprocal and compositional inverse. The confusion is somewhat mitigated by the fact that each of the reciprocal trigonometric functions has its own name—for example, (cos(x))−1 = sec(x). Nevertheless, certain authors advise against using it for its ambiguity. Another precarious convention used by a tiny number of authors is to use an uppercase first letter, along with a −1 superscript: Sin−1(x), Cos−1(x), Tan−1(x), etc. Although its intention is to avoid confusion with the multiplicative inverse, which should be represented by sin−1(x), cos−1(x), etc., or, better, by sin−1 x, cos−1 x, etc., it in turn creates yet another major source of ambiguity: especially in light of the fact that many popular high-level programming languages (e.g. Wolfram's Mathematica, and University of Sydney's MAGMA) use those very same capitalised representations for the standard trig functions; whereas others (Python (ie SymPy and NumPy), Matlab, MAPLE etc) use lower-case.

Hence, since 2009, the ISO 80000-2 standard has specified solely the "arc" prefix for the inverse functions.


£#h5#£Basic concepts£#/h5#£
£#h5#£Principal values£#/h5#£
Since none of the six trigonometric functions are one-to-one, they must be restricted in order to have inverse functions. Therefore, the result ranges of the inverse functions are proper (i.e. strict) subsets of the domains of the original functions.

For example, using function in the sense of multivalued functions, just as the square root function ${\displaystyle y={\sqrt {x}}}$ could be defined from ${\displaystyle y^{2}=x,}$ the function ${\displaystyle y=\arcsin(x)}$ is defined so that ${\displaystyle \sin(y)=x.}$ For a given real number ${\displaystyle x,}$ with ${\displaystyle -1\leq x\leq 1,}$ there are multiple (in fact, countably infinitely many) numbers ${\displaystyle y}$ such that ${\displaystyle \sin(y)=x}$ ; for example, ${\displaystyle \sin(0)=0,}$ but also ${\displaystyle \sin(\pi )=0,}$ ${\displaystyle \sin(2\pi )=0,}$ etc. When only one value is desired, the function may be restricted to its principal branch. With this restriction, for each ${\displaystyle x}$ in the domain, the expression ${\displaystyle \arcsin(x)}$ will evaluate only to a single value, called its principal value. These properties apply to all the inverse trigonometric functions.

The principal inverses are listed in the following table.

Note: Some authors define the range of arcsecant to be ${\textstyle (0\leq y<{\frac {\pi }{2}}{\text{ or }}\pi \leq y<{\frac {3\pi }{2}})}$ , because the tangent function is nonnegative on this domain. This makes some computations more consistent. For example, using this range, ${\displaystyle \tan(\operatorname {arcsec}(x))={\sqrt {x^{2}-1}},}$ whereas with the range ${\textstyle (0\leq y<{\frac {\pi }{2}}{\text{ or }}{\frac {\pi }{2}}<y\leq \pi )}$ , we would have to write ${\displaystyle \tan(\operatorname {arcsec}(x))=\pm {\sqrt {x^{2}-1}},}$ since tangent is nonnegative on ${\textstyle 0\leq y<{\frac {\pi }{2}},}$ but nonpositive on ${\textstyle {\frac {\pi }{2}}<y\leq \pi .}$ For a similar reason, the same authors define the range of arccosecant to be ${\textstyle (-\pi <y\leq -{\frac {\pi }{2}}}$ or ${\textstyle 0<y\leq {\frac {\pi }{2}}).}$

If ${\displaystyle x}$ is allowed to be a complex number, then the range of ${\displaystyle y}$ applies only to its real part.

The table below displays names and domains of the inverse trigonometric functions along with the range of their usual principal values in radians.

The symbol ${\displaystyle \mathbb {R} =(-\infty ,\infty )}$ denotes the set of all real numbers and ${\displaystyle \mathbb {Z} =\{\ldots ,\,-2,\,-1,\,0,\,1,\,2,\,\ldots \}}$ denotes the set of all integers. The set of all integer multiples of ${\displaystyle \pi }$ is denoted by

The symbol ${\displaystyle \,\setminus \,}$ denotes set subtraction so that, for instance, ${\displaystyle \mathbb {R} \setminus (-1,1)=(-\infty ,-1]\cup [1,\infty )}$ is the set of points in ${\displaystyle \mathbb {R} }$ (that is, real numbers) that are not in the interval ${\displaystyle (-1,1).}$

The Minkowski sum notation ${\textstyle \pi \mathbb {Z} +(0,\pi )}$ and ${\displaystyle \pi \mathbb {Z} +{\bigl (}{-{\tfrac {\pi }{2}}},{\tfrac {\pi }{2}}{\bigr )}}$ that is used above to concisely write the domains of ${\displaystyle \cot ,\csc ,\tan ,{\text{ and }}\sec }$ is now explained.

Domain of cotangent ${\displaystyle \cot }$ and cosecant ${\displaystyle \csc }$ : The domains of ${\displaystyle \,\cot \,}$ and ${\displaystyle \,\csc \,}$ are the same. They are the set of all angles ${\displaystyle \theta }$ at which ${\displaystyle \sin \theta \neq 0,}$ i.e. all real numbers that are not of the form ${\displaystyle \pi n}$ for some integer ${\displaystyle n,}$

Domain of tangent ${\displaystyle \tan }$ and secant ${\displaystyle \sec }$ : The domains of ${\displaystyle \,\tan \,}$ and ${\displaystyle \,\sec \,}$ are the same. They are the set of all angles ${\displaystyle \theta }$ at which ${\displaystyle \cos \theta \neq 0,}$


£#h5#£Solutions to elementary trigonometric equations£#/h5#£
Each of the trigonometric functions is periodic in the real part of its argument, running through all its values twice in each interval of ${\displaystyle 2\pi }$ :

£#ul#££#li#£Sine and cosecant begin their period at ${\textstyle 2\pi k-{\frac {\pi }{2}}}$ (where ${\displaystyle k}$ is an integer), finish it at ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ and then reverse themselves over ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ to ${\displaystyle 2\pi k+{\frac {3\pi }{2}}.}$ £#/li#£ £#li#£Cosine and secant begin their period at ${\displaystyle 2\pi k,}$ finish it at ${\displaystyle 2\pi k+\pi .}$ and then reverse themselves over ${\displaystyle 2\pi k+\pi }$ to ${\displaystyle 2\pi k+2\pi .}$ £#/li#£ £#li#£Tangent begins its period at ${\textstyle 2\pi k-{\frac {\pi }{2}},}$ finishes it at ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ and then repeats it (forward) over ${\textstyle 2\pi k+{\frac {\pi }{2}}}$ to ${\textstyle 2\pi k+{\frac {3\pi }{2}}.}$ £#/li#£ £#li#£Cotangent begins its period at ${\displaystyle 2\pi k,}$ finishes it at ${\displaystyle 2\pi k+\pi ,}$ and then repeats it (forward) over ${\displaystyle 2\pi k+\pi }$ to ${\displaystyle 2\pi k+2\pi .}$ £#/li#££#/ul#£
This periodicity is reflected in the general inverses, where ${\displaystyle k}$ is some integer.

The following table shows how inverse trigonometric functions may be used to solve equalities involving the six standard trigonometric functions. It is assumed that the given values ${\displaystyle \theta ,}$ ${\displaystyle r,}$ ${\displaystyle s,}$ ${\displaystyle x,}$ and ${\displaystyle y}$ all lie within appropriate ranges so that the relevant expressions below are well-defined. Note that "for some ${\displaystyle k\in \mathbb {Z} }$ " is just another way of saying "for some integer ${\displaystyle k.}$ "

The symbol ${\displaystyle \,\iff \,}$ is logical equality. The expression "LHS ${\displaystyle \,\iff \,}$ RHS" indicates that either (a) the left hand side (i.e. LHS) and right hand side (i.e. RHS) are both true, or else (b) the left hand side and right hand side are both false; there is no option (c) (e.g. it is not possible for the LHS statement to be true and also simultaneously for the RHS statement to false), because otherwise "LHS ${\displaystyle \,\iff \,}$ RHS" would not have been written (see this footnote for an example illustrating this concept).

For example, if ${\displaystyle \cos \theta =-1}$ then ${\displaystyle \theta =\pi +2\pi k=-\pi +2\pi (1+k)}$ for some ${\displaystyle k\in \mathbb {Z} .}$ While if ${\displaystyle \sin \theta =\pm 1}$ then ${\textstyle \theta ={\frac {\pi }{2}}+\pi k=-{\frac {\pi }{2}}+\pi (k+1)}$ for some ${\displaystyle k\in \mathbb {Z} ,}$ where ${\displaystyle k}$ will be even if ${\displaystyle \sin \theta =1}$ and it will be odd if ${\displaystyle \sin \theta =-1.}$ The equations ${\displaystyle \sec \theta =-1}$ and ${\displaystyle \csc \theta =\pm 1}$ have the same solutions as ${\displaystyle \cos \theta =-1}$ and ${\displaystyle \sin \theta =\pm 1,}$ respectively. In all equations above except for those just solved (i.e. except for ${\displaystyle \sin }$ / ${\displaystyle \csc \theta =\pm 1}$ and ${\displaystyle \cos }$ / ${\displaystyle \sec \theta =-1}$ ), the integer ${\displaystyle k}$ in the solution's formula is uniquely determined by ${\displaystyle \theta }$ (for fixed ${\displaystyle r,s,x,}$ and ${\displaystyle y}$ ).

Detailed example and explanation of the "plus or minus" symbol ${\displaystyle \pm }$
The solutions to ${\displaystyle \cos \theta =x}$ and ${\displaystyle \sec \theta =x}$ involve the "plus or minus" symbol ${\displaystyle \,\pm ,\,}$ whose meaning is now clarified. Only the solution to ${\displaystyle \cos \theta =x}$ will be discussed since the discussion for ${\displaystyle \sec \theta =x}$ is the same. We are given ${\displaystyle x}$ between ${\displaystyle -1\leq x\leq 1}$ and we know that there is an angle ${\displaystyle \theta }$ in some give interval that satisfies ${\displaystyle \cos \theta =x.}$ We want to find this ${\displaystyle \theta .}$ The formula for the solution involves:

If ${\displaystyle \,\arccos x=0\,}$ (which only happens when ${\displaystyle x=1}$ ) then ${\displaystyle \,+\arccos x=0\,}$ and ${\displaystyle \,-\arccos x=0\,}$ so either way, ${\displaystyle \,\pm \arccos x\,}$ can only be equal to ${\displaystyle 0.}$ But if ${\displaystyle \,\arccos x\neq 0,\,}$ which will now be assumed, then the solution to ${\displaystyle \cos \theta =x,}$ which is written above as is shorthand for the following statement:
Either £#ul#££#li#£ ${\displaystyle \,\theta =\arccos x+2\pi k\,}$ for some integer ${\displaystyle k,}$
or else£#/li#£ £#li#£ ${\displaystyle \,\theta =-\arccos x+2\pi k\,}$ for some integer ${\displaystyle k.}$ £#/li#££#/ul#£
Because ${\displaystyle \,\arccos x\neq 0\,}$ and ${\displaystyle \,0<\arccos x\leq \pi \,}$ exactly one of these two equalities can hold. Additional information about ${\displaystyle \theta }$ is needed to determine which one holds. For example, suppose that ${\displaystyle x=0}$ and that all that is known about ${\displaystyle \theta }$ is that ${\displaystyle \,-\pi \leq \theta \leq \pi \,}$ (and nothing more is known). Then

and moreover, in this particular case ${\displaystyle k=0}$ (for both the ${\displaystyle \,+\,}$ case and the ${\displaystyle \,-\,}$ case) and so consequently, This means that ${\displaystyle \theta }$ could be either ${\displaystyle \,\pi /2\,}$ or ${\displaystyle \,-\pi /2.}$ Without additional information it is not possible to determine which of these values ${\displaystyle \theta }$ has. An example of some additional information that could determine the value of ${\displaystyle \theta }$ would be knowing that the angle is above the ${\displaystyle x}$ -axis(in which case ${\displaystyle \theta =\pi /2}$ ) or alternatively, knowing that it is below the ${\displaystyle x}$ -axis(in which case ${\displaystyle \theta =-\pi /2}$ ).
Transforming equations

The equations above can be transformed by using the reflection and shift identities:

These formulas imply, in particular, that the following hold:

where swapping ${\displaystyle \sin \leftrightarrow \csc ,}$ swapping ${\displaystyle \cos \leftrightarrow \sec ,}$ and swapping ${\displaystyle \tan \leftrightarrow \cot }$ gives the analogous equations for ${\displaystyle \csc ,\sec ,{\text{ and }}\cot ,}$ respectively.
So for example, by using the equality ${\textstyle \sin \left({\frac {\pi }{2}}-\theta \right)=\cos \theta ,}$ the equation ${\displaystyle \cos \theta =x}$ can be transformed into ${\textstyle \sin \left({\frac {\pi }{2}}-\theta \right)=x,}$ which allows for the solution to the equation ${\displaystyle \;\sin \varphi =x\;}$ (where ${\textstyle \varphi :={\frac {\pi }{2}}-\theta }$ ) to be used; that solution being: ${\displaystyle \varphi =(-1)^{k}\arcsin(x)+\pi k\;{\text{ for some }}k\in \mathbb {Z} ,}$ which becomes:

where using the fact that ${\displaystyle (-1)^{k}=(-1)^{-k}}$ and substituting ${\displaystyle h:=-k}$ proves that another solution to ${\displaystyle \;\cos \theta =x\;}$ is: The substitution ${\displaystyle \;\arcsin x={\frac {\pi }{2}}-\arccos x\;}$ may be used express the right hand side of the above formula in terms of ${\displaystyle \;\arccos x\;}$ instead of ${\displaystyle \;\arcsin x.\;}$
£#h5#£Equal identical trigonometric functions£#/h5#£
The table below shows how two angles ${\displaystyle \theta }$ and ${\displaystyle \varphi }$ must be related if their values under a given trigonometric function are equal or negatives of each other.


£#h5#£Relationships between trigonometric functions and inverse trigonometric functions£#/h5#£
Trigonometric functions of inverse trigonometric functions are tabulated below. A quick way to derive them is by considering the geometry of a right-angled triangle, with one side of length 1 and another side of length ${\displaystyle x,}$ then applying the Pythagorean theorem and definitions of the trigonometric ratios. Purely algebraic derivations are longer. It is worth noting that for arcsecant and arccosecant, the diagram assumes that ${\displaystyle x}$ is positive, and thus the result has to be corrected through the use of absolute values and the signum (sgn) operation.


£#h5#£Relationships among the inverse trigonometric functions£#/h5#£
Complementary angles:

${\displaystyle {\begin{aligned}\arccos(x)&={\frac {\pi }{2}}-\arcsin(x)\\[0.5em]\operatorname {arccot}(x)&={\frac {\pi }{2}}-\arctan(x)\\[0.5em]\operatorname {arccsc}(x)&={\frac {\pi }{2}}-\operatorname {arcsec}(x)\end{aligned}}}$
Negative arguments:

${\displaystyle {\begin{aligned}\arcsin(-x)&=-\arcsin(x)\\\arccos(-x)&=\pi -\arccos(x)\\\arctan(-x)&=-\arctan(x)\\\operatorname {arccot}(-x)&=\pi -\operatorname {arccot}(x)\\\operatorname {arcsec}(-x)&=\pi -\operatorname {arcsec}(x)\\\operatorname {arccsc}(-x)&=-\operatorname {arccsc}(x)\end{aligned}}}$
Reciprocal arguments:

${\displaystyle {\begin{aligned}\arccos \left({\frac {1}{x}}\right)&=\operatorname {arcsec}(x)\\[0.3em]\arcsin \left({\frac {1}{x}}\right)&=\operatorname {arccsc}(x)\\[0.3em]\arctan \left({\frac {1}{x}}\right)&={\frac {\pi }{2}}-\arctan(x)=\operatorname {arccot}(x)\,,{\text{ if }}x>0\\[0.3em]\arctan \left({\frac {1}{x}}\right)&=-{\frac {\pi }{2}}-\arctan(x)=\operatorname {arccot}(x)-\pi \,,{\text{ if }}x<0\\[0.3em]\operatorname {arccot} \left({\frac {1}{x}}\right)&={\frac {\pi }{2}}-\operatorname {arccot}(x)=\arctan(x)\,,{\text{ if }}x>0\\[0.3em]\operatorname {arccot} \left({\frac {1}{x}}\right)&={\frac {3\pi }{2}}-\operatorname {arccot}(x)=\pi +\arctan(x)\,,{\text{ if }}x<0\\[0.3em]\operatorname {arcsec} \left({\frac {1}{x}}\right)&=\arccos(x)\\[0.3em]\operatorname {arccsc} \left({\frac {1}{x}}\right)&=\arcsin(x)\end{aligned}}}$
Useful identities if one only has a fragment of a sine table:

${\displaystyle {\begin{aligned}\arccos(x)&=\arcsin \left({\sqrt {1-x^{2}}}\right)\,,{\text{ if }}0\leq x\leq 1{\text{ , from which you get }}\\\arccos &\left({\frac {1-x^{2}}{1+x^{2}}}\right)=\arcsin \left({\frac {2x}{1+x^{2}}}\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin &\left({\sqrt {1-x^{2}}}\right)={\frac {\pi }{2}}-\operatorname {sgn}(x)\arcsin(x)\\\arccos(x)&={\frac {1}{2}}\arccos \left(2x^{2}-1\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin(x)&={\frac {1}{2}}\arccos \left(1-2x^{2}\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin(x)&=\arctan \left({\frac {x}{\sqrt {1-x^{2}}}}\right)\\\arccos(x)&=\arctan \left({\frac {\sqrt {1-x^{2}}}{x}}\right)\\\arctan(x)&=\arcsin \left({\frac {x}{\sqrt {1+x^{2}}}}\right)\\\operatorname {arccot}(x)&=\arccos \left({\frac {x}{\sqrt {1+x^{2}}}}\right)\end{aligned}}}$
Whenever the square root of a complex number is used here, we choose the root with the positive real part (or positive imaginary part if the square was negative real).

A useful form that follows directly from the table above is

${\displaystyle \arctan \left(x\right)=\arccos \left({\sqrt {\frac {1}{1+x^{2}}}}\right)\,,{\text{ if }}x\geq 0}$ .
It is obtained by recognizing that ${\displaystyle \cos \left(\arctan \left(x\right)\right)={\sqrt {\frac {1}{1+x^{2}}}}=\cos \left(\arccos \left({\sqrt {\frac {1}{1+x^{2}}}}\right)\right)}$ .

From the half-angle formula, ${\displaystyle \tan \left({\tfrac {\theta }{2}}\right)={\tfrac {\sin(\theta )}{1+\cos(\theta )}}}$ , we get:

${\displaystyle {\begin{aligned}\arcsin(x)&=2\arctan \left({\frac {x}{1+{\sqrt {1-x^{2}}}}}\right)\\[0.5em]\arccos(x)&=2\arctan \left({\frac {\sqrt {1-x^{2}}}{1+x}}\right)\,,{\text{ if }}-1<x\leq 1\\[0.5em]\arctan(x)&=2\arctan \left({\frac {x}{1+{\sqrt {1+x^{2}}}}}\right)\end{aligned}}}$

£#h5#£Arctangent addition formula£#/h5#£
${\displaystyle \arctan(u)\pm \arctan(v)=\arctan \left({\frac {u\pm v}{1\mp uv}}\right){\pmod {\pi }}\,,\quad uv\neq 1\,.}$
This is derived from the tangent addition formula

${\displaystyle \tan(\alpha \pm \beta )={\frac {\tan(\alpha )\pm \tan(\beta )}{1\mp \tan(\alpha )\tan(\beta )}}\,,}$
by letting

${\displaystyle \alpha =\arctan(u)\,,\quad \beta =\arctan(v)\,.}$

£#h5#£In calculus£#/h5#£
£#h5#£Derivatives of inverse trigonometric functions£#/h5#£
The derivatives for complex values of z are as follows:

${\displaystyle {\begin{aligned}{\frac {d}{dz}}\arcsin(z)&{}={\frac {1}{\sqrt {1-z^{2}}}}\;;&z&{}\neq -1,+1\\{\frac {d}{dz}}\arccos(z)&{}=-{\frac {1}{\sqrt {1-z^{2}}}}\;;&z&{}\neq -1,+1\\{\frac {d}{dz}}\arctan(z)&{}={\frac {1}{1+z^{2}}}\;;&z&{}\neq -i,+i\\{\frac {d}{dz}}\operatorname {arccot}(z)&{}=-{\frac {1}{1+z^{2}}}\;;&z&{}\neq -i,+i\\{\frac {d}{dz}}\operatorname {arcsec}(z)&{}={\frac {1}{z^{2}{\sqrt {1-{\frac {1}{z^{2}}}}}}}\;;&z&{}\neq -1,0,+1\\{\frac {d}{dz}}\operatorname {arccsc}(z)&{}=-{\frac {1}{z^{2}{\sqrt {1-{\frac {1}{z^{2}}}}}}}\;;&z&{}\neq -1,0,+1\end{aligned}}}$
Only for real values of x:

${\displaystyle {\begin{aligned}{\frac {d}{dx}}\operatorname {arcsec}(x)&{}={\frac {1}{|x|{\sqrt {x^{2}-1}}}}\;;&|x|>1\\{\frac {d}{dx}}\operatorname {arccsc}(x)&{}=-{\frac {1}{|x|{\sqrt {x^{2}-1}}}}\;;&|x|>1\end{aligned}}}$
For a sample derivation: if ${\displaystyle \theta =\arcsin(x)}$ , we get:

${\displaystyle {\frac {d\arcsin(x)}{dx}}={\frac {d\theta }{d\sin(\theta )}}={\frac {d\theta }{\cos(\theta )\,d\theta }}={\frac {1}{\cos(\theta )}}={\frac {1}{\sqrt {1-\sin ^{2}(\theta )}}}={\frac {1}{\sqrt {1-x^{2}}}}}$

£#h5#£Expression as definite integrals£#/h5#£
Integrating the derivative and fixing the value at one point gives an expression for the inverse trigonometric function as a definite integral:

${\displaystyle {\begin{aligned}\arcsin(x)&{}=\int _{0}^{x}{\frac {1}{\sqrt {1-z^{2}}}}\,dz\;,&|x|&{}\leq 1\\\arccos(x)&{}=\int _{x}^{1}{\frac {1}{\sqrt {1-z^{2}}}}\,dz\;,&|x|&{}\leq 1\\\arctan(x)&{}=\int _{0}^{x}{\frac {1}{z^{2}+1}}\,dz\;,\\\operatorname {arccot}(x)&{}=\int _{x}^{\infty }{\frac {1}{z^{2}+1}}\,dz\;,\\\operatorname {arcsec}(x)&{}=\int _{1}^{x}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz=\pi +\int _{-x}^{-1}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz\;,&x&{}\geq 1\\\operatorname {arccsc}(x)&{}=\int _{x}^{\infty }{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz=\int _{-\infty }^{-x}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz\;,&x&{}\geq 1\\\end{aligned}}}$
When x equals 1, the integrals with limited domains are improper integrals, but still well-defined.


£#h5#£Infinite series£#/h5#£
Similar to the sine and cosine functions, the inverse trigonometric functions can also be calculated using power series, as follows. For arcsine, the series can be derived by expanding its derivative, ${\textstyle {\tfrac {1}{\sqrt {1-z^{2}}}}}$ , as a binomial series, and integrating term by term (using the integral definition as above). The series for arctangent can similarly be derived by expanding its derivative ${\textstyle {\frac {1}{1+z^{2}}}}$ in a geometric series, and applying the integral definition above (see Leibniz series).

${\displaystyle {\begin{aligned}\arcsin(z)&=z+\left({\frac {1}{2}}\right){\frac {z^{3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {z^{5}}{5}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {z^{7}}{7}}+\cdots \\[5pt]&=\sum _{n=0}^{\infty }{\frac {(2n-1)!!}{(2n)!!}}{\frac {z^{2n+1}}{2n+1}}\\[5pt]&=\sum _{n=0}^{\infty }{\frac {(2n)!}{(2^{n}n!)^{2}}}{\frac {z^{2n+1}}{2n+1}}\,;\qquad |z|\leq 1\end{aligned}}}$
${\displaystyle \arctan(z)=z-{\frac {z^{3}}{3}}+{\frac {z^{5}}{5}}-{\frac {z^{7}}{7}}+\cdots =\sum _{n=0}^{\infty }{\frac {(-1)^{n}z^{2n+1}}{2n+1}}\,;\qquad |z|\leq 1\qquad z\neq i,-i}$
Series for the other inverse trigonometric functions can be given in terms of these according to the relationships given above. For example, ${\displaystyle \arccos(x)=\pi /2-\arcsin(x)}$ , ${\displaystyle \operatorname {arccsc}(x)=\arcsin(1/x)}$ , and so on. Another series is given by:

${\displaystyle 2\left(\arcsin \left({\frac {x}{2}}\right)\right)^{2}=\sum _{n=1}^{\infty }{\frac {x^{2n}}{n^{2}{\binom {2n}{n}}}}.}$
Leonhard Euler found a series for the arctangent that converges more quickly than its Taylor series:

${\displaystyle \arctan(z)={\frac {z}{1+z^{2}}}\sum _{n=0}^{\infty }\prod _{k=1}^{n}{\frac {2kz^{2}}{(2k+1)(1+z^{2})}}.}$
(The term in the sum for n = 0 is the empty product, so is 1.)

Alternatively, this can be expressed as

${\displaystyle \arctan(z)=\sum _{n=0}^{\infty }{\frac {2^{2n}(n!)^{2}}{(2n+1)!}}{\frac {z^{2n+1}}{(1+z^{2})^{n+1}}}.}$
Another series for the arctangent function is given by

${\displaystyle \arctan(z)=i\sum _{n=1}^{\infty }{\frac {1}{2n-1}}\left({\frac {1}{(1+2i/z)^{2n-1}}}-{\frac {1}{(1-2i/z)^{2n-1}}}\right),}$
where ${\displaystyle i={\sqrt {-1}}}$ is the imaginary unit.


£#h5#£Continued fractions for arctangent£#/h5#£
Two alternatives to the power series for arctangent are these generalized continued fractions:

${\displaystyle \arctan(z)={\frac {z}{1+{\cfrac {(1z)^{2}}{3-1z^{2}+{\cfrac {(3z)^{2}}{5-3z^{2}+{\cfrac {(5z)^{2}}{7-5z^{2}+{\cfrac {(7z)^{2}}{9-7z^{2}+\ddots }}}}}}}}}}={\frac {z}{1+{\cfrac {(1z)^{2}}{3+{\cfrac {(2z)^{2}}{5+{\cfrac {(3z)^{2}}{7+{\cfrac {(4z)^{2}}{9+\ddots }}}}}}}}}}}$
The second of these is valid in the cut complex plane. There are two cuts, from −i to the point at infinity, going down the imaginary axis, and from i to the point at infinity, going up the same axis. It works best for real numbers running from −1 to 1. The partial denominators are the odd natural numbers, and the partial numerators (after the first) are just (nz)2, with each perfect square appearing once. The first was developed by Leonhard Euler; the second by Carl Friedrich Gauss utilizing the Gaussian hypergeometric series.


£#h5#£Indefinite integrals of inverse trigonometric functions£#/h5#£
For real and complex values of z:

${\displaystyle {\begin{aligned}\int \arcsin(z)\,dz&{}=z\,\arcsin(z)+{\sqrt {1-z^{2}}}+C\\\int \arccos(z)\,dz&{}=z\,\arccos(z)-{\sqrt {1-z^{2}}}+C\\\int \arctan(z)\,dz&{}=z\,\arctan(z)-{\frac {1}{2}}\ln \left(1+z^{2}\right)+C\\\int \operatorname {arccot}(z)\,dz&{}=z\,\operatorname {arccot}(z)+{\frac {1}{2}}\ln \left(1+z^{2}\right)+C\\\int \operatorname {arcsec}(z)\,dz&{}=z\,\operatorname {arcsec}(z)-\ln \left[z\left(1+{\sqrt {\frac {z^{2}-1}{z^{2}}}}\right)\right]+C\\\int \operatorname {arccsc}(z)\,dz&{}=z\,\operatorname {arccsc}(z)+\ln \left[z\left(1+{\sqrt {\frac {z^{2}-1}{z^{2}}}}\right)\right]+C\end{aligned}}}$
For real x ≥ 1:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\ln \left(x+{\sqrt {x^{2}-1}}\right)+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\ln \left(x+{\sqrt {x^{2}-1}}\right)+C\end{aligned}}}$
For all real x not between -1 and 1:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\operatorname {sgn}(x)\ln \left|x+{\sqrt {x^{2}-1}}\right|+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\operatorname {sgn}(x)\ln \left|x+{\sqrt {x^{2}-1}}\right|+C\end{aligned}}}$
The absolute value is necessary to compensate for both negative and positive values of the arcsecant and arccosecant functions. The signum function is also necessary due to the absolute values in the derivatives of the two functions, which create two different solutions for positive and negative values of x. These can be further simplified using the logarithmic definitions of the inverse hyperbolic functions:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\operatorname {arcosh} (|x|)+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\operatorname {arcosh} (|x|)+C\\\end{aligned}}}$
The absolute value in the argument of the arcosh function creates a negative half of its graph, making it identical to the signum logarithmic function shown above.

All of these antiderivatives can be derived using integration by parts and the simple derivative forms shown above.


£#h5#£Example£#/h5#£
Using ${\displaystyle \int u\,dv=uv-\int v\,du}$ (i.e. integration by parts), set

${\displaystyle {\begin{aligned}u&=\arcsin(x)&dv&=dx\\du&={\frac {dx}{\sqrt {1-x^{2}}}}&v&=x\end{aligned}}}$
Then

${\displaystyle \int \arcsin(x)\,dx=x\arcsin(x)-\int {\frac {x}{\sqrt {1-x^{2}}}}\,dx,}$
which by the simple substitution ${\displaystyle w=1-x^{2},\ dw=-2x\,dx}$ yields the final result:

${\displaystyle \int \arcsin(x)\,dx=x\arcsin(x)+{\sqrt {1-x^{2}}}+C}$

£#h5#£Extension to complex plane£#/h5#£
Since the inverse trigonometric functions are analytic functions, they can be extended from the real line to the complex plane. This results in functions with multiple sheets and branch points. One possible way of defining the extension is:

${\displaystyle \arctan(z)=\int _{0}^{z}{\frac {dx}{1+x^{2}}}\quad z\neq -i,+i}$
where the part of the imaginary axis which does not lie strictly between the branch points (−i and +i) is the branch cut between the principal sheet and other sheets. The path of the integral must not cross a branch cut. For z not on a branch cut, a straight line path from 0 to z is such a path. For z on a branch cut, the path must approach from Re[x] > 0 for the upper branch cut and from Re[x] < 0 for the lower branch cut.

The arcsine function may then be defined as:

${\displaystyle \arcsin(z)=\arctan \left({\frac {z}{\sqrt {1-z^{2}}}}\right)\quad z\neq -1,+1}$
where (the square-root function has its cut along the negative real axis and) the part of the real axis which does not lie strictly between −1 and +1 is the branch cut between the principal sheet of arcsin and other sheets;

${\displaystyle \arccos(z)={\frac {\pi }{2}}-\arcsin(z)\quad z\neq -1,+1}$
which has the same cut as arcsin;

${\displaystyle \operatorname {arccot}(z)={\frac {\pi }{2}}-\arctan(z)\quad z\neq -i,i}$
which has the same cut as arctan;

${\displaystyle \operatorname {arcsec}(z)=\arccos \left({\frac {1}{z}}\right)\quad z\neq -1,0,+1}$
where the part of the real axis between −1 and +1 inclusive is the cut between the principal sheet of arcsec and other sheets;

${\displaystyle \operatorname {arccsc}(z)=\arcsin \left({\frac {1}{z}}\right)\quad z\neq -1,0,+1}$
which has the same cut as arcsec.


£#h5#£Logarithmic forms£#/h5#£
These functions may also be expressed using complex logarithms. This extends their domains to the complex plane in a natural fashion. The following identities for principal values of the functions hold everywhere that they are defined, even on their branch cuts.

${\displaystyle {\begin{aligned}\arcsin(z)&{}=-i\ln \left({\sqrt {1-z^{2}}}+iz\right)=i\ln \left({\sqrt {1-z^{2}}}-iz\right)&{}=\operatorname {arccsc} \left({\frac {1}{z}}\right)\\[10pt]\arccos(z)&{}=-i\ln \left(i{\sqrt {1-z^{2}}}+z\right)={\frac {\pi }{2}}-\arcsin(z)&{}=\operatorname {arcsec} \left({\frac {1}{z}}\right)\\[10pt]\arctan(z)&{}=-{\frac {i}{2}}\ln \left({\frac {i-z}{i+z}}\right)=-{\frac {i}{2}}\ln \left({\frac {1+iz}{1-iz}}\right)&{}=\operatorname {arccot} \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arccot}(z)&{}=-{\frac {i}{2}}\ln \left({\frac {z+i}{z-i}}\right)=-{\frac {i}{2}}\ln \left({\frac {iz-1}{iz+1}}\right)&{}=\arctan \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arcsec}(z)&{}=-i\ln \left(i{\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {1}{z}}\right)={\frac {\pi }{2}}-\operatorname {arccsc}(z)&{}=\arccos \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arccsc}(z)&{}=-i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {i}{z}}\right)=i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}-{\frac {i}{z}}\right)&{}=\arcsin \left({\frac {1}{z}}\right)\end{aligned}}}$

£#h5#£Generalization£#/h5#£
Because all of the inverse trigonometric functions output an angle of a right triangle, they can be generalized by using Euler's formula to form a right triangle in the complex plane. Algebraically, this gives us:

${\displaystyle ce^{\theta i}=c\cos(\theta )+ci\sin(\theta )}$
or

${\displaystyle ce^{\theta i}=a+bi}$
where ${\displaystyle a}$ is the adjacent side, ${\displaystyle b}$ is the opposite side, and ${\displaystyle c}$ is the hypotenuse. From here, we can solve for ${\displaystyle \theta }$ .

${\displaystyle {\begin{aligned}e^{\ln(c)+\theta i}&=a+bi\\\ln c+\theta i&=\ln(a+bi)\\\theta &=\operatorname {Im} \left(\ln(a+bi)\right)\end{aligned}}}$
or

${\displaystyle \theta =-i\ln \left({\frac {a+bi}{c}}\right)=i\ln \left({\frac {c}{a+bi}}\right)}$
Simply taking the imaginary part works for any real-valued ${\displaystyle a}$ and ${\displaystyle b}$ , but if ${\displaystyle a}$ or ${\displaystyle b}$ is complex-valued, we have to use the final equation so that the real part of the result isn't excluded. Since the length of the hypotenuse doesn't change the angle, ignoring the real part of ${\displaystyle \ln(a+bi)}$ also removes ${\displaystyle c}$ from the equation. In the final equation, we see that the angle of the triangle in the complex plane can be found by inputting the lengths of each side. By setting one of the three sides equal to 1 and one of the remaining sides equal to our input ${\displaystyle z}$ , we obtain a formula for one of the inverse trig functions, for a total of six equations. Because the inverse trig functions require only one input, we must put the final side of the triangle in terms of the other two using the Pythagorean Theorem relation

${\displaystyle a^{2}+b^{2}=c^{2}}$
The table below shows the values of a, b, and c for each of the inverse trig functions and the equivalent expressions for ${\displaystyle \theta }$ that result from plugging the values into the equations above and simplifying.

${\displaystyle {\begin{aligned}&a&&b&&c&&\theta &&\theta _{\text{simplified}}&&\theta _{a,b\in \mathbb {R} }\\\arcsin(z)\ \ &{\sqrt {1-z^{2}}}&&z&&1&&-i\ln \left({\frac {{\sqrt {1-z^{2}}}+zi}{1}}\right)&&=-i\ln \left({\sqrt {1-z^{2}}}+zi\right)&&\operatorname {Im} \left(\ln \left({\sqrt {1-z^{2}}}+zi\right)\right)\\\arccos(z)\ \ &z&&{\sqrt {1-z^{2}}}&&1&&-i\ln \left({\frac {z+i{\sqrt {1-z^{2}}}}{1}}\right)&&=-i\ln \left(z+{\sqrt {z^{2}-1}}\right)&&\operatorname {Im} \left(\ln \left(z+{\sqrt {z^{2}-1}}\right)\right)\\\arctan(z)\ \ &1&&z&&{\sqrt {1+z^{2}}}&&i\ln \left({\frac {\sqrt {1+z^{2}}}{1+zi}}\right)&&={\frac {i}{2}}\ln \left({\frac {i+z}{i-z}}\right)&&\operatorname {Im} \left(\ln \left(1+zi\right)\right)\\\operatorname {arccot}(z)\ \ &z&&1&&{\sqrt {z^{2}+1}}&&i\ln \left({\frac {\sqrt {z^{2}+1}}{z+i}}\right)&&={\frac {i}{2}}\ln \left({\frac {z-i}{z+i}}\right)&&\operatorname {Im} \left(\ln \left(z+i\right)\right)\\\operatorname {arcsec}(z)\ \ &1&&{\sqrt {z^{2}-1}}&&z&&-i\ln \left({\frac {1+i{\sqrt {z^{2}-1}}}{z}}\right)&&=-i\ln \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z^{2}}}-1}}\right)&&\operatorname {Im} \left(\ln \left(1+{\sqrt {1-z^{2}}}\right)\right)\\\operatorname {arccsc}(z)\ \ &{\sqrt {z^{2}-1}}&&1&&z&&-i\ln \left({\frac {{\sqrt {z^{2}-1}}+i}{z}}\right)&&=-i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {i}{z}}\right)&&\operatorname {Im} \left(\ln \left({\sqrt {z^{2}-1}}+i\right)\right)\\\end{aligned}}}$
In this sense, all of the inverse trig functions can be thought of as specific cases of the complex-valued log function. Since this definition works for any complex-valued ${\displaystyle z}$ , this definition allows for hyperbolic angles as outputs and can be used to further define the inverse hyperbolic functions. Elementary proofs of the relations may also proceed via expansion to exponential forms of the trigonometric functions.


£#h5#£Example proof£#/h5#£
${\displaystyle {\begin{aligned}\sin(\phi )&=z\\\phi &=\arcsin(z)\end{aligned}}}$
Using the exponential definition of sine, and letting ${\displaystyle \xi =e^{\phi i},}$

${\displaystyle {\begin{aligned}z&={\frac {e^{\phi i}-e^{-\phi i}}{2i}}\\[10mu]2iz&=\xi -{\frac {1}{\xi }}\\[5mu]0&=\xi ^{2}-2i\xi z-1\\[5mu]\xi &=iz\pm {\sqrt {1-z^{2}}}\\[5mu]\phi &=-i\ln \left(iz\pm {\sqrt {1-z^{2}}}\right)\end{aligned}}}$
(the positive branch is chosen)

${\displaystyle \phi =\arcsin(z)=-i\ln \left(iz+{\sqrt {1-z^{2}}}\right)}$

£#h5#£Applications£#/h5#£
£#h5#£Finding the angle of a right triangle£#/h5#£
Inverse trigonometric functions are useful when trying to determine the remaining two angles of a right triangle when the lengths of the sides of the triangle are known. Recalling the right-triangle definitions of sine and cosine, it follows that

${\displaystyle \theta =\arcsin \left({\frac {\text{opposite}}{\text{hypotenuse}}}\right)=\arccos \left({\frac {\text{adjacent}}{\text{hypotenuse}}}\right).}$
Often, the hypotenuse is unknown and would need to be calculated before using arcsine or arccosine using the Pythagorean Theorem: ${\displaystyle a^{2}+b^{2}=h^{2}}$ where ${\displaystyle h}$ is the length of the hypotenuse. Arctangent comes in handy in this situation, as the length of the hypotenuse is not needed.

${\displaystyle \theta =\arctan \left({\frac {\text{opposite}}{\text{adjacent}}}\right)\,.}$
For example, suppose a roof drops 8 feet as it runs out 20 feet. The roof makes an angle θ with the horizontal, where θ may be computed as follows:

${\displaystyle \theta =\arctan \left({\frac {\text{opposite}}{\text{adjacent}}}\right)=\arctan \left({\frac {\text{rise}}{\text{run}}}\right)=\arctan \left({\frac {8}{20}}\right)\approx 21.8^{\circ }\,.}$

£#h5#£In computer science and engineering£#/h5#£
£#h5#£Two-argument variant of arctangent£#/h5#£

The two-argument atan2 function computes the arctangent of y / x given y and x, but with a range of (−π, π]. In other words, atan2(y, x) is the angle between the positive x-axis of a plane and the point (x, y) on it, with positive sign for counter-clockwise angles (upper half-plane, y > 0), and negative sign for clockwise angles (lower half-plane, y < 0). It was first introduced in many computer programming languages, but it is now also common in other fields of science and engineering.

In terms of the standard arctan function, that is with range of (−π/2, π/2), it can be expressed as follows:

${\displaystyle \operatorname {atan2} (y,x)={\begin{cases}\arctan \left({\frac {y}{x}}\right)&\quad x>0\\\arctan \left({\frac {y}{x}}\right)+\pi &\quad y\geq 0,\;x<0\\\arctan \left({\frac {y}{x}}\right)-\pi &\quad y<0,\;x<0\\{\frac {\pi }{2}}&\quad y>0,\;x=0\\-{\frac {\pi }{2}}&\quad y<0,\;x=0\\{\text{undefined}}&\quad y=0,\;x=0\end{cases}}}$
It also equals the principal value of the argument of the complex number x + iy.

This limited version of the function above may also be defined using the tangent half-angle formulae as follows:

${\displaystyle \operatorname {atan2} (y,x)=2\arctan \left({\frac {y}{{\sqrt {x^{2}+y^{2}}}+x}}\right)}$
provided that either x > 0 or y ≠ 0. However this fails if given x ≤ 0 and y = 0 so the expression is unsuitable for computational use.

The above argument order (y, x) seems to be the most common, and in particular is used in ISO standards such as the C programming language, but a few authors may use the opposite convention (x, y) so some caution is warranted. These variations are detailed at atan2.


£#h5#£Arctangent function with location parameter£#/h5#£
In many applications the solution ${\displaystyle y}$ of the equation ${\displaystyle x=\tan(y)}$ is to come as close as possible to a given value ${\displaystyle -\infty <\eta <\infty }$ . The adequate solution is produced by the parameter modified arctangent function

${\displaystyle y=\arctan _{\eta }(x):=\arctan(x)+\pi \,\operatorname {rni} \left({\frac {\eta -\arctan(x)}{\pi }}\right)\,.}$
The function ${\displaystyle \operatorname {rni} }$ rounds to the nearest integer.


£#h5#£Numerical accuracy£#/h5#£
For angles near 0 and π, arccosine is ill-conditioned and will thus calculate the angle with reduced accuracy in a computer implementation (due to the limited number of digits). Similarly, arcsine is inaccurate for angles near −π/2 and π/2.


£#h5#£See also£#/h5#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Abramowitz, Milton; Stegun, Irene A., eds. (1972). Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables. New York: Dover Publications. ISBN 978-0-486-61272-0.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Inverse Tangent". MathWorld.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Trigonometric Functions £#/li#££#/ul#£




£#h3#£Arcsine£#/h3#£

In mathematics, the inverse trigonometric functions (occasionally also called arcus functions, antitrigonometric functions or cyclometric functions) are the inverse functions of the trigonometric functions (with suitably restricted domains). Specifically, they are the inverses of the sine, cosine, tangent, cotangent, secant, and cosecant functions, and are used to obtain an angle from any of the angle's trigonometric ratios. Inverse trigonometric functions are widely used in engineering, navigation, physics, and geometry.


£#h5#£Notation£#/h5#£
Several notations for the inverse trigonometric functions exist. The most common convention is to name inverse trigonometric functions using an arc- prefix: arcsin(x), arccos(x), arctan(x), etc. (This convention is used throughout this article.) This notation arises from the following geometric relationships: when measuring in radians, an angle of θ radians will correspond to an arc whose length is rθ, where r is the radius of the circle. Thus in the unit circle, "the arc whose cosine is x" is the same as "the angle whose cosine is x", because the length of the arc of the circle in radii is the same as the measurement of the angle in radians. In computer programming languages, the inverse trigonometric functions are often called by the abbreviated forms asin, acos, atan.

The notations sin−1(x), cos−1(x), tan−1(x), etc., as introduced by John Herschel in 1813, are often used as well in English-language sources, much more than the also established sin[−1](x), cos[−1](x), tan[−1](x),—conventions consistent with the notation of an inverse function, that is useful e.g. to define the multivalued version of each inverse trigonometric function: e.g. ${\displaystyle \tan ^{-1}(x)=\{\arctan(x)+\pi k\mid k\in \mathbb {Z} \}}$ . However, this might appear to conflict logically with the common semantics for expressions such as sin2(x) (although only sin2 x, without parentheses, is the really common one), which refer to numeric power rather than function composition, and therefore may result in confusion between multiplicative inverse or reciprocal and compositional inverse. The confusion is somewhat mitigated by the fact that each of the reciprocal trigonometric functions has its own name—for example, (cos(x))−1 = sec(x). Nevertheless, certain authors advise against using it for its ambiguity. Another precarious convention used by a tiny number of authors is to use an uppercase first letter, along with a −1 superscript: Sin−1(x), Cos−1(x), Tan−1(x), etc. Although its intention is to avoid confusion with the multiplicative inverse, which should be represented by sin−1(x), cos−1(x), etc., or, better, by sin−1 x, cos−1 x, etc., it in turn creates yet another major source of ambiguity: especially in light of the fact that many popular high-level programming languages (e.g. Wolfram's Mathematica, and University of Sydney's MAGMA) use those very same capitalised representations for the standard trig functions; whereas others (Python (ie SymPy and NumPy), Matlab, MAPLE etc) use lower-case.

Hence, since 2009, the ISO 80000-2 standard has specified solely the "arc" prefix for the inverse functions.


£#h5#£Basic concepts£#/h5#£
£#h5#£Principal values£#/h5#£
Since none of the six trigonometric functions are one-to-one, they must be restricted in order to have inverse functions. Therefore, the result ranges of the inverse functions are proper (i.e. strict) subsets of the domains of the original functions.

For example, using function in the sense of multivalued functions, just as the square root function ${\displaystyle y={\sqrt {x}}}$ could be defined from ${\displaystyle y^{2}=x,}$ the function ${\displaystyle y=\arcsin(x)}$ is defined so that ${\displaystyle \sin(y)=x.}$ For a given real number ${\displaystyle x,}$ with ${\displaystyle -1\leq x\leq 1,}$ there are multiple (in fact, countably infinitely many) numbers ${\displaystyle y}$ such that ${\displaystyle \sin(y)=x}$ ; for example, ${\displaystyle \sin(0)=0,}$ but also ${\displaystyle \sin(\pi )=0,}$ ${\displaystyle \sin(2\pi )=0,}$ etc. When only one value is desired, the function may be restricted to its principal branch. With this restriction, for each ${\displaystyle x}$ in the domain, the expression ${\displaystyle \arcsin(x)}$ will evaluate only to a single value, called its principal value. These properties apply to all the inverse trigonometric functions.

The principal inverses are listed in the following table.

Note: Some authors define the range of arcsecant to be ${\textstyle (0\leq y<{\frac {\pi }{2}}{\text{ or }}\pi \leq y<{\frac {3\pi }{2}})}$ , because the tangent function is nonnegative on this domain. This makes some computations more consistent. For example, using this range, ${\displaystyle \tan(\operatorname {arcsec}(x))={\sqrt {x^{2}-1}},}$ whereas with the range ${\textstyle (0\leq y<{\frac {\pi }{2}}{\text{ or }}{\frac {\pi }{2}}<y\leq \pi )}$ , we would have to write ${\displaystyle \tan(\operatorname {arcsec}(x))=\pm {\sqrt {x^{2}-1}},}$ since tangent is nonnegative on ${\textstyle 0\leq y<{\frac {\pi }{2}},}$ but nonpositive on ${\textstyle {\frac {\pi }{2}}<y\leq \pi .}$ For a similar reason, the same authors define the range of arccosecant to be ${\textstyle (-\pi <y\leq -{\frac {\pi }{2}}}$ or ${\textstyle 0<y\leq {\frac {\pi }{2}}).}$

If ${\displaystyle x}$ is allowed to be a complex number, then the range of ${\displaystyle y}$ applies only to its real part.

The table below displays names and domains of the inverse trigonometric functions along with the range of their usual principal values in radians.

The symbol ${\displaystyle \mathbb {R} =(-\infty ,\infty )}$ denotes the set of all real numbers and ${\displaystyle \mathbb {Z} =\{\ldots ,\,-2,\,-1,\,0,\,1,\,2,\,\ldots \}}$ denotes the set of all integers. The set of all integer multiples of ${\displaystyle \pi }$ is denoted by

The symbol ${\displaystyle \,\setminus \,}$ denotes set subtraction so that, for instance, ${\displaystyle \mathbb {R} \setminus (-1,1)=(-\infty ,-1]\cup [1,\infty )}$ is the set of points in ${\displaystyle \mathbb {R} }$ (that is, real numbers) that are not in the interval ${\displaystyle (-1,1).}$

The Minkowski sum notation ${\textstyle \pi \mathbb {Z} +(0,\pi )}$ and ${\displaystyle \pi \mathbb {Z} +{\bigl (}{-{\tfrac {\pi }{2}}},{\tfrac {\pi }{2}}{\bigr )}}$ that is used above to concisely write the domains of ${\displaystyle \cot ,\csc ,\tan ,{\text{ and }}\sec }$ is now explained.

Domain of cotangent ${\displaystyle \cot }$ and cosecant ${\displaystyle \csc }$ : The domains of ${\displaystyle \,\cot \,}$ and ${\displaystyle \,\csc \,}$ are the same. They are the set of all angles ${\displaystyle \theta }$ at which ${\displaystyle \sin \theta \neq 0,}$ i.e. all real numbers that are not of the form ${\displaystyle \pi n}$ for some integer ${\displaystyle n,}$

Domain of tangent ${\displaystyle \tan }$ and secant ${\displaystyle \sec }$ : The domains of ${\displaystyle \,\tan \,}$ and ${\displaystyle \,\sec \,}$ are the same. They are the set of all angles ${\displaystyle \theta }$ at which ${\displaystyle \cos \theta \neq 0,}$


£#h5#£Solutions to elementary trigonometric equations£#/h5#£
Each of the trigonometric functions is periodic in the real part of its argument, running through all its values twice in each interval of ${\displaystyle 2\pi }$ :

£#ul#££#li#£Sine and cosecant begin their period at ${\textstyle 2\pi k-{\frac {\pi }{2}}}$ (where ${\displaystyle k}$ is an integer), finish it at ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ and then reverse themselves over ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ to ${\displaystyle 2\pi k+{\frac {3\pi }{2}}.}$ £#/li#£ £#li#£Cosine and secant begin their period at ${\displaystyle 2\pi k,}$ finish it at ${\displaystyle 2\pi k+\pi .}$ and then reverse themselves over ${\displaystyle 2\pi k+\pi }$ to ${\displaystyle 2\pi k+2\pi .}$ £#/li#£ £#li#£Tangent begins its period at ${\textstyle 2\pi k-{\frac {\pi }{2}},}$ finishes it at ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ and then repeats it (forward) over ${\textstyle 2\pi k+{\frac {\pi }{2}}}$ to ${\textstyle 2\pi k+{\frac {3\pi }{2}}.}$ £#/li#£ £#li#£Cotangent begins its period at ${\displaystyle 2\pi k,}$ finishes it at ${\displaystyle 2\pi k+\pi ,}$ and then repeats it (forward) over ${\displaystyle 2\pi k+\pi }$ to ${\displaystyle 2\pi k+2\pi .}$ £#/li#££#/ul#£
This periodicity is reflected in the general inverses, where ${\displaystyle k}$ is some integer.

The following table shows how inverse trigonometric functions may be used to solve equalities involving the six standard trigonometric functions. It is assumed that the given values ${\displaystyle \theta ,}$ ${\displaystyle r,}$ ${\displaystyle s,}$ ${\displaystyle x,}$ and ${\displaystyle y}$ all lie within appropriate ranges so that the relevant expressions below are well-defined. Note that "for some ${\displaystyle k\in \mathbb {Z} }$ " is just another way of saying "for some integer ${\displaystyle k.}$ "

The symbol ${\displaystyle \,\iff \,}$ is logical equality. The expression "LHS ${\displaystyle \,\iff \,}$ RHS" indicates that either (a) the left hand side (i.e. LHS) and right hand side (i.e. RHS) are both true, or else (b) the left hand side and right hand side are both false; there is no option (c) (e.g. it is not possible for the LHS statement to be true and also simultaneously for the RHS statement to false), because otherwise "LHS ${\displaystyle \,\iff \,}$ RHS" would not have been written (see this footnote for an example illustrating this concept).

For example, if ${\displaystyle \cos \theta =-1}$ then ${\displaystyle \theta =\pi +2\pi k=-\pi +2\pi (1+k)}$ for some ${\displaystyle k\in \mathbb {Z} .}$ While if ${\displaystyle \sin \theta =\pm 1}$ then ${\textstyle \theta ={\frac {\pi }{2}}+\pi k=-{\frac {\pi }{2}}+\pi (k+1)}$ for some ${\displaystyle k\in \mathbb {Z} ,}$ where ${\displaystyle k}$ will be even if ${\displaystyle \sin \theta =1}$ and it will be odd if ${\displaystyle \sin \theta =-1.}$ The equations ${\displaystyle \sec \theta =-1}$ and ${\displaystyle \csc \theta =\pm 1}$ have the same solutions as ${\displaystyle \cos \theta =-1}$ and ${\displaystyle \sin \theta =\pm 1,}$ respectively. In all equations above except for those just solved (i.e. except for ${\displaystyle \sin }$ / ${\displaystyle \csc \theta =\pm 1}$ and ${\displaystyle \cos }$ / ${\displaystyle \sec \theta =-1}$ ), the integer ${\displaystyle k}$ in the solution's formula is uniquely determined by ${\displaystyle \theta }$ (for fixed ${\displaystyle r,s,x,}$ and ${\displaystyle y}$ ).

Detailed example and explanation of the "plus or minus" symbol ${\displaystyle \pm }$
The solutions to ${\displaystyle \cos \theta =x}$ and ${\displaystyle \sec \theta =x}$ involve the "plus or minus" symbol ${\displaystyle \,\pm ,\,}$ whose meaning is now clarified. Only the solution to ${\displaystyle \cos \theta =x}$ will be discussed since the discussion for ${\displaystyle \sec \theta =x}$ is the same. We are given ${\displaystyle x}$ between ${\displaystyle -1\leq x\leq 1}$ and we know that there is an angle ${\displaystyle \theta }$ in some give interval that satisfies ${\displaystyle \cos \theta =x.}$ We want to find this ${\displaystyle \theta .}$ The formula for the solution involves:

If ${\displaystyle \,\arccos x=0\,}$ (which only happens when ${\displaystyle x=1}$ ) then ${\displaystyle \,+\arccos x=0\,}$ and ${\displaystyle \,-\arccos x=0\,}$ so either way, ${\displaystyle \,\pm \arccos x\,}$ can only be equal to ${\displaystyle 0.}$ But if ${\displaystyle \,\arccos x\neq 0,\,}$ which will now be assumed, then the solution to ${\displaystyle \cos \theta =x,}$ which is written above as is shorthand for the following statement:
Either £#ul#££#li#£ ${\displaystyle \,\theta =\arccos x+2\pi k\,}$ for some integer ${\displaystyle k,}$
or else£#/li#£ £#li#£ ${\displaystyle \,\theta =-\arccos x+2\pi k\,}$ for some integer ${\displaystyle k.}$ £#/li#££#/ul#£
Because ${\displaystyle \,\arccos x\neq 0\,}$ and ${\displaystyle \,0<\arccos x\leq \pi \,}$ exactly one of these two equalities can hold. Additional information about ${\displaystyle \theta }$ is needed to determine which one holds. For example, suppose that ${\displaystyle x=0}$ and that all that is known about ${\displaystyle \theta }$ is that ${\displaystyle \,-\pi \leq \theta \leq \pi \,}$ (and nothing more is known). Then

and moreover, in this particular case ${\displaystyle k=0}$ (for both the ${\displaystyle \,+\,}$ case and the ${\displaystyle \,-\,}$ case) and so consequently, This means that ${\displaystyle \theta }$ could be either ${\displaystyle \,\pi /2\,}$ or ${\displaystyle \,-\pi /2.}$ Without additional information it is not possible to determine which of these values ${\displaystyle \theta }$ has. An example of some additional information that could determine the value of ${\displaystyle \theta }$ would be knowing that the angle is above the ${\displaystyle x}$ -axis(in which case ${\displaystyle \theta =\pi /2}$ ) or alternatively, knowing that it is below the ${\displaystyle x}$ -axis(in which case ${\displaystyle \theta =-\pi /2}$ ).
Transforming equations

The equations above can be transformed by using the reflection and shift identities:

These formulas imply, in particular, that the following hold:

where swapping ${\displaystyle \sin \leftrightarrow \csc ,}$ swapping ${\displaystyle \cos \leftrightarrow \sec ,}$ and swapping ${\displaystyle \tan \leftrightarrow \cot }$ gives the analogous equations for ${\displaystyle \csc ,\sec ,{\text{ and }}\cot ,}$ respectively.
So for example, by using the equality ${\textstyle \sin \left({\frac {\pi }{2}}-\theta \right)=\cos \theta ,}$ the equation ${\displaystyle \cos \theta =x}$ can be transformed into ${\textstyle \sin \left({\frac {\pi }{2}}-\theta \right)=x,}$ which allows for the solution to the equation ${\displaystyle \;\sin \varphi =x\;}$ (where ${\textstyle \varphi :={\frac {\pi }{2}}-\theta }$ ) to be used; that solution being: ${\displaystyle \varphi =(-1)^{k}\arcsin(x)+\pi k\;{\text{ for some }}k\in \mathbb {Z} ,}$ which becomes:

where using the fact that ${\displaystyle (-1)^{k}=(-1)^{-k}}$ and substituting ${\displaystyle h:=-k}$ proves that another solution to ${\displaystyle \;\cos \theta =x\;}$ is: The substitution ${\displaystyle \;\arcsin x={\frac {\pi }{2}}-\arccos x\;}$ may be used express the right hand side of the above formula in terms of ${\displaystyle \;\arccos x\;}$ instead of ${\displaystyle \;\arcsin x.\;}$
£#h5#£Equal identical trigonometric functions£#/h5#£
The table below shows how two angles ${\displaystyle \theta }$ and ${\displaystyle \varphi }$ must be related if their values under a given trigonometric function are equal or negatives of each other.


£#h5#£Relationships between trigonometric functions and inverse trigonometric functions£#/h5#£
Trigonometric functions of inverse trigonometric functions are tabulated below. A quick way to derive them is by considering the geometry of a right-angled triangle, with one side of length 1 and another side of length ${\displaystyle x,}$ then applying the Pythagorean theorem and definitions of the trigonometric ratios. Purely algebraic derivations are longer. It is worth noting that for arcsecant and arccosecant, the diagram assumes that ${\displaystyle x}$ is positive, and thus the result has to be corrected through the use of absolute values and the signum (sgn) operation.


£#h5#£Relationships among the inverse trigonometric functions£#/h5#£
Complementary angles:

${\displaystyle {\begin{aligned}\arccos(x)&={\frac {\pi }{2}}-\arcsin(x)\\[0.5em]\operatorname {arccot}(x)&={\frac {\pi }{2}}-\arctan(x)\\[0.5em]\operatorname {arccsc}(x)&={\frac {\pi }{2}}-\operatorname {arcsec}(x)\end{aligned}}}$
Negative arguments:

${\displaystyle {\begin{aligned}\arcsin(-x)&=-\arcsin(x)\\\arccos(-x)&=\pi -\arccos(x)\\\arctan(-x)&=-\arctan(x)\\\operatorname {arccot}(-x)&=\pi -\operatorname {arccot}(x)\\\operatorname {arcsec}(-x)&=\pi -\operatorname {arcsec}(x)\\\operatorname {arccsc}(-x)&=-\operatorname {arccsc}(x)\end{aligned}}}$
Reciprocal arguments:

${\displaystyle {\begin{aligned}\arccos \left({\frac {1}{x}}\right)&=\operatorname {arcsec}(x)\\[0.3em]\arcsin \left({\frac {1}{x}}\right)&=\operatorname {arccsc}(x)\\[0.3em]\arctan \left({\frac {1}{x}}\right)&={\frac {\pi }{2}}-\arctan(x)=\operatorname {arccot}(x)\,,{\text{ if }}x>0\\[0.3em]\arctan \left({\frac {1}{x}}\right)&=-{\frac {\pi }{2}}-\arctan(x)=\operatorname {arccot}(x)-\pi \,,{\text{ if }}x<0\\[0.3em]\operatorname {arccot} \left({\frac {1}{x}}\right)&={\frac {\pi }{2}}-\operatorname {arccot}(x)=\arctan(x)\,,{\text{ if }}x>0\\[0.3em]\operatorname {arccot} \left({\frac {1}{x}}\right)&={\frac {3\pi }{2}}-\operatorname {arccot}(x)=\pi +\arctan(x)\,,{\text{ if }}x<0\\[0.3em]\operatorname {arcsec} \left({\frac {1}{x}}\right)&=\arccos(x)\\[0.3em]\operatorname {arccsc} \left({\frac {1}{x}}\right)&=\arcsin(x)\end{aligned}}}$
Useful identities if one only has a fragment of a sine table:

${\displaystyle {\begin{aligned}\arccos(x)&=\arcsin \left({\sqrt {1-x^{2}}}\right)\,,{\text{ if }}0\leq x\leq 1{\text{ , from which you get }}\\\arccos &\left({\frac {1-x^{2}}{1+x^{2}}}\right)=\arcsin \left({\frac {2x}{1+x^{2}}}\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin &\left({\sqrt {1-x^{2}}}\right)={\frac {\pi }{2}}-\operatorname {sgn}(x)\arcsin(x)\\\arccos(x)&={\frac {1}{2}}\arccos \left(2x^{2}-1\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin(x)&={\frac {1}{2}}\arccos \left(1-2x^{2}\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin(x)&=\arctan \left({\frac {x}{\sqrt {1-x^{2}}}}\right)\\\arccos(x)&=\arctan \left({\frac {\sqrt {1-x^{2}}}{x}}\right)\\\arctan(x)&=\arcsin \left({\frac {x}{\sqrt {1+x^{2}}}}\right)\\\operatorname {arccot}(x)&=\arccos \left({\frac {x}{\sqrt {1+x^{2}}}}\right)\end{aligned}}}$
Whenever the square root of a complex number is used here, we choose the root with the positive real part (or positive imaginary part if the square was negative real).

A useful form that follows directly from the table above is

${\displaystyle \arctan \left(x\right)=\arccos \left({\sqrt {\frac {1}{1+x^{2}}}}\right)\,,{\text{ if }}x\geq 0}$ .
It is obtained by recognizing that ${\displaystyle \cos \left(\arctan \left(x\right)\right)={\sqrt {\frac {1}{1+x^{2}}}}=\cos \left(\arccos \left({\sqrt {\frac {1}{1+x^{2}}}}\right)\right)}$ .

From the half-angle formula, ${\displaystyle \tan \left({\tfrac {\theta }{2}}\right)={\tfrac {\sin(\theta )}{1+\cos(\theta )}}}$ , we get:

${\displaystyle {\begin{aligned}\arcsin(x)&=2\arctan \left({\frac {x}{1+{\sqrt {1-x^{2}}}}}\right)\\[0.5em]\arccos(x)&=2\arctan \left({\frac {\sqrt {1-x^{2}}}{1+x}}\right)\,,{\text{ if }}-1<x\leq 1\\[0.5em]\arctan(x)&=2\arctan \left({\frac {x}{1+{\sqrt {1+x^{2}}}}}\right)\end{aligned}}}$

£#h5#£Arctangent addition formula£#/h5#£
${\displaystyle \arctan(u)\pm \arctan(v)=\arctan \left({\frac {u\pm v}{1\mp uv}}\right){\pmod {\pi }}\,,\quad uv\neq 1\,.}$
This is derived from the tangent addition formula

${\displaystyle \tan(\alpha \pm \beta )={\frac {\tan(\alpha )\pm \tan(\beta )}{1\mp \tan(\alpha )\tan(\beta )}}\,,}$
by letting

${\displaystyle \alpha =\arctan(u)\,,\quad \beta =\arctan(v)\,.}$

£#h5#£In calculus£#/h5#£
£#h5#£Derivatives of inverse trigonometric functions£#/h5#£
The derivatives for complex values of z are as follows:

${\displaystyle {\begin{aligned}{\frac {d}{dz}}\arcsin(z)&{}={\frac {1}{\sqrt {1-z^{2}}}}\;;&z&{}\neq -1,+1\\{\frac {d}{dz}}\arccos(z)&{}=-{\frac {1}{\sqrt {1-z^{2}}}}\;;&z&{}\neq -1,+1\\{\frac {d}{dz}}\arctan(z)&{}={\frac {1}{1+z^{2}}}\;;&z&{}\neq -i,+i\\{\frac {d}{dz}}\operatorname {arccot}(z)&{}=-{\frac {1}{1+z^{2}}}\;;&z&{}\neq -i,+i\\{\frac {d}{dz}}\operatorname {arcsec}(z)&{}={\frac {1}{z^{2}{\sqrt {1-{\frac {1}{z^{2}}}}}}}\;;&z&{}\neq -1,0,+1\\{\frac {d}{dz}}\operatorname {arccsc}(z)&{}=-{\frac {1}{z^{2}{\sqrt {1-{\frac {1}{z^{2}}}}}}}\;;&z&{}\neq -1,0,+1\end{aligned}}}$
Only for real values of x:

${\displaystyle {\begin{aligned}{\frac {d}{dx}}\operatorname {arcsec}(x)&{}={\frac {1}{|x|{\sqrt {x^{2}-1}}}}\;;&|x|>1\\{\frac {d}{dx}}\operatorname {arccsc}(x)&{}=-{\frac {1}{|x|{\sqrt {x^{2}-1}}}}\;;&|x|>1\end{aligned}}}$
For a sample derivation: if ${\displaystyle \theta =\arcsin(x)}$ , we get:

${\displaystyle {\frac {d\arcsin(x)}{dx}}={\frac {d\theta }{d\sin(\theta )}}={\frac {d\theta }{\cos(\theta )\,d\theta }}={\frac {1}{\cos(\theta )}}={\frac {1}{\sqrt {1-\sin ^{2}(\theta )}}}={\frac {1}{\sqrt {1-x^{2}}}}}$

£#h5#£Expression as definite integrals£#/h5#£
Integrating the derivative and fixing the value at one point gives an expression for the inverse trigonometric function as a definite integral:

${\displaystyle {\begin{aligned}\arcsin(x)&{}=\int _{0}^{x}{\frac {1}{\sqrt {1-z^{2}}}}\,dz\;,&|x|&{}\leq 1\\\arccos(x)&{}=\int _{x}^{1}{\frac {1}{\sqrt {1-z^{2}}}}\,dz\;,&|x|&{}\leq 1\\\arctan(x)&{}=\int _{0}^{x}{\frac {1}{z^{2}+1}}\,dz\;,\\\operatorname {arccot}(x)&{}=\int _{x}^{\infty }{\frac {1}{z^{2}+1}}\,dz\;,\\\operatorname {arcsec}(x)&{}=\int _{1}^{x}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz=\pi +\int _{-x}^{-1}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz\;,&x&{}\geq 1\\\operatorname {arccsc}(x)&{}=\int _{x}^{\infty }{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz=\int _{-\infty }^{-x}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz\;,&x&{}\geq 1\\\end{aligned}}}$
When x equals 1, the integrals with limited domains are improper integrals, but still well-defined.


£#h5#£Infinite series£#/h5#£
Similar to the sine and cosine functions, the inverse trigonometric functions can also be calculated using power series, as follows. For arcsine, the series can be derived by expanding its derivative, ${\textstyle {\tfrac {1}{\sqrt {1-z^{2}}}}}$ , as a binomial series, and integrating term by term (using the integral definition as above). The series for arctangent can similarly be derived by expanding its derivative ${\textstyle {\frac {1}{1+z^{2}}}}$ in a geometric series, and applying the integral definition above (see Leibniz series).

${\displaystyle {\begin{aligned}\arcsin(z)&=z+\left({\frac {1}{2}}\right){\frac {z^{3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {z^{5}}{5}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {z^{7}}{7}}+\cdots \\[5pt]&=\sum _{n=0}^{\infty }{\frac {(2n-1)!!}{(2n)!!}}{\frac {z^{2n+1}}{2n+1}}\\[5pt]&=\sum _{n=0}^{\infty }{\frac {(2n)!}{(2^{n}n!)^{2}}}{\frac {z^{2n+1}}{2n+1}}\,;\qquad |z|\leq 1\end{aligned}}}$
${\displaystyle \arctan(z)=z-{\frac {z^{3}}{3}}+{\frac {z^{5}}{5}}-{\frac {z^{7}}{7}}+\cdots =\sum _{n=0}^{\infty }{\frac {(-1)^{n}z^{2n+1}}{2n+1}}\,;\qquad |z|\leq 1\qquad z\neq i,-i}$
Series for the other inverse trigonometric functions can be given in terms of these according to the relationships given above. For example, ${\displaystyle \arccos(x)=\pi /2-\arcsin(x)}$ , ${\displaystyle \operatorname {arccsc}(x)=\arcsin(1/x)}$ , and so on. Another series is given by:

${\displaystyle 2\left(\arcsin \left({\frac {x}{2}}\right)\right)^{2}=\sum _{n=1}^{\infty }{\frac {x^{2n}}{n^{2}{\binom {2n}{n}}}}.}$
Leonhard Euler found a series for the arctangent that converges more quickly than its Taylor series:

${\displaystyle \arctan(z)={\frac {z}{1+z^{2}}}\sum _{n=0}^{\infty }\prod _{k=1}^{n}{\frac {2kz^{2}}{(2k+1)(1+z^{2})}}.}$
(The term in the sum for n = 0 is the empty product, so is 1.)

Alternatively, this can be expressed as

${\displaystyle \arctan(z)=\sum _{n=0}^{\infty }{\frac {2^{2n}(n!)^{2}}{(2n+1)!}}{\frac {z^{2n+1}}{(1+z^{2})^{n+1}}}.}$
Another series for the arctangent function is given by

${\displaystyle \arctan(z)=i\sum _{n=1}^{\infty }{\frac {1}{2n-1}}\left({\frac {1}{(1+2i/z)^{2n-1}}}-{\frac {1}{(1-2i/z)^{2n-1}}}\right),}$
where ${\displaystyle i={\sqrt {-1}}}$ is the imaginary unit.


£#h5#£Continued fractions for arctangent£#/h5#£
Two alternatives to the power series for arctangent are these generalized continued fractions:

${\displaystyle \arctan(z)={\frac {z}{1+{\cfrac {(1z)^{2}}{3-1z^{2}+{\cfrac {(3z)^{2}}{5-3z^{2}+{\cfrac {(5z)^{2}}{7-5z^{2}+{\cfrac {(7z)^{2}}{9-7z^{2}+\ddots }}}}}}}}}}={\frac {z}{1+{\cfrac {(1z)^{2}}{3+{\cfrac {(2z)^{2}}{5+{\cfrac {(3z)^{2}}{7+{\cfrac {(4z)^{2}}{9+\ddots }}}}}}}}}}}$
The second of these is valid in the cut complex plane. There are two cuts, from −i to the point at infinity, going down the imaginary axis, and from i to the point at infinity, going up the same axis. It works best for real numbers running from −1 to 1. The partial denominators are the odd natural numbers, and the partial numerators (after the first) are just (nz)2, with each perfect square appearing once. The first was developed by Leonhard Euler; the second by Carl Friedrich Gauss utilizing the Gaussian hypergeometric series.


£#h5#£Indefinite integrals of inverse trigonometric functions£#/h5#£
For real and complex values of z:

${\displaystyle {\begin{aligned}\int \arcsin(z)\,dz&{}=z\,\arcsin(z)+{\sqrt {1-z^{2}}}+C\\\int \arccos(z)\,dz&{}=z\,\arccos(z)-{\sqrt {1-z^{2}}}+C\\\int \arctan(z)\,dz&{}=z\,\arctan(z)-{\frac {1}{2}}\ln \left(1+z^{2}\right)+C\\\int \operatorname {arccot}(z)\,dz&{}=z\,\operatorname {arccot}(z)+{\frac {1}{2}}\ln \left(1+z^{2}\right)+C\\\int \operatorname {arcsec}(z)\,dz&{}=z\,\operatorname {arcsec}(z)-\ln \left[z\left(1+{\sqrt {\frac {z^{2}-1}{z^{2}}}}\right)\right]+C\\\int \operatorname {arccsc}(z)\,dz&{}=z\,\operatorname {arccsc}(z)+\ln \left[z\left(1+{\sqrt {\frac {z^{2}-1}{z^{2}}}}\right)\right]+C\end{aligned}}}$
For real x ≥ 1:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\ln \left(x+{\sqrt {x^{2}-1}}\right)+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\ln \left(x+{\sqrt {x^{2}-1}}\right)+C\end{aligned}}}$
For all real x not between -1 and 1:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\operatorname {sgn}(x)\ln \left|x+{\sqrt {x^{2}-1}}\right|+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\operatorname {sgn}(x)\ln \left|x+{\sqrt {x^{2}-1}}\right|+C\end{aligned}}}$
The absolute value is necessary to compensate for both negative and positive values of the arcsecant and arccosecant functions. The signum function is also necessary due to the absolute values in the derivatives of the two functions, which create two different solutions for positive and negative values of x. These can be further simplified using the logarithmic definitions of the inverse hyperbolic functions:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\operatorname {arcosh} (|x|)+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\operatorname {arcosh} (|x|)+C\\\end{aligned}}}$
The absolute value in the argument of the arcosh function creates a negative half of its graph, making it identical to the signum logarithmic function shown above.

All of these antiderivatives can be derived using integration by parts and the simple derivative forms shown above.


£#h5#£Example£#/h5#£
Using ${\displaystyle \int u\,dv=uv-\int v\,du}$ (i.e. integration by parts), set

${\displaystyle {\begin{aligned}u&=\arcsin(x)&dv&=dx\\du&={\frac {dx}{\sqrt {1-x^{2}}}}&v&=x\end{aligned}}}$
Then

${\displaystyle \int \arcsin(x)\,dx=x\arcsin(x)-\int {\frac {x}{\sqrt {1-x^{2}}}}\,dx,}$
which by the simple substitution ${\displaystyle w=1-x^{2},\ dw=-2x\,dx}$ yields the final result:

${\displaystyle \int \arcsin(x)\,dx=x\arcsin(x)+{\sqrt {1-x^{2}}}+C}$

£#h5#£Extension to complex plane£#/h5#£
Since the inverse trigonometric functions are analytic functions, they can be extended from the real line to the complex plane. This results in functions with multiple sheets and branch points. One possible way of defining the extension is:

${\displaystyle \arctan(z)=\int _{0}^{z}{\frac {dx}{1+x^{2}}}\quad z\neq -i,+i}$
where the part of the imaginary axis which does not lie strictly between the branch points (−i and +i) is the branch cut between the principal sheet and other sheets. The path of the integral must not cross a branch cut. For z not on a branch cut, a straight line path from 0 to z is such a path. For z on a branch cut, the path must approach from Re[x] > 0 for the upper branch cut and from Re[x] < 0 for the lower branch cut.

The arcsine function may then be defined as:

${\displaystyle \arcsin(z)=\arctan \left({\frac {z}{\sqrt {1-z^{2}}}}\right)\quad z\neq -1,+1}$
where (the square-root function has its cut along the negative real axis and) the part of the real axis which does not lie strictly between −1 and +1 is the branch cut between the principal sheet of arcsin and other sheets;

${\displaystyle \arccos(z)={\frac {\pi }{2}}-\arcsin(z)\quad z\neq -1,+1}$
which has the same cut as arcsin;

${\displaystyle \operatorname {arccot}(z)={\frac {\pi }{2}}-\arctan(z)\quad z\neq -i,i}$
which has the same cut as arctan;

${\displaystyle \operatorname {arcsec}(z)=\arccos \left({\frac {1}{z}}\right)\quad z\neq -1,0,+1}$
where the part of the real axis between −1 and +1 inclusive is the cut between the principal sheet of arcsec and other sheets;

${\displaystyle \operatorname {arccsc}(z)=\arcsin \left({\frac {1}{z}}\right)\quad z\neq -1,0,+1}$
which has the same cut as arcsec.


£#h5#£Logarithmic forms£#/h5#£
These functions may also be expressed using complex logarithms. This extends their domains to the complex plane in a natural fashion. The following identities for principal values of the functions hold everywhere that they are defined, even on their branch cuts.

${\displaystyle {\begin{aligned}\arcsin(z)&{}=-i\ln \left({\sqrt {1-z^{2}}}+iz\right)=i\ln \left({\sqrt {1-z^{2}}}-iz\right)&{}=\operatorname {arccsc} \left({\frac {1}{z}}\right)\\[10pt]\arccos(z)&{}=-i\ln \left(i{\sqrt {1-z^{2}}}+z\right)={\frac {\pi }{2}}-\arcsin(z)&{}=\operatorname {arcsec} \left({\frac {1}{z}}\right)\\[10pt]\arctan(z)&{}=-{\frac {i}{2}}\ln \left({\frac {i-z}{i+z}}\right)=-{\frac {i}{2}}\ln \left({\frac {1+iz}{1-iz}}\right)&{}=\operatorname {arccot} \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arccot}(z)&{}=-{\frac {i}{2}}\ln \left({\frac {z+i}{z-i}}\right)=-{\frac {i}{2}}\ln \left({\frac {iz-1}{iz+1}}\right)&{}=\arctan \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arcsec}(z)&{}=-i\ln \left(i{\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {1}{z}}\right)={\frac {\pi }{2}}-\operatorname {arccsc}(z)&{}=\arccos \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arccsc}(z)&{}=-i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {i}{z}}\right)=i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}-{\frac {i}{z}}\right)&{}=\arcsin \left({\frac {1}{z}}\right)\end{aligned}}}$

£#h5#£Generalization£#/h5#£
Because all of the inverse trigonometric functions output an angle of a right triangle, they can be generalized by using Euler's formula to form a right triangle in the complex plane. Algebraically, this gives us:

${\displaystyle ce^{\theta i}=c\cos(\theta )+ci\sin(\theta )}$
or

${\displaystyle ce^{\theta i}=a+bi}$
where ${\displaystyle a}$ is the adjacent side, ${\displaystyle b}$ is the opposite side, and ${\displaystyle c}$ is the hypotenuse. From here, we can solve for ${\displaystyle \theta }$ .

${\displaystyle {\begin{aligned}e^{\ln(c)+\theta i}&=a+bi\\\ln c+\theta i&=\ln(a+bi)\\\theta &=\operatorname {Im} \left(\ln(a+bi)\right)\end{aligned}}}$
or

${\displaystyle \theta =-i\ln \left({\frac {a+bi}{c}}\right)=i\ln \left({\frac {c}{a+bi}}\right)}$
Simply taking the imaginary part works for any real-valued ${\displaystyle a}$ and ${\displaystyle b}$ , but if ${\displaystyle a}$ or ${\displaystyle b}$ is complex-valued, we have to use the final equation so that the real part of the result isn't excluded. Since the length of the hypotenuse doesn't change the angle, ignoring the real part of ${\displaystyle \ln(a+bi)}$ also removes ${\displaystyle c}$ from the equation. In the final equation, we see that the angle of the triangle in the complex plane can be found by inputting the lengths of each side. By setting one of the three sides equal to 1 and one of the remaining sides equal to our input ${\displaystyle z}$ , we obtain a formula for one of the inverse trig functions, for a total of six equations. Because the inverse trig functions require only one input, we must put the final side of the triangle in terms of the other two using the Pythagorean Theorem relation

${\displaystyle a^{2}+b^{2}=c^{2}}$
The table below shows the values of a, b, and c for each of the inverse trig functions and the equivalent expressions for ${\displaystyle \theta }$ that result from plugging the values into the equations above and simplifying.

${\displaystyle {\begin{aligned}&a&&b&&c&&\theta &&\theta _{\text{simplified}}&&\theta _{a,b\in \mathbb {R} }\\\arcsin(z)\ \ &{\sqrt {1-z^{2}}}&&z&&1&&-i\ln \left({\frac {{\sqrt {1-z^{2}}}+zi}{1}}\right)&&=-i\ln \left({\sqrt {1-z^{2}}}+zi\right)&&\operatorname {Im} \left(\ln \left({\sqrt {1-z^{2}}}+zi\right)\right)\\\arccos(z)\ \ &z&&{\sqrt {1-z^{2}}}&&1&&-i\ln \left({\frac {z+i{\sqrt {1-z^{2}}}}{1}}\right)&&=-i\ln \left(z+{\sqrt {z^{2}-1}}\right)&&\operatorname {Im} \left(\ln \left(z+{\sqrt {z^{2}-1}}\right)\right)\\\arctan(z)\ \ &1&&z&&{\sqrt {1+z^{2}}}&&i\ln \left({\frac {\sqrt {1+z^{2}}}{1+zi}}\right)&&={\frac {i}{2}}\ln \left({\frac {i+z}{i-z}}\right)&&\operatorname {Im} \left(\ln \left(1+zi\right)\right)\\\operatorname {arccot}(z)\ \ &z&&1&&{\sqrt {z^{2}+1}}&&i\ln \left({\frac {\sqrt {z^{2}+1}}{z+i}}\right)&&={\frac {i}{2}}\ln \left({\frac {z-i}{z+i}}\right)&&\operatorname {Im} \left(\ln \left(z+i\right)\right)\\\operatorname {arcsec}(z)\ \ &1&&{\sqrt {z^{2}-1}}&&z&&-i\ln \left({\frac {1+i{\sqrt {z^{2}-1}}}{z}}\right)&&=-i\ln \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z^{2}}}-1}}\right)&&\operatorname {Im} \left(\ln \left(1+{\sqrt {1-z^{2}}}\right)\right)\\\operatorname {arccsc}(z)\ \ &{\sqrt {z^{2}-1}}&&1&&z&&-i\ln \left({\frac {{\sqrt {z^{2}-1}}+i}{z}}\right)&&=-i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {i}{z}}\right)&&\operatorname {Im} \left(\ln \left({\sqrt {z^{2}-1}}+i\right)\right)\\\end{aligned}}}$
In this sense, all of the inverse trig functions can be thought of as specific cases of the complex-valued log function. Since this definition works for any complex-valued ${\displaystyle z}$ , this definition allows for hyperbolic angles as outputs and can be used to further define the inverse hyperbolic functions. Elementary proofs of the relations may also proceed via expansion to exponential forms of the trigonometric functions.


£#h5#£Example proof£#/h5#£
${\displaystyle {\begin{aligned}\sin(\phi )&=z\\\phi &=\arcsin(z)\end{aligned}}}$
Using the exponential definition of sine, and letting ${\displaystyle \xi =e^{\phi i},}$

${\displaystyle {\begin{aligned}z&={\frac {e^{\phi i}-e^{-\phi i}}{2i}}\\[10mu]2iz&=\xi -{\frac {1}{\xi }}\\[5mu]0&=\xi ^{2}-2i\xi z-1\\[5mu]\xi &=iz\pm {\sqrt {1-z^{2}}}\\[5mu]\phi &=-i\ln \left(iz\pm {\sqrt {1-z^{2}}}\right)\end{aligned}}}$
(the positive branch is chosen)

${\displaystyle \phi =\arcsin(z)=-i\ln \left(iz+{\sqrt {1-z^{2}}}\right)}$

£#h5#£Applications£#/h5#£
£#h5#£Finding the angle of a right triangle£#/h5#£
Inverse trigonometric functions are useful when trying to determine the remaining two angles of a right triangle when the lengths of the sides of the triangle are known. Recalling the right-triangle definitions of sine and cosine, it follows that

${\displaystyle \theta =\arcsin \left({\frac {\text{opposite}}{\text{hypotenuse}}}\right)=\arccos \left({\frac {\text{adjacent}}{\text{hypotenuse}}}\right).}$
Often, the hypotenuse is unknown and would need to be calculated before using arcsine or arccosine using the Pythagorean Theorem: ${\displaystyle a^{2}+b^{2}=h^{2}}$ where ${\displaystyle h}$ is the length of the hypotenuse. Arctangent comes in handy in this situation, as the length of the hypotenuse is not needed.

${\displaystyle \theta =\arctan \left({\frac {\text{opposite}}{\text{adjacent}}}\right)\,.}$
For example, suppose a roof drops 8 feet as it runs out 20 feet. The roof makes an angle θ with the horizontal, where θ may be computed as follows:

${\displaystyle \theta =\arctan \left({\frac {\text{opposite}}{\text{adjacent}}}\right)=\arctan \left({\frac {\text{rise}}{\text{run}}}\right)=\arctan \left({\frac {8}{20}}\right)\approx 21.8^{\circ }\,.}$

£#h5#£In computer science and engineering£#/h5#£
£#h5#£Two-argument variant of arctangent£#/h5#£

The two-argument atan2 function computes the arctangent of y / x given y and x, but with a range of (−π, π]. In other words, atan2(y, x) is the angle between the positive x-axis of a plane and the point (x, y) on it, with positive sign for counter-clockwise angles (upper half-plane, y > 0), and negative sign for clockwise angles (lower half-plane, y < 0). It was first introduced in many computer programming languages, but it is now also common in other fields of science and engineering.

In terms of the standard arctan function, that is with range of (−π/2, π/2), it can be expressed as follows:

${\displaystyle \operatorname {atan2} (y,x)={\begin{cases}\arctan \left({\frac {y}{x}}\right)&\quad x>0\\\arctan \left({\frac {y}{x}}\right)+\pi &\quad y\geq 0,\;x<0\\\arctan \left({\frac {y}{x}}\right)-\pi &\quad y<0,\;x<0\\{\frac {\pi }{2}}&\quad y>0,\;x=0\\-{\frac {\pi }{2}}&\quad y<0,\;x=0\\{\text{undefined}}&\quad y=0,\;x=0\end{cases}}}$
It also equals the principal value of the argument of the complex number x + iy.

This limited version of the function above may also be defined using the tangent half-angle formulae as follows:

${\displaystyle \operatorname {atan2} (y,x)=2\arctan \left({\frac {y}{{\sqrt {x^{2}+y^{2}}}+x}}\right)}$
provided that either x > 0 or y ≠ 0. However this fails if given x ≤ 0 and y = 0 so the expression is unsuitable for computational use.

The above argument order (y, x) seems to be the most common, and in particular is used in ISO standards such as the C programming language, but a few authors may use the opposite convention (x, y) so some caution is warranted. These variations are detailed at atan2.


£#h5#£Arctangent function with location parameter£#/h5#£
In many applications the solution ${\displaystyle y}$ of the equation ${\displaystyle x=\tan(y)}$ is to come as close as possible to a given value ${\displaystyle -\infty <\eta <\infty }$ . The adequate solution is produced by the parameter modified arctangent function

${\displaystyle y=\arctan _{\eta }(x):=\arctan(x)+\pi \,\operatorname {rni} \left({\frac {\eta -\arctan(x)}{\pi }}\right)\,.}$
The function ${\displaystyle \operatorname {rni} }$ rounds to the nearest integer.


£#h5#£Numerical accuracy£#/h5#£
For angles near 0 and π, arccosine is ill-conditioned and will thus calculate the angle with reduced accuracy in a computer implementation (due to the limited number of digits). Similarly, arcsine is inaccurate for angles near −π/2 and π/2.


£#h5#£See also£#/h5#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Abramowitz, Milton; Stegun, Irene A., eds. (1972). Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables. New York: Dover Publications. ISBN 978-0-486-61272-0.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Inverse Tangent". MathWorld.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Trigonometric Functions £#/li#££#/ul#£




£#h3#£Arcsinh£#/h3#£


In mathematics, the inverse hyperbolic functions are the inverse functions of the hyperbolic functions.

For a given value of a hyperbolic function, the corresponding inverse hyperbolic function provides the corresponding hyperbolic angle. The size of the hyperbolic angle is equal to the area of the corresponding hyperbolic sector of the hyperbola xy = 1, or twice the area of the corresponding sector of the unit hyperbola x2 − y2 = 1, just as a circular angle is twice the area of the circular sector of the unit circle. Some authors have called inverse hyperbolic functions "area functions" to realize the hyperbolic angles.

Hyperbolic functions occur in the calculations of angles and distances in hyperbolic geometry. It also occurs in the solutions of many linear differential equations (such as the equation defining a catenary), cubic equations, and Laplace's equation in Cartesian coordinates. Laplace's equations are important in many areas of physics, including electromagnetic theory, heat transfer, fluid dynamics, and special relativity.


£#h5#£Notation£#/h5#£
The ISO 80000-2 standard abbreviations consist of ar- followed by the abbreviation of the corresponding hyperbolic function (e.g., arsinh, arcosh). The prefix arc- followed by the corresponding hyperbolic function (e.g., arcsinh, arccosh) is also commonly seen, by analogy with the nomenclature for inverse trigonometric functions. These are misnomers, since the prefix arc is the abbreviation for arcus, while the prefix ar stands for area; the hyperbolic functions are not directly related to arcs.

Other authors prefer to use the notation argsinh, argcosh, argtanh, and so on, where the prefix arg is the abbreviation of the Latin argumentum. In computer science, this is often shortened to asinh.

The notation sinh−1(x), cosh−1(x), etc., is also used, despite the fact that care must be taken to avoid misinterpretations of the superscript −1 as a power, as opposed to a shorthand to denote the inverse function (e.g., cosh−1(x) versus cosh(x)−1).


£#h5#£Definitions in terms of logarithms£#/h5#£
Since the hyperbolic functions are rational functions of ex whose numerator and denominator are of degree at most two, these functions may be solved in terms of ex, by using the quadratic formula; then, taking the natural logarithm gives the following expressions for the inverse hyperbolic functions.

For complex arguments, the inverse hyperbolic functions, the square root and the logarithm are multi-valued functions, and the equalities of the next subsections may be viewed as equalities of multi-valued functions.

For all inverse hyperbolic functions (save the inverse hyperbolic cotangent and the inverse hyperbolic cosecant), the domain of the real function is connected.


£#h5#£Inverse hyperbolic sine£#/h5#£
Inverse hyperbolic sine (a.k.a. area hyperbolic sine) (Latin: Area sinus hyperbolicus):

${\displaystyle \operatorname {arsinh} x=\ln \left(x+{\sqrt {x^{2}+1}}\right)}$
The domain is the whole real line.


£#h5#£Inverse hyperbolic cosine£#/h5#£
Inverse hyperbolic cosine (a.k.a. area hyperbolic cosine) (Latin: Area cosinus hyperbolicus):

${\displaystyle \operatorname {arcosh} x=\ln \left(x+{\sqrt {x^{2}-1}}\right)}$
The domain is the closed interval [1, +∞ ).


£#h5#£Inverse hyperbolic tangent£#/h5#£
Inverse hyperbolic tangent (a.k.a. area hyperbolic tangent) (Latin: Area tangens hyperbolicus):

${\displaystyle \operatorname {artanh} x={\frac {1}{2}}\ln \left({\frac {1+x}{1-x}}\right)}$
The domain is the open interval (−1, 1).


£#h5#£Inverse hyperbolic cotangent£#/h5#£
Inverse hyperbolic cotangent (a.k.a., area hyperbolic cotangent) (Latin: Area cotangens hyperbolicus):

${\displaystyle \operatorname {arcoth} x={\frac {1}{2}}\ln \left({\frac {x+1}{x-1}}\right)}$
The domain is the union of the open intervals (−∞, −1) and (1, +∞).


£#h5#£Inverse hyperbolic secant£#/h5#£
Inverse hyperbolic secant (a.k.a., area hyperbolic secant) (Latin: Area secans hyperbolicus):

${\displaystyle \operatorname {arsech} x=\ln \left({\frac {1}{x}}+{\sqrt {{\frac {1}{x^{2}}}-1}}\right)=\ln \left({\frac {1+{\sqrt {1-x^{2}}}}{x}}\right)}$
The domain is the semi-open interval (0, 1].


£#h5#£Inverse hyperbolic cosecant£#/h5#£
Inverse hyperbolic cosecant (a.k.a., area hyperbolic cosecant) (Latin: Area cosecans hyperbolicus):

${\displaystyle \operatorname {arcsch} x=\ln \left({\frac {1}{x}}+{\sqrt {{\frac {1}{x^{2}}}+1}}\right)}$
The domain is the real line with 0 removed.


£#h5#£Addition formulae£#/h5#£
${\displaystyle \operatorname {arsinh} u\pm \operatorname {arsinh} v=\operatorname {arsinh} \left(u{\sqrt {1+v^{2}}}\pm v{\sqrt {1+u^{2}}}\right)}$
${\displaystyle \operatorname {arcosh} u\pm \operatorname {arcosh} v=\operatorname {arcosh} \left(uv\pm {\sqrt {(u^{2}-1)(v^{2}-1)}}\right)}$
${\displaystyle \operatorname {artanh} u\pm \operatorname {artanh} v=\operatorname {artanh} \left({\frac {u\pm v}{1\pm uv}}\right)}$
${\displaystyle \operatorname {arcoth} u\pm \operatorname {arcoth} v=\operatorname {arcoth} \left({\frac {1\pm uv}{u\pm v}}\right)}$
${\displaystyle {\begin{aligned}\operatorname {arsinh} u+\operatorname {arcosh} v&=\operatorname {arsinh} \left(uv+{\sqrt {(1+u^{2})(v^{2}-1)}}\right)\\&=\operatorname {arcosh} \left(v{\sqrt {1+u^{2}}}+u{\sqrt {v^{2}-1}}\right)\end{aligned}}}$

£#h5#£Other identities£#/h5#£
${\displaystyle {\begin{aligned}2\operatorname {arcosh} x&=\operatorname {arcosh} (2x^{2}-1)&\quad {\hbox{ for }}x\geq 1\\4\operatorname {arcosh} x&=\operatorname {arcosh} (8x^{4}-8x^{2}+1)&\quad {\hbox{ for }}x\geq 1\\2\operatorname {arsinh} x&=\operatorname {arcosh} (2x^{2}+1)&\quad {\hbox{ for }}x\geq 0\\4\operatorname {arsinh} x&=\operatorname {arcosh} (8x^{4}+8x^{2}+1)&\quad {\hbox{ for }}x\geq 0\end{aligned}}}$
${\displaystyle \ln(x)=\operatorname {arcosh} \left({\frac {x^{2}+1}{2x}}\right)=\operatorname {arsinh} \left({\frac {x^{2}-1}{2x}}\right)=\operatorname {artanh} \left({\frac {x^{2}-1}{x^{2}+1}}\right)}$

£#h5#£Composition of hyperbolic and inverse hyperbolic functions£#/h5#£
${\displaystyle {\begin{aligned}&\sinh(\operatorname {arcosh} x)={\sqrt {x^{2}-1}}\quad {\text{for}}\quad |x|>1\\&\sinh(\operatorname {artanh} x)={\frac {x}{\sqrt {1-x^{2}}}}\quad {\text{for}}\quad -1<x<1\\&\cosh(\operatorname {arsinh} x)={\sqrt {1+x^{2}}}\\&\cosh(\operatorname {artanh} x)={\frac {1}{\sqrt {1-x^{2}}}}\quad {\text{for}}\quad -1<x<1\\&\tanh(\operatorname {arsinh} x)={\frac {x}{\sqrt {1+x^{2}}}}\\&\tanh(\operatorname {arcosh} x)={\frac {\sqrt {x^{2}-1}}{x}}\quad {\text{for}}\quad |x|>1\end{aligned}}}$

£#h5#£Composition of inverse hyperbolic and trigonometric functions£#/h5#£
${\displaystyle \operatorname {arsinh} \left(\tan \alpha \right)=\operatorname {artanh} \left(\sin \alpha \right)=\ln \left({\frac {1+\sin \alpha }{\cos \alpha }}\right)=\pm \operatorname {arcosh} \left({\frac {1}{\cos \alpha }}\right)}$
${\displaystyle \ln \left(\left|\tan \alpha \right|\right)=-\operatorname {artanh} \left(\cos 2\alpha \right)}$

£#h5#£Conversions£#/h5#£
${\displaystyle \ln x=\operatorname {artanh} \left({\frac {x^{2}-1}{x^{2}+1}}\right)=\operatorname {arsinh} \left({\frac {x^{2}-1}{2x}}\right)=\pm \operatorname {arcosh} \left({\frac {x^{2}+1}{2x}}\right)}$
${\displaystyle \operatorname {artanh} x=\operatorname {arsinh} \left({\frac {x}{\sqrt {1-x^{2}}}}\right)=\pm \operatorname {arcosh} \left({\frac {1}{\sqrt {1-x^{2}}}}\right)}$
${\displaystyle \operatorname {arsinh} x=\operatorname {artanh} \left({\frac {x}{\sqrt {1+x^{2}}}}\right)=\pm \operatorname {arcosh} \left({\sqrt {1+x^{2}}}\right)}$
${\displaystyle \operatorname {arcosh} x=\left|\operatorname {arsinh} \left({\sqrt {x^{2}-1}}\right)\right|=\left|\operatorname {artanh} \left({\frac {\sqrt {x^{2}-1}}{x}}\right)\right|}$

£#h5#£Derivatives£#/h5#£
${\displaystyle {\begin{aligned}{\frac {d}{dx}}\operatorname {arsinh} x&{}={\frac {1}{\sqrt {x^{2}+1}}},{\text{ for all real }}x\\{\frac {d}{dx}}\operatorname {arcosh} x&{}={\frac {1}{\sqrt {x^{2}-1}}},{\text{ for all real }}x>1\\{\frac {d}{dx}}\operatorname {artanh} x&{}={\frac {1}{1-x^{2}}},{\text{ for all real }}|x|<1\\{\frac {d}{dx}}\operatorname {arcoth} x&{}={\frac {1}{1-x^{2}}},{\text{ for all real }}|x|>1\\{\frac {d}{dx}}\operatorname {arsech} x&{}={\frac {-1}{x{\sqrt {1-x^{2}}}}},{\text{ for all real }}x\in (0,1)\\{\frac {d}{dx}}\operatorname {arcsch} x&{}={\frac {-1}{|x|{\sqrt {1+x^{2}}}}},{\text{ for all real }}x{\text{, except }}0\\\end{aligned}}}$
For an example differentiation: let θ = arsinh x, so (where sinh2 θ = (sinh θ)2):

${\displaystyle {\frac {d\,\operatorname {arsinh} x}{dx}}={\frac {d\theta }{d\sinh \theta }}={\frac {1}{\cosh \theta }}={\frac {1}{\sqrt {1+\sinh ^{2}\theta }}}={\frac {1}{\sqrt {1+x^{2}}}}.}$

£#h5#£Series expansions£#/h5#£
Expansion series can be obtained for the above functions:

${\displaystyle {\begin{aligned}\operatorname {arsinh} x&=x-\left({\frac {1}{2}}\right){\frac {x^{3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{5}}{5}}-\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{7}}{7}}\pm \cdots \\&=\sum _{n=0}^{\infty }\left({\frac {(-1)^{n}(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{2n+1}}{2n+1}},\qquad \left|x\right|<1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcosh} x&=\ln(2x)-\left(\left({\frac {1}{2}}\right){\frac {x^{-2}}{2}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{-4}}{4}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{-6}}{6}}+\cdots \right)\\&=\ln(2x)-\sum _{n=1}^{\infty }\left({\frac {(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{-2n}}{2n}},\qquad \left|x\right|>1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {artanh} x&=x+{\frac {x^{3}}{3}}+{\frac {x^{5}}{5}}+{\frac {x^{7}}{7}}+\cdots \\&=\sum _{n=0}^{\infty }{\frac {x^{2n+1}}{2n+1}},\qquad \left|x\right|<1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcsch} x=\operatorname {arsinh} {\frac {1}{x}}&=x^{-1}-\left({\frac {1}{2}}\right){\frac {x^{-3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{-5}}{5}}-\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{-7}}{7}}\pm \cdots \\&=\sum _{n=0}^{\infty }\left({\frac {(-1)^{n}(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{-(2n+1)}}{2n+1}},\qquad \left|x\right|>1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arsech} x=\operatorname {arcosh} {\frac {1}{x}}&=\ln {\frac {2}{x}}-\left(\left({\frac {1}{2}}\right){\frac {x^{2}}{2}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{4}}{4}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{6}}{6}}+\cdots \right)\\&=\ln {\frac {2}{x}}-\sum _{n=1}^{\infty }\left({\frac {(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{2n}}{2n}},\qquad 0<x\leq 1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcoth} x=\operatorname {artanh} {\frac {1}{x}}&=x^{-1}+{\frac {x^{-3}}{3}}+{\frac {x^{-5}}{5}}+{\frac {x^{-7}}{7}}+\cdots \\&=\sum _{n=0}^{\infty }{\frac {x^{-(2n+1)}}{2n+1}},\qquad \left|x\right|>1\end{aligned}}}$
Asymptotic expansion for the arsinh x is given by

${\displaystyle \operatorname {arsinh} x=\ln(2x)+\sum \limits _{n=1}^{\infty }{\left({-1}\right)^{n-1}{\frac {\left({2n-1}\right)!!}{2n\left({2n}\right)!!}}}{\frac {1}{x^{2n}}}}$



£#h5#£Principal values in the complex plane£#/h5#£
As functions of a complex variable, inverse hyperbolic functions are multivalued functions that are analytic, except at a finite number of points. For such a function, it is common to define a principal value, which is a single valued analytic function which coincides with one specific branch of the multivalued function, over a domain consisting of the complex plane in which a finite number of arcs (usually half lines or line segments) have been removed. These arcs are called branch cuts. For specifying the branch, that is, defining which value of the multivalued function is considered at each point, one generally define it at a particular point, and deduce the value everywhere in the domain of definition of the principal value by analytic continuation. When possible, it is better to define the principal value directly—without referring to analytic continuation.

For example, for the square root, the principal value is defined as the square root that has a positive real part. This defines a single valued analytic function, which is defined everywhere, except for non-positive real values of the variables (where the two square roots have a zero real part). This principal value of the square root function is denoted ${\displaystyle {\sqrt {x}}}$ in what follows. Similarly, the principal value of the logarithm, denoted ${\displaystyle \operatorname {Log} }$ in what follows, is defined as the value for which the imaginary part has the smallest absolute value. It is defined everywhere except for non-positive real values of the variable, for which two different values of the logarithm reach the minimum.

For all inverse hyperbolic functions, the principal value may be defined in terms of principal values of the square root and the logarithm function. However, in some cases, the formulas of § Definitions in terms of logarithms do not give a correct principal value, as giving a domain of definition which is too small and, in one case non-connected.


£#h5#£Principal value of the inverse hyperbolic sine£#/h5#£
The principal value of the inverse hyperbolic sine is given by

${\displaystyle \operatorname {arsinh} z=\operatorname {Log} (z+{\sqrt {z^{2}+1}}\,)\,.}$
The argument of the square root is a non-positive real number, if and only if z belongs to one of the intervals [i, +i∞) and (−i∞, −i] of the imaginary axis. If the argument of the logarithm is real, then it is positive. Thus this formula defines a principal value for arsinh, with branch cuts [i, +i∞) and (−i∞, −i]. This is optimal, as the branch cuts must connect the singular points i and −i to the infinity.


£#h5#£Principal value of the inverse hyperbolic cosine£#/h5#£
The formula for the inverse hyperbolic cosine given in § Inverse hyperbolic cosine is not convenient, since similar to the principal values of the logarithm and the square root, the principal value of arcosh would not be defined for imaginary z. Thus the square root has to be factorized, leading to

${\displaystyle \operatorname {arcosh} z=\operatorname {Log} (z+{\sqrt {z+1}}{\sqrt {z-1}}\,)\,.}$
The principal values of the square roots are both defined, except if z belongs to the real interval (−∞, 1]. If the argument of the logarithm is real, then z is real and has the same sign. Thus, the above formula defines a principal value of arcosh outside the real interval (−∞, 1], which is thus the unique branch cut.


£#h5#£Principal values of the inverse hyperbolic tangent and cotangent£#/h5#£
The formulas given in § Definitions in terms of logarithms suggests

${\displaystyle {\begin{aligned}\operatorname {artanh} z&={\frac {1}{2}}\operatorname {Log} \left({\frac {1+z}{1-z}}\right)\\\operatorname {arcoth} z&={\frac {1}{2}}\operatorname {Log} \left({\frac {z+1}{z-1}}\right)\end{aligned}}}$
for the definition of the principal values of the inverse hyperbolic tangent and cotangent. In these formulas, the argument of the logarithm is real if and only if z is real. For artanh, this argument is in the real interval (−∞, 0], if z belongs either to (−∞, −1] or to [1, ∞). For arcoth, the argument of the logarithm is in (−∞, 0], if and only if z belongs to the real interval [−1, 1].

Therefore, these formulas define convenient principal values, for which the branch cuts are (−∞, −1] and [1, ∞) for the inverse hyperbolic tangent, and [−1, 1] for the inverse hyperbolic cotangent.

In view of a better numerical evaluation near the branch cuts, some authors use the following definitions of the principal values, although the second one introduces a removable singularity at z = 0. The two definitions of ${\displaystyle \operatorname {artanh} }$ differ for real values of ${\displaystyle z}$ with ${\displaystyle z>1}$ . The ones of ${\displaystyle \operatorname {arcoth} }$ differ for real values of ${\displaystyle z}$ with ${\displaystyle z\in [0,1)}$ .

${\displaystyle {\begin{aligned}\operatorname {artanh} z&={\tfrac {1}{2}}\operatorname {Log} \left({1+z}\right)-{\tfrac {1}{2}}\operatorname {Log} \left({1-z}\right)\\\operatorname {arcoth} z&={\tfrac {1}{2}}\operatorname {Log} \left({1+{\frac {1}{z}}}\right)-{\tfrac {1}{2}}\operatorname {Log} \left({1-{\frac {1}{z}}}\right)\end{aligned}}}$

£#h5#£Principal value of the inverse hyperbolic cosecant£#/h5#£
For the inverse hyperbolic cosecant, the principal value is defined as

${\displaystyle \operatorname {arcsch} z=\operatorname {Log} \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z^{2}}}+1}}\,\right)}$ .
It is defined when the arguments of the logarithm and the square root are not non-positive real numbers. The principal value of the square root is thus defined outside the interval [−i, i] of the imaginary line. If the argument of the logarithm is real, then z is a non-zero real number, and this implies that the argument of the logarithm is positive.

Thus, the principal value is defined by the above formula outside the branch cut, consisting of the interval [−i, i] of the imaginary line.

For z = 0, there is a singular point that is included in the branch cut.


£#h5#£Principal value of the inverse hyperbolic secant£#/h5#£
Here, as in the case of the inverse hyperbolic cosine, we have to factorize the square root. This gives the principal value

${\displaystyle \operatorname {arsech} z=\operatorname {Log} \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z}}+1}}\,{\sqrt {{\frac {1}{z}}-1}}\right).}$
If the argument of a square root is real, then z is real, and it follows that both principal values of square roots are defined, except if z is real and belongs to one of the intervals (−∞, 0] and [1, +∞). If the argument of the logarithm is real and negative, then z is also real and negative. It follows that the principal value of arsech is well defined, by the above formula outside two branch cuts, the real intervals (−∞, 0] and [1, +∞).

For z = 0, there is a singular point that is included in one of the branch cuts.


£#h5#£Graphical representation£#/h5#£
In the following graphical representation of the principal values of the inverse hyperbolic functions, the branch cuts appear as discontinuities of the color. The fact that the whole branch cuts appear as discontinuities, shows that these principal values may not be extended into analytic functions defined over larger domains. In other words, the above defined branch cuts are minimal.


£#h5#£See also£#/h5#£ £#ul#££#li#£Complex logarithm£#/li#£ £#li#£Hyperbolic secant distribution£#/li#£ £#li#£ISO 80000-2£#/li#£ £#li#£List of integrals of inverse hyperbolic functions£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£Bibliography£#/h5#£ £#ul#££#li#£Herbert Busemann and Paul J. Kelly (1953) Projective Geometry and Projective Metrics, page 207, Academic Press.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Inverse hyperbolic functions", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Hyperbolic Functions £#/li#££#/ul#£




£#h3#£Arctan£#/h3#£

In mathematics, the inverse trigonometric functions (occasionally also called arcus functions, antitrigonometric functions or cyclometric functions) are the inverse functions of the trigonometric functions (with suitably restricted domains). Specifically, they are the inverses of the sine, cosine, tangent, cotangent, secant, and cosecant functions, and are used to obtain an angle from any of the angle's trigonometric ratios. Inverse trigonometric functions are widely used in engineering, navigation, physics, and geometry.


£#h5#£Notation£#/h5#£
Several notations for the inverse trigonometric functions exist. The most common convention is to name inverse trigonometric functions using an arc- prefix: arcsin(x), arccos(x), arctan(x), etc. (This convention is used throughout this article.) This notation arises from the following geometric relationships: when measuring in radians, an angle of θ radians will correspond to an arc whose length is rθ, where r is the radius of the circle. Thus in the unit circle, "the arc whose cosine is x" is the same as "the angle whose cosine is x", because the length of the arc of the circle in radii is the same as the measurement of the angle in radians. In computer programming languages, the inverse trigonometric functions are often called by the abbreviated forms asin, acos, atan.

The notations sin−1(x), cos−1(x), tan−1(x), etc., as introduced by John Herschel in 1813, are often used as well in English-language sources, much more than the also established sin[−1](x), cos[−1](x), tan[−1](x),—conventions consistent with the notation of an inverse function, that is useful e.g. to define the multivalued version of each inverse trigonometric function: e.g. ${\displaystyle \tan ^{-1}(x)=\{\arctan(x)+\pi k\mid k\in \mathbb {Z} \}}$ . However, this might appear to conflict logically with the common semantics for expressions such as sin2(x) (although only sin2 x, without parentheses, is the really common one), which refer to numeric power rather than function composition, and therefore may result in confusion between multiplicative inverse or reciprocal and compositional inverse. The confusion is somewhat mitigated by the fact that each of the reciprocal trigonometric functions has its own name—for example, (cos(x))−1 = sec(x). Nevertheless, certain authors advise against using it for its ambiguity. Another precarious convention used by a tiny number of authors is to use an uppercase first letter, along with a −1 superscript: Sin−1(x), Cos−1(x), Tan−1(x), etc. Although its intention is to avoid confusion with the multiplicative inverse, which should be represented by sin−1(x), cos−1(x), etc., or, better, by sin−1 x, cos−1 x, etc., it in turn creates yet another major source of ambiguity: especially in light of the fact that many popular high-level programming languages (e.g. Wolfram's Mathematica, and University of Sydney's MAGMA) use those very same capitalised representations for the standard trig functions; whereas others (Python (ie SymPy and NumPy), Matlab, MAPLE etc) use lower-case.

Hence, since 2009, the ISO 80000-2 standard has specified solely the "arc" prefix for the inverse functions.


£#h5#£Basic concepts£#/h5#£
£#h5#£Principal values£#/h5#£
Since none of the six trigonometric functions are one-to-one, they must be restricted in order to have inverse functions. Therefore, the result ranges of the inverse functions are proper (i.e. strict) subsets of the domains of the original functions.

For example, using function in the sense of multivalued functions, just as the square root function ${\displaystyle y={\sqrt {x}}}$ could be defined from ${\displaystyle y^{2}=x,}$ the function ${\displaystyle y=\arcsin(x)}$ is defined so that ${\displaystyle \sin(y)=x.}$ For a given real number ${\displaystyle x,}$ with ${\displaystyle -1\leq x\leq 1,}$ there are multiple (in fact, countably infinitely many) numbers ${\displaystyle y}$ such that ${\displaystyle \sin(y)=x}$ ; for example, ${\displaystyle \sin(0)=0,}$ but also ${\displaystyle \sin(\pi )=0,}$ ${\displaystyle \sin(2\pi )=0,}$ etc. When only one value is desired, the function may be restricted to its principal branch. With this restriction, for each ${\displaystyle x}$ in the domain, the expression ${\displaystyle \arcsin(x)}$ will evaluate only to a single value, called its principal value. These properties apply to all the inverse trigonometric functions.

The principal inverses are listed in the following table.

Note: Some authors define the range of arcsecant to be ${\textstyle (0\leq y<{\frac {\pi }{2}}{\text{ or }}\pi \leq y<{\frac {3\pi }{2}})}$ , because the tangent function is nonnegative on this domain. This makes some computations more consistent. For example, using this range, ${\displaystyle \tan(\operatorname {arcsec}(x))={\sqrt {x^{2}-1}},}$ whereas with the range ${\textstyle (0\leq y<{\frac {\pi }{2}}{\text{ or }}{\frac {\pi }{2}}<y\leq \pi )}$ , we would have to write ${\displaystyle \tan(\operatorname {arcsec}(x))=\pm {\sqrt {x^{2}-1}},}$ since tangent is nonnegative on ${\textstyle 0\leq y<{\frac {\pi }{2}},}$ but nonpositive on ${\textstyle {\frac {\pi }{2}}<y\leq \pi .}$ For a similar reason, the same authors define the range of arccosecant to be ${\textstyle (-\pi <y\leq -{\frac {\pi }{2}}}$ or ${\textstyle 0<y\leq {\frac {\pi }{2}}).}$

If ${\displaystyle x}$ is allowed to be a complex number, then the range of ${\displaystyle y}$ applies only to its real part.

The table below displays names and domains of the inverse trigonometric functions along with the range of their usual principal values in radians.

The symbol ${\displaystyle \mathbb {R} =(-\infty ,\infty )}$ denotes the set of all real numbers and ${\displaystyle \mathbb {Z} =\{\ldots ,\,-2,\,-1,\,0,\,1,\,2,\,\ldots \}}$ denotes the set of all integers. The set of all integer multiples of ${\displaystyle \pi }$ is denoted by

The symbol ${\displaystyle \,\setminus \,}$ denotes set subtraction so that, for instance, ${\displaystyle \mathbb {R} \setminus (-1,1)=(-\infty ,-1]\cup [1,\infty )}$ is the set of points in ${\displaystyle \mathbb {R} }$ (that is, real numbers) that are not in the interval ${\displaystyle (-1,1).}$

The Minkowski sum notation ${\textstyle \pi \mathbb {Z} +(0,\pi )}$ and ${\displaystyle \pi \mathbb {Z} +{\bigl (}{-{\tfrac {\pi }{2}}},{\tfrac {\pi }{2}}{\bigr )}}$ that is used above to concisely write the domains of ${\displaystyle \cot ,\csc ,\tan ,{\text{ and }}\sec }$ is now explained.

Domain of cotangent ${\displaystyle \cot }$ and cosecant ${\displaystyle \csc }$ : The domains of ${\displaystyle \,\cot \,}$ and ${\displaystyle \,\csc \,}$ are the same. They are the set of all angles ${\displaystyle \theta }$ at which ${\displaystyle \sin \theta \neq 0,}$ i.e. all real numbers that are not of the form ${\displaystyle \pi n}$ for some integer ${\displaystyle n,}$

Domain of tangent ${\displaystyle \tan }$ and secant ${\displaystyle \sec }$ : The domains of ${\displaystyle \,\tan \,}$ and ${\displaystyle \,\sec \,}$ are the same. They are the set of all angles ${\displaystyle \theta }$ at which ${\displaystyle \cos \theta \neq 0,}$


£#h5#£Solutions to elementary trigonometric equations£#/h5#£
Each of the trigonometric functions is periodic in the real part of its argument, running through all its values twice in each interval of ${\displaystyle 2\pi }$ :

£#ul#££#li#£Sine and cosecant begin their period at ${\textstyle 2\pi k-{\frac {\pi }{2}}}$ (where ${\displaystyle k}$ is an integer), finish it at ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ and then reverse themselves over ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ to ${\displaystyle 2\pi k+{\frac {3\pi }{2}}.}$ £#/li#£ £#li#£Cosine and secant begin their period at ${\displaystyle 2\pi k,}$ finish it at ${\displaystyle 2\pi k+\pi .}$ and then reverse themselves over ${\displaystyle 2\pi k+\pi }$ to ${\displaystyle 2\pi k+2\pi .}$ £#/li#£ £#li#£Tangent begins its period at ${\textstyle 2\pi k-{\frac {\pi }{2}},}$ finishes it at ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ and then repeats it (forward) over ${\textstyle 2\pi k+{\frac {\pi }{2}}}$ to ${\textstyle 2\pi k+{\frac {3\pi }{2}}.}$ £#/li#£ £#li#£Cotangent begins its period at ${\displaystyle 2\pi k,}$ finishes it at ${\displaystyle 2\pi k+\pi ,}$ and then repeats it (forward) over ${\displaystyle 2\pi k+\pi }$ to ${\displaystyle 2\pi k+2\pi .}$ £#/li#££#/ul#£
This periodicity is reflected in the general inverses, where ${\displaystyle k}$ is some integer.

The following table shows how inverse trigonometric functions may be used to solve equalities involving the six standard trigonometric functions. It is assumed that the given values ${\displaystyle \theta ,}$ ${\displaystyle r,}$ ${\displaystyle s,}$ ${\displaystyle x,}$ and ${\displaystyle y}$ all lie within appropriate ranges so that the relevant expressions below are well-defined. Note that "for some ${\displaystyle k\in \mathbb {Z} }$ " is just another way of saying "for some integer ${\displaystyle k.}$ "

The symbol ${\displaystyle \,\iff \,}$ is logical equality. The expression "LHS ${\displaystyle \,\iff \,}$ RHS" indicates that either (a) the left hand side (i.e. LHS) and right hand side (i.e. RHS) are both true, or else (b) the left hand side and right hand side are both false; there is no option (c) (e.g. it is not possible for the LHS statement to be true and also simultaneously for the RHS statement to false), because otherwise "LHS ${\displaystyle \,\iff \,}$ RHS" would not have been written (see this footnote for an example illustrating this concept).

For example, if ${\displaystyle \cos \theta =-1}$ then ${\displaystyle \theta =\pi +2\pi k=-\pi +2\pi (1+k)}$ for some ${\displaystyle k\in \mathbb {Z} .}$ While if ${\displaystyle \sin \theta =\pm 1}$ then ${\textstyle \theta ={\frac {\pi }{2}}+\pi k=-{\frac {\pi }{2}}+\pi (k+1)}$ for some ${\displaystyle k\in \mathbb {Z} ,}$ where ${\displaystyle k}$ will be even if ${\displaystyle \sin \theta =1}$ and it will be odd if ${\displaystyle \sin \theta =-1.}$ The equations ${\displaystyle \sec \theta =-1}$ and ${\displaystyle \csc \theta =\pm 1}$ have the same solutions as ${\displaystyle \cos \theta =-1}$ and ${\displaystyle \sin \theta =\pm 1,}$ respectively. In all equations above except for those just solved (i.e. except for ${\displaystyle \sin }$ / ${\displaystyle \csc \theta =\pm 1}$ and ${\displaystyle \cos }$ / ${\displaystyle \sec \theta =-1}$ ), the integer ${\displaystyle k}$ in the solution's formula is uniquely determined by ${\displaystyle \theta }$ (for fixed ${\displaystyle r,s,x,}$ and ${\displaystyle y}$ ).

Detailed example and explanation of the "plus or minus" symbol ${\displaystyle \pm }$
The solutions to ${\displaystyle \cos \theta =x}$ and ${\displaystyle \sec \theta =x}$ involve the "plus or minus" symbol ${\displaystyle \,\pm ,\,}$ whose meaning is now clarified. Only the solution to ${\displaystyle \cos \theta =x}$ will be discussed since the discussion for ${\displaystyle \sec \theta =x}$ is the same. We are given ${\displaystyle x}$ between ${\displaystyle -1\leq x\leq 1}$ and we know that there is an angle ${\displaystyle \theta }$ in some give interval that satisfies ${\displaystyle \cos \theta =x.}$ We want to find this ${\displaystyle \theta .}$ The formula for the solution involves:

If ${\displaystyle \,\arccos x=0\,}$ (which only happens when ${\displaystyle x=1}$ ) then ${\displaystyle \,+\arccos x=0\,}$ and ${\displaystyle \,-\arccos x=0\,}$ so either way, ${\displaystyle \,\pm \arccos x\,}$ can only be equal to ${\displaystyle 0.}$ But if ${\displaystyle \,\arccos x\neq 0,\,}$ which will now be assumed, then the solution to ${\displaystyle \cos \theta =x,}$ which is written above as is shorthand for the following statement:
Either £#ul#££#li#£ ${\displaystyle \,\theta =\arccos x+2\pi k\,}$ for some integer ${\displaystyle k,}$
or else£#/li#£ £#li#£ ${\displaystyle \,\theta =-\arccos x+2\pi k\,}$ for some integer ${\displaystyle k.}$ £#/li#££#/ul#£
Because ${\displaystyle \,\arccos x\neq 0\,}$ and ${\displaystyle \,0<\arccos x\leq \pi \,}$ exactly one of these two equalities can hold. Additional information about ${\displaystyle \theta }$ is needed to determine which one holds. For example, suppose that ${\displaystyle x=0}$ and that all that is known about ${\displaystyle \theta }$ is that ${\displaystyle \,-\pi \leq \theta \leq \pi \,}$ (and nothing more is known). Then

and moreover, in this particular case ${\displaystyle k=0}$ (for both the ${\displaystyle \,+\,}$ case and the ${\displaystyle \,-\,}$ case) and so consequently, This means that ${\displaystyle \theta }$ could be either ${\displaystyle \,\pi /2\,}$ or ${\displaystyle \,-\pi /2.}$ Without additional information it is not possible to determine which of these values ${\displaystyle \theta }$ has. An example of some additional information that could determine the value of ${\displaystyle \theta }$ would be knowing that the angle is above the ${\displaystyle x}$ -axis(in which case ${\displaystyle \theta =\pi /2}$ ) or alternatively, knowing that it is below the ${\displaystyle x}$ -axis(in which case ${\displaystyle \theta =-\pi /2}$ ).
Transforming equations

The equations above can be transformed by using the reflection and shift identities:

These formulas imply, in particular, that the following hold:

where swapping ${\displaystyle \sin \leftrightarrow \csc ,}$ swapping ${\displaystyle \cos \leftrightarrow \sec ,}$ and swapping ${\displaystyle \tan \leftrightarrow \cot }$ gives the analogous equations for ${\displaystyle \csc ,\sec ,{\text{ and }}\cot ,}$ respectively.
So for example, by using the equality ${\textstyle \sin \left({\frac {\pi }{2}}-\theta \right)=\cos \theta ,}$ the equation ${\displaystyle \cos \theta =x}$ can be transformed into ${\textstyle \sin \left({\frac {\pi }{2}}-\theta \right)=x,}$ which allows for the solution to the equation ${\displaystyle \;\sin \varphi =x\;}$ (where ${\textstyle \varphi :={\frac {\pi }{2}}-\theta }$ ) to be used; that solution being: ${\displaystyle \varphi =(-1)^{k}\arcsin(x)+\pi k\;{\text{ for some }}k\in \mathbb {Z} ,}$ which becomes:

where using the fact that ${\displaystyle (-1)^{k}=(-1)^{-k}}$ and substituting ${\displaystyle h:=-k}$ proves that another solution to ${\displaystyle \;\cos \theta =x\;}$ is: The substitution ${\displaystyle \;\arcsin x={\frac {\pi }{2}}-\arccos x\;}$ may be used express the right hand side of the above formula in terms of ${\displaystyle \;\arccos x\;}$ instead of ${\displaystyle \;\arcsin x.\;}$
£#h5#£Equal identical trigonometric functions£#/h5#£
The table below shows how two angles ${\displaystyle \theta }$ and ${\displaystyle \varphi }$ must be related if their values under a given trigonometric function are equal or negatives of each other.


£#h5#£Relationships between trigonometric functions and inverse trigonometric functions£#/h5#£
Trigonometric functions of inverse trigonometric functions are tabulated below. A quick way to derive them is by considering the geometry of a right-angled triangle, with one side of length 1 and another side of length ${\displaystyle x,}$ then applying the Pythagorean theorem and definitions of the trigonometric ratios. Purely algebraic derivations are longer. It is worth noting that for arcsecant and arccosecant, the diagram assumes that ${\displaystyle x}$ is positive, and thus the result has to be corrected through the use of absolute values and the signum (sgn) operation.


£#h5#£Relationships among the inverse trigonometric functions£#/h5#£
Complementary angles:

${\displaystyle {\begin{aligned}\arccos(x)&={\frac {\pi }{2}}-\arcsin(x)\\[0.5em]\operatorname {arccot}(x)&={\frac {\pi }{2}}-\arctan(x)\\[0.5em]\operatorname {arccsc}(x)&={\frac {\pi }{2}}-\operatorname {arcsec}(x)\end{aligned}}}$
Negative arguments:

${\displaystyle {\begin{aligned}\arcsin(-x)&=-\arcsin(x)\\\arccos(-x)&=\pi -\arccos(x)\\\arctan(-x)&=-\arctan(x)\\\operatorname {arccot}(-x)&=\pi -\operatorname {arccot}(x)\\\operatorname {arcsec}(-x)&=\pi -\operatorname {arcsec}(x)\\\operatorname {arccsc}(-x)&=-\operatorname {arccsc}(x)\end{aligned}}}$
Reciprocal arguments:

${\displaystyle {\begin{aligned}\arccos \left({\frac {1}{x}}\right)&=\operatorname {arcsec}(x)\\[0.3em]\arcsin \left({\frac {1}{x}}\right)&=\operatorname {arccsc}(x)\\[0.3em]\arctan \left({\frac {1}{x}}\right)&={\frac {\pi }{2}}-\arctan(x)=\operatorname {arccot}(x)\,,{\text{ if }}x>0\\[0.3em]\arctan \left({\frac {1}{x}}\right)&=-{\frac {\pi }{2}}-\arctan(x)=\operatorname {arccot}(x)-\pi \,,{\text{ if }}x<0\\[0.3em]\operatorname {arccot} \left({\frac {1}{x}}\right)&={\frac {\pi }{2}}-\operatorname {arccot}(x)=\arctan(x)\,,{\text{ if }}x>0\\[0.3em]\operatorname {arccot} \left({\frac {1}{x}}\right)&={\frac {3\pi }{2}}-\operatorname {arccot}(x)=\pi +\arctan(x)\,,{\text{ if }}x<0\\[0.3em]\operatorname {arcsec} \left({\frac {1}{x}}\right)&=\arccos(x)\\[0.3em]\operatorname {arccsc} \left({\frac {1}{x}}\right)&=\arcsin(x)\end{aligned}}}$
Useful identities if one only has a fragment of a sine table:

${\displaystyle {\begin{aligned}\arccos(x)&=\arcsin \left({\sqrt {1-x^{2}}}\right)\,,{\text{ if }}0\leq x\leq 1{\text{ , from which you get }}\\\arccos &\left({\frac {1-x^{2}}{1+x^{2}}}\right)=\arcsin \left({\frac {2x}{1+x^{2}}}\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin &\left({\sqrt {1-x^{2}}}\right)={\frac {\pi }{2}}-\operatorname {sgn}(x)\arcsin(x)\\\arccos(x)&={\frac {1}{2}}\arccos \left(2x^{2}-1\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin(x)&={\frac {1}{2}}\arccos \left(1-2x^{2}\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin(x)&=\arctan \left({\frac {x}{\sqrt {1-x^{2}}}}\right)\\\arccos(x)&=\arctan \left({\frac {\sqrt {1-x^{2}}}{x}}\right)\\\arctan(x)&=\arcsin \left({\frac {x}{\sqrt {1+x^{2}}}}\right)\\\operatorname {arccot}(x)&=\arccos \left({\frac {x}{\sqrt {1+x^{2}}}}\right)\end{aligned}}}$
Whenever the square root of a complex number is used here, we choose the root with the positive real part (or positive imaginary part if the square was negative real).

A useful form that follows directly from the table above is

${\displaystyle \arctan \left(x\right)=\arccos \left({\sqrt {\frac {1}{1+x^{2}}}}\right)\,,{\text{ if }}x\geq 0}$ .
It is obtained by recognizing that ${\displaystyle \cos \left(\arctan \left(x\right)\right)={\sqrt {\frac {1}{1+x^{2}}}}=\cos \left(\arccos \left({\sqrt {\frac {1}{1+x^{2}}}}\right)\right)}$ .

From the half-angle formula, ${\displaystyle \tan \left({\tfrac {\theta }{2}}\right)={\tfrac {\sin(\theta )}{1+\cos(\theta )}}}$ , we get:

${\displaystyle {\begin{aligned}\arcsin(x)&=2\arctan \left({\frac {x}{1+{\sqrt {1-x^{2}}}}}\right)\\[0.5em]\arccos(x)&=2\arctan \left({\frac {\sqrt {1-x^{2}}}{1+x}}\right)\,,{\text{ if }}-1<x\leq 1\\[0.5em]\arctan(x)&=2\arctan \left({\frac {x}{1+{\sqrt {1+x^{2}}}}}\right)\end{aligned}}}$

£#h5#£Arctangent addition formula£#/h5#£
${\displaystyle \arctan(u)\pm \arctan(v)=\arctan \left({\frac {u\pm v}{1\mp uv}}\right){\pmod {\pi }}\,,\quad uv\neq 1\,.}$
This is derived from the tangent addition formula

${\displaystyle \tan(\alpha \pm \beta )={\frac {\tan(\alpha )\pm \tan(\beta )}{1\mp \tan(\alpha )\tan(\beta )}}\,,}$
by letting

${\displaystyle \alpha =\arctan(u)\,,\quad \beta =\arctan(v)\,.}$

£#h5#£In calculus£#/h5#£
£#h5#£Derivatives of inverse trigonometric functions£#/h5#£
The derivatives for complex values of z are as follows:

${\displaystyle {\begin{aligned}{\frac {d}{dz}}\arcsin(z)&{}={\frac {1}{\sqrt {1-z^{2}}}}\;;&z&{}\neq -1,+1\\{\frac {d}{dz}}\arccos(z)&{}=-{\frac {1}{\sqrt {1-z^{2}}}}\;;&z&{}\neq -1,+1\\{\frac {d}{dz}}\arctan(z)&{}={\frac {1}{1+z^{2}}}\;;&z&{}\neq -i,+i\\{\frac {d}{dz}}\operatorname {arccot}(z)&{}=-{\frac {1}{1+z^{2}}}\;;&z&{}\neq -i,+i\\{\frac {d}{dz}}\operatorname {arcsec}(z)&{}={\frac {1}{z^{2}{\sqrt {1-{\frac {1}{z^{2}}}}}}}\;;&z&{}\neq -1,0,+1\\{\frac {d}{dz}}\operatorname {arccsc}(z)&{}=-{\frac {1}{z^{2}{\sqrt {1-{\frac {1}{z^{2}}}}}}}\;;&z&{}\neq -1,0,+1\end{aligned}}}$
Only for real values of x:

${\displaystyle {\begin{aligned}{\frac {d}{dx}}\operatorname {arcsec}(x)&{}={\frac {1}{|x|{\sqrt {x^{2}-1}}}}\;;&|x|>1\\{\frac {d}{dx}}\operatorname {arccsc}(x)&{}=-{\frac {1}{|x|{\sqrt {x^{2}-1}}}}\;;&|x|>1\end{aligned}}}$
For a sample derivation: if ${\displaystyle \theta =\arcsin(x)}$ , we get:

${\displaystyle {\frac {d\arcsin(x)}{dx}}={\frac {d\theta }{d\sin(\theta )}}={\frac {d\theta }{\cos(\theta )\,d\theta }}={\frac {1}{\cos(\theta )}}={\frac {1}{\sqrt {1-\sin ^{2}(\theta )}}}={\frac {1}{\sqrt {1-x^{2}}}}}$

£#h5#£Expression as definite integrals£#/h5#£
Integrating the derivative and fixing the value at one point gives an expression for the inverse trigonometric function as a definite integral:

${\displaystyle {\begin{aligned}\arcsin(x)&{}=\int _{0}^{x}{\frac {1}{\sqrt {1-z^{2}}}}\,dz\;,&|x|&{}\leq 1\\\arccos(x)&{}=\int _{x}^{1}{\frac {1}{\sqrt {1-z^{2}}}}\,dz\;,&|x|&{}\leq 1\\\arctan(x)&{}=\int _{0}^{x}{\frac {1}{z^{2}+1}}\,dz\;,\\\operatorname {arccot}(x)&{}=\int _{x}^{\infty }{\frac {1}{z^{2}+1}}\,dz\;,\\\operatorname {arcsec}(x)&{}=\int _{1}^{x}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz=\pi +\int _{-x}^{-1}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz\;,&x&{}\geq 1\\\operatorname {arccsc}(x)&{}=\int _{x}^{\infty }{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz=\int _{-\infty }^{-x}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz\;,&x&{}\geq 1\\\end{aligned}}}$
When x equals 1, the integrals with limited domains are improper integrals, but still well-defined.


£#h5#£Infinite series£#/h5#£
Similar to the sine and cosine functions, the inverse trigonometric functions can also be calculated using power series, as follows. For arcsine, the series can be derived by expanding its derivative, ${\textstyle {\tfrac {1}{\sqrt {1-z^{2}}}}}$ , as a binomial series, and integrating term by term (using the integral definition as above). The series for arctangent can similarly be derived by expanding its derivative ${\textstyle {\frac {1}{1+z^{2}}}}$ in a geometric series, and applying the integral definition above (see Leibniz series).

${\displaystyle {\begin{aligned}\arcsin(z)&=z+\left({\frac {1}{2}}\right){\frac {z^{3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {z^{5}}{5}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {z^{7}}{7}}+\cdots \\[5pt]&=\sum _{n=0}^{\infty }{\frac {(2n-1)!!}{(2n)!!}}{\frac {z^{2n+1}}{2n+1}}\\[5pt]&=\sum _{n=0}^{\infty }{\frac {(2n)!}{(2^{n}n!)^{2}}}{\frac {z^{2n+1}}{2n+1}}\,;\qquad |z|\leq 1\end{aligned}}}$
${\displaystyle \arctan(z)=z-{\frac {z^{3}}{3}}+{\frac {z^{5}}{5}}-{\frac {z^{7}}{7}}+\cdots =\sum _{n=0}^{\infty }{\frac {(-1)^{n}z^{2n+1}}{2n+1}}\,;\qquad |z|\leq 1\qquad z\neq i,-i}$
Series for the other inverse trigonometric functions can be given in terms of these according to the relationships given above. For example, ${\displaystyle \arccos(x)=\pi /2-\arcsin(x)}$ , ${\displaystyle \operatorname {arccsc}(x)=\arcsin(1/x)}$ , and so on. Another series is given by:

${\displaystyle 2\left(\arcsin \left({\frac {x}{2}}\right)\right)^{2}=\sum _{n=1}^{\infty }{\frac {x^{2n}}{n^{2}{\binom {2n}{n}}}}.}$
Leonhard Euler found a series for the arctangent that converges more quickly than its Taylor series:

${\displaystyle \arctan(z)={\frac {z}{1+z^{2}}}\sum _{n=0}^{\infty }\prod _{k=1}^{n}{\frac {2kz^{2}}{(2k+1)(1+z^{2})}}.}$
(The term in the sum for n = 0 is the empty product, so is 1.)

Alternatively, this can be expressed as

${\displaystyle \arctan(z)=\sum _{n=0}^{\infty }{\frac {2^{2n}(n!)^{2}}{(2n+1)!}}{\frac {z^{2n+1}}{(1+z^{2})^{n+1}}}.}$
Another series for the arctangent function is given by

${\displaystyle \arctan(z)=i\sum _{n=1}^{\infty }{\frac {1}{2n-1}}\left({\frac {1}{(1+2i/z)^{2n-1}}}-{\frac {1}{(1-2i/z)^{2n-1}}}\right),}$
where ${\displaystyle i={\sqrt {-1}}}$ is the imaginary unit.


£#h5#£Continued fractions for arctangent£#/h5#£
Two alternatives to the power series for arctangent are these generalized continued fractions:

${\displaystyle \arctan(z)={\frac {z}{1+{\cfrac {(1z)^{2}}{3-1z^{2}+{\cfrac {(3z)^{2}}{5-3z^{2}+{\cfrac {(5z)^{2}}{7-5z^{2}+{\cfrac {(7z)^{2}}{9-7z^{2}+\ddots }}}}}}}}}}={\frac {z}{1+{\cfrac {(1z)^{2}}{3+{\cfrac {(2z)^{2}}{5+{\cfrac {(3z)^{2}}{7+{\cfrac {(4z)^{2}}{9+\ddots }}}}}}}}}}}$
The second of these is valid in the cut complex plane. There are two cuts, from −i to the point at infinity, going down the imaginary axis, and from i to the point at infinity, going up the same axis. It works best for real numbers running from −1 to 1. The partial denominators are the odd natural numbers, and the partial numerators (after the first) are just (nz)2, with each perfect square appearing once. The first was developed by Leonhard Euler; the second by Carl Friedrich Gauss utilizing the Gaussian hypergeometric series.


£#h5#£Indefinite integrals of inverse trigonometric functions£#/h5#£
For real and complex values of z:

${\displaystyle {\begin{aligned}\int \arcsin(z)\,dz&{}=z\,\arcsin(z)+{\sqrt {1-z^{2}}}+C\\\int \arccos(z)\,dz&{}=z\,\arccos(z)-{\sqrt {1-z^{2}}}+C\\\int \arctan(z)\,dz&{}=z\,\arctan(z)-{\frac {1}{2}}\ln \left(1+z^{2}\right)+C\\\int \operatorname {arccot}(z)\,dz&{}=z\,\operatorname {arccot}(z)+{\frac {1}{2}}\ln \left(1+z^{2}\right)+C\\\int \operatorname {arcsec}(z)\,dz&{}=z\,\operatorname {arcsec}(z)-\ln \left[z\left(1+{\sqrt {\frac {z^{2}-1}{z^{2}}}}\right)\right]+C\\\int \operatorname {arccsc}(z)\,dz&{}=z\,\operatorname {arccsc}(z)+\ln \left[z\left(1+{\sqrt {\frac {z^{2}-1}{z^{2}}}}\right)\right]+C\end{aligned}}}$
For real x ≥ 1:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\ln \left(x+{\sqrt {x^{2}-1}}\right)+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\ln \left(x+{\sqrt {x^{2}-1}}\right)+C\end{aligned}}}$
For all real x not between -1 and 1:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\operatorname {sgn}(x)\ln \left|x+{\sqrt {x^{2}-1}}\right|+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\operatorname {sgn}(x)\ln \left|x+{\sqrt {x^{2}-1}}\right|+C\end{aligned}}}$
The absolute value is necessary to compensate for both negative and positive values of the arcsecant and arccosecant functions. The signum function is also necessary due to the absolute values in the derivatives of the two functions, which create two different solutions for positive and negative values of x. These can be further simplified using the logarithmic definitions of the inverse hyperbolic functions:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\operatorname {arcosh} (|x|)+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\operatorname {arcosh} (|x|)+C\\\end{aligned}}}$
The absolute value in the argument of the arcosh function creates a negative half of its graph, making it identical to the signum logarithmic function shown above.

All of these antiderivatives can be derived using integration by parts and the simple derivative forms shown above.


£#h5#£Example£#/h5#£
Using ${\displaystyle \int u\,dv=uv-\int v\,du}$ (i.e. integration by parts), set

${\displaystyle {\begin{aligned}u&=\arcsin(x)&dv&=dx\\du&={\frac {dx}{\sqrt {1-x^{2}}}}&v&=x\end{aligned}}}$
Then

${\displaystyle \int \arcsin(x)\,dx=x\arcsin(x)-\int {\frac {x}{\sqrt {1-x^{2}}}}\,dx,}$
which by the simple substitution ${\displaystyle w=1-x^{2},\ dw=-2x\,dx}$ yields the final result:

${\displaystyle \int \arcsin(x)\,dx=x\arcsin(x)+{\sqrt {1-x^{2}}}+C}$

£#h5#£Extension to complex plane£#/h5#£
Since the inverse trigonometric functions are analytic functions, they can be extended from the real line to the complex plane. This results in functions with multiple sheets and branch points. One possible way of defining the extension is:

${\displaystyle \arctan(z)=\int _{0}^{z}{\frac {dx}{1+x^{2}}}\quad z\neq -i,+i}$
where the part of the imaginary axis which does not lie strictly between the branch points (−i and +i) is the branch cut between the principal sheet and other sheets. The path of the integral must not cross a branch cut. For z not on a branch cut, a straight line path from 0 to z is such a path. For z on a branch cut, the path must approach from Re[x] > 0 for the upper branch cut and from Re[x] < 0 for the lower branch cut.

The arcsine function may then be defined as:

${\displaystyle \arcsin(z)=\arctan \left({\frac {z}{\sqrt {1-z^{2}}}}\right)\quad z\neq -1,+1}$
where (the square-root function has its cut along the negative real axis and) the part of the real axis which does not lie strictly between −1 and +1 is the branch cut between the principal sheet of arcsin and other sheets;

${\displaystyle \arccos(z)={\frac {\pi }{2}}-\arcsin(z)\quad z\neq -1,+1}$
which has the same cut as arcsin;

${\displaystyle \operatorname {arccot}(z)={\frac {\pi }{2}}-\arctan(z)\quad z\neq -i,i}$
which has the same cut as arctan;

${\displaystyle \operatorname {arcsec}(z)=\arccos \left({\frac {1}{z}}\right)\quad z\neq -1,0,+1}$
where the part of the real axis between −1 and +1 inclusive is the cut between the principal sheet of arcsec and other sheets;

${\displaystyle \operatorname {arccsc}(z)=\arcsin \left({\frac {1}{z}}\right)\quad z\neq -1,0,+1}$
which has the same cut as arcsec.


£#h5#£Logarithmic forms£#/h5#£
These functions may also be expressed using complex logarithms. This extends their domains to the complex plane in a natural fashion. The following identities for principal values of the functions hold everywhere that they are defined, even on their branch cuts.

${\displaystyle {\begin{aligned}\arcsin(z)&{}=-i\ln \left({\sqrt {1-z^{2}}}+iz\right)=i\ln \left({\sqrt {1-z^{2}}}-iz\right)&{}=\operatorname {arccsc} \left({\frac {1}{z}}\right)\\[10pt]\arccos(z)&{}=-i\ln \left(i{\sqrt {1-z^{2}}}+z\right)={\frac {\pi }{2}}-\arcsin(z)&{}=\operatorname {arcsec} \left({\frac {1}{z}}\right)\\[10pt]\arctan(z)&{}=-{\frac {i}{2}}\ln \left({\frac {i-z}{i+z}}\right)=-{\frac {i}{2}}\ln \left({\frac {1+iz}{1-iz}}\right)&{}=\operatorname {arccot} \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arccot}(z)&{}=-{\frac {i}{2}}\ln \left({\frac {z+i}{z-i}}\right)=-{\frac {i}{2}}\ln \left({\frac {iz-1}{iz+1}}\right)&{}=\arctan \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arcsec}(z)&{}=-i\ln \left(i{\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {1}{z}}\right)={\frac {\pi }{2}}-\operatorname {arccsc}(z)&{}=\arccos \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arccsc}(z)&{}=-i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {i}{z}}\right)=i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}-{\frac {i}{z}}\right)&{}=\arcsin \left({\frac {1}{z}}\right)\end{aligned}}}$

£#h5#£Generalization£#/h5#£
Because all of the inverse trigonometric functions output an angle of a right triangle, they can be generalized by using Euler's formula to form a right triangle in the complex plane. Algebraically, this gives us:

${\displaystyle ce^{\theta i}=c\cos(\theta )+ci\sin(\theta )}$
or

${\displaystyle ce^{\theta i}=a+bi}$
where ${\displaystyle a}$ is the adjacent side, ${\displaystyle b}$ is the opposite side, and ${\displaystyle c}$ is the hypotenuse. From here, we can solve for ${\displaystyle \theta }$ .

${\displaystyle {\begin{aligned}e^{\ln(c)+\theta i}&=a+bi\\\ln c+\theta i&=\ln(a+bi)\\\theta &=\operatorname {Im} \left(\ln(a+bi)\right)\end{aligned}}}$
or

${\displaystyle \theta =-i\ln \left({\frac {a+bi}{c}}\right)=i\ln \left({\frac {c}{a+bi}}\right)}$
Simply taking the imaginary part works for any real-valued ${\displaystyle a}$ and ${\displaystyle b}$ , but if ${\displaystyle a}$ or ${\displaystyle b}$ is complex-valued, we have to use the final equation so that the real part of the result isn't excluded. Since the length of the hypotenuse doesn't change the angle, ignoring the real part of ${\displaystyle \ln(a+bi)}$ also removes ${\displaystyle c}$ from the equation. In the final equation, we see that the angle of the triangle in the complex plane can be found by inputting the lengths of each side. By setting one of the three sides equal to 1 and one of the remaining sides equal to our input ${\displaystyle z}$ , we obtain a formula for one of the inverse trig functions, for a total of six equations. Because the inverse trig functions require only one input, we must put the final side of the triangle in terms of the other two using the Pythagorean Theorem relation

${\displaystyle a^{2}+b^{2}=c^{2}}$
The table below shows the values of a, b, and c for each of the inverse trig functions and the equivalent expressions for ${\displaystyle \theta }$ that result from plugging the values into the equations above and simplifying.

${\displaystyle {\begin{aligned}&a&&b&&c&&\theta &&\theta _{\text{simplified}}&&\theta _{a,b\in \mathbb {R} }\\\arcsin(z)\ \ &{\sqrt {1-z^{2}}}&&z&&1&&-i\ln \left({\frac {{\sqrt {1-z^{2}}}+zi}{1}}\right)&&=-i\ln \left({\sqrt {1-z^{2}}}+zi\right)&&\operatorname {Im} \left(\ln \left({\sqrt {1-z^{2}}}+zi\right)\right)\\\arccos(z)\ \ &z&&{\sqrt {1-z^{2}}}&&1&&-i\ln \left({\frac {z+i{\sqrt {1-z^{2}}}}{1}}\right)&&=-i\ln \left(z+{\sqrt {z^{2}-1}}\right)&&\operatorname {Im} \left(\ln \left(z+{\sqrt {z^{2}-1}}\right)\right)\\\arctan(z)\ \ &1&&z&&{\sqrt {1+z^{2}}}&&i\ln \left({\frac {\sqrt {1+z^{2}}}{1+zi}}\right)&&={\frac {i}{2}}\ln \left({\frac {i+z}{i-z}}\right)&&\operatorname {Im} \left(\ln \left(1+zi\right)\right)\\\operatorname {arccot}(z)\ \ &z&&1&&{\sqrt {z^{2}+1}}&&i\ln \left({\frac {\sqrt {z^{2}+1}}{z+i}}\right)&&={\frac {i}{2}}\ln \left({\frac {z-i}{z+i}}\right)&&\operatorname {Im} \left(\ln \left(z+i\right)\right)\\\operatorname {arcsec}(z)\ \ &1&&{\sqrt {z^{2}-1}}&&z&&-i\ln \left({\frac {1+i{\sqrt {z^{2}-1}}}{z}}\right)&&=-i\ln \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z^{2}}}-1}}\right)&&\operatorname {Im} \left(\ln \left(1+{\sqrt {1-z^{2}}}\right)\right)\\\operatorname {arccsc}(z)\ \ &{\sqrt {z^{2}-1}}&&1&&z&&-i\ln \left({\frac {{\sqrt {z^{2}-1}}+i}{z}}\right)&&=-i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {i}{z}}\right)&&\operatorname {Im} \left(\ln \left({\sqrt {z^{2}-1}}+i\right)\right)\\\end{aligned}}}$
In this sense, all of the inverse trig functions can be thought of as specific cases of the complex-valued log function. Since this definition works for any complex-valued ${\displaystyle z}$ , this definition allows for hyperbolic angles as outputs and can be used to further define the inverse hyperbolic functions. Elementary proofs of the relations may also proceed via expansion to exponential forms of the trigonometric functions.


£#h5#£Example proof£#/h5#£
${\displaystyle {\begin{aligned}\sin(\phi )&=z\\\phi &=\arcsin(z)\end{aligned}}}$
Using the exponential definition of sine, and letting ${\displaystyle \xi =e^{\phi i},}$

${\displaystyle {\begin{aligned}z&={\frac {e^{\phi i}-e^{-\phi i}}{2i}}\\[10mu]2iz&=\xi -{\frac {1}{\xi }}\\[5mu]0&=\xi ^{2}-2i\xi z-1\\[5mu]\xi &=iz\pm {\sqrt {1-z^{2}}}\\[5mu]\phi &=-i\ln \left(iz\pm {\sqrt {1-z^{2}}}\right)\end{aligned}}}$
(the positive branch is chosen)

${\displaystyle \phi =\arcsin(z)=-i\ln \left(iz+{\sqrt {1-z^{2}}}\right)}$

£#h5#£Applications£#/h5#£
£#h5#£Finding the angle of a right triangle£#/h5#£
Inverse trigonometric functions are useful when trying to determine the remaining two angles of a right triangle when the lengths of the sides of the triangle are known. Recalling the right-triangle definitions of sine and cosine, it follows that

${\displaystyle \theta =\arcsin \left({\frac {\text{opposite}}{\text{hypotenuse}}}\right)=\arccos \left({\frac {\text{adjacent}}{\text{hypotenuse}}}\right).}$
Often, the hypotenuse is unknown and would need to be calculated before using arcsine or arccosine using the Pythagorean Theorem: ${\displaystyle a^{2}+b^{2}=h^{2}}$ where ${\displaystyle h}$ is the length of the hypotenuse. Arctangent comes in handy in this situation, as the length of the hypotenuse is not needed.

${\displaystyle \theta =\arctan \left({\frac {\text{opposite}}{\text{adjacent}}}\right)\,.}$
For example, suppose a roof drops 8 feet as it runs out 20 feet. The roof makes an angle θ with the horizontal, where θ may be computed as follows:

${\displaystyle \theta =\arctan \left({\frac {\text{opposite}}{\text{adjacent}}}\right)=\arctan \left({\frac {\text{rise}}{\text{run}}}\right)=\arctan \left({\frac {8}{20}}\right)\approx 21.8^{\circ }\,.}$

£#h5#£In computer science and engineering£#/h5#£
£#h5#£Two-argument variant of arctangent£#/h5#£

The two-argument atan2 function computes the arctangent of y / x given y and x, but with a range of (−π, π]. In other words, atan2(y, x) is the angle between the positive x-axis of a plane and the point (x, y) on it, with positive sign for counter-clockwise angles (upper half-plane, y > 0), and negative sign for clockwise angles (lower half-plane, y < 0). It was first introduced in many computer programming languages, but it is now also common in other fields of science and engineering.

In terms of the standard arctan function, that is with range of (−π/2, π/2), it can be expressed as follows:

${\displaystyle \operatorname {atan2} (y,x)={\begin{cases}\arctan \left({\frac {y}{x}}\right)&\quad x>0\\\arctan \left({\frac {y}{x}}\right)+\pi &\quad y\geq 0,\;x<0\\\arctan \left({\frac {y}{x}}\right)-\pi &\quad y<0,\;x<0\\{\frac {\pi }{2}}&\quad y>0,\;x=0\\-{\frac {\pi }{2}}&\quad y<0,\;x=0\\{\text{undefined}}&\quad y=0,\;x=0\end{cases}}}$
It also equals the principal value of the argument of the complex number x + iy.

This limited version of the function above may also be defined using the tangent half-angle formulae as follows:

${\displaystyle \operatorname {atan2} (y,x)=2\arctan \left({\frac {y}{{\sqrt {x^{2}+y^{2}}}+x}}\right)}$
provided that either x > 0 or y ≠ 0. However this fails if given x ≤ 0 and y = 0 so the expression is unsuitable for computational use.

The above argument order (y, x) seems to be the most common, and in particular is used in ISO standards such as the C programming language, but a few authors may use the opposite convention (x, y) so some caution is warranted. These variations are detailed at atan2.


£#h5#£Arctangent function with location parameter£#/h5#£
In many applications the solution ${\displaystyle y}$ of the equation ${\displaystyle x=\tan(y)}$ is to come as close as possible to a given value ${\displaystyle -\infty <\eta <\infty }$ . The adequate solution is produced by the parameter modified arctangent function

${\displaystyle y=\arctan _{\eta }(x):=\arctan(x)+\pi \,\operatorname {rni} \left({\frac {\eta -\arctan(x)}{\pi }}\right)\,.}$
The function ${\displaystyle \operatorname {rni} }$ rounds to the nearest integer.


£#h5#£Numerical accuracy£#/h5#£
For angles near 0 and π, arccosine is ill-conditioned and will thus calculate the angle with reduced accuracy in a computer implementation (due to the limited number of digits). Similarly, arcsine is inaccurate for angles near −π/2 and π/2.


£#h5#£See also£#/h5#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Abramowitz, Milton; Stegun, Irene A., eds. (1972). Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables. New York: Dover Publications. ISBN 978-0-486-61272-0.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Inverse Tangent". MathWorld.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Trigonometric Functions £#/li#££#/ul#£




£#h3#£Arctangent£#/h3#£

In mathematics, the inverse trigonometric functions (occasionally also called arcus functions, antitrigonometric functions or cyclometric functions) are the inverse functions of the trigonometric functions (with suitably restricted domains). Specifically, they are the inverses of the sine, cosine, tangent, cotangent, secant, and cosecant functions, and are used to obtain an angle from any of the angle's trigonometric ratios. Inverse trigonometric functions are widely used in engineering, navigation, physics, and geometry.


£#h5#£Notation£#/h5#£
Several notations for the inverse trigonometric functions exist. The most common convention is to name inverse trigonometric functions using an arc- prefix: arcsin(x), arccos(x), arctan(x), etc. (This convention is used throughout this article.) This notation arises from the following geometric relationships: when measuring in radians, an angle of θ radians will correspond to an arc whose length is rθ, where r is the radius of the circle. Thus in the unit circle, "the arc whose cosine is x" is the same as "the angle whose cosine is x", because the length of the arc of the circle in radii is the same as the measurement of the angle in radians. In computer programming languages, the inverse trigonometric functions are often called by the abbreviated forms asin, acos, atan.

The notations sin−1(x), cos−1(x), tan−1(x), etc., as introduced by John Herschel in 1813, are often used as well in English-language sources, much more than the also established sin[−1](x), cos[−1](x), tan[−1](x),—conventions consistent with the notation of an inverse function, that is useful e.g. to define the multivalued version of each inverse trigonometric function: e.g. ${\displaystyle \tan ^{-1}(x)=\{\arctan(x)+\pi k\mid k\in \mathbb {Z} \}}$ . However, this might appear to conflict logically with the common semantics for expressions such as sin2(x) (although only sin2 x, without parentheses, is the really common one), which refer to numeric power rather than function composition, and therefore may result in confusion between multiplicative inverse or reciprocal and compositional inverse. The confusion is somewhat mitigated by the fact that each of the reciprocal trigonometric functions has its own name—for example, (cos(x))−1 = sec(x). Nevertheless, certain authors advise against using it for its ambiguity. Another precarious convention used by a tiny number of authors is to use an uppercase first letter, along with a −1 superscript: Sin−1(x), Cos−1(x), Tan−1(x), etc. Although its intention is to avoid confusion with the multiplicative inverse, which should be represented by sin−1(x), cos−1(x), etc., or, better, by sin−1 x, cos−1 x, etc., it in turn creates yet another major source of ambiguity: especially in light of the fact that many popular high-level programming languages (e.g. Wolfram's Mathematica, and University of Sydney's MAGMA) use those very same capitalised representations for the standard trig functions; whereas others (Python (ie SymPy and NumPy), Matlab, MAPLE etc) use lower-case.

Hence, since 2009, the ISO 80000-2 standard has specified solely the "arc" prefix for the inverse functions.


£#h5#£Basic concepts£#/h5#£
£#h5#£Principal values£#/h5#£
Since none of the six trigonometric functions are one-to-one, they must be restricted in order to have inverse functions. Therefore, the result ranges of the inverse functions are proper (i.e. strict) subsets of the domains of the original functions.

For example, using function in the sense of multivalued functions, just as the square root function ${\displaystyle y={\sqrt {x}}}$ could be defined from ${\displaystyle y^{2}=x,}$ the function ${\displaystyle y=\arcsin(x)}$ is defined so that ${\displaystyle \sin(y)=x.}$ For a given real number ${\displaystyle x,}$ with ${\displaystyle -1\leq x\leq 1,}$ there are multiple (in fact, countably infinitely many) numbers ${\displaystyle y}$ such that ${\displaystyle \sin(y)=x}$ ; for example, ${\displaystyle \sin(0)=0,}$ but also ${\displaystyle \sin(\pi )=0,}$ ${\displaystyle \sin(2\pi )=0,}$ etc. When only one value is desired, the function may be restricted to its principal branch. With this restriction, for each ${\displaystyle x}$ in the domain, the expression ${\displaystyle \arcsin(x)}$ will evaluate only to a single value, called its principal value. These properties apply to all the inverse trigonometric functions.

The principal inverses are listed in the following table.

Note: Some authors define the range of arcsecant to be ${\textstyle (0\leq y<{\frac {\pi }{2}}{\text{ or }}\pi \leq y<{\frac {3\pi }{2}})}$ , because the tangent function is nonnegative on this domain. This makes some computations more consistent. For example, using this range, ${\displaystyle \tan(\operatorname {arcsec}(x))={\sqrt {x^{2}-1}},}$ whereas with the range ${\textstyle (0\leq y<{\frac {\pi }{2}}{\text{ or }}{\frac {\pi }{2}}<y\leq \pi )}$ , we would have to write ${\displaystyle \tan(\operatorname {arcsec}(x))=\pm {\sqrt {x^{2}-1}},}$ since tangent is nonnegative on ${\textstyle 0\leq y<{\frac {\pi }{2}},}$ but nonpositive on ${\textstyle {\frac {\pi }{2}}<y\leq \pi .}$ For a similar reason, the same authors define the range of arccosecant to be ${\textstyle (-\pi <y\leq -{\frac {\pi }{2}}}$ or ${\textstyle 0<y\leq {\frac {\pi }{2}}).}$

If ${\displaystyle x}$ is allowed to be a complex number, then the range of ${\displaystyle y}$ applies only to its real part.

The table below displays names and domains of the inverse trigonometric functions along with the range of their usual principal values in radians.

The symbol ${\displaystyle \mathbb {R} =(-\infty ,\infty )}$ denotes the set of all real numbers and ${\displaystyle \mathbb {Z} =\{\ldots ,\,-2,\,-1,\,0,\,1,\,2,\,\ldots \}}$ denotes the set of all integers. The set of all integer multiples of ${\displaystyle \pi }$ is denoted by

The symbol ${\displaystyle \,\setminus \,}$ denotes set subtraction so that, for instance, ${\displaystyle \mathbb {R} \setminus (-1,1)=(-\infty ,-1]\cup [1,\infty )}$ is the set of points in ${\displaystyle \mathbb {R} }$ (that is, real numbers) that are not in the interval ${\displaystyle (-1,1).}$

The Minkowski sum notation ${\textstyle \pi \mathbb {Z} +(0,\pi )}$ and ${\displaystyle \pi \mathbb {Z} +{\bigl (}{-{\tfrac {\pi }{2}}},{\tfrac {\pi }{2}}{\bigr )}}$ that is used above to concisely write the domains of ${\displaystyle \cot ,\csc ,\tan ,{\text{ and }}\sec }$ is now explained.

Domain of cotangent ${\displaystyle \cot }$ and cosecant ${\displaystyle \csc }$ : The domains of ${\displaystyle \,\cot \,}$ and ${\displaystyle \,\csc \,}$ are the same. They are the set of all angles ${\displaystyle \theta }$ at which ${\displaystyle \sin \theta \neq 0,}$ i.e. all real numbers that are not of the form ${\displaystyle \pi n}$ for some integer ${\displaystyle n,}$

Domain of tangent ${\displaystyle \tan }$ and secant ${\displaystyle \sec }$ : The domains of ${\displaystyle \,\tan \,}$ and ${\displaystyle \,\sec \,}$ are the same. They are the set of all angles ${\displaystyle \theta }$ at which ${\displaystyle \cos \theta \neq 0,}$


£#h5#£Solutions to elementary trigonometric equations£#/h5#£
Each of the trigonometric functions is periodic in the real part of its argument, running through all its values twice in each interval of ${\displaystyle 2\pi }$ :

£#ul#££#li#£Sine and cosecant begin their period at ${\textstyle 2\pi k-{\frac {\pi }{2}}}$ (where ${\displaystyle k}$ is an integer), finish it at ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ and then reverse themselves over ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ to ${\displaystyle 2\pi k+{\frac {3\pi }{2}}.}$ £#/li#£ £#li#£Cosine and secant begin their period at ${\displaystyle 2\pi k,}$ finish it at ${\displaystyle 2\pi k+\pi .}$ and then reverse themselves over ${\displaystyle 2\pi k+\pi }$ to ${\displaystyle 2\pi k+2\pi .}$ £#/li#£ £#li#£Tangent begins its period at ${\textstyle 2\pi k-{\frac {\pi }{2}},}$ finishes it at ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ and then repeats it (forward) over ${\textstyle 2\pi k+{\frac {\pi }{2}}}$ to ${\textstyle 2\pi k+{\frac {3\pi }{2}}.}$ £#/li#£ £#li#£Cotangent begins its period at ${\displaystyle 2\pi k,}$ finishes it at ${\displaystyle 2\pi k+\pi ,}$ and then repeats it (forward) over ${\displaystyle 2\pi k+\pi }$ to ${\displaystyle 2\pi k+2\pi .}$ £#/li#££#/ul#£
This periodicity is reflected in the general inverses, where ${\displaystyle k}$ is some integer.

The following table shows how inverse trigonometric functions may be used to solve equalities involving the six standard trigonometric functions. It is assumed that the given values ${\displaystyle \theta ,}$ ${\displaystyle r,}$ ${\displaystyle s,}$ ${\displaystyle x,}$ and ${\displaystyle y}$ all lie within appropriate ranges so that the relevant expressions below are well-defined. Note that "for some ${\displaystyle k\in \mathbb {Z} }$ " is just another way of saying "for some integer ${\displaystyle k.}$ "

The symbol ${\displaystyle \,\iff \,}$ is logical equality. The expression "LHS ${\displaystyle \,\iff \,}$ RHS" indicates that either (a) the left hand side (i.e. LHS) and right hand side (i.e. RHS) are both true, or else (b) the left hand side and right hand side are both false; there is no option (c) (e.g. it is not possible for the LHS statement to be true and also simultaneously for the RHS statement to false), because otherwise "LHS ${\displaystyle \,\iff \,}$ RHS" would not have been written (see this footnote for an example illustrating this concept).

For example, if ${\displaystyle \cos \theta =-1}$ then ${\displaystyle \theta =\pi +2\pi k=-\pi +2\pi (1+k)}$ for some ${\displaystyle k\in \mathbb {Z} .}$ While if ${\displaystyle \sin \theta =\pm 1}$ then ${\textstyle \theta ={\frac {\pi }{2}}+\pi k=-{\frac {\pi }{2}}+\pi (k+1)}$ for some ${\displaystyle k\in \mathbb {Z} ,}$ where ${\displaystyle k}$ will be even if ${\displaystyle \sin \theta =1}$ and it will be odd if ${\displaystyle \sin \theta =-1.}$ The equations ${\displaystyle \sec \theta =-1}$ and ${\displaystyle \csc \theta =\pm 1}$ have the same solutions as ${\displaystyle \cos \theta =-1}$ and ${\displaystyle \sin \theta =\pm 1,}$ respectively. In all equations above except for those just solved (i.e. except for ${\displaystyle \sin }$ / ${\displaystyle \csc \theta =\pm 1}$ and ${\displaystyle \cos }$ / ${\displaystyle \sec \theta =-1}$ ), the integer ${\displaystyle k}$ in the solution's formula is uniquely determined by ${\displaystyle \theta }$ (for fixed ${\displaystyle r,s,x,}$ and ${\displaystyle y}$ ).

Detailed example and explanation of the "plus or minus" symbol ${\displaystyle \pm }$
The solutions to ${\displaystyle \cos \theta =x}$ and ${\displaystyle \sec \theta =x}$ involve the "plus or minus" symbol ${\displaystyle \,\pm ,\,}$ whose meaning is now clarified. Only the solution to ${\displaystyle \cos \theta =x}$ will be discussed since the discussion for ${\displaystyle \sec \theta =x}$ is the same. We are given ${\displaystyle x}$ between ${\displaystyle -1\leq x\leq 1}$ and we know that there is an angle ${\displaystyle \theta }$ in some give interval that satisfies ${\displaystyle \cos \theta =x.}$ We want to find this ${\displaystyle \theta .}$ The formula for the solution involves:

If ${\displaystyle \,\arccos x=0\,}$ (which only happens when ${\displaystyle x=1}$ ) then ${\displaystyle \,+\arccos x=0\,}$ and ${\displaystyle \,-\arccos x=0\,}$ so either way, ${\displaystyle \,\pm \arccos x\,}$ can only be equal to ${\displaystyle 0.}$ But if ${\displaystyle \,\arccos x\neq 0,\,}$ which will now be assumed, then the solution to ${\displaystyle \cos \theta =x,}$ which is written above as is shorthand for the following statement:
Either £#ul#££#li#£ ${\displaystyle \,\theta =\arccos x+2\pi k\,}$ for some integer ${\displaystyle k,}$
or else£#/li#£ £#li#£ ${\displaystyle \,\theta =-\arccos x+2\pi k\,}$ for some integer ${\displaystyle k.}$ £#/li#££#/ul#£
Because ${\displaystyle \,\arccos x\neq 0\,}$ and ${\displaystyle \,0<\arccos x\leq \pi \,}$ exactly one of these two equalities can hold. Additional information about ${\displaystyle \theta }$ is needed to determine which one holds. For example, suppose that ${\displaystyle x=0}$ and that all that is known about ${\displaystyle \theta }$ is that ${\displaystyle \,-\pi \leq \theta \leq \pi \,}$ (and nothing more is known). Then

and moreover, in this particular case ${\displaystyle k=0}$ (for both the ${\displaystyle \,+\,}$ case and the ${\displaystyle \,-\,}$ case) and so consequently, This means that ${\displaystyle \theta }$ could be either ${\displaystyle \,\pi /2\,}$ or ${\displaystyle \,-\pi /2.}$ Without additional information it is not possible to determine which of these values ${\displaystyle \theta }$ has. An example of some additional information that could determine the value of ${\displaystyle \theta }$ would be knowing that the angle is above the ${\displaystyle x}$ -axis(in which case ${\displaystyle \theta =\pi /2}$ ) or alternatively, knowing that it is below the ${\displaystyle x}$ -axis(in which case ${\displaystyle \theta =-\pi /2}$ ).
Transforming equations

The equations above can be transformed by using the reflection and shift identities:

These formulas imply, in particular, that the following hold:

where swapping ${\displaystyle \sin \leftrightarrow \csc ,}$ swapping ${\displaystyle \cos \leftrightarrow \sec ,}$ and swapping ${\displaystyle \tan \leftrightarrow \cot }$ gives the analogous equations for ${\displaystyle \csc ,\sec ,{\text{ and }}\cot ,}$ respectively.
So for example, by using the equality ${\textstyle \sin \left({\frac {\pi }{2}}-\theta \right)=\cos \theta ,}$ the equation ${\displaystyle \cos \theta =x}$ can be transformed into ${\textstyle \sin \left({\frac {\pi }{2}}-\theta \right)=x,}$ which allows for the solution to the equation ${\displaystyle \;\sin \varphi =x\;}$ (where ${\textstyle \varphi :={\frac {\pi }{2}}-\theta }$ ) to be used; that solution being: ${\displaystyle \varphi =(-1)^{k}\arcsin(x)+\pi k\;{\text{ for some }}k\in \mathbb {Z} ,}$ which becomes:

where using the fact that ${\displaystyle (-1)^{k}=(-1)^{-k}}$ and substituting ${\displaystyle h:=-k}$ proves that another solution to ${\displaystyle \;\cos \theta =x\;}$ is: The substitution ${\displaystyle \;\arcsin x={\frac {\pi }{2}}-\arccos x\;}$ may be used express the right hand side of the above formula in terms of ${\displaystyle \;\arccos x\;}$ instead of ${\displaystyle \;\arcsin x.\;}$
£#h5#£Equal identical trigonometric functions£#/h5#£
The table below shows how two angles ${\displaystyle \theta }$ and ${\displaystyle \varphi }$ must be related if their values under a given trigonometric function are equal or negatives of each other.


£#h5#£Relationships between trigonometric functions and inverse trigonometric functions£#/h5#£
Trigonometric functions of inverse trigonometric functions are tabulated below. A quick way to derive them is by considering the geometry of a right-angled triangle, with one side of length 1 and another side of length ${\displaystyle x,}$ then applying the Pythagorean theorem and definitions of the trigonometric ratios. Purely algebraic derivations are longer. It is worth noting that for arcsecant and arccosecant, the diagram assumes that ${\displaystyle x}$ is positive, and thus the result has to be corrected through the use of absolute values and the signum (sgn) operation.


£#h5#£Relationships among the inverse trigonometric functions£#/h5#£
Complementary angles:

${\displaystyle {\begin{aligned}\arccos(x)&={\frac {\pi }{2}}-\arcsin(x)\\[0.5em]\operatorname {arccot}(x)&={\frac {\pi }{2}}-\arctan(x)\\[0.5em]\operatorname {arccsc}(x)&={\frac {\pi }{2}}-\operatorname {arcsec}(x)\end{aligned}}}$
Negative arguments:

${\displaystyle {\begin{aligned}\arcsin(-x)&=-\arcsin(x)\\\arccos(-x)&=\pi -\arccos(x)\\\arctan(-x)&=-\arctan(x)\\\operatorname {arccot}(-x)&=\pi -\operatorname {arccot}(x)\\\operatorname {arcsec}(-x)&=\pi -\operatorname {arcsec}(x)\\\operatorname {arccsc}(-x)&=-\operatorname {arccsc}(x)\end{aligned}}}$
Reciprocal arguments:

${\displaystyle {\begin{aligned}\arccos \left({\frac {1}{x}}\right)&=\operatorname {arcsec}(x)\\[0.3em]\arcsin \left({\frac {1}{x}}\right)&=\operatorname {arccsc}(x)\\[0.3em]\arctan \left({\frac {1}{x}}\right)&={\frac {\pi }{2}}-\arctan(x)=\operatorname {arccot}(x)\,,{\text{ if }}x>0\\[0.3em]\arctan \left({\frac {1}{x}}\right)&=-{\frac {\pi }{2}}-\arctan(x)=\operatorname {arccot}(x)-\pi \,,{\text{ if }}x<0\\[0.3em]\operatorname {arccot} \left({\frac {1}{x}}\right)&={\frac {\pi }{2}}-\operatorname {arccot}(x)=\arctan(x)\,,{\text{ if }}x>0\\[0.3em]\operatorname {arccot} \left({\frac {1}{x}}\right)&={\frac {3\pi }{2}}-\operatorname {arccot}(x)=\pi +\arctan(x)\,,{\text{ if }}x<0\\[0.3em]\operatorname {arcsec} \left({\frac {1}{x}}\right)&=\arccos(x)\\[0.3em]\operatorname {arccsc} \left({\frac {1}{x}}\right)&=\arcsin(x)\end{aligned}}}$
Useful identities if one only has a fragment of a sine table:

${\displaystyle {\begin{aligned}\arccos(x)&=\arcsin \left({\sqrt {1-x^{2}}}\right)\,,{\text{ if }}0\leq x\leq 1{\text{ , from which you get }}\\\arccos &\left({\frac {1-x^{2}}{1+x^{2}}}\right)=\arcsin \left({\frac {2x}{1+x^{2}}}\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin &\left({\sqrt {1-x^{2}}}\right)={\frac {\pi }{2}}-\operatorname {sgn}(x)\arcsin(x)\\\arccos(x)&={\frac {1}{2}}\arccos \left(2x^{2}-1\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin(x)&={\frac {1}{2}}\arccos \left(1-2x^{2}\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin(x)&=\arctan \left({\frac {x}{\sqrt {1-x^{2}}}}\right)\\\arccos(x)&=\arctan \left({\frac {\sqrt {1-x^{2}}}{x}}\right)\\\arctan(x)&=\arcsin \left({\frac {x}{\sqrt {1+x^{2}}}}\right)\\\operatorname {arccot}(x)&=\arccos \left({\frac {x}{\sqrt {1+x^{2}}}}\right)\end{aligned}}}$
Whenever the square root of a complex number is used here, we choose the root with the positive real part (or positive imaginary part if the square was negative real).

A useful form that follows directly from the table above is

${\displaystyle \arctan \left(x\right)=\arccos \left({\sqrt {\frac {1}{1+x^{2}}}}\right)\,,{\text{ if }}x\geq 0}$ .
It is obtained by recognizing that ${\displaystyle \cos \left(\arctan \left(x\right)\right)={\sqrt {\frac {1}{1+x^{2}}}}=\cos \left(\arccos \left({\sqrt {\frac {1}{1+x^{2}}}}\right)\right)}$ .

From the half-angle formula, ${\displaystyle \tan \left({\tfrac {\theta }{2}}\right)={\tfrac {\sin(\theta )}{1+\cos(\theta )}}}$ , we get:

${\displaystyle {\begin{aligned}\arcsin(x)&=2\arctan \left({\frac {x}{1+{\sqrt {1-x^{2}}}}}\right)\\[0.5em]\arccos(x)&=2\arctan \left({\frac {\sqrt {1-x^{2}}}{1+x}}\right)\,,{\text{ if }}-1<x\leq 1\\[0.5em]\arctan(x)&=2\arctan \left({\frac {x}{1+{\sqrt {1+x^{2}}}}}\right)\end{aligned}}}$

£#h5#£Arctangent addition formula£#/h5#£
${\displaystyle \arctan(u)\pm \arctan(v)=\arctan \left({\frac {u\pm v}{1\mp uv}}\right){\pmod {\pi }}\,,\quad uv\neq 1\,.}$
This is derived from the tangent addition formula

${\displaystyle \tan(\alpha \pm \beta )={\frac {\tan(\alpha )\pm \tan(\beta )}{1\mp \tan(\alpha )\tan(\beta )}}\,,}$
by letting

${\displaystyle \alpha =\arctan(u)\,,\quad \beta =\arctan(v)\,.}$

£#h5#£In calculus£#/h5#£
£#h5#£Derivatives of inverse trigonometric functions£#/h5#£
The derivatives for complex values of z are as follows:

${\displaystyle {\begin{aligned}{\frac {d}{dz}}\arcsin(z)&{}={\frac {1}{\sqrt {1-z^{2}}}}\;;&z&{}\neq -1,+1\\{\frac {d}{dz}}\arccos(z)&{}=-{\frac {1}{\sqrt {1-z^{2}}}}\;;&z&{}\neq -1,+1\\{\frac {d}{dz}}\arctan(z)&{}={\frac {1}{1+z^{2}}}\;;&z&{}\neq -i,+i\\{\frac {d}{dz}}\operatorname {arccot}(z)&{}=-{\frac {1}{1+z^{2}}}\;;&z&{}\neq -i,+i\\{\frac {d}{dz}}\operatorname {arcsec}(z)&{}={\frac {1}{z^{2}{\sqrt {1-{\frac {1}{z^{2}}}}}}}\;;&z&{}\neq -1,0,+1\\{\frac {d}{dz}}\operatorname {arccsc}(z)&{}=-{\frac {1}{z^{2}{\sqrt {1-{\frac {1}{z^{2}}}}}}}\;;&z&{}\neq -1,0,+1\end{aligned}}}$
Only for real values of x:

${\displaystyle {\begin{aligned}{\frac {d}{dx}}\operatorname {arcsec}(x)&{}={\frac {1}{|x|{\sqrt {x^{2}-1}}}}\;;&|x|>1\\{\frac {d}{dx}}\operatorname {arccsc}(x)&{}=-{\frac {1}{|x|{\sqrt {x^{2}-1}}}}\;;&|x|>1\end{aligned}}}$
For a sample derivation: if ${\displaystyle \theta =\arcsin(x)}$ , we get:

${\displaystyle {\frac {d\arcsin(x)}{dx}}={\frac {d\theta }{d\sin(\theta )}}={\frac {d\theta }{\cos(\theta )\,d\theta }}={\frac {1}{\cos(\theta )}}={\frac {1}{\sqrt {1-\sin ^{2}(\theta )}}}={\frac {1}{\sqrt {1-x^{2}}}}}$

£#h5#£Expression as definite integrals£#/h5#£
Integrating the derivative and fixing the value at one point gives an expression for the inverse trigonometric function as a definite integral:

${\displaystyle {\begin{aligned}\arcsin(x)&{}=\int _{0}^{x}{\frac {1}{\sqrt {1-z^{2}}}}\,dz\;,&|x|&{}\leq 1\\\arccos(x)&{}=\int _{x}^{1}{\frac {1}{\sqrt {1-z^{2}}}}\,dz\;,&|x|&{}\leq 1\\\arctan(x)&{}=\int _{0}^{x}{\frac {1}{z^{2}+1}}\,dz\;,\\\operatorname {arccot}(x)&{}=\int _{x}^{\infty }{\frac {1}{z^{2}+1}}\,dz\;,\\\operatorname {arcsec}(x)&{}=\int _{1}^{x}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz=\pi +\int _{-x}^{-1}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz\;,&x&{}\geq 1\\\operatorname {arccsc}(x)&{}=\int _{x}^{\infty }{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz=\int _{-\infty }^{-x}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz\;,&x&{}\geq 1\\\end{aligned}}}$
When x equals 1, the integrals with limited domains are improper integrals, but still well-defined.


£#h5#£Infinite series£#/h5#£
Similar to the sine and cosine functions, the inverse trigonometric functions can also be calculated using power series, as follows. For arcsine, the series can be derived by expanding its derivative, ${\textstyle {\tfrac {1}{\sqrt {1-z^{2}}}}}$ , as a binomial series, and integrating term by term (using the integral definition as above). The series for arctangent can similarly be derived by expanding its derivative ${\textstyle {\frac {1}{1+z^{2}}}}$ in a geometric series, and applying the integral definition above (see Leibniz series).

${\displaystyle {\begin{aligned}\arcsin(z)&=z+\left({\frac {1}{2}}\right){\frac {z^{3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {z^{5}}{5}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {z^{7}}{7}}+\cdots \\[5pt]&=\sum _{n=0}^{\infty }{\frac {(2n-1)!!}{(2n)!!}}{\frac {z^{2n+1}}{2n+1}}\\[5pt]&=\sum _{n=0}^{\infty }{\frac {(2n)!}{(2^{n}n!)^{2}}}{\frac {z^{2n+1}}{2n+1}}\,;\qquad |z|\leq 1\end{aligned}}}$
${\displaystyle \arctan(z)=z-{\frac {z^{3}}{3}}+{\frac {z^{5}}{5}}-{\frac {z^{7}}{7}}+\cdots =\sum _{n=0}^{\infty }{\frac {(-1)^{n}z^{2n+1}}{2n+1}}\,;\qquad |z|\leq 1\qquad z\neq i,-i}$
Series for the other inverse trigonometric functions can be given in terms of these according to the relationships given above. For example, ${\displaystyle \arccos(x)=\pi /2-\arcsin(x)}$ , ${\displaystyle \operatorname {arccsc}(x)=\arcsin(1/x)}$ , and so on. Another series is given by:

${\displaystyle 2\left(\arcsin \left({\frac {x}{2}}\right)\right)^{2}=\sum _{n=1}^{\infty }{\frac {x^{2n}}{n^{2}{\binom {2n}{n}}}}.}$
Leonhard Euler found a series for the arctangent that converges more quickly than its Taylor series:

${\displaystyle \arctan(z)={\frac {z}{1+z^{2}}}\sum _{n=0}^{\infty }\prod _{k=1}^{n}{\frac {2kz^{2}}{(2k+1)(1+z^{2})}}.}$
(The term in the sum for n = 0 is the empty product, so is 1.)

Alternatively, this can be expressed as

${\displaystyle \arctan(z)=\sum _{n=0}^{\infty }{\frac {2^{2n}(n!)^{2}}{(2n+1)!}}{\frac {z^{2n+1}}{(1+z^{2})^{n+1}}}.}$
Another series for the arctangent function is given by

${\displaystyle \arctan(z)=i\sum _{n=1}^{\infty }{\frac {1}{2n-1}}\left({\frac {1}{(1+2i/z)^{2n-1}}}-{\frac {1}{(1-2i/z)^{2n-1}}}\right),}$
where ${\displaystyle i={\sqrt {-1}}}$ is the imaginary unit.


£#h5#£Continued fractions for arctangent£#/h5#£
Two alternatives to the power series for arctangent are these generalized continued fractions:

${\displaystyle \arctan(z)={\frac {z}{1+{\cfrac {(1z)^{2}}{3-1z^{2}+{\cfrac {(3z)^{2}}{5-3z^{2}+{\cfrac {(5z)^{2}}{7-5z^{2}+{\cfrac {(7z)^{2}}{9-7z^{2}+\ddots }}}}}}}}}}={\frac {z}{1+{\cfrac {(1z)^{2}}{3+{\cfrac {(2z)^{2}}{5+{\cfrac {(3z)^{2}}{7+{\cfrac {(4z)^{2}}{9+\ddots }}}}}}}}}}}$
The second of these is valid in the cut complex plane. There are two cuts, from −i to the point at infinity, going down the imaginary axis, and from i to the point at infinity, going up the same axis. It works best for real numbers running from −1 to 1. The partial denominators are the odd natural numbers, and the partial numerators (after the first) are just (nz)2, with each perfect square appearing once. The first was developed by Leonhard Euler; the second by Carl Friedrich Gauss utilizing the Gaussian hypergeometric series.


£#h5#£Indefinite integrals of inverse trigonometric functions£#/h5#£
For real and complex values of z:

${\displaystyle {\begin{aligned}\int \arcsin(z)\,dz&{}=z\,\arcsin(z)+{\sqrt {1-z^{2}}}+C\\\int \arccos(z)\,dz&{}=z\,\arccos(z)-{\sqrt {1-z^{2}}}+C\\\int \arctan(z)\,dz&{}=z\,\arctan(z)-{\frac {1}{2}}\ln \left(1+z^{2}\right)+C\\\int \operatorname {arccot}(z)\,dz&{}=z\,\operatorname {arccot}(z)+{\frac {1}{2}}\ln \left(1+z^{2}\right)+C\\\int \operatorname {arcsec}(z)\,dz&{}=z\,\operatorname {arcsec}(z)-\ln \left[z\left(1+{\sqrt {\frac {z^{2}-1}{z^{2}}}}\right)\right]+C\\\int \operatorname {arccsc}(z)\,dz&{}=z\,\operatorname {arccsc}(z)+\ln \left[z\left(1+{\sqrt {\frac {z^{2}-1}{z^{2}}}}\right)\right]+C\end{aligned}}}$
For real x ≥ 1:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\ln \left(x+{\sqrt {x^{2}-1}}\right)+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\ln \left(x+{\sqrt {x^{2}-1}}\right)+C\end{aligned}}}$
For all real x not between -1 and 1:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\operatorname {sgn}(x)\ln \left|x+{\sqrt {x^{2}-1}}\right|+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\operatorname {sgn}(x)\ln \left|x+{\sqrt {x^{2}-1}}\right|+C\end{aligned}}}$
The absolute value is necessary to compensate for both negative and positive values of the arcsecant and arccosecant functions. The signum function is also necessary due to the absolute values in the derivatives of the two functions, which create two different solutions for positive and negative values of x. These can be further simplified using the logarithmic definitions of the inverse hyperbolic functions:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\operatorname {arcosh} (|x|)+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\operatorname {arcosh} (|x|)+C\\\end{aligned}}}$
The absolute value in the argument of the arcosh function creates a negative half of its graph, making it identical to the signum logarithmic function shown above.

All of these antiderivatives can be derived using integration by parts and the simple derivative forms shown above.


£#h5#£Example£#/h5#£
Using ${\displaystyle \int u\,dv=uv-\int v\,du}$ (i.e. integration by parts), set

${\displaystyle {\begin{aligned}u&=\arcsin(x)&dv&=dx\\du&={\frac {dx}{\sqrt {1-x^{2}}}}&v&=x\end{aligned}}}$
Then

${\displaystyle \int \arcsin(x)\,dx=x\arcsin(x)-\int {\frac {x}{\sqrt {1-x^{2}}}}\,dx,}$
which by the simple substitution ${\displaystyle w=1-x^{2},\ dw=-2x\,dx}$ yields the final result:

${\displaystyle \int \arcsin(x)\,dx=x\arcsin(x)+{\sqrt {1-x^{2}}}+C}$

£#h5#£Extension to complex plane£#/h5#£
Since the inverse trigonometric functions are analytic functions, they can be extended from the real line to the complex plane. This results in functions with multiple sheets and branch points. One possible way of defining the extension is:

${\displaystyle \arctan(z)=\int _{0}^{z}{\frac {dx}{1+x^{2}}}\quad z\neq -i,+i}$
where the part of the imaginary axis which does not lie strictly between the branch points (−i and +i) is the branch cut between the principal sheet and other sheets. The path of the integral must not cross a branch cut. For z not on a branch cut, a straight line path from 0 to z is such a path. For z on a branch cut, the path must approach from Re[x] > 0 for the upper branch cut and from Re[x] < 0 for the lower branch cut.

The arcsine function may then be defined as:

${\displaystyle \arcsin(z)=\arctan \left({\frac {z}{\sqrt {1-z^{2}}}}\right)\quad z\neq -1,+1}$
where (the square-root function has its cut along the negative real axis and) the part of the real axis which does not lie strictly between −1 and +1 is the branch cut between the principal sheet of arcsin and other sheets;

${\displaystyle \arccos(z)={\frac {\pi }{2}}-\arcsin(z)\quad z\neq -1,+1}$
which has the same cut as arcsin;

${\displaystyle \operatorname {arccot}(z)={\frac {\pi }{2}}-\arctan(z)\quad z\neq -i,i}$
which has the same cut as arctan;

${\displaystyle \operatorname {arcsec}(z)=\arccos \left({\frac {1}{z}}\right)\quad z\neq -1,0,+1}$
where the part of the real axis between −1 and +1 inclusive is the cut between the principal sheet of arcsec and other sheets;

${\displaystyle \operatorname {arccsc}(z)=\arcsin \left({\frac {1}{z}}\right)\quad z\neq -1,0,+1}$
which has the same cut as arcsec.


£#h5#£Logarithmic forms£#/h5#£
These functions may also be expressed using complex logarithms. This extends their domains to the complex plane in a natural fashion. The following identities for principal values of the functions hold everywhere that they are defined, even on their branch cuts.

${\displaystyle {\begin{aligned}\arcsin(z)&{}=-i\ln \left({\sqrt {1-z^{2}}}+iz\right)=i\ln \left({\sqrt {1-z^{2}}}-iz\right)&{}=\operatorname {arccsc} \left({\frac {1}{z}}\right)\\[10pt]\arccos(z)&{}=-i\ln \left(i{\sqrt {1-z^{2}}}+z\right)={\frac {\pi }{2}}-\arcsin(z)&{}=\operatorname {arcsec} \left({\frac {1}{z}}\right)\\[10pt]\arctan(z)&{}=-{\frac {i}{2}}\ln \left({\frac {i-z}{i+z}}\right)=-{\frac {i}{2}}\ln \left({\frac {1+iz}{1-iz}}\right)&{}=\operatorname {arccot} \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arccot}(z)&{}=-{\frac {i}{2}}\ln \left({\frac {z+i}{z-i}}\right)=-{\frac {i}{2}}\ln \left({\frac {iz-1}{iz+1}}\right)&{}=\arctan \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arcsec}(z)&{}=-i\ln \left(i{\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {1}{z}}\right)={\frac {\pi }{2}}-\operatorname {arccsc}(z)&{}=\arccos \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arccsc}(z)&{}=-i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {i}{z}}\right)=i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}-{\frac {i}{z}}\right)&{}=\arcsin \left({\frac {1}{z}}\right)\end{aligned}}}$

£#h5#£Generalization£#/h5#£
Because all of the inverse trigonometric functions output an angle of a right triangle, they can be generalized by using Euler's formula to form a right triangle in the complex plane. Algebraically, this gives us:

${\displaystyle ce^{\theta i}=c\cos(\theta )+ci\sin(\theta )}$
or

${\displaystyle ce^{\theta i}=a+bi}$
where ${\displaystyle a}$ is the adjacent side, ${\displaystyle b}$ is the opposite side, and ${\displaystyle c}$ is the hypotenuse. From here, we can solve for ${\displaystyle \theta }$ .

${\displaystyle {\begin{aligned}e^{\ln(c)+\theta i}&=a+bi\\\ln c+\theta i&=\ln(a+bi)\\\theta &=\operatorname {Im} \left(\ln(a+bi)\right)\end{aligned}}}$
or

${\displaystyle \theta =-i\ln \left({\frac {a+bi}{c}}\right)=i\ln \left({\frac {c}{a+bi}}\right)}$
Simply taking the imaginary part works for any real-valued ${\displaystyle a}$ and ${\displaystyle b}$ , but if ${\displaystyle a}$ or ${\displaystyle b}$ is complex-valued, we have to use the final equation so that the real part of the result isn't excluded. Since the length of the hypotenuse doesn't change the angle, ignoring the real part of ${\displaystyle \ln(a+bi)}$ also removes ${\displaystyle c}$ from the equation. In the final equation, we see that the angle of the triangle in the complex plane can be found by inputting the lengths of each side. By setting one of the three sides equal to 1 and one of the remaining sides equal to our input ${\displaystyle z}$ , we obtain a formula for one of the inverse trig functions, for a total of six equations. Because the inverse trig functions require only one input, we must put the final side of the triangle in terms of the other two using the Pythagorean Theorem relation

${\displaystyle a^{2}+b^{2}=c^{2}}$
The table below shows the values of a, b, and c for each of the inverse trig functions and the equivalent expressions for ${\displaystyle \theta }$ that result from plugging the values into the equations above and simplifying.

${\displaystyle {\begin{aligned}&a&&b&&c&&\theta &&\theta _{\text{simplified}}&&\theta _{a,b\in \mathbb {R} }\\\arcsin(z)\ \ &{\sqrt {1-z^{2}}}&&z&&1&&-i\ln \left({\frac {{\sqrt {1-z^{2}}}+zi}{1}}\right)&&=-i\ln \left({\sqrt {1-z^{2}}}+zi\right)&&\operatorname {Im} \left(\ln \left({\sqrt {1-z^{2}}}+zi\right)\right)\\\arccos(z)\ \ &z&&{\sqrt {1-z^{2}}}&&1&&-i\ln \left({\frac {z+i{\sqrt {1-z^{2}}}}{1}}\right)&&=-i\ln \left(z+{\sqrt {z^{2}-1}}\right)&&\operatorname {Im} \left(\ln \left(z+{\sqrt {z^{2}-1}}\right)\right)\\\arctan(z)\ \ &1&&z&&{\sqrt {1+z^{2}}}&&i\ln \left({\frac {\sqrt {1+z^{2}}}{1+zi}}\right)&&={\frac {i}{2}}\ln \left({\frac {i+z}{i-z}}\right)&&\operatorname {Im} \left(\ln \left(1+zi\right)\right)\\\operatorname {arccot}(z)\ \ &z&&1&&{\sqrt {z^{2}+1}}&&i\ln \left({\frac {\sqrt {z^{2}+1}}{z+i}}\right)&&={\frac {i}{2}}\ln \left({\frac {z-i}{z+i}}\right)&&\operatorname {Im} \left(\ln \left(z+i\right)\right)\\\operatorname {arcsec}(z)\ \ &1&&{\sqrt {z^{2}-1}}&&z&&-i\ln \left({\frac {1+i{\sqrt {z^{2}-1}}}{z}}\right)&&=-i\ln \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z^{2}}}-1}}\right)&&\operatorname {Im} \left(\ln \left(1+{\sqrt {1-z^{2}}}\right)\right)\\\operatorname {arccsc}(z)\ \ &{\sqrt {z^{2}-1}}&&1&&z&&-i\ln \left({\frac {{\sqrt {z^{2}-1}}+i}{z}}\right)&&=-i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {i}{z}}\right)&&\operatorname {Im} \left(\ln \left({\sqrt {z^{2}-1}}+i\right)\right)\\\end{aligned}}}$
In this sense, all of the inverse trig functions can be thought of as specific cases of the complex-valued log function. Since this definition works for any complex-valued ${\displaystyle z}$ , this definition allows for hyperbolic angles as outputs and can be used to further define the inverse hyperbolic functions. Elementary proofs of the relations may also proceed via expansion to exponential forms of the trigonometric functions.


£#h5#£Example proof£#/h5#£
${\displaystyle {\begin{aligned}\sin(\phi )&=z\\\phi &=\arcsin(z)\end{aligned}}}$
Using the exponential definition of sine, and letting ${\displaystyle \xi =e^{\phi i},}$

${\displaystyle {\begin{aligned}z&={\frac {e^{\phi i}-e^{-\phi i}}{2i}}\\[10mu]2iz&=\xi -{\frac {1}{\xi }}\\[5mu]0&=\xi ^{2}-2i\xi z-1\\[5mu]\xi &=iz\pm {\sqrt {1-z^{2}}}\\[5mu]\phi &=-i\ln \left(iz\pm {\sqrt {1-z^{2}}}\right)\end{aligned}}}$
(the positive branch is chosen)

${\displaystyle \phi =\arcsin(z)=-i\ln \left(iz+{\sqrt {1-z^{2}}}\right)}$

£#h5#£Applications£#/h5#£
£#h5#£Finding the angle of a right triangle£#/h5#£
Inverse trigonometric functions are useful when trying to determine the remaining two angles of a right triangle when the lengths of the sides of the triangle are known. Recalling the right-triangle definitions of sine and cosine, it follows that

${\displaystyle \theta =\arcsin \left({\frac {\text{opposite}}{\text{hypotenuse}}}\right)=\arccos \left({\frac {\text{adjacent}}{\text{hypotenuse}}}\right).}$
Often, the hypotenuse is unknown and would need to be calculated before using arcsine or arccosine using the Pythagorean Theorem: ${\displaystyle a^{2}+b^{2}=h^{2}}$ where ${\displaystyle h}$ is the length of the hypotenuse. Arctangent comes in handy in this situation, as the length of the hypotenuse is not needed.

${\displaystyle \theta =\arctan \left({\frac {\text{opposite}}{\text{adjacent}}}\right)\,.}$
For example, suppose a roof drops 8 feet as it runs out 20 feet. The roof makes an angle θ with the horizontal, where θ may be computed as follows:

${\displaystyle \theta =\arctan \left({\frac {\text{opposite}}{\text{adjacent}}}\right)=\arctan \left({\frac {\text{rise}}{\text{run}}}\right)=\arctan \left({\frac {8}{20}}\right)\approx 21.8^{\circ }\,.}$

£#h5#£In computer science and engineering£#/h5#£
£#h5#£Two-argument variant of arctangent£#/h5#£

The two-argument atan2 function computes the arctangent of y / x given y and x, but with a range of (−π, π]. In other words, atan2(y, x) is the angle between the positive x-axis of a plane and the point (x, y) on it, with positive sign for counter-clockwise angles (upper half-plane, y > 0), and negative sign for clockwise angles (lower half-plane, y < 0). It was first introduced in many computer programming languages, but it is now also common in other fields of science and engineering.

In terms of the standard arctan function, that is with range of (−π/2, π/2), it can be expressed as follows:

${\displaystyle \operatorname {atan2} (y,x)={\begin{cases}\arctan \left({\frac {y}{x}}\right)&\quad x>0\\\arctan \left({\frac {y}{x}}\right)+\pi &\quad y\geq 0,\;x<0\\\arctan \left({\frac {y}{x}}\right)-\pi &\quad y<0,\;x<0\\{\frac {\pi }{2}}&\quad y>0,\;x=0\\-{\frac {\pi }{2}}&\quad y<0,\;x=0\\{\text{undefined}}&\quad y=0,\;x=0\end{cases}}}$
It also equals the principal value of the argument of the complex number x + iy.

This limited version of the function above may also be defined using the tangent half-angle formulae as follows:

${\displaystyle \operatorname {atan2} (y,x)=2\arctan \left({\frac {y}{{\sqrt {x^{2}+y^{2}}}+x}}\right)}$
provided that either x > 0 or y ≠ 0. However this fails if given x ≤ 0 and y = 0 so the expression is unsuitable for computational use.

The above argument order (y, x) seems to be the most common, and in particular is used in ISO standards such as the C programming language, but a few authors may use the opposite convention (x, y) so some caution is warranted. These variations are detailed at atan2.


£#h5#£Arctangent function with location parameter£#/h5#£
In many applications the solution ${\displaystyle y}$ of the equation ${\displaystyle x=\tan(y)}$ is to come as close as possible to a given value ${\displaystyle -\infty <\eta <\infty }$ . The adequate solution is produced by the parameter modified arctangent function

${\displaystyle y=\arctan _{\eta }(x):=\arctan(x)+\pi \,\operatorname {rni} \left({\frac {\eta -\arctan(x)}{\pi }}\right)\,.}$
The function ${\displaystyle \operatorname {rni} }$ rounds to the nearest integer.


£#h5#£Numerical accuracy£#/h5#£
For angles near 0 and π, arccosine is ill-conditioned and will thus calculate the angle with reduced accuracy in a computer implementation (due to the limited number of digits). Similarly, arcsine is inaccurate for angles near −π/2 and π/2.


£#h5#£See also£#/h5#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Abramowitz, Milton; Stegun, Irene A., eds. (1972). Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables. New York: Dover Publications. ISBN 978-0-486-61272-0.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Inverse Tangent". MathWorld.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Trigonometric Functions £#/li#££#/ul#£




£#h3#£Arctangent Integral£#/h3#£

In mathematics, the polylogarithm (also known as Jonquière's function, for Alfred Jonquière) is a special function Lis(z) of order s and argument z. Only for special values of s does the polylogarithm reduce to an elementary function such as the natural logarithm or a rational function. In quantum statistics, the polylogarithm function appears as the closed form of integrals of the Fermi–Dirac distribution and the Bose–Einstein distribution, and is also known as the Fermi–Dirac integral or the Bose–Einstein integral. In quantum electrodynamics, polylogarithms of positive integer order arise in the calculation of processes represented by higher-order Feynman diagrams.

The polylogarithm function is equivalent to the Hurwitz zeta function — either function can be expressed in terms of the other — and both functions are special cases of the Lerch transcendent. Polylogarithms should not be confused with polylogarithmic functions nor with the offset logarithmic integral which has the same notation, but with one variable.

Different polylogarithm functions in the complex plane£#/li#£
£#/ul#£
The polylogarithm function is defined by a power series in z, which is also a Dirichlet series in s:

${\displaystyle \operatorname {Li} _{s}(z)=\sum _{k=1}^{\infty }{z^{k} \over k^{s}}=z+{z^{2} \over 2^{s}}+{z^{3} \over 3^{s}}+\cdots }$
This definition is valid for arbitrary complex order s and for all complex arguments z with |z| < 1; it can be extended to |z| ≥ 1 by the process of analytic continuation. (Here the denominator ns is understood as exp(s ln(n)). The special case s = 1 involves the ordinary natural logarithm, Li1(z) = −ln(1−z), while the special cases s = 2 and s = 3 are called the dilogarithm (also referred to as Spence's function) and trilogarithm respectively. The name of the function comes from the fact that it may also be defined as the repeated integral of itself:

${\displaystyle \operatorname {Li} _{s+1}(z)=\int _{0}^{z}{\frac {\operatorname {Li} _{s}(t)}{t}}dt}$
thus the dilogarithm is an integral of a function involving the logarithm, and so on. For nonpositive integer orders s, the polylogarithm is a rational function.


£#h5#£Properties£#/h5#£
In the case where the polylogarithm order ${\displaystyle s}$ is an integer, it will be represented by ${\displaystyle n}$ (or ${\displaystyle -n}$ when negative). It is often convenient to define ${\displaystyle \mu =\ln(z)}$ where ${\displaystyle \ln(z)}$ is the principal branch of the complex logarithm ${\displaystyle \operatorname {Ln} (z)}$ so that ${\displaystyle -\pi <\operatorname {Im} (\mu )\leq \pi .}$ Also, all exponentiation will be assumed to be single-valued: ${\displaystyle z^{s}=\exp(s\ln(z)).}$

Depending on the order ${\displaystyle s}$ , the polylogarithm may be multi-valued. The principal branch of ${\displaystyle \operatorname {Li} _{s}(z)}$ is taken to be given for ${\displaystyle |z|<1}$ by the above series definition and taken to be continuous except on the positive real axis, where a cut is made from ${\displaystyle z=1}$ to ${\displaystyle \infty }$ such that the axis is placed on the lower half plane of ${\displaystyle z}$ . In terms of ${\displaystyle \mu }$ , this amounts to ${\displaystyle -\pi <\operatorname {arg} (-\mu )\leq \pi }$ . The discontinuity of the polylogarithm in dependence on ${\displaystyle \mu }$ can sometimes be confusing.

For real argument ${\displaystyle z}$ , the polylogarithm of real order ${\displaystyle s}$ is real if ${\displaystyle z<1}$ , and its imaginary part for ${\displaystyle z\geq 1}$ is (Wood 1992, § 3):

Going across the cut, if ε is an infinitesimally small positive real number, then:

Both can be concluded from the series expansion (see below) of Lis(eµ) about µ = 0.

The derivatives of the polylogarithm follow from the defining power series:

The square relationship is seen from the series definition, and is related to the duplication formula (see also Clunie (1954), Schrödinger (1952)):

Kummer's function obeys a very similar duplication formula. This is a special case of the multiplication formula, for any positive integer p:

which can be proved using the series definition of the polylogarithm and the orthogonality of the exponential terms (see e.g. discrete Fourier transform).

Another important property, the inversion formula, involves the Hurwitz zeta function or the Bernoulli polynomials and is found under relationship to other functions below.


£#h5#£Particular values£#/h5#£
For particular cases, the polylogarithm may be expressed in terms of other functions (see below). Particular values for the polylogarithm may thus also be found as particular values of these other functions.

£#li#£ For integer values of the polylogarithm order, the following explicit expressions are obtained by repeated application of z·∂/∂z to Li1(z): Accordingly the polylogarithm reduces to a ratio of polynomials in z, and is therefore a rational function of z, for all nonpositive integer orders. The general case may be expressed as a finite sum: where S(n,k) are the Stirling numbers of the second kind. Equivalent formulae applicable to negative integer orders are (Wood 1992, § 6): and: where ${\displaystyle \scriptstyle \left\langle {n \atop k}\right\rangle }$ are the Eulerian numbers. All roots of Li−n(z) are distinct and real; they include z = 0, while the remainder is negative and centered about z = −1 on a logarithmic scale. As n becomes large, the numerical evaluation of these rational expressions increasingly suffers from cancellation (Wood 1992, § 6); full accuracy can be obtained, however, by computing Li−n(z) via the general relation with the Hurwitz zeta function (see below). £#/li#£ £#li#£ Some particular expressions for half-integer values of the argument z are: where ζ is the Riemann zeta function. No formulae of this type are known for higher integer orders (Lewin 1991, p. 2), but one has for instance (Borwein, Borwein & Girgensohn 1995): which involves the alternating double sum In general one has for integer orders n ≥ 2 (Broadhurst 1996, p. 9): where ζ(s1, …, sk) is the multiple zeta function; for example: £#/li#£ £#li#£ As a straightforward consequence of the series definition, values of the polylogarithm at the pth complex roots of unity are given by the Fourier sum: where ζ is the Hurwitz zeta function. For Re(s) > 1, where Lis(1) is finite, the relation also holds with m = 0 or m = p. While this formula is not as simple as that implied by the more general relation with the Hurwitz zeta function listed under relationship to other functions below, it has the advantage of applying to non-negative integer values of s as well. As usual, the relation may be inverted to express ζ(s, m⁄p) for any m = 1, …, p as a Fourier sum of Lis(exp(2πi k⁄p)) over k = 1, …, p. £#/li#£

£#h5#£Relationship to other functions£#/h5#£ £#ul#££#li#£For z = 1, the polylogarithm reduces to the Riemann zeta function £#/li#£ £#li#£The polylogarithm is related to Dirichlet eta function and the Dirichlet beta function: where η(s) is the Dirichlet eta function. For pure imaginary arguments, we have: where β(s) is the Dirichlet beta function.£#/li#£ £#li#£The polylogarithm is related to the complete Fermi–Dirac integral as: £#/li#£ £#li#£The polylogarithm is a special case of the incomplete polylogarithm function £#/li#£ £#li#£The polylogarithm is a special case of the Lerch transcendent (Erdélyi et al. 1981, § 1.11-14) £#/li#£ £#li#£The polylogarithm is related to the Hurwitz zeta function by:£#/li#££#/ul#£
which relation, however, is invalidated at positive integer s by poles of the gamma function Γ(1 − s), and at s = 0 by a pole of both zeta functions; a derivation of this formula is given under series representations below. With a little help from a functional equation for the Hurwitz zeta function, the polylogarithm is consequently also related to that function via (Jonquière 1889):

which relation holds for 0 ≤ Re(x) < 1 if Im(x) ≥ 0, and for 0 < Re(x) ≤ 1 if Im(x) < 0. Equivalently, for all complex s and for complex z ∉ ]0;1], the inversion formula reads

and for all complex s and for complex z ∉ ]1;∞[

For z ∉ ]0;∞[, one has ln(−z) = −ln(−1⁄z), and both expressions agree. These relations furnish the analytic continuation of the polylogarithm beyond the circle of convergence |z| = 1 of the defining power series. (The corresponding equation of Jonquière (1889, eq. 5) and Erdélyi et al. (1981, § 1.11-16) is not correct if one assumes that the principal branches of the polylogarithm and the logarithm are used simultaneously.) See the next item for a simplified formula when s is an integer.

£#ul#££#li#£For positive integer polylogarithm orders s, the Hurwitz zeta function ζ(1−s, x) reduces to Bernoulli polynomials, ζ(1−n, x) = −Bn(x) / n, and Jonquière's inversion formula for n = 1, 2, 3, … becomes:£#/li#££#/ul#£
where again 0 ≤ Re(x) < 1 if Im(x) ≥ 0, and 0 < Re(x) ≤ 1 if Im(x) < 0. Upon restriction of the polylogarithm argument to the unit circle, Im(x) = 0, the left hand side of this formula simplifies to 2 Re(Lin(e2πix)) if n is even, and to 2i Im(Lin(e2πix)) if n is odd. For negative integer orders, on the other hand, the divergence of Γ(s) implies for all z that (Erdélyi et al. 1981, § 1.11-17):

More generally, one has for n = 0, ±1, ±2, ±3, …:

where both expressions agree for z ∉ ]0;∞[. (The corresponding equation of Jonquière (1889, eq. 1) and Erdélyi et al. (1981, § 1.11-18) is again not correct.)

£#ul#££#li#£The polylogarithm with pure imaginary μ may be expressed in terms of the Clausen functions Cis(θ) and Sis(θ), and vice versa (Lewin 1958, Ch. VII § 1.4; Abramowitz & Stegun 1972, § 27.8):£#/li#££#/ul#£
£#ul#££#li#£The inverse tangent integral Tis(z) (Lewin 1958, Ch. VII § 1.2) can be expressed in terms of polylogarithms:£#/li#££#/ul#£
The relation in particular implies:

which explains the function name.

£#ul#££#li#£The Legendre chi function χs(z) (Lewin 1958, Ch. VII § 1.1; Boersma & Dempsey 1992) can be expressed in terms of polylogarithms:£#/li#££#/ul#£
£#ul#££#li#£The polylogarithm of integer order can be expressed as a generalized hypergeometric function:£#/li#££#/ul#£
£#ul#££#li#£In terms of the incomplete zeta functions or "Debye functions" (Abramowitz & Stegun 1972, § 27.1):£#/li#££#/ul#£
the polylogarithm Lin(z) for positive integer n may be expressed as the finite sum (Wood 1992, § 16):

A remarkably similar expression relates the "Debye functions" Zn(z) to the polylogarithm:

£#ul#££#li#£Using Lambert series, if ${\displaystyle J_{s}(n)}$ is Jordan's totient function, then£#/li#££#/ul#£

£#h5#£Integral representations£#/h5#£
Any of the following integral representations furnishes the analytic continuation of the polylogarithm beyond the circle of convergence |z| = 1 of the defining power series.

£#li#£ The polylogarithm can be expressed in terms of the integral of the Bose–Einstein distribution: This converges for Re(s) > 0 and all z except for z real and ≥ 1. The polylogarithm in this context is sometimes referred to as a Bose integral but more commonly as a Bose–Einstein integral. Similarly, the polylogarithm can be expressed in terms of the integral of the Fermi–Dirac distribution: This converges for Re(s) > 0 and all z except for z real and ≤ −1. The polylogarithm in this context is sometimes referred to as a Fermi integral or a Fermi–Dirac integral (GSL 2010). These representations are readily verified by Taylor expansion of the integrand with respect to z and termwise integration. The papers of Dingle contain detailed investigations of both types of integrals. The polylogarithm is also related to the integral of the Maxwell–Boltzmann distribution: This also gives the asymptotic behavior of polylogarithm at the vicinity of origin. £#/li#£ £#li#£ A complementary integral representation applies to Re(s) < 0 and to all z except to z real and ≥ 0: This integral follows from the general relation of the polylogarithm with the Hurwitz zeta function (see above) and a familiar integral representation of the latter. £#/li#£ £#li#£ The polylogarithm may be quite generally represented by a Hankel contour integral (Whittaker & Watson 1927, § 12.22, § 13.13), which extends the Bose–Einstein representation to negative orders s. As long as the t = μ pole of the integrand does not lie on the non-negative real axis, and s ≠ 1, 2, 3, …, we have: where H represents the Hankel contour. The integrand has a cut along the real axis from zero to infinity, with the axis belonging to the lower half plane of t. The integration starts at +∞ on the upper half plane (Im(t) > 0), circles the origin without enclosing any of the poles t = µ + 2kπi, and terminates at +∞ on the lower half plane (Im(t) < 0). For the case where µ is real and non-negative, we can simply subtract the contribution of the enclosed t = µ pole: where R is the residue of the pole: £#/li#£ £#li#£ When the Abel–Plana formula is applied to the defining series of the polylogarithm, a Hermite-type integral representation results that is valid for all complex z and for all complex s: where Γ is the upper incomplete gamma-function. All (but not part) of the ln(z) in this expression can be replaced by −ln(1⁄z). A related representation which also holds for all complex s, avoids the use of the incomplete gamma function, but this integral fails for z on the positive real axis if Re(s) ≤ 0. This expression is found by writing 2s Lis(−z) / (−z) = Φ(z2, s, 1⁄2) − z Φ(z2, s, 1), where Φ is the Lerch transcendent, and applying the Abel–Plana formula to the first Φ series and a complementary formula that involves 1 / (e2πt + 1) in place of 1 / (e2πt − 1) to the second Φ series. £#/li#£ £#li#£ As cited in, we can express an integral for the polylogarithm by integrating the ordinary geometric series termwise for ${\displaystyle s\in \mathbb {N} }$ as £#/li#£

£#h5#£Series representations£#/h5#£
£#li#£ As noted under integral representations above, the Bose–Einstein integral representation of the polylogarithm may be extended to negative orders s by means of Hankel contour integration: where H is the Hankel contour, s ≠ 1, 2, 3, …, and the t = μ pole of the integrand does not lie on the non-negative real axis. The contour can be modified so that it encloses the poles of the integrand at t − µ = 2kπi, and the integral can be evaluated as the sum of the residues (Wood 1992, § 12, 13; Gradshteyn & Ryzhik 1980, § 9.553): This will hold for Re(s) < 0 and all μ except where eμ = 1. For 0 < Im(µ) ≤ 2π the sum can be split as: where the two series can now be identified with the Hurwitz zeta function: This relation, which has already been given under relationship to other functions above, holds for all complex s ≠ 0, 1, 2, 3, … and was first derived in (Jonquière 1889, eq. 6). £#/li#£ £#li#£ In order to represent the polylogarithm as a power series about µ = 0, we write the series derived from the Hankel contour integral as: When the binomial powers in the sum are expanded about µ = 0 and the order of summation is reversed, the sum over h can be expressed in closed form: This result holds for |µ| < 2π and, thanks to the analytic continuation provided by the zeta functions, for all s ≠ 1, 2, 3, … . If the order is a positive integer, s = n, both the term with k = n − 1 and the gamma function become infinite, although their sum does not. One obtains (Wood 1992, § 9; Gradshteyn & Ryzhik 1980, § 9.554): where the sum over h vanishes if k = 0. So, for positive integer orders and for |μ| < 2π we have the series: where Hn denotes the nth harmonic number: The problem terms now contain −ln(−μ) which, when multiplied by μn−1, will tend to zero as μ → 0, except for n = 1. This reflects the fact that Lis(z) exhibits a true logarithmic singularity at s = 1 and z = 1 since: For s close, but not equal, to a positive integer, the divergent terms in the expansion about µ = 0 can be expected to cause computational difficulties (Wood 1992, § 9). Erdélyi's corresponding expansion (Erdélyi et al. 1981, § 1.11-15) in powers of ln(z) is not correct if one assumes that the principal branches of the polylogarithm and the logarithm are used simultaneously, since ln(1⁄z) is not uniformly equal to −ln(z). For nonpositive integer values of s, the zeta function ζ(s − k) in the expansion about µ = 0 reduces to Bernoulli numbers: ζ(−n − k) = −B1+n+k / (1 + n + k). Numerical evaluation of Li−n(z) by this series does not suffer from the cancellation effects that the finite rational expressions given under particular values above exhibit for large n. £#/li#£ £#li#£ By use of the identity the Bose–Einstein integral representation of the polylogarithm (see above) may be cast in the form: Replacing the hyperbolic cotangent with a bilateral series, then reversing the order of integral and sum, and finally identifying the summands with an integral representation of the upper incomplete gamma function, one obtains: For both the bilateral series of this result and that for the hyperbolic cotangent, symmetric partial sums from −kmax to kmax converge unconditionally as kmax → ∞. Provided the summation is performed symmetrically, this series for Lis(z) thus holds for all complex s as well as all complex z. £#/li#£ £#li#£ Introducing an explicit expression for the Stirling numbers of the second kind into the finite sum for the polylogarithm of nonpositive integer order (see above) one may write: The infinite series obtained by simply extending the outer summation to ∞ (Guillera & Sondow 2008, Theorem 2.1): turns out to converge to the polylogarithm for all complex s and for complex z with Re(z) < 1⁄2, as can be verified for |−z⁄(1−z)| < 1⁄2 by reversing the order of summation and using: The inner coefficients of these series can be expressed by Stirling-number-related formulas involving the generalized harmonic numbers. For example, see generating function transformations to find proofs (references to proofs) of the following identities: For the other arguments with Re(z) < 1⁄2 the result follows by analytic continuation. This procedure is equivalent to applying Euler's transformation to the series in z that defines the polylogarithm. £#/li#£

£#h5#£Asymptotic expansions£#/h5#£
For |z| ≫ 1, the polylogarithm can be expanded into asymptotic series in terms of ln(−z):

where B2k are the Bernoulli numbers. Both versions hold for all s and for any arg(z). As usual, the summation should be terminated when the terms start growing in magnitude. For negative integer s, the expansions vanish entirely; for non-negative integer s, they break off after a finite number of terms. Wood (1992, § 11) describes a method for obtaining these series from the Bose–Einstein integral representation (his equation 11.2 for Lis(eµ) requires −2π < Im(µ) ≤ 0).


£#h5#£Limiting behavior£#/h5#£
The following limits result from the various representations of the polylogarithm (Wood 1992, § 22):

Wood's first limit for Re(µ) → ∞ has been corrected in accordance with his equation 11.3. The limit for Re(s) → −∞ follows from the general relation of the polylogarithm with the Hurwitz zeta function (see above).


£#h5#£Dilogarithm£#/h5#£
The dilogarithm is the polylogarithm of order s = 2. An alternate integral expression of the dilogarithm for arbitrary complex argument z is (Abramowitz & Stegun 1972, § 27.7):

A source of confusion is that some computer algebra systems define the dilogarithm as dilog(z) = Li2(1−z).

In the case of real z ≥ 1 the first integral expression for the dilogarithm can be written as

from which expanding ln(t−1) and integrating term by term we obtain

The Abel identity for the dilogarithm is given by (Abel 1881)

This is immediately seen to hold for either x = 0 or y = 0, and for general arguments is then easily verified by differentiation ∂/∂x ∂/∂y. For y = 1−x the identity reduces to Euler's reflection formula

where Li2(1) = ζ(2) = 1⁄6 π2 has been used and x may take any complex value.
In terms of the new variables u = x/(1−y), v = y/(1−x) the Abel identity reads

which corresponds to the pentagon identity given in (Rogers 1907).
From the Abel identity for x = y = 1−z and the square relationship we have Landen's identity

and applying the reflection formula to each dilogarithm we find the inversion formula
and for real z ≥ 1 also

Known closed-form evaluations of the dilogarithm at special arguments are collected in the table below. Arguments in the first column are related by reflection x ↔ 1−x or inversion x ↔ 1⁄x to either x = 0 or x = −1; arguments in the third column are all interrelated by these operations.

Maximon (2003) discusses the 17th to 19th century references. The reflection formula was already published by Landen in 1760, prior to its appearance in a 1768 book by Euler (Maximon 2003, § 10); an equivalent to Abel's identity was already published by Spence in 1809, before Abel wrote his manuscript in 1826 (Zagier 1989, § 2). The designation bilogarithmische Function was introduced by Carl Johan Danielsson Hill (professor in Lund, Sweden) in 1828 (Maximon 2003, § 10). Don Zagier (1989) has remarked that the dilogarithm is the only mathematical function possessing a sense of humor.

Here ${\displaystyle \phi ={\tfrac {1}{2}}({\sqrt {5}}+1)}$ denotes the golden ratio.

£#h5#£Polylogarithm ladders£#/h5#£
Leonard Lewin discovered a remarkable and broad generalization of a number of classical relationships on the polylogarithm for special values. These are now called polylogarithm ladders. Define ${\displaystyle \rho ={\tfrac {1}{2}}({\sqrt {5}}-1)}$ as the reciprocal of the golden ratio. Then two simple examples of dilogarithm ladders are

given by Coxeter (1935) and

given by Landen. Polylogarithm ladders occur naturally and deeply in K-theory and algebraic geometry. Polylogarithm ladders provide the basis for the rapid computations of various mathematical constants by means of the BBP algorithm (Bailey, Borwein & Plouffe 1997).


£#h5#£Monodromy£#/h5#£
The polylogarithm has two branch points; one at z = 1 and another at z = 0. The second branch point, at z = 0, is not visible on the main sheet of the polylogarithm; it becomes visible only when the function is analytically continued to its other sheets. The monodromy group for the polylogarithm consists of the homotopy classes of loops that wind around the two branch points. Denoting these two by m0 and m1, the monodromy group has the group presentation

For the special case of the dilogarithm, one also has that wm0 = m0w, and the monodromy group becomes the Heisenberg group (identifying m0, m1 and w with x, y, z) (Vepstas 2008).


£#h5#£References£#/h5#£ £#ul#££#li#£Abel, N.H. (1881) [1826]. "Note sur la fonction ${\displaystyle \scriptstyle \psi x=x+{\frac {x^{2}}{2^{2}}}+{\frac {x^{3}}{3^{2}}}+\cdots +{\frac {x^{n}}{n^{2}}}+\cdots }$ " (PDF). In Sylow, L.; Lie, S. (eds.). Œuvres complètes de Niels Henrik Abel − Nouvelle édition, Tome II (in French). Christiania [Oslo]: Grøndahl & Søn. pp. 189–193. (this 1826 manuscript was only published posthumously.)£#/li#£ £#li#£Abramowitz, M.; Stegun, I.A. (1972). Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables. New York: Dover Publications. ISBN 978-0-486-61272-0.£#/li#£ £#li#£Apostol, T.M. (2010), "Polylogarithm", in Olver, Frank W. J.; Lozier, Daniel M.; Boisvert, Ronald F.; Clark, Charles W. (eds.), NIST Handbook of Mathematical Functions, Cambridge University Press, ISBN 978-0-521-19225-5, MR 2723248£#/li#£ £#li#£Bailey, D.H.; Borwein, P.B.; Plouffe, S. (April 1997). "On the Rapid Computation of Various Polylogarithmic Constants" (PDF). Mathematics of Computation. 66 (218): 903–913. Bibcode:1997MaCom..66..903B. doi:10.1090/S0025-5718-97-00856-9.£#/li#£ £#li#£Bailey, D.H.; Broadhurst, D.J. (June 20, 1999). "A Seventeenth-Order Polylogarithm Ladder". arXiv:math.CA/9906134.£#/li#£ £#li#£Berndt, B.C. (1994). Ramanujan's Notebooks, Part IV. New York: Springer-Verlag. pp. 323–326. ISBN 978-0-387-94109-7.£#/li#£ £#li#£Boersma, J.; Dempsey, J.P. (1992). "On the evaluation of Legendre's chi-function". Mathematics of Computation. 59 (199): 157–163. doi:10.2307/2152987. JSTOR 2152987.£#/li#£ £#li#£Borwein, D.; Borwein, J.M.; Girgensohn, R. (1995). "Explicit evaluation of Euler sums" (PDF). Proceedings of the Edinburgh Mathematical Society. Series 2. 38 (2): 277–294. doi:10.1017/S0013091500019088.£#/li#£ £#li#£Borwein, J.M.; Bradley, D.M.; Broadhurst, D.J.; Lisonek, P. (2001). "Special Values of Multiple Polylogarithms". Transactions of the American Mathematical Society. 353 (3): 907–941. arXiv:math/9910045. doi:10.1090/S0002-9947-00-02616-7. S2CID 11373360.£#/li#£ £#li#£Broadhurst, D.J. (April 21, 1996). "On the enumeration of irreducible k-fold Euler sums and their roles in knot theory and field theory". arXiv:hep-th/9604128.£#/li#£ £#li#£Clunie, J. (1954). "On Bose-Einstein functions". Proceedings of the Physical Society. Series A. 67 (7): 632–636. Bibcode:1954PPSA...67..632C. doi:10.1088/0370-1298/67/7/308.£#/li#£ £#li#£Cohen, H.; Lewin, L.; Zagier, D. (1992). "A Sixteenth-Order Polylogarithm Ladder" (PS). Experimental Mathematics. 1 (1): 25–34.£#/li#£ £#li#£Coxeter, H.S.M. (1935). "The functions of Schläfli and Lobatschefsky". Quarterly Journal of Mathematics. 6 (1): 13–29. Bibcode:1935QJMat...6...13C. doi:10.1093/qmath/os-6.1.13. JFM 61.0395.02.£#/li#£ £#li#£Cvijovic, D.; Klinowski, J. (1997). "Continued-fraction expansions for the Riemann zeta function and polylogarithms" (PDF). Proceedings of the American Mathematical Society. 125 (9): 2543–2550. doi:10.1090/S0002-9939-97-04102-6.£#/li#£ £#li#£Cvijovic, D. (2007). "New integral representations of the polylogarithm function". Proceedings of the Royal Society A. 463 (2080): 897–905. arXiv:0911.4452. Bibcode:2007RSPSA.463..897C. doi:10.1098/rspa.2006.1794. S2CID 115156743.£#/li#£ £#li#£Erdélyi, A.; Magnus, W.; Oberhettinger, F.; Tricomi, F.G. (1981). Higher Transcendental Functions, Vol. 1 (PDF). Malabar, FL: R.E. Krieger Publishing. ISBN 978-0-89874-206-0. (this is a reprint of the McGraw–Hill original of 1953.)£#/li#£ £#li#£Fornberg, B.; Kölbig, K.S. (1975). "Complex zeros of the Jonquière or polylogarithm function". Mathematics of Computation. 29 (130): 582–599. doi:10.2307/2005579. JSTOR 2005579.£#/li#£ £#li#£GNU Scientific Library (2010). "Reference Manual". Retrieved 2010-06-13.£#/li#£ £#li#£Gradshteyn, Izrail Solomonovich; Ryzhik, Iosif Moiseevich; Geronimus, Yuri Veniaminovich; Tseytlin, Michail Yulyevich; Jeffrey, Alan (2015) [October 2014]. "9.553.". In Zwillinger, Daniel; Moll, Victor Hugo (eds.). Table of Integrals, Series, and Products. Translated by Scripta Technica, Inc. (8 ed.). Academic Press, Inc. p. 1050. ISBN 978-0-12-384933-5. LCCN 2014010276.£#/li#£ £#li#£Guillera, J.; Sondow, J. (2008). "Double integrals and infinite products for some classical constants via analytic continuations of Lerch's transcendent". The Ramanujan Journal. 16 (3): 247–270. arXiv:math.NT/0506319. doi:10.1007/s11139-007-9102-0. S2CID 119131640.£#/li#£ £#li#£Hain, R.M. (March 25, 1992). "Classical polylogarithms". arXiv:alg-geom/9202022.£#/li#£ £#li#£Jahnke, E.; Emde, F. (1945). Tables of Functions with Formulae and Curves (4th ed.). New York: Dover Publications.£#/li#£ £#li#£Jonquière, A. (1889). "Note sur la série ${\displaystyle \scriptstyle \sum _{n=1}^{\infty }{\frac {x^{n}}{n^{s}}}}$ " (PDF). Bulletin de la Société Mathématique de France (in French). 17: 142–152. doi:10.24033/bsmf.392. JFM 21.0246.02.£#/li#£ £#li#£Kölbig, K.S.; Mignaco, J.A.; Remiddi, E. (1970). "On Nielsen's generalized polylogarithms and their numerical calculation". BIT. 10: 38–74. doi:10.1007/BF01940890. S2CID 119672619.£#/li#£ £#li#£Kirillov, A.N. (1995). "Dilogarithm identities". Progress of Theoretical Physics Supplement. 118: 61–142. arXiv:hep-th/9408113. Bibcode:1995PThPS.118...61K. doi:10.1143/PTPS.118.61. S2CID 119177149.£#/li#£ £#li#£Lewin, L. (1958). Dilogarithms and Associated Functions. London: Macdonald. MR 0105524.£#/li#£ £#li#£Lewin, L. (1981). Polylogarithms and Associated Functions. New York: North-Holland. ISBN 978-0-444-00550-2.£#/li#£ £#li#£Lewin, L., ed. (1991). Structural Properties of Polylogarithms. Mathematical Surveys and Monographs. Vol. 37. Providence, RI: Amer. Math. Soc. ISBN 978-0-8218-1634-9.£#/li#£ £#li#£Markman, B. (1965). "The Riemann Zeta Function". BIT. 5: 138–141.£#/li#£ £#li#£Maximon, L.C. (2003). "The Dilogarithm Function for Complex Argument". Proceedings of the Royal Society A. 459 (2039): 2807–2819. Bibcode:2003RSPSA.459.2807M. doi:10.1098/rspa.2003.1156. S2CID 122271244.£#/li#£ £#li#£McDougall, J.; Stoner, E.C. (1938). "The computation of Fermi-Dirac functions". Philosophical Transactions of the Royal Society A. 237 (773): 67–104. Bibcode:1938RSPTA.237...67M. doi:10.1098/rsta.1938.0004. JFM 64.1500.04.£#/li#£ £#li#£Nielsen, N. (1909). "Der Eulersche Dilogarithmus und seine Verallgemeinerungen. Eine Monographie". Nova Acta Leopoldina (in German). Halle – Leipzig, Germany: Kaiserlich-Leopoldinisch-Carolinische Deutsche Akademie der Naturforscher. XC (3): 121–212. JFM 40.0478.01.£#/li#£ £#li#£Prudnikov, A.P.; Marichev, O.I.; Brychkov, Yu.A. (1990). Integrals and Series, Vol. 3: More Special Functions. Newark, NJ: Gordon and Breach. ISBN 978-2-88124-682-1. (see § 1.2, "The generalized zeta function, Bernoulli polynomials, Euler polynomials, and polylogarithms", p. 23.)£#/li#£ £#li#£Robinson, J.E. (1951). "Note on the Bose-Einstein integral functions". Physical Review. Series 2. 83 (3): 678–679. Bibcode:1951PhRv...83..678R. doi:10.1103/PhysRev.83.678.£#/li#£ £#li#£Rogers, L.J. (1907). "On function sum theorems connected with the series ${\displaystyle \scriptstyle \sum _{n=1}^{\infty }{\frac {x^{n}}{n^{2}}}}$ ". Proceedings of the London Mathematical Society (2). 4 (1): 169–189. doi:10.1112/plms/s2-4.1.169. JFM 37.0428.03.£#/li#£ £#li#£Schrödinger, E. (1952). Statistical Thermodynamics (2nd ed.). Cambridge, UK: Cambridge University Press.£#/li#£ £#li#£Truesdell, C. (1945). "On a function which occurs in the theory of the structure of polymers". Annals of Mathematics. Second Series. 46 (1): 144–157. doi:10.2307/1969153. JSTOR 1969153.£#/li#£ £#li#£Vepstas, L. (2008). "An efficient algorithm for accelerating the convergence of oscillatory series, useful for computing the polylogarithm and Hurwitz zeta functions". Numerical Algorithms. 47 (3): 211–252. arXiv:math.CA/0702243. Bibcode:2008NuAlg..47..211V. doi:10.1007/s11075-007-9153-8. S2CID 15131811.£#/li#£ £#li#£Whittaker, E.T.; Watson, G.N. (1927). A Course of Modern Analysis (4th ed.). Cambridge, UK: Cambridge University Press. (this edition has been reprinted many times, a 1996 paperback has ISBN 0-521-09189-6.)£#/li#£ £#li#£Wirtinger, W. (1905). "Über eine besondere Dirichletsche Reihe". Journal für die Reine und Angewandte Mathematik (in German). 1905 (129): 214–219. doi:10.1515/crll.1905.129.214. JFM 37.0434.01. S2CID 199545536.£#/li#£ £#li#£Wood, D.C. (June 1992). "The Computation of Polylogarithms. Technical Report 15-92*" (PS). Canterbury, UK: University of Kent Computing Laboratory. Retrieved 2005-11-01.£#/li#£ £#li#£Zagier, D. (1989). "The dilogarithm function in geometry and number theory". Number Theory and Related Topics: papers presented at the Ramanujan Colloquium, Bombay, 1988. Studies in Mathematics. Vol. 12. Bombay: Tata Institute of Fundamental Research and Oxford University Press. pp. 231–249. ISBN 0-19-562367-3. (also appeared as "The remarkable dilogarithm" in Journal of Mathematical and Physical Sciences 22 (1988), pp. 131–145, and as Chapter I of (Zagier 2007).)£#/li#£ £#li#£Zagier, D. (2007). "The Dilogarithm Function" (PDF). In Cartier, P.E.; et al. (eds.). Frontiers in Number Theory, Physics, and Geometry II – On Conformal Field Theories, Discrete Groups and Renormalization. Berlin: Springer-Verlag. pp. 3–65. ISBN 978-3-540-30307-7.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Polylogarithm". MathWorld.£#/li#£ £#li#£Weisstein, Eric W. "Dilogarithm". MathWorld.£#/li#£ £#li#£Algorithms in Analytic Number Theory provides an arbitrary-precision, GMP-based, GPL-licensed implementation.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Polylogarithms £#/li#££#/ul#£




£#h3#£Arctanh£#/h3#£


In mathematics, the inverse hyperbolic functions are the inverse functions of the hyperbolic functions.

For a given value of a hyperbolic function, the corresponding inverse hyperbolic function provides the corresponding hyperbolic angle. The size of the hyperbolic angle is equal to the area of the corresponding hyperbolic sector of the hyperbola xy = 1, or twice the area of the corresponding sector of the unit hyperbola x2 − y2 = 1, just as a circular angle is twice the area of the circular sector of the unit circle. Some authors have called inverse hyperbolic functions "area functions" to realize the hyperbolic angles.

Hyperbolic functions occur in the calculations of angles and distances in hyperbolic geometry. It also occurs in the solutions of many linear differential equations (such as the equation defining a catenary), cubic equations, and Laplace's equation in Cartesian coordinates. Laplace's equations are important in many areas of physics, including electromagnetic theory, heat transfer, fluid dynamics, and special relativity.


£#h5#£Notation£#/h5#£
The ISO 80000-2 standard abbreviations consist of ar- followed by the abbreviation of the corresponding hyperbolic function (e.g., arsinh, arcosh). The prefix arc- followed by the corresponding hyperbolic function (e.g., arcsinh, arccosh) is also commonly seen, by analogy with the nomenclature for inverse trigonometric functions. These are misnomers, since the prefix arc is the abbreviation for arcus, while the prefix ar stands for area; the hyperbolic functions are not directly related to arcs.

Other authors prefer to use the notation argsinh, argcosh, argtanh, and so on, where the prefix arg is the abbreviation of the Latin argumentum. In computer science, this is often shortened to asinh.

The notation sinh−1(x), cosh−1(x), etc., is also used, despite the fact that care must be taken to avoid misinterpretations of the superscript −1 as a power, as opposed to a shorthand to denote the inverse function (e.g., cosh−1(x) versus cosh(x)−1).


£#h5#£Definitions in terms of logarithms£#/h5#£
Since the hyperbolic functions are rational functions of ex whose numerator and denominator are of degree at most two, these functions may be solved in terms of ex, by using the quadratic formula; then, taking the natural logarithm gives the following expressions for the inverse hyperbolic functions.

For complex arguments, the inverse hyperbolic functions, the square root and the logarithm are multi-valued functions, and the equalities of the next subsections may be viewed as equalities of multi-valued functions.

For all inverse hyperbolic functions (save the inverse hyperbolic cotangent and the inverse hyperbolic cosecant), the domain of the real function is connected.


£#h5#£Inverse hyperbolic sine£#/h5#£
Inverse hyperbolic sine (a.k.a. area hyperbolic sine) (Latin: Area sinus hyperbolicus):

${\displaystyle \operatorname {arsinh} x=\ln \left(x+{\sqrt {x^{2}+1}}\right)}$
The domain is the whole real line.


£#h5#£Inverse hyperbolic cosine£#/h5#£
Inverse hyperbolic cosine (a.k.a. area hyperbolic cosine) (Latin: Area cosinus hyperbolicus):

${\displaystyle \operatorname {arcosh} x=\ln \left(x+{\sqrt {x^{2}-1}}\right)}$
The domain is the closed interval [1, +∞ ).


£#h5#£Inverse hyperbolic tangent£#/h5#£
Inverse hyperbolic tangent (a.k.a. area hyperbolic tangent) (Latin: Area tangens hyperbolicus):

${\displaystyle \operatorname {artanh} x={\frac {1}{2}}\ln \left({\frac {1+x}{1-x}}\right)}$
The domain is the open interval (−1, 1).


£#h5#£Inverse hyperbolic cotangent£#/h5#£
Inverse hyperbolic cotangent (a.k.a., area hyperbolic cotangent) (Latin: Area cotangens hyperbolicus):

${\displaystyle \operatorname {arcoth} x={\frac {1}{2}}\ln \left({\frac {x+1}{x-1}}\right)}$
The domain is the union of the open intervals (−∞, −1) and (1, +∞).


£#h5#£Inverse hyperbolic secant£#/h5#£
Inverse hyperbolic secant (a.k.a., area hyperbolic secant) (Latin: Area secans hyperbolicus):

${\displaystyle \operatorname {arsech} x=\ln \left({\frac {1}{x}}+{\sqrt {{\frac {1}{x^{2}}}-1}}\right)=\ln \left({\frac {1+{\sqrt {1-x^{2}}}}{x}}\right)}$
The domain is the semi-open interval (0, 1].


£#h5#£Inverse hyperbolic cosecant£#/h5#£
Inverse hyperbolic cosecant (a.k.a., area hyperbolic cosecant) (Latin: Area cosecans hyperbolicus):

${\displaystyle \operatorname {arcsch} x=\ln \left({\frac {1}{x}}+{\sqrt {{\frac {1}{x^{2}}}+1}}\right)}$
The domain is the real line with 0 removed.


£#h5#£Addition formulae£#/h5#£
${\displaystyle \operatorname {arsinh} u\pm \operatorname {arsinh} v=\operatorname {arsinh} \left(u{\sqrt {1+v^{2}}}\pm v{\sqrt {1+u^{2}}}\right)}$
${\displaystyle \operatorname {arcosh} u\pm \operatorname {arcosh} v=\operatorname {arcosh} \left(uv\pm {\sqrt {(u^{2}-1)(v^{2}-1)}}\right)}$
${\displaystyle \operatorname {artanh} u\pm \operatorname {artanh} v=\operatorname {artanh} \left({\frac {u\pm v}{1\pm uv}}\right)}$
${\displaystyle \operatorname {arcoth} u\pm \operatorname {arcoth} v=\operatorname {arcoth} \left({\frac {1\pm uv}{u\pm v}}\right)}$
${\displaystyle {\begin{aligned}\operatorname {arsinh} u+\operatorname {arcosh} v&=\operatorname {arsinh} \left(uv+{\sqrt {(1+u^{2})(v^{2}-1)}}\right)\\&=\operatorname {arcosh} \left(v{\sqrt {1+u^{2}}}+u{\sqrt {v^{2}-1}}\right)\end{aligned}}}$

£#h5#£Other identities£#/h5#£
${\displaystyle {\begin{aligned}2\operatorname {arcosh} x&=\operatorname {arcosh} (2x^{2}-1)&\quad {\hbox{ for }}x\geq 1\\4\operatorname {arcosh} x&=\operatorname {arcosh} (8x^{4}-8x^{2}+1)&\quad {\hbox{ for }}x\geq 1\\2\operatorname {arsinh} x&=\operatorname {arcosh} (2x^{2}+1)&\quad {\hbox{ for }}x\geq 0\\4\operatorname {arsinh} x&=\operatorname {arcosh} (8x^{4}+8x^{2}+1)&\quad {\hbox{ for }}x\geq 0\end{aligned}}}$
${\displaystyle \ln(x)=\operatorname {arcosh} \left({\frac {x^{2}+1}{2x}}\right)=\operatorname {arsinh} \left({\frac {x^{2}-1}{2x}}\right)=\operatorname {artanh} \left({\frac {x^{2}-1}{x^{2}+1}}\right)}$

£#h5#£Composition of hyperbolic and inverse hyperbolic functions£#/h5#£
${\displaystyle {\begin{aligned}&\sinh(\operatorname {arcosh} x)={\sqrt {x^{2}-1}}\quad {\text{for}}\quad |x|>1\\&\sinh(\operatorname {artanh} x)={\frac {x}{\sqrt {1-x^{2}}}}\quad {\text{for}}\quad -1<x<1\\&\cosh(\operatorname {arsinh} x)={\sqrt {1+x^{2}}}\\&\cosh(\operatorname {artanh} x)={\frac {1}{\sqrt {1-x^{2}}}}\quad {\text{for}}\quad -1<x<1\\&\tanh(\operatorname {arsinh} x)={\frac {x}{\sqrt {1+x^{2}}}}\\&\tanh(\operatorname {arcosh} x)={\frac {\sqrt {x^{2}-1}}{x}}\quad {\text{for}}\quad |x|>1\end{aligned}}}$

£#h5#£Composition of inverse hyperbolic and trigonometric functions£#/h5#£
${\displaystyle \operatorname {arsinh} \left(\tan \alpha \right)=\operatorname {artanh} \left(\sin \alpha \right)=\ln \left({\frac {1+\sin \alpha }{\cos \alpha }}\right)=\pm \operatorname {arcosh} \left({\frac {1}{\cos \alpha }}\right)}$
${\displaystyle \ln \left(\left|\tan \alpha \right|\right)=-\operatorname {artanh} \left(\cos 2\alpha \right)}$

£#h5#£Conversions£#/h5#£
${\displaystyle \ln x=\operatorname {artanh} \left({\frac {x^{2}-1}{x^{2}+1}}\right)=\operatorname {arsinh} \left({\frac {x^{2}-1}{2x}}\right)=\pm \operatorname {arcosh} \left({\frac {x^{2}+1}{2x}}\right)}$
${\displaystyle \operatorname {artanh} x=\operatorname {arsinh} \left({\frac {x}{\sqrt {1-x^{2}}}}\right)=\pm \operatorname {arcosh} \left({\frac {1}{\sqrt {1-x^{2}}}}\right)}$
${\displaystyle \operatorname {arsinh} x=\operatorname {artanh} \left({\frac {x}{\sqrt {1+x^{2}}}}\right)=\pm \operatorname {arcosh} \left({\sqrt {1+x^{2}}}\right)}$
${\displaystyle \operatorname {arcosh} x=\left|\operatorname {arsinh} \left({\sqrt {x^{2}-1}}\right)\right|=\left|\operatorname {artanh} \left({\frac {\sqrt {x^{2}-1}}{x}}\right)\right|}$

£#h5#£Derivatives£#/h5#£
${\displaystyle {\begin{aligned}{\frac {d}{dx}}\operatorname {arsinh} x&{}={\frac {1}{\sqrt {x^{2}+1}}},{\text{ for all real }}x\\{\frac {d}{dx}}\operatorname {arcosh} x&{}={\frac {1}{\sqrt {x^{2}-1}}},{\text{ for all real }}x>1\\{\frac {d}{dx}}\operatorname {artanh} x&{}={\frac {1}{1-x^{2}}},{\text{ for all real }}|x|<1\\{\frac {d}{dx}}\operatorname {arcoth} x&{}={\frac {1}{1-x^{2}}},{\text{ for all real }}|x|>1\\{\frac {d}{dx}}\operatorname {arsech} x&{}={\frac {-1}{x{\sqrt {1-x^{2}}}}},{\text{ for all real }}x\in (0,1)\\{\frac {d}{dx}}\operatorname {arcsch} x&{}={\frac {-1}{|x|{\sqrt {1+x^{2}}}}},{\text{ for all real }}x{\text{, except }}0\\\end{aligned}}}$
For an example differentiation: let θ = arsinh x, so (where sinh2 θ = (sinh θ)2):

${\displaystyle {\frac {d\,\operatorname {arsinh} x}{dx}}={\frac {d\theta }{d\sinh \theta }}={\frac {1}{\cosh \theta }}={\frac {1}{\sqrt {1+\sinh ^{2}\theta }}}={\frac {1}{\sqrt {1+x^{2}}}}.}$

£#h5#£Series expansions£#/h5#£
Expansion series can be obtained for the above functions:

${\displaystyle {\begin{aligned}\operatorname {arsinh} x&=x-\left({\frac {1}{2}}\right){\frac {x^{3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{5}}{5}}-\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{7}}{7}}\pm \cdots \\&=\sum _{n=0}^{\infty }\left({\frac {(-1)^{n}(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{2n+1}}{2n+1}},\qquad \left|x\right|<1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcosh} x&=\ln(2x)-\left(\left({\frac {1}{2}}\right){\frac {x^{-2}}{2}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{-4}}{4}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{-6}}{6}}+\cdots \right)\\&=\ln(2x)-\sum _{n=1}^{\infty }\left({\frac {(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{-2n}}{2n}},\qquad \left|x\right|>1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {artanh} x&=x+{\frac {x^{3}}{3}}+{\frac {x^{5}}{5}}+{\frac {x^{7}}{7}}+\cdots \\&=\sum _{n=0}^{\infty }{\frac {x^{2n+1}}{2n+1}},\qquad \left|x\right|<1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcsch} x=\operatorname {arsinh} {\frac {1}{x}}&=x^{-1}-\left({\frac {1}{2}}\right){\frac {x^{-3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{-5}}{5}}-\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{-7}}{7}}\pm \cdots \\&=\sum _{n=0}^{\infty }\left({\frac {(-1)^{n}(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{-(2n+1)}}{2n+1}},\qquad \left|x\right|>1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arsech} x=\operatorname {arcosh} {\frac {1}{x}}&=\ln {\frac {2}{x}}-\left(\left({\frac {1}{2}}\right){\frac {x^{2}}{2}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{4}}{4}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{6}}{6}}+\cdots \right)\\&=\ln {\frac {2}{x}}-\sum _{n=1}^{\infty }\left({\frac {(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{2n}}{2n}},\qquad 0<x\leq 1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcoth} x=\operatorname {artanh} {\frac {1}{x}}&=x^{-1}+{\frac {x^{-3}}{3}}+{\frac {x^{-5}}{5}}+{\frac {x^{-7}}{7}}+\cdots \\&=\sum _{n=0}^{\infty }{\frac {x^{-(2n+1)}}{2n+1}},\qquad \left|x\right|>1\end{aligned}}}$
Asymptotic expansion for the arsinh x is given by

${\displaystyle \operatorname {arsinh} x=\ln(2x)+\sum \limits _{n=1}^{\infty }{\left({-1}\right)^{n-1}{\frac {\left({2n-1}\right)!!}{2n\left({2n}\right)!!}}}{\frac {1}{x^{2n}}}}$



£#h5#£Principal values in the complex plane£#/h5#£
As functions of a complex variable, inverse hyperbolic functions are multivalued functions that are analytic, except at a finite number of points. For such a function, it is common to define a principal value, which is a single valued analytic function which coincides with one specific branch of the multivalued function, over a domain consisting of the complex plane in which a finite number of arcs (usually half lines or line segments) have been removed. These arcs are called branch cuts. For specifying the branch, that is, defining which value of the multivalued function is considered at each point, one generally define it at a particular point, and deduce the value everywhere in the domain of definition of the principal value by analytic continuation. When possible, it is better to define the principal value directly—without referring to analytic continuation.

For example, for the square root, the principal value is defined as the square root that has a positive real part. This defines a single valued analytic function, which is defined everywhere, except for non-positive real values of the variables (where the two square roots have a zero real part). This principal value of the square root function is denoted ${\displaystyle {\sqrt {x}}}$ in what follows. Similarly, the principal value of the logarithm, denoted ${\displaystyle \operatorname {Log} }$ in what follows, is defined as the value for which the imaginary part has the smallest absolute value. It is defined everywhere except for non-positive real values of the variable, for which two different values of the logarithm reach the minimum.

For all inverse hyperbolic functions, the principal value may be defined in terms of principal values of the square root and the logarithm function. However, in some cases, the formulas of § Definitions in terms of logarithms do not give a correct principal value, as giving a domain of definition which is too small and, in one case non-connected.


£#h5#£Principal value of the inverse hyperbolic sine£#/h5#£
The principal value of the inverse hyperbolic sine is given by

${\displaystyle \operatorname {arsinh} z=\operatorname {Log} (z+{\sqrt {z^{2}+1}}\,)\,.}$
The argument of the square root is a non-positive real number, if and only if z belongs to one of the intervals [i, +i∞) and (−i∞, −i] of the imaginary axis. If the argument of the logarithm is real, then it is positive. Thus this formula defines a principal value for arsinh, with branch cuts [i, +i∞) and (−i∞, −i]. This is optimal, as the branch cuts must connect the singular points i and −i to the infinity.


£#h5#£Principal value of the inverse hyperbolic cosine£#/h5#£
The formula for the inverse hyperbolic cosine given in § Inverse hyperbolic cosine is not convenient, since similar to the principal values of the logarithm and the square root, the principal value of arcosh would not be defined for imaginary z. Thus the square root has to be factorized, leading to

${\displaystyle \operatorname {arcosh} z=\operatorname {Log} (z+{\sqrt {z+1}}{\sqrt {z-1}}\,)\,.}$
The principal values of the square roots are both defined, except if z belongs to the real interval (−∞, 1]. If the argument of the logarithm is real, then z is real and has the same sign. Thus, the above formula defines a principal value of arcosh outside the real interval (−∞, 1], which is thus the unique branch cut.


£#h5#£Principal values of the inverse hyperbolic tangent and cotangent£#/h5#£
The formulas given in § Definitions in terms of logarithms suggests

${\displaystyle {\begin{aligned}\operatorname {artanh} z&={\frac {1}{2}}\operatorname {Log} \left({\frac {1+z}{1-z}}\right)\\\operatorname {arcoth} z&={\frac {1}{2}}\operatorname {Log} \left({\frac {z+1}{z-1}}\right)\end{aligned}}}$
for the definition of the principal values of the inverse hyperbolic tangent and cotangent. In these formulas, the argument of the logarithm is real if and only if z is real. For artanh, this argument is in the real interval (−∞, 0], if z belongs either to (−∞, −1] or to [1, ∞). For arcoth, the argument of the logarithm is in (−∞, 0], if and only if z belongs to the real interval [−1, 1].

Therefore, these formulas define convenient principal values, for which the branch cuts are (−∞, −1] and [1, ∞) for the inverse hyperbolic tangent, and [−1, 1] for the inverse hyperbolic cotangent.

In view of a better numerical evaluation near the branch cuts, some authors use the following definitions of the principal values, although the second one introduces a removable singularity at z = 0. The two definitions of ${\displaystyle \operatorname {artanh} }$ differ for real values of ${\displaystyle z}$ with ${\displaystyle z>1}$ . The ones of ${\displaystyle \operatorname {arcoth} }$ differ for real values of ${\displaystyle z}$ with ${\displaystyle z\in [0,1)}$ .

${\displaystyle {\begin{aligned}\operatorname {artanh} z&={\tfrac {1}{2}}\operatorname {Log} \left({1+z}\right)-{\tfrac {1}{2}}\operatorname {Log} \left({1-z}\right)\\\operatorname {arcoth} z&={\tfrac {1}{2}}\operatorname {Log} \left({1+{\frac {1}{z}}}\right)-{\tfrac {1}{2}}\operatorname {Log} \left({1-{\frac {1}{z}}}\right)\end{aligned}}}$

£#h5#£Principal value of the inverse hyperbolic cosecant£#/h5#£
For the inverse hyperbolic cosecant, the principal value is defined as

${\displaystyle \operatorname {arcsch} z=\operatorname {Log} \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z^{2}}}+1}}\,\right)}$ .
It is defined when the arguments of the logarithm and the square root are not non-positive real numbers. The principal value of the square root is thus defined outside the interval [−i, i] of the imaginary line. If the argument of the logarithm is real, then z is a non-zero real number, and this implies that the argument of the logarithm is positive.

Thus, the principal value is defined by the above formula outside the branch cut, consisting of the interval [−i, i] of the imaginary line.

For z = 0, there is a singular point that is included in the branch cut.


£#h5#£Principal value of the inverse hyperbolic secant£#/h5#£
Here, as in the case of the inverse hyperbolic cosine, we have to factorize the square root. This gives the principal value

${\displaystyle \operatorname {arsech} z=\operatorname {Log} \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z}}+1}}\,{\sqrt {{\frac {1}{z}}-1}}\right).}$
If the argument of a square root is real, then z is real, and it follows that both principal values of square roots are defined, except if z is real and belongs to one of the intervals (−∞, 0] and [1, +∞). If the argument of the logarithm is real and negative, then z is also real and negative. It follows that the principal value of arsech is well defined, by the above formula outside two branch cuts, the real intervals (−∞, 0] and [1, +∞).

For z = 0, there is a singular point that is included in one of the branch cuts.


£#h5#£Graphical representation£#/h5#£
In the following graphical representation of the principal values of the inverse hyperbolic functions, the branch cuts appear as discontinuities of the color. The fact that the whole branch cuts appear as discontinuities, shows that these principal values may not be extended into analytic functions defined over larger domains. In other words, the above defined branch cuts are minimal.


£#h5#£See also£#/h5#£ £#ul#££#li#£Complex logarithm£#/li#£ £#li#£Hyperbolic secant distribution£#/li#£ £#li#£ISO 80000-2£#/li#£ £#li#£List of integrals of inverse hyperbolic functions£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£Bibliography£#/h5#£ £#ul#££#li#£Herbert Busemann and Paul J. Kelly (1953) Projective Geometry and Projective Metrics, page 207, Academic Press.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Inverse hyperbolic functions", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Hyperbolic Functions £#/li#££#/ul#£




£#h3#£Arctg£#/h3#£

In mathematics, the inverse trigonometric functions (occasionally also called arcus functions, antitrigonometric functions or cyclometric functions) are the inverse functions of the trigonometric functions (with suitably restricted domains). Specifically, they are the inverses of the sine, cosine, tangent, cotangent, secant, and cosecant functions, and are used to obtain an angle from any of the angle's trigonometric ratios. Inverse trigonometric functions are widely used in engineering, navigation, physics, and geometry.


£#h5#£Notation£#/h5#£
Several notations for the inverse trigonometric functions exist. The most common convention is to name inverse trigonometric functions using an arc- prefix: arcsin(x), arccos(x), arctan(x), etc. (This convention is used throughout this article.) This notation arises from the following geometric relationships: when measuring in radians, an angle of θ radians will correspond to an arc whose length is rθ, where r is the radius of the circle. Thus in the unit circle, "the arc whose cosine is x" is the same as "the angle whose cosine is x", because the length of the arc of the circle in radii is the same as the measurement of the angle in radians. In computer programming languages, the inverse trigonometric functions are often called by the abbreviated forms asin, acos, atan.

The notations sin−1(x), cos−1(x), tan−1(x), etc., as introduced by John Herschel in 1813, are often used as well in English-language sources, much more than the also established sin[−1](x), cos[−1](x), tan[−1](x),—conventions consistent with the notation of an inverse function, that is useful e.g. to define the multivalued version of each inverse trigonometric function: e.g. ${\displaystyle \tan ^{-1}(x)=\{\arctan(x)+\pi k\mid k\in \mathbb {Z} \}}$ . However, this might appear to conflict logically with the common semantics for expressions such as sin2(x) (although only sin2 x, without parentheses, is the really common one), which refer to numeric power rather than function composition, and therefore may result in confusion between multiplicative inverse or reciprocal and compositional inverse. The confusion is somewhat mitigated by the fact that each of the reciprocal trigonometric functions has its own name—for example, (cos(x))−1 = sec(x). Nevertheless, certain authors advise against using it for its ambiguity. Another precarious convention used by a tiny number of authors is to use an uppercase first letter, along with a −1 superscript: Sin−1(x), Cos−1(x), Tan−1(x), etc. Although its intention is to avoid confusion with the multiplicative inverse, which should be represented by sin−1(x), cos−1(x), etc., or, better, by sin−1 x, cos−1 x, etc., it in turn creates yet another major source of ambiguity: especially in light of the fact that many popular high-level programming languages (e.g. Wolfram's Mathematica, and University of Sydney's MAGMA) use those very same capitalised representations for the standard trig functions; whereas others (Python (ie SymPy and NumPy), Matlab, MAPLE etc) use lower-case.

Hence, since 2009, the ISO 80000-2 standard has specified solely the "arc" prefix for the inverse functions.


£#h5#£Basic concepts£#/h5#£
£#h5#£Principal values£#/h5#£
Since none of the six trigonometric functions are one-to-one, they must be restricted in order to have inverse functions. Therefore, the result ranges of the inverse functions are proper (i.e. strict) subsets of the domains of the original functions.

For example, using function in the sense of multivalued functions, just as the square root function ${\displaystyle y={\sqrt {x}}}$ could be defined from ${\displaystyle y^{2}=x,}$ the function ${\displaystyle y=\arcsin(x)}$ is defined so that ${\displaystyle \sin(y)=x.}$ For a given real number ${\displaystyle x,}$ with ${\displaystyle -1\leq x\leq 1,}$ there are multiple (in fact, countably infinitely many) numbers ${\displaystyle y}$ such that ${\displaystyle \sin(y)=x}$ ; for example, ${\displaystyle \sin(0)=0,}$ but also ${\displaystyle \sin(\pi )=0,}$ ${\displaystyle \sin(2\pi )=0,}$ etc. When only one value is desired, the function may be restricted to its principal branch. With this restriction, for each ${\displaystyle x}$ in the domain, the expression ${\displaystyle \arcsin(x)}$ will evaluate only to a single value, called its principal value. These properties apply to all the inverse trigonometric functions.

The principal inverses are listed in the following table.

Note: Some authors define the range of arcsecant to be ${\textstyle (0\leq y<{\frac {\pi }{2}}{\text{ or }}\pi \leq y<{\frac {3\pi }{2}})}$ , because the tangent function is nonnegative on this domain. This makes some computations more consistent. For example, using this range, ${\displaystyle \tan(\operatorname {arcsec}(x))={\sqrt {x^{2}-1}},}$ whereas with the range ${\textstyle (0\leq y<{\frac {\pi }{2}}{\text{ or }}{\frac {\pi }{2}}<y\leq \pi )}$ , we would have to write ${\displaystyle \tan(\operatorname {arcsec}(x))=\pm {\sqrt {x^{2}-1}},}$ since tangent is nonnegative on ${\textstyle 0\leq y<{\frac {\pi }{2}},}$ but nonpositive on ${\textstyle {\frac {\pi }{2}}<y\leq \pi .}$ For a similar reason, the same authors define the range of arccosecant to be ${\textstyle (-\pi <y\leq -{\frac {\pi }{2}}}$ or ${\textstyle 0<y\leq {\frac {\pi }{2}}).}$

If ${\displaystyle x}$ is allowed to be a complex number, then the range of ${\displaystyle y}$ applies only to its real part.

The table below displays names and domains of the inverse trigonometric functions along with the range of their usual principal values in radians.

The symbol ${\displaystyle \mathbb {R} =(-\infty ,\infty )}$ denotes the set of all real numbers and ${\displaystyle \mathbb {Z} =\{\ldots ,\,-2,\,-1,\,0,\,1,\,2,\,\ldots \}}$ denotes the set of all integers. The set of all integer multiples of ${\displaystyle \pi }$ is denoted by

The symbol ${\displaystyle \,\setminus \,}$ denotes set subtraction so that, for instance, ${\displaystyle \mathbb {R} \setminus (-1,1)=(-\infty ,-1]\cup [1,\infty )}$ is the set of points in ${\displaystyle \mathbb {R} }$ (that is, real numbers) that are not in the interval ${\displaystyle (-1,1).}$

The Minkowski sum notation ${\textstyle \pi \mathbb {Z} +(0,\pi )}$ and ${\displaystyle \pi \mathbb {Z} +{\bigl (}{-{\tfrac {\pi }{2}}},{\tfrac {\pi }{2}}{\bigr )}}$ that is used above to concisely write the domains of ${\displaystyle \cot ,\csc ,\tan ,{\text{ and }}\sec }$ is now explained.

Domain of cotangent ${\displaystyle \cot }$ and cosecant ${\displaystyle \csc }$ : The domains of ${\displaystyle \,\cot \,}$ and ${\displaystyle \,\csc \,}$ are the same. They are the set of all angles ${\displaystyle \theta }$ at which ${\displaystyle \sin \theta \neq 0,}$ i.e. all real numbers that are not of the form ${\displaystyle \pi n}$ for some integer ${\displaystyle n,}$

Domain of tangent ${\displaystyle \tan }$ and secant ${\displaystyle \sec }$ : The domains of ${\displaystyle \,\tan \,}$ and ${\displaystyle \,\sec \,}$ are the same. They are the set of all angles ${\displaystyle \theta }$ at which ${\displaystyle \cos \theta \neq 0,}$


£#h5#£Solutions to elementary trigonometric equations£#/h5#£
Each of the trigonometric functions is periodic in the real part of its argument, running through all its values twice in each interval of ${\displaystyle 2\pi }$ :

£#ul#££#li#£Sine and cosecant begin their period at ${\textstyle 2\pi k-{\frac {\pi }{2}}}$ (where ${\displaystyle k}$ is an integer), finish it at ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ and then reverse themselves over ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ to ${\displaystyle 2\pi k+{\frac {3\pi }{2}}.}$ £#/li#£ £#li#£Cosine and secant begin their period at ${\displaystyle 2\pi k,}$ finish it at ${\displaystyle 2\pi k+\pi .}$ and then reverse themselves over ${\displaystyle 2\pi k+\pi }$ to ${\displaystyle 2\pi k+2\pi .}$ £#/li#£ £#li#£Tangent begins its period at ${\textstyle 2\pi k-{\frac {\pi }{2}},}$ finishes it at ${\textstyle 2\pi k+{\frac {\pi }{2}},}$ and then repeats it (forward) over ${\textstyle 2\pi k+{\frac {\pi }{2}}}$ to ${\textstyle 2\pi k+{\frac {3\pi }{2}}.}$ £#/li#£ £#li#£Cotangent begins its period at ${\displaystyle 2\pi k,}$ finishes it at ${\displaystyle 2\pi k+\pi ,}$ and then repeats it (forward) over ${\displaystyle 2\pi k+\pi }$ to ${\displaystyle 2\pi k+2\pi .}$ £#/li#££#/ul#£
This periodicity is reflected in the general inverses, where ${\displaystyle k}$ is some integer.

The following table shows how inverse trigonometric functions may be used to solve equalities involving the six standard trigonometric functions. It is assumed that the given values ${\displaystyle \theta ,}$ ${\displaystyle r,}$ ${\displaystyle s,}$ ${\displaystyle x,}$ and ${\displaystyle y}$ all lie within appropriate ranges so that the relevant expressions below are well-defined. Note that "for some ${\displaystyle k\in \mathbb {Z} }$ " is just another way of saying "for some integer ${\displaystyle k.}$ "

The symbol ${\displaystyle \,\iff \,}$ is logical equality. The expression "LHS ${\displaystyle \,\iff \,}$ RHS" indicates that either (a) the left hand side (i.e. LHS) and right hand side (i.e. RHS) are both true, or else (b) the left hand side and right hand side are both false; there is no option (c) (e.g. it is not possible for the LHS statement to be true and also simultaneously for the RHS statement to false), because otherwise "LHS ${\displaystyle \,\iff \,}$ RHS" would not have been written (see this footnote for an example illustrating this concept).

For example, if ${\displaystyle \cos \theta =-1}$ then ${\displaystyle \theta =\pi +2\pi k=-\pi +2\pi (1+k)}$ for some ${\displaystyle k\in \mathbb {Z} .}$ While if ${\displaystyle \sin \theta =\pm 1}$ then ${\textstyle \theta ={\frac {\pi }{2}}+\pi k=-{\frac {\pi }{2}}+\pi (k+1)}$ for some ${\displaystyle k\in \mathbb {Z} ,}$ where ${\displaystyle k}$ will be even if ${\displaystyle \sin \theta =1}$ and it will be odd if ${\displaystyle \sin \theta =-1.}$ The equations ${\displaystyle \sec \theta =-1}$ and ${\displaystyle \csc \theta =\pm 1}$ have the same solutions as ${\displaystyle \cos \theta =-1}$ and ${\displaystyle \sin \theta =\pm 1,}$ respectively. In all equations above except for those just solved (i.e. except for ${\displaystyle \sin }$ / ${\displaystyle \csc \theta =\pm 1}$ and ${\displaystyle \cos }$ / ${\displaystyle \sec \theta =-1}$ ), the integer ${\displaystyle k}$ in the solution's formula is uniquely determined by ${\displaystyle \theta }$ (for fixed ${\displaystyle r,s,x,}$ and ${\displaystyle y}$ ).

Detailed example and explanation of the "plus or minus" symbol ${\displaystyle \pm }$
The solutions to ${\displaystyle \cos \theta =x}$ and ${\displaystyle \sec \theta =x}$ involve the "plus or minus" symbol ${\displaystyle \,\pm ,\,}$ whose meaning is now clarified. Only the solution to ${\displaystyle \cos \theta =x}$ will be discussed since the discussion for ${\displaystyle \sec \theta =x}$ is the same. We are given ${\displaystyle x}$ between ${\displaystyle -1\leq x\leq 1}$ and we know that there is an angle ${\displaystyle \theta }$ in some give interval that satisfies ${\displaystyle \cos \theta =x.}$ We want to find this ${\displaystyle \theta .}$ The formula for the solution involves:

If ${\displaystyle \,\arccos x=0\,}$ (which only happens when ${\displaystyle x=1}$ ) then ${\displaystyle \,+\arccos x=0\,}$ and ${\displaystyle \,-\arccos x=0\,}$ so either way, ${\displaystyle \,\pm \arccos x\,}$ can only be equal to ${\displaystyle 0.}$ But if ${\displaystyle \,\arccos x\neq 0,\,}$ which will now be assumed, then the solution to ${\displaystyle \cos \theta =x,}$ which is written above as is shorthand for the following statement:
Either £#ul#££#li#£ ${\displaystyle \,\theta =\arccos x+2\pi k\,}$ for some integer ${\displaystyle k,}$
or else£#/li#£ £#li#£ ${\displaystyle \,\theta =-\arccos x+2\pi k\,}$ for some integer ${\displaystyle k.}$ £#/li#££#/ul#£
Because ${\displaystyle \,\arccos x\neq 0\,}$ and ${\displaystyle \,0<\arccos x\leq \pi \,}$ exactly one of these two equalities can hold. Additional information about ${\displaystyle \theta }$ is needed to determine which one holds. For example, suppose that ${\displaystyle x=0}$ and that all that is known about ${\displaystyle \theta }$ is that ${\displaystyle \,-\pi \leq \theta \leq \pi \,}$ (and nothing more is known). Then

and moreover, in this particular case ${\displaystyle k=0}$ (for both the ${\displaystyle \,+\,}$ case and the ${\displaystyle \,-\,}$ case) and so consequently, This means that ${\displaystyle \theta }$ could be either ${\displaystyle \,\pi /2\,}$ or ${\displaystyle \,-\pi /2.}$ Without additional information it is not possible to determine which of these values ${\displaystyle \theta }$ has. An example of some additional information that could determine the value of ${\displaystyle \theta }$ would be knowing that the angle is above the ${\displaystyle x}$ -axis(in which case ${\displaystyle \theta =\pi /2}$ ) or alternatively, knowing that it is below the ${\displaystyle x}$ -axis(in which case ${\displaystyle \theta =-\pi /2}$ ).
Transforming equations

The equations above can be transformed by using the reflection and shift identities:

These formulas imply, in particular, that the following hold:

where swapping ${\displaystyle \sin \leftrightarrow \csc ,}$ swapping ${\displaystyle \cos \leftrightarrow \sec ,}$ and swapping ${\displaystyle \tan \leftrightarrow \cot }$ gives the analogous equations for ${\displaystyle \csc ,\sec ,{\text{ and }}\cot ,}$ respectively.
So for example, by using the equality ${\textstyle \sin \left({\frac {\pi }{2}}-\theta \right)=\cos \theta ,}$ the equation ${\displaystyle \cos \theta =x}$ can be transformed into ${\textstyle \sin \left({\frac {\pi }{2}}-\theta \right)=x,}$ which allows for the solution to the equation ${\displaystyle \;\sin \varphi =x\;}$ (where ${\textstyle \varphi :={\frac {\pi }{2}}-\theta }$ ) to be used; that solution being: ${\displaystyle \varphi =(-1)^{k}\arcsin(x)+\pi k\;{\text{ for some }}k\in \mathbb {Z} ,}$ which becomes:

where using the fact that ${\displaystyle (-1)^{k}=(-1)^{-k}}$ and substituting ${\displaystyle h:=-k}$ proves that another solution to ${\displaystyle \;\cos \theta =x\;}$ is: The substitution ${\displaystyle \;\arcsin x={\frac {\pi }{2}}-\arccos x\;}$ may be used express the right hand side of the above formula in terms of ${\displaystyle \;\arccos x\;}$ instead of ${\displaystyle \;\arcsin x.\;}$
£#h5#£Equal identical trigonometric functions£#/h5#£
The table below shows how two angles ${\displaystyle \theta }$ and ${\displaystyle \varphi }$ must be related if their values under a given trigonometric function are equal or negatives of each other.


£#h5#£Relationships between trigonometric functions and inverse trigonometric functions£#/h5#£
Trigonometric functions of inverse trigonometric functions are tabulated below. A quick way to derive them is by considering the geometry of a right-angled triangle, with one side of length 1 and another side of length ${\displaystyle x,}$ then applying the Pythagorean theorem and definitions of the trigonometric ratios. Purely algebraic derivations are longer. It is worth noting that for arcsecant and arccosecant, the diagram assumes that ${\displaystyle x}$ is positive, and thus the result has to be corrected through the use of absolute values and the signum (sgn) operation.


£#h5#£Relationships among the inverse trigonometric functions£#/h5#£
Complementary angles:

${\displaystyle {\begin{aligned}\arccos(x)&={\frac {\pi }{2}}-\arcsin(x)\\[0.5em]\operatorname {arccot}(x)&={\frac {\pi }{2}}-\arctan(x)\\[0.5em]\operatorname {arccsc}(x)&={\frac {\pi }{2}}-\operatorname {arcsec}(x)\end{aligned}}}$
Negative arguments:

${\displaystyle {\begin{aligned}\arcsin(-x)&=-\arcsin(x)\\\arccos(-x)&=\pi -\arccos(x)\\\arctan(-x)&=-\arctan(x)\\\operatorname {arccot}(-x)&=\pi -\operatorname {arccot}(x)\\\operatorname {arcsec}(-x)&=\pi -\operatorname {arcsec}(x)\\\operatorname {arccsc}(-x)&=-\operatorname {arccsc}(x)\end{aligned}}}$
Reciprocal arguments:

${\displaystyle {\begin{aligned}\arccos \left({\frac {1}{x}}\right)&=\operatorname {arcsec}(x)\\[0.3em]\arcsin \left({\frac {1}{x}}\right)&=\operatorname {arccsc}(x)\\[0.3em]\arctan \left({\frac {1}{x}}\right)&={\frac {\pi }{2}}-\arctan(x)=\operatorname {arccot}(x)\,,{\text{ if }}x>0\\[0.3em]\arctan \left({\frac {1}{x}}\right)&=-{\frac {\pi }{2}}-\arctan(x)=\operatorname {arccot}(x)-\pi \,,{\text{ if }}x<0\\[0.3em]\operatorname {arccot} \left({\frac {1}{x}}\right)&={\frac {\pi }{2}}-\operatorname {arccot}(x)=\arctan(x)\,,{\text{ if }}x>0\\[0.3em]\operatorname {arccot} \left({\frac {1}{x}}\right)&={\frac {3\pi }{2}}-\operatorname {arccot}(x)=\pi +\arctan(x)\,,{\text{ if }}x<0\\[0.3em]\operatorname {arcsec} \left({\frac {1}{x}}\right)&=\arccos(x)\\[0.3em]\operatorname {arccsc} \left({\frac {1}{x}}\right)&=\arcsin(x)\end{aligned}}}$
Useful identities if one only has a fragment of a sine table:

${\displaystyle {\begin{aligned}\arccos(x)&=\arcsin \left({\sqrt {1-x^{2}}}\right)\,,{\text{ if }}0\leq x\leq 1{\text{ , from which you get }}\\\arccos &\left({\frac {1-x^{2}}{1+x^{2}}}\right)=\arcsin \left({\frac {2x}{1+x^{2}}}\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin &\left({\sqrt {1-x^{2}}}\right)={\frac {\pi }{2}}-\operatorname {sgn}(x)\arcsin(x)\\\arccos(x)&={\frac {1}{2}}\arccos \left(2x^{2}-1\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin(x)&={\frac {1}{2}}\arccos \left(1-2x^{2}\right)\,,{\text{ if }}0\leq x\leq 1\\\arcsin(x)&=\arctan \left({\frac {x}{\sqrt {1-x^{2}}}}\right)\\\arccos(x)&=\arctan \left({\frac {\sqrt {1-x^{2}}}{x}}\right)\\\arctan(x)&=\arcsin \left({\frac {x}{\sqrt {1+x^{2}}}}\right)\\\operatorname {arccot}(x)&=\arccos \left({\frac {x}{\sqrt {1+x^{2}}}}\right)\end{aligned}}}$
Whenever the square root of a complex number is used here, we choose the root with the positive real part (or positive imaginary part if the square was negative real).

A useful form that follows directly from the table above is

${\displaystyle \arctan \left(x\right)=\arccos \left({\sqrt {\frac {1}{1+x^{2}}}}\right)\,,{\text{ if }}x\geq 0}$ .
It is obtained by recognizing that ${\displaystyle \cos \left(\arctan \left(x\right)\right)={\sqrt {\frac {1}{1+x^{2}}}}=\cos \left(\arccos \left({\sqrt {\frac {1}{1+x^{2}}}}\right)\right)}$ .

From the half-angle formula, ${\displaystyle \tan \left({\tfrac {\theta }{2}}\right)={\tfrac {\sin(\theta )}{1+\cos(\theta )}}}$ , we get:

${\displaystyle {\begin{aligned}\arcsin(x)&=2\arctan \left({\frac {x}{1+{\sqrt {1-x^{2}}}}}\right)\\[0.5em]\arccos(x)&=2\arctan \left({\frac {\sqrt {1-x^{2}}}{1+x}}\right)\,,{\text{ if }}-1<x\leq 1\\[0.5em]\arctan(x)&=2\arctan \left({\frac {x}{1+{\sqrt {1+x^{2}}}}}\right)\end{aligned}}}$

£#h5#£Arctangent addition formula£#/h5#£
${\displaystyle \arctan(u)\pm \arctan(v)=\arctan \left({\frac {u\pm v}{1\mp uv}}\right){\pmod {\pi }}\,,\quad uv\neq 1\,.}$
This is derived from the tangent addition formula

${\displaystyle \tan(\alpha \pm \beta )={\frac {\tan(\alpha )\pm \tan(\beta )}{1\mp \tan(\alpha )\tan(\beta )}}\,,}$
by letting

${\displaystyle \alpha =\arctan(u)\,,\quad \beta =\arctan(v)\,.}$

£#h5#£In calculus£#/h5#£
£#h5#£Derivatives of inverse trigonometric functions£#/h5#£
The derivatives for complex values of z are as follows:

${\displaystyle {\begin{aligned}{\frac {d}{dz}}\arcsin(z)&{}={\frac {1}{\sqrt {1-z^{2}}}}\;;&z&{}\neq -1,+1\\{\frac {d}{dz}}\arccos(z)&{}=-{\frac {1}{\sqrt {1-z^{2}}}}\;;&z&{}\neq -1,+1\\{\frac {d}{dz}}\arctan(z)&{}={\frac {1}{1+z^{2}}}\;;&z&{}\neq -i,+i\\{\frac {d}{dz}}\operatorname {arccot}(z)&{}=-{\frac {1}{1+z^{2}}}\;;&z&{}\neq -i,+i\\{\frac {d}{dz}}\operatorname {arcsec}(z)&{}={\frac {1}{z^{2}{\sqrt {1-{\frac {1}{z^{2}}}}}}}\;;&z&{}\neq -1,0,+1\\{\frac {d}{dz}}\operatorname {arccsc}(z)&{}=-{\frac {1}{z^{2}{\sqrt {1-{\frac {1}{z^{2}}}}}}}\;;&z&{}\neq -1,0,+1\end{aligned}}}$
Only for real values of x:

${\displaystyle {\begin{aligned}{\frac {d}{dx}}\operatorname {arcsec}(x)&{}={\frac {1}{|x|{\sqrt {x^{2}-1}}}}\;;&|x|>1\\{\frac {d}{dx}}\operatorname {arccsc}(x)&{}=-{\frac {1}{|x|{\sqrt {x^{2}-1}}}}\;;&|x|>1\end{aligned}}}$
For a sample derivation: if ${\displaystyle \theta =\arcsin(x)}$ , we get:

${\displaystyle {\frac {d\arcsin(x)}{dx}}={\frac {d\theta }{d\sin(\theta )}}={\frac {d\theta }{\cos(\theta )\,d\theta }}={\frac {1}{\cos(\theta )}}={\frac {1}{\sqrt {1-\sin ^{2}(\theta )}}}={\frac {1}{\sqrt {1-x^{2}}}}}$

£#h5#£Expression as definite integrals£#/h5#£
Integrating the derivative and fixing the value at one point gives an expression for the inverse trigonometric function as a definite integral:

${\displaystyle {\begin{aligned}\arcsin(x)&{}=\int _{0}^{x}{\frac {1}{\sqrt {1-z^{2}}}}\,dz\;,&|x|&{}\leq 1\\\arccos(x)&{}=\int _{x}^{1}{\frac {1}{\sqrt {1-z^{2}}}}\,dz\;,&|x|&{}\leq 1\\\arctan(x)&{}=\int _{0}^{x}{\frac {1}{z^{2}+1}}\,dz\;,\\\operatorname {arccot}(x)&{}=\int _{x}^{\infty }{\frac {1}{z^{2}+1}}\,dz\;,\\\operatorname {arcsec}(x)&{}=\int _{1}^{x}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz=\pi +\int _{-x}^{-1}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz\;,&x&{}\geq 1\\\operatorname {arccsc}(x)&{}=\int _{x}^{\infty }{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz=\int _{-\infty }^{-x}{\frac {1}{z{\sqrt {z^{2}-1}}}}\,dz\;,&x&{}\geq 1\\\end{aligned}}}$
When x equals 1, the integrals with limited domains are improper integrals, but still well-defined.


£#h5#£Infinite series£#/h5#£
Similar to the sine and cosine functions, the inverse trigonometric functions can also be calculated using power series, as follows. For arcsine, the series can be derived by expanding its derivative, ${\textstyle {\tfrac {1}{\sqrt {1-z^{2}}}}}$ , as a binomial series, and integrating term by term (using the integral definition as above). The series for arctangent can similarly be derived by expanding its derivative ${\textstyle {\frac {1}{1+z^{2}}}}$ in a geometric series, and applying the integral definition above (see Leibniz series).

${\displaystyle {\begin{aligned}\arcsin(z)&=z+\left({\frac {1}{2}}\right){\frac {z^{3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {z^{5}}{5}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {z^{7}}{7}}+\cdots \\[5pt]&=\sum _{n=0}^{\infty }{\frac {(2n-1)!!}{(2n)!!}}{\frac {z^{2n+1}}{2n+1}}\\[5pt]&=\sum _{n=0}^{\infty }{\frac {(2n)!}{(2^{n}n!)^{2}}}{\frac {z^{2n+1}}{2n+1}}\,;\qquad |z|\leq 1\end{aligned}}}$
${\displaystyle \arctan(z)=z-{\frac {z^{3}}{3}}+{\frac {z^{5}}{5}}-{\frac {z^{7}}{7}}+\cdots =\sum _{n=0}^{\infty }{\frac {(-1)^{n}z^{2n+1}}{2n+1}}\,;\qquad |z|\leq 1\qquad z\neq i,-i}$
Series for the other inverse trigonometric functions can be given in terms of these according to the relationships given above. For example, ${\displaystyle \arccos(x)=\pi /2-\arcsin(x)}$ , ${\displaystyle \operatorname {arccsc}(x)=\arcsin(1/x)}$ , and so on. Another series is given by:

${\displaystyle 2\left(\arcsin \left({\frac {x}{2}}\right)\right)^{2}=\sum _{n=1}^{\infty }{\frac {x^{2n}}{n^{2}{\binom {2n}{n}}}}.}$
Leonhard Euler found a series for the arctangent that converges more quickly than its Taylor series:

${\displaystyle \arctan(z)={\frac {z}{1+z^{2}}}\sum _{n=0}^{\infty }\prod _{k=1}^{n}{\frac {2kz^{2}}{(2k+1)(1+z^{2})}}.}$
(The term in the sum for n = 0 is the empty product, so is 1.)

Alternatively, this can be expressed as

${\displaystyle \arctan(z)=\sum _{n=0}^{\infty }{\frac {2^{2n}(n!)^{2}}{(2n+1)!}}{\frac {z^{2n+1}}{(1+z^{2})^{n+1}}}.}$
Another series for the arctangent function is given by

${\displaystyle \arctan(z)=i\sum _{n=1}^{\infty }{\frac {1}{2n-1}}\left({\frac {1}{(1+2i/z)^{2n-1}}}-{\frac {1}{(1-2i/z)^{2n-1}}}\right),}$
where ${\displaystyle i={\sqrt {-1}}}$ is the imaginary unit.


£#h5#£Continued fractions for arctangent£#/h5#£
Two alternatives to the power series for arctangent are these generalized continued fractions:

${\displaystyle \arctan(z)={\frac {z}{1+{\cfrac {(1z)^{2}}{3-1z^{2}+{\cfrac {(3z)^{2}}{5-3z^{2}+{\cfrac {(5z)^{2}}{7-5z^{2}+{\cfrac {(7z)^{2}}{9-7z^{2}+\ddots }}}}}}}}}}={\frac {z}{1+{\cfrac {(1z)^{2}}{3+{\cfrac {(2z)^{2}}{5+{\cfrac {(3z)^{2}}{7+{\cfrac {(4z)^{2}}{9+\ddots }}}}}}}}}}}$
The second of these is valid in the cut complex plane. There are two cuts, from −i to the point at infinity, going down the imaginary axis, and from i to the point at infinity, going up the same axis. It works best for real numbers running from −1 to 1. The partial denominators are the odd natural numbers, and the partial numerators (after the first) are just (nz)2, with each perfect square appearing once. The first was developed by Leonhard Euler; the second by Carl Friedrich Gauss utilizing the Gaussian hypergeometric series.


£#h5#£Indefinite integrals of inverse trigonometric functions£#/h5#£
For real and complex values of z:

${\displaystyle {\begin{aligned}\int \arcsin(z)\,dz&{}=z\,\arcsin(z)+{\sqrt {1-z^{2}}}+C\\\int \arccos(z)\,dz&{}=z\,\arccos(z)-{\sqrt {1-z^{2}}}+C\\\int \arctan(z)\,dz&{}=z\,\arctan(z)-{\frac {1}{2}}\ln \left(1+z^{2}\right)+C\\\int \operatorname {arccot}(z)\,dz&{}=z\,\operatorname {arccot}(z)+{\frac {1}{2}}\ln \left(1+z^{2}\right)+C\\\int \operatorname {arcsec}(z)\,dz&{}=z\,\operatorname {arcsec}(z)-\ln \left[z\left(1+{\sqrt {\frac {z^{2}-1}{z^{2}}}}\right)\right]+C\\\int \operatorname {arccsc}(z)\,dz&{}=z\,\operatorname {arccsc}(z)+\ln \left[z\left(1+{\sqrt {\frac {z^{2}-1}{z^{2}}}}\right)\right]+C\end{aligned}}}$
For real x ≥ 1:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\ln \left(x+{\sqrt {x^{2}-1}}\right)+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\ln \left(x+{\sqrt {x^{2}-1}}\right)+C\end{aligned}}}$
For all real x not between -1 and 1:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\operatorname {sgn}(x)\ln \left|x+{\sqrt {x^{2}-1}}\right|+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\operatorname {sgn}(x)\ln \left|x+{\sqrt {x^{2}-1}}\right|+C\end{aligned}}}$
The absolute value is necessary to compensate for both negative and positive values of the arcsecant and arccosecant functions. The signum function is also necessary due to the absolute values in the derivatives of the two functions, which create two different solutions for positive and negative values of x. These can be further simplified using the logarithmic definitions of the inverse hyperbolic functions:

${\displaystyle {\begin{aligned}\int \operatorname {arcsec}(x)\,dx&{}=x\,\operatorname {arcsec}(x)-\operatorname {arcosh} (|x|)+C\\\int \operatorname {arccsc}(x)\,dx&{}=x\,\operatorname {arccsc}(x)+\operatorname {arcosh} (|x|)+C\\\end{aligned}}}$
The absolute value in the argument of the arcosh function creates a negative half of its graph, making it identical to the signum logarithmic function shown above.

All of these antiderivatives can be derived using integration by parts and the simple derivative forms shown above.


£#h5#£Example£#/h5#£
Using ${\displaystyle \int u\,dv=uv-\int v\,du}$ (i.e. integration by parts), set

${\displaystyle {\begin{aligned}u&=\arcsin(x)&dv&=dx\\du&={\frac {dx}{\sqrt {1-x^{2}}}}&v&=x\end{aligned}}}$
Then

${\displaystyle \int \arcsin(x)\,dx=x\arcsin(x)-\int {\frac {x}{\sqrt {1-x^{2}}}}\,dx,}$
which by the simple substitution ${\displaystyle w=1-x^{2},\ dw=-2x\,dx}$ yields the final result:

${\displaystyle \int \arcsin(x)\,dx=x\arcsin(x)+{\sqrt {1-x^{2}}}+C}$

£#h5#£Extension to complex plane£#/h5#£
Since the inverse trigonometric functions are analytic functions, they can be extended from the real line to the complex plane. This results in functions with multiple sheets and branch points. One possible way of defining the extension is:

${\displaystyle \arctan(z)=\int _{0}^{z}{\frac {dx}{1+x^{2}}}\quad z\neq -i,+i}$
where the part of the imaginary axis which does not lie strictly between the branch points (−i and +i) is the branch cut between the principal sheet and other sheets. The path of the integral must not cross a branch cut. For z not on a branch cut, a straight line path from 0 to z is such a path. For z on a branch cut, the path must approach from Re[x] > 0 for the upper branch cut and from Re[x] < 0 for the lower branch cut.

The arcsine function may then be defined as:

${\displaystyle \arcsin(z)=\arctan \left({\frac {z}{\sqrt {1-z^{2}}}}\right)\quad z\neq -1,+1}$
where (the square-root function has its cut along the negative real axis and) the part of the real axis which does not lie strictly between −1 and +1 is the branch cut between the principal sheet of arcsin and other sheets;

${\displaystyle \arccos(z)={\frac {\pi }{2}}-\arcsin(z)\quad z\neq -1,+1}$
which has the same cut as arcsin;

${\displaystyle \operatorname {arccot}(z)={\frac {\pi }{2}}-\arctan(z)\quad z\neq -i,i}$
which has the same cut as arctan;

${\displaystyle \operatorname {arcsec}(z)=\arccos \left({\frac {1}{z}}\right)\quad z\neq -1,0,+1}$
where the part of the real axis between −1 and +1 inclusive is the cut between the principal sheet of arcsec and other sheets;

${\displaystyle \operatorname {arccsc}(z)=\arcsin \left({\frac {1}{z}}\right)\quad z\neq -1,0,+1}$
which has the same cut as arcsec.


£#h5#£Logarithmic forms£#/h5#£
These functions may also be expressed using complex logarithms. This extends their domains to the complex plane in a natural fashion. The following identities for principal values of the functions hold everywhere that they are defined, even on their branch cuts.

${\displaystyle {\begin{aligned}\arcsin(z)&{}=-i\ln \left({\sqrt {1-z^{2}}}+iz\right)=i\ln \left({\sqrt {1-z^{2}}}-iz\right)&{}=\operatorname {arccsc} \left({\frac {1}{z}}\right)\\[10pt]\arccos(z)&{}=-i\ln \left(i{\sqrt {1-z^{2}}}+z\right)={\frac {\pi }{2}}-\arcsin(z)&{}=\operatorname {arcsec} \left({\frac {1}{z}}\right)\\[10pt]\arctan(z)&{}=-{\frac {i}{2}}\ln \left({\frac {i-z}{i+z}}\right)=-{\frac {i}{2}}\ln \left({\frac {1+iz}{1-iz}}\right)&{}=\operatorname {arccot} \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arccot}(z)&{}=-{\frac {i}{2}}\ln \left({\frac {z+i}{z-i}}\right)=-{\frac {i}{2}}\ln \left({\frac {iz-1}{iz+1}}\right)&{}=\arctan \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arcsec}(z)&{}=-i\ln \left(i{\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {1}{z}}\right)={\frac {\pi }{2}}-\operatorname {arccsc}(z)&{}=\arccos \left({\frac {1}{z}}\right)\\[10pt]\operatorname {arccsc}(z)&{}=-i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {i}{z}}\right)=i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}-{\frac {i}{z}}\right)&{}=\arcsin \left({\frac {1}{z}}\right)\end{aligned}}}$

£#h5#£Generalization£#/h5#£
Because all of the inverse trigonometric functions output an angle of a right triangle, they can be generalized by using Euler's formula to form a right triangle in the complex plane. Algebraically, this gives us:

${\displaystyle ce^{\theta i}=c\cos(\theta )+ci\sin(\theta )}$
or

${\displaystyle ce^{\theta i}=a+bi}$
where ${\displaystyle a}$ is the adjacent side, ${\displaystyle b}$ is the opposite side, and ${\displaystyle c}$ is the hypotenuse. From here, we can solve for ${\displaystyle \theta }$ .

${\displaystyle {\begin{aligned}e^{\ln(c)+\theta i}&=a+bi\\\ln c+\theta i&=\ln(a+bi)\\\theta &=\operatorname {Im} \left(\ln(a+bi)\right)\end{aligned}}}$
or

${\displaystyle \theta =-i\ln \left({\frac {a+bi}{c}}\right)=i\ln \left({\frac {c}{a+bi}}\right)}$
Simply taking the imaginary part works for any real-valued ${\displaystyle a}$ and ${\displaystyle b}$ , but if ${\displaystyle a}$ or ${\displaystyle b}$ is complex-valued, we have to use the final equation so that the real part of the result isn't excluded. Since the length of the hypotenuse doesn't change the angle, ignoring the real part of ${\displaystyle \ln(a+bi)}$ also removes ${\displaystyle c}$ from the equation. In the final equation, we see that the angle of the triangle in the complex plane can be found by inputting the lengths of each side. By setting one of the three sides equal to 1 and one of the remaining sides equal to our input ${\displaystyle z}$ , we obtain a formula for one of the inverse trig functions, for a total of six equations. Because the inverse trig functions require only one input, we must put the final side of the triangle in terms of the other two using the Pythagorean Theorem relation

${\displaystyle a^{2}+b^{2}=c^{2}}$
The table below shows the values of a, b, and c for each of the inverse trig functions and the equivalent expressions for ${\displaystyle \theta }$ that result from plugging the values into the equations above and simplifying.

${\displaystyle {\begin{aligned}&a&&b&&c&&\theta &&\theta _{\text{simplified}}&&\theta _{a,b\in \mathbb {R} }\\\arcsin(z)\ \ &{\sqrt {1-z^{2}}}&&z&&1&&-i\ln \left({\frac {{\sqrt {1-z^{2}}}+zi}{1}}\right)&&=-i\ln \left({\sqrt {1-z^{2}}}+zi\right)&&\operatorname {Im} \left(\ln \left({\sqrt {1-z^{2}}}+zi\right)\right)\\\arccos(z)\ \ &z&&{\sqrt {1-z^{2}}}&&1&&-i\ln \left({\frac {z+i{\sqrt {1-z^{2}}}}{1}}\right)&&=-i\ln \left(z+{\sqrt {z^{2}-1}}\right)&&\operatorname {Im} \left(\ln \left(z+{\sqrt {z^{2}-1}}\right)\right)\\\arctan(z)\ \ &1&&z&&{\sqrt {1+z^{2}}}&&i\ln \left({\frac {\sqrt {1+z^{2}}}{1+zi}}\right)&&={\frac {i}{2}}\ln \left({\frac {i+z}{i-z}}\right)&&\operatorname {Im} \left(\ln \left(1+zi\right)\right)\\\operatorname {arccot}(z)\ \ &z&&1&&{\sqrt {z^{2}+1}}&&i\ln \left({\frac {\sqrt {z^{2}+1}}{z+i}}\right)&&={\frac {i}{2}}\ln \left({\frac {z-i}{z+i}}\right)&&\operatorname {Im} \left(\ln \left(z+i\right)\right)\\\operatorname {arcsec}(z)\ \ &1&&{\sqrt {z^{2}-1}}&&z&&-i\ln \left({\frac {1+i{\sqrt {z^{2}-1}}}{z}}\right)&&=-i\ln \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z^{2}}}-1}}\right)&&\operatorname {Im} \left(\ln \left(1+{\sqrt {1-z^{2}}}\right)\right)\\\operatorname {arccsc}(z)\ \ &{\sqrt {z^{2}-1}}&&1&&z&&-i\ln \left({\frac {{\sqrt {z^{2}-1}}+i}{z}}\right)&&=-i\ln \left({\sqrt {1-{\frac {1}{z^{2}}}}}+{\frac {i}{z}}\right)&&\operatorname {Im} \left(\ln \left({\sqrt {z^{2}-1}}+i\right)\right)\\\end{aligned}}}$
In this sense, all of the inverse trig functions can be thought of as specific cases of the complex-valued log function. Since this definition works for any complex-valued ${\displaystyle z}$ , this definition allows for hyperbolic angles as outputs and can be used to further define the inverse hyperbolic functions. Elementary proofs of the relations may also proceed via expansion to exponential forms of the trigonometric functions.


£#h5#£Example proof£#/h5#£
${\displaystyle {\begin{aligned}\sin(\phi )&=z\\\phi &=\arcsin(z)\end{aligned}}}$
Using the exponential definition of sine, and letting ${\displaystyle \xi =e^{\phi i},}$

${\displaystyle {\begin{aligned}z&={\frac {e^{\phi i}-e^{-\phi i}}{2i}}\\[10mu]2iz&=\xi -{\frac {1}{\xi }}\\[5mu]0&=\xi ^{2}-2i\xi z-1\\[5mu]\xi &=iz\pm {\sqrt {1-z^{2}}}\\[5mu]\phi &=-i\ln \left(iz\pm {\sqrt {1-z^{2}}}\right)\end{aligned}}}$
(the positive branch is chosen)

${\displaystyle \phi =\arcsin(z)=-i\ln \left(iz+{\sqrt {1-z^{2}}}\right)}$

£#h5#£Applications£#/h5#£
£#h5#£Finding the angle of a right triangle£#/h5#£
Inverse trigonometric functions are useful when trying to determine the remaining two angles of a right triangle when the lengths of the sides of the triangle are known. Recalling the right-triangle definitions of sine and cosine, it follows that

${\displaystyle \theta =\arcsin \left({\frac {\text{opposite}}{\text{hypotenuse}}}\right)=\arccos \left({\frac {\text{adjacent}}{\text{hypotenuse}}}\right).}$
Often, the hypotenuse is unknown and would need to be calculated before using arcsine or arccosine using the Pythagorean Theorem: ${\displaystyle a^{2}+b^{2}=h^{2}}$ where ${\displaystyle h}$ is the length of the hypotenuse. Arctangent comes in handy in this situation, as the length of the hypotenuse is not needed.

${\displaystyle \theta =\arctan \left({\frac {\text{opposite}}{\text{adjacent}}}\right)\,.}$
For example, suppose a roof drops 8 feet as it runs out 20 feet. The roof makes an angle θ with the horizontal, where θ may be computed as follows:

${\displaystyle \theta =\arctan \left({\frac {\text{opposite}}{\text{adjacent}}}\right)=\arctan \left({\frac {\text{rise}}{\text{run}}}\right)=\arctan \left({\frac {8}{20}}\right)\approx 21.8^{\circ }\,.}$

£#h5#£In computer science and engineering£#/h5#£
£#h5#£Two-argument variant of arctangent£#/h5#£

The two-argument atan2 function computes the arctangent of y / x given y and x, but with a range of (−π, π]. In other words, atan2(y, x) is the angle between the positive x-axis of a plane and the point (x, y) on it, with positive sign for counter-clockwise angles (upper half-plane, y > 0), and negative sign for clockwise angles (lower half-plane, y < 0). It was first introduced in many computer programming languages, but it is now also common in other fields of science and engineering.

In terms of the standard arctan function, that is with range of (−π/2, π/2), it can be expressed as follows:

${\displaystyle \operatorname {atan2} (y,x)={\begin{cases}\arctan \left({\frac {y}{x}}\right)&\quad x>0\\\arctan \left({\frac {y}{x}}\right)+\pi &\quad y\geq 0,\;x<0\\\arctan \left({\frac {y}{x}}\right)-\pi &\quad y<0,\;x<0\\{\frac {\pi }{2}}&\quad y>0,\;x=0\\-{\frac {\pi }{2}}&\quad y<0,\;x=0\\{\text{undefined}}&\quad y=0,\;x=0\end{cases}}}$
It also equals the principal value of the argument of the complex number x + iy.

This limited version of the function above may also be defined using the tangent half-angle formulae as follows:

${\displaystyle \operatorname {atan2} (y,x)=2\arctan \left({\frac {y}{{\sqrt {x^{2}+y^{2}}}+x}}\right)}$
provided that either x > 0 or y ≠ 0. However this fails if given x ≤ 0 and y = 0 so the expression is unsuitable for computational use.

The above argument order (y, x) seems to be the most common, and in particular is used in ISO standards such as the C programming language, but a few authors may use the opposite convention (x, y) so some caution is warranted. These variations are detailed at atan2.


£#h5#£Arctangent function with location parameter£#/h5#£
In many applications the solution ${\displaystyle y}$ of the equation ${\displaystyle x=\tan(y)}$ is to come as close as possible to a given value ${\displaystyle -\infty <\eta <\infty }$ . The adequate solution is produced by the parameter modified arctangent function

${\displaystyle y=\arctan _{\eta }(x):=\arctan(x)+\pi \,\operatorname {rni} \left({\frac {\eta -\arctan(x)}{\pi }}\right)\,.}$
The function ${\displaystyle \operatorname {rni} }$ rounds to the nearest integer.


£#h5#£Numerical accuracy£#/h5#£
For angles near 0 and π, arccosine is ill-conditioned and will thus calculate the angle with reduced accuracy in a computer implementation (due to the limited number of digits). Similarly, arcsine is inaccurate for angles near −π/2 and π/2.


£#h5#£See also£#/h5#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Abramowitz, Milton; Stegun, Irene A., eds. (1972). Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables. New York: Dover Publications. ISBN 978-0-486-61272-0.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Inverse Tangent". MathWorld.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Trigonometric Functions £#/li#££#/ul#£




£#h3#£Arcth£#/h3#£


In mathematics, the inverse hyperbolic functions are the inverse functions of the hyperbolic functions.

For a given value of a hyperbolic function, the corresponding inverse hyperbolic function provides the corresponding hyperbolic angle. The size of the hyperbolic angle is equal to the area of the corresponding hyperbolic sector of the hyperbola xy = 1, or twice the area of the corresponding sector of the unit hyperbola x2 − y2 = 1, just as a circular angle is twice the area of the circular sector of the unit circle. Some authors have called inverse hyperbolic functions "area functions" to realize the hyperbolic angles.

Hyperbolic functions occur in the calculations of angles and distances in hyperbolic geometry. It also occurs in the solutions of many linear differential equations (such as the equation defining a catenary), cubic equations, and Laplace's equation in Cartesian coordinates. Laplace's equations are important in many areas of physics, including electromagnetic theory, heat transfer, fluid dynamics, and special relativity.


£#h5#£Notation£#/h5#£
The ISO 80000-2 standard abbreviations consist of ar- followed by the abbreviation of the corresponding hyperbolic function (e.g., arsinh, arcosh). The prefix arc- followed by the corresponding hyperbolic function (e.g., arcsinh, arccosh) is also commonly seen, by analogy with the nomenclature for inverse trigonometric functions. These are misnomers, since the prefix arc is the abbreviation for arcus, while the prefix ar stands for area; the hyperbolic functions are not directly related to arcs.

Other authors prefer to use the notation argsinh, argcosh, argtanh, and so on, where the prefix arg is the abbreviation of the Latin argumentum. In computer science, this is often shortened to asinh.

The notation sinh−1(x), cosh−1(x), etc., is also used, despite the fact that care must be taken to avoid misinterpretations of the superscript −1 as a power, as opposed to a shorthand to denote the inverse function (e.g., cosh−1(x) versus cosh(x)−1).


£#h5#£Definitions in terms of logarithms£#/h5#£
Since the hyperbolic functions are rational functions of ex whose numerator and denominator are of degree at most two, these functions may be solved in terms of ex, by using the quadratic formula; then, taking the natural logarithm gives the following expressions for the inverse hyperbolic functions.

For complex arguments, the inverse hyperbolic functions, the square root and the logarithm are multi-valued functions, and the equalities of the next subsections may be viewed as equalities of multi-valued functions.

For all inverse hyperbolic functions (save the inverse hyperbolic cotangent and the inverse hyperbolic cosecant), the domain of the real function is connected.


£#h5#£Inverse hyperbolic sine£#/h5#£
Inverse hyperbolic sine (a.k.a. area hyperbolic sine) (Latin: Area sinus hyperbolicus):

${\displaystyle \operatorname {arsinh} x=\ln \left(x+{\sqrt {x^{2}+1}}\right)}$
The domain is the whole real line.


£#h5#£Inverse hyperbolic cosine£#/h5#£
Inverse hyperbolic cosine (a.k.a. area hyperbolic cosine) (Latin: Area cosinus hyperbolicus):

${\displaystyle \operatorname {arcosh} x=\ln \left(x+{\sqrt {x^{2}-1}}\right)}$
The domain is the closed interval [1, +∞ ).


£#h5#£Inverse hyperbolic tangent£#/h5#£
Inverse hyperbolic tangent (a.k.a. area hyperbolic tangent) (Latin: Area tangens hyperbolicus):

${\displaystyle \operatorname {artanh} x={\frac {1}{2}}\ln \left({\frac {1+x}{1-x}}\right)}$
The domain is the open interval (−1, 1).


£#h5#£Inverse hyperbolic cotangent£#/h5#£
Inverse hyperbolic cotangent (a.k.a., area hyperbolic cotangent) (Latin: Area cotangens hyperbolicus):

${\displaystyle \operatorname {arcoth} x={\frac {1}{2}}\ln \left({\frac {x+1}{x-1}}\right)}$
The domain is the union of the open intervals (−∞, −1) and (1, +∞).


£#h5#£Inverse hyperbolic secant£#/h5#£
Inverse hyperbolic secant (a.k.a., area hyperbolic secant) (Latin: Area secans hyperbolicus):

${\displaystyle \operatorname {arsech} x=\ln \left({\frac {1}{x}}+{\sqrt {{\frac {1}{x^{2}}}-1}}\right)=\ln \left({\frac {1+{\sqrt {1-x^{2}}}}{x}}\right)}$
The domain is the semi-open interval (0, 1].


£#h5#£Inverse hyperbolic cosecant£#/h5#£
Inverse hyperbolic cosecant (a.k.a., area hyperbolic cosecant) (Latin: Area cosecans hyperbolicus):

${\displaystyle \operatorname {arcsch} x=\ln \left({\frac {1}{x}}+{\sqrt {{\frac {1}{x^{2}}}+1}}\right)}$
The domain is the real line with 0 removed.


£#h5#£Addition formulae£#/h5#£
${\displaystyle \operatorname {arsinh} u\pm \operatorname {arsinh} v=\operatorname {arsinh} \left(u{\sqrt {1+v^{2}}}\pm v{\sqrt {1+u^{2}}}\right)}$
${\displaystyle \operatorname {arcosh} u\pm \operatorname {arcosh} v=\operatorname {arcosh} \left(uv\pm {\sqrt {(u^{2}-1)(v^{2}-1)}}\right)}$
${\displaystyle \operatorname {artanh} u\pm \operatorname {artanh} v=\operatorname {artanh} \left({\frac {u\pm v}{1\pm uv}}\right)}$
${\displaystyle \operatorname {arcoth} u\pm \operatorname {arcoth} v=\operatorname {arcoth} \left({\frac {1\pm uv}{u\pm v}}\right)}$
${\displaystyle {\begin{aligned}\operatorname {arsinh} u+\operatorname {arcosh} v&=\operatorname {arsinh} \left(uv+{\sqrt {(1+u^{2})(v^{2}-1)}}\right)\\&=\operatorname {arcosh} \left(v{\sqrt {1+u^{2}}}+u{\sqrt {v^{2}-1}}\right)\end{aligned}}}$

£#h5#£Other identities£#/h5#£
${\displaystyle {\begin{aligned}2\operatorname {arcosh} x&=\operatorname {arcosh} (2x^{2}-1)&\quad {\hbox{ for }}x\geq 1\\4\operatorname {arcosh} x&=\operatorname {arcosh} (8x^{4}-8x^{2}+1)&\quad {\hbox{ for }}x\geq 1\\2\operatorname {arsinh} x&=\operatorname {arcosh} (2x^{2}+1)&\quad {\hbox{ for }}x\geq 0\\4\operatorname {arsinh} x&=\operatorname {arcosh} (8x^{4}+8x^{2}+1)&\quad {\hbox{ for }}x\geq 0\end{aligned}}}$
${\displaystyle \ln(x)=\operatorname {arcosh} \left({\frac {x^{2}+1}{2x}}\right)=\operatorname {arsinh} \left({\frac {x^{2}-1}{2x}}\right)=\operatorname {artanh} \left({\frac {x^{2}-1}{x^{2}+1}}\right)}$

£#h5#£Composition of hyperbolic and inverse hyperbolic functions£#/h5#£
${\displaystyle {\begin{aligned}&\sinh(\operatorname {arcosh} x)={\sqrt {x^{2}-1}}\quad {\text{for}}\quad |x|>1\\&\sinh(\operatorname {artanh} x)={\frac {x}{\sqrt {1-x^{2}}}}\quad {\text{for}}\quad -1<x<1\\&\cosh(\operatorname {arsinh} x)={\sqrt {1+x^{2}}}\\&\cosh(\operatorname {artanh} x)={\frac {1}{\sqrt {1-x^{2}}}}\quad {\text{for}}\quad -1<x<1\\&\tanh(\operatorname {arsinh} x)={\frac {x}{\sqrt {1+x^{2}}}}\\&\tanh(\operatorname {arcosh} x)={\frac {\sqrt {x^{2}-1}}{x}}\quad {\text{for}}\quad |x|>1\end{aligned}}}$

£#h5#£Composition of inverse hyperbolic and trigonometric functions£#/h5#£
${\displaystyle \operatorname {arsinh} \left(\tan \alpha \right)=\operatorname {artanh} \left(\sin \alpha \right)=\ln \left({\frac {1+\sin \alpha }{\cos \alpha }}\right)=\pm \operatorname {arcosh} \left({\frac {1}{\cos \alpha }}\right)}$
${\displaystyle \ln \left(\left|\tan \alpha \right|\right)=-\operatorname {artanh} \left(\cos 2\alpha \right)}$

£#h5#£Conversions£#/h5#£
${\displaystyle \ln x=\operatorname {artanh} \left({\frac {x^{2}-1}{x^{2}+1}}\right)=\operatorname {arsinh} \left({\frac {x^{2}-1}{2x}}\right)=\pm \operatorname {arcosh} \left({\frac {x^{2}+1}{2x}}\right)}$
${\displaystyle \operatorname {artanh} x=\operatorname {arsinh} \left({\frac {x}{\sqrt {1-x^{2}}}}\right)=\pm \operatorname {arcosh} \left({\frac {1}{\sqrt {1-x^{2}}}}\right)}$
${\displaystyle \operatorname {arsinh} x=\operatorname {artanh} \left({\frac {x}{\sqrt {1+x^{2}}}}\right)=\pm \operatorname {arcosh} \left({\sqrt {1+x^{2}}}\right)}$
${\displaystyle \operatorname {arcosh} x=\left|\operatorname {arsinh} \left({\sqrt {x^{2}-1}}\right)\right|=\left|\operatorname {artanh} \left({\frac {\sqrt {x^{2}-1}}{x}}\right)\right|}$

£#h5#£Derivatives£#/h5#£
${\displaystyle {\begin{aligned}{\frac {d}{dx}}\operatorname {arsinh} x&{}={\frac {1}{\sqrt {x^{2}+1}}},{\text{ for all real }}x\\{\frac {d}{dx}}\operatorname {arcosh} x&{}={\frac {1}{\sqrt {x^{2}-1}}},{\text{ for all real }}x>1\\{\frac {d}{dx}}\operatorname {artanh} x&{}={\frac {1}{1-x^{2}}},{\text{ for all real }}|x|<1\\{\frac {d}{dx}}\operatorname {arcoth} x&{}={\frac {1}{1-x^{2}}},{\text{ for all real }}|x|>1\\{\frac {d}{dx}}\operatorname {arsech} x&{}={\frac {-1}{x{\sqrt {1-x^{2}}}}},{\text{ for all real }}x\in (0,1)\\{\frac {d}{dx}}\operatorname {arcsch} x&{}={\frac {-1}{|x|{\sqrt {1+x^{2}}}}},{\text{ for all real }}x{\text{, except }}0\\\end{aligned}}}$
For an example differentiation: let θ = arsinh x, so (where sinh2 θ = (sinh θ)2):

${\displaystyle {\frac {d\,\operatorname {arsinh} x}{dx}}={\frac {d\theta }{d\sinh \theta }}={\frac {1}{\cosh \theta }}={\frac {1}{\sqrt {1+\sinh ^{2}\theta }}}={\frac {1}{\sqrt {1+x^{2}}}}.}$

£#h5#£Series expansions£#/h5#£
Expansion series can be obtained for the above functions:

${\displaystyle {\begin{aligned}\operatorname {arsinh} x&=x-\left({\frac {1}{2}}\right){\frac {x^{3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{5}}{5}}-\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{7}}{7}}\pm \cdots \\&=\sum _{n=0}^{\infty }\left({\frac {(-1)^{n}(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{2n+1}}{2n+1}},\qquad \left|x\right|<1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcosh} x&=\ln(2x)-\left(\left({\frac {1}{2}}\right){\frac {x^{-2}}{2}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{-4}}{4}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{-6}}{6}}+\cdots \right)\\&=\ln(2x)-\sum _{n=1}^{\infty }\left({\frac {(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{-2n}}{2n}},\qquad \left|x\right|>1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {artanh} x&=x+{\frac {x^{3}}{3}}+{\frac {x^{5}}{5}}+{\frac {x^{7}}{7}}+\cdots \\&=\sum _{n=0}^{\infty }{\frac {x^{2n+1}}{2n+1}},\qquad \left|x\right|<1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcsch} x=\operatorname {arsinh} {\frac {1}{x}}&=x^{-1}-\left({\frac {1}{2}}\right){\frac {x^{-3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{-5}}{5}}-\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{-7}}{7}}\pm \cdots \\&=\sum _{n=0}^{\infty }\left({\frac {(-1)^{n}(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{-(2n+1)}}{2n+1}},\qquad \left|x\right|>1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arsech} x=\operatorname {arcosh} {\frac {1}{x}}&=\ln {\frac {2}{x}}-\left(\left({\frac {1}{2}}\right){\frac {x^{2}}{2}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{4}}{4}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{6}}{6}}+\cdots \right)\\&=\ln {\frac {2}{x}}-\sum _{n=1}^{\infty }\left({\frac {(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{2n}}{2n}},\qquad 0<x\leq 1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcoth} x=\operatorname {artanh} {\frac {1}{x}}&=x^{-1}+{\frac {x^{-3}}{3}}+{\frac {x^{-5}}{5}}+{\frac {x^{-7}}{7}}+\cdots \\&=\sum _{n=0}^{\infty }{\frac {x^{-(2n+1)}}{2n+1}},\qquad \left|x\right|>1\end{aligned}}}$
Asymptotic expansion for the arsinh x is given by

${\displaystyle \operatorname {arsinh} x=\ln(2x)+\sum \limits _{n=1}^{\infty }{\left({-1}\right)^{n-1}{\frac {\left({2n-1}\right)!!}{2n\left({2n}\right)!!}}}{\frac {1}{x^{2n}}}}$



£#h5#£Principal values in the complex plane£#/h5#£
As functions of a complex variable, inverse hyperbolic functions are multivalued functions that are analytic, except at a finite number of points. For such a function, it is common to define a principal value, which is a single valued analytic function which coincides with one specific branch of the multivalued function, over a domain consisting of the complex plane in which a finite number of arcs (usually half lines or line segments) have been removed. These arcs are called branch cuts. For specifying the branch, that is, defining which value of the multivalued function is considered at each point, one generally define it at a particular point, and deduce the value everywhere in the domain of definition of the principal value by analytic continuation. When possible, it is better to define the principal value directly—without referring to analytic continuation.

For example, for the square root, the principal value is defined as the square root that has a positive real part. This defines a single valued analytic function, which is defined everywhere, except for non-positive real values of the variables (where the two square roots have a zero real part). This principal value of the square root function is denoted ${\displaystyle {\sqrt {x}}}$ in what follows. Similarly, the principal value of the logarithm, denoted ${\displaystyle \operatorname {Log} }$ in what follows, is defined as the value for which the imaginary part has the smallest absolute value. It is defined everywhere except for non-positive real values of the variable, for which two different values of the logarithm reach the minimum.

For all inverse hyperbolic functions, the principal value may be defined in terms of principal values of the square root and the logarithm function. However, in some cases, the formulas of § Definitions in terms of logarithms do not give a correct principal value, as giving a domain of definition which is too small and, in one case non-connected.


£#h5#£Principal value of the inverse hyperbolic sine£#/h5#£
The principal value of the inverse hyperbolic sine is given by

${\displaystyle \operatorname {arsinh} z=\operatorname {Log} (z+{\sqrt {z^{2}+1}}\,)\,.}$
The argument of the square root is a non-positive real number, if and only if z belongs to one of the intervals [i, +i∞) and (−i∞, −i] of the imaginary axis. If the argument of the logarithm is real, then it is positive. Thus this formula defines a principal value for arsinh, with branch cuts [i, +i∞) and (−i∞, −i]. This is optimal, as the branch cuts must connect the singular points i and −i to the infinity.


£#h5#£Principal value of the inverse hyperbolic cosine£#/h5#£
The formula for the inverse hyperbolic cosine given in § Inverse hyperbolic cosine is not convenient, since similar to the principal values of the logarithm and the square root, the principal value of arcosh would not be defined for imaginary z. Thus the square root has to be factorized, leading to

${\displaystyle \operatorname {arcosh} z=\operatorname {Log} (z+{\sqrt {z+1}}{\sqrt {z-1}}\,)\,.}$
The principal values of the square roots are both defined, except if z belongs to the real interval (−∞, 1]. If the argument of the logarithm is real, then z is real and has the same sign. Thus, the above formula defines a principal value of arcosh outside the real interval (−∞, 1], which is thus the unique branch cut.


£#h5#£Principal values of the inverse hyperbolic tangent and cotangent£#/h5#£
The formulas given in § Definitions in terms of logarithms suggests

${\displaystyle {\begin{aligned}\operatorname {artanh} z&={\frac {1}{2}}\operatorname {Log} \left({\frac {1+z}{1-z}}\right)\\\operatorname {arcoth} z&={\frac {1}{2}}\operatorname {Log} \left({\frac {z+1}{z-1}}\right)\end{aligned}}}$
for the definition of the principal values of the inverse hyperbolic tangent and cotangent. In these formulas, the argument of the logarithm is real if and only if z is real. For artanh, this argument is in the real interval (−∞, 0], if z belongs either to (−∞, −1] or to [1, ∞). For arcoth, the argument of the logarithm is in (−∞, 0], if and only if z belongs to the real interval [−1, 1].

Therefore, these formulas define convenient principal values, for which the branch cuts are (−∞, −1] and [1, ∞) for the inverse hyperbolic tangent, and [−1, 1] for the inverse hyperbolic cotangent.

In view of a better numerical evaluation near the branch cuts, some authors use the following definitions of the principal values, although the second one introduces a removable singularity at z = 0. The two definitions of ${\displaystyle \operatorname {artanh} }$ differ for real values of ${\displaystyle z}$ with ${\displaystyle z>1}$ . The ones of ${\displaystyle \operatorname {arcoth} }$ differ for real values of ${\displaystyle z}$ with ${\displaystyle z\in [0,1)}$ .

${\displaystyle {\begin{aligned}\operatorname {artanh} z&={\tfrac {1}{2}}\operatorname {Log} \left({1+z}\right)-{\tfrac {1}{2}}\operatorname {Log} \left({1-z}\right)\\\operatorname {arcoth} z&={\tfrac {1}{2}}\operatorname {Log} \left({1+{\frac {1}{z}}}\right)-{\tfrac {1}{2}}\operatorname {Log} \left({1-{\frac {1}{z}}}\right)\end{aligned}}}$

£#h5#£Principal value of the inverse hyperbolic cosecant£#/h5#£
For the inverse hyperbolic cosecant, the principal value is defined as

${\displaystyle \operatorname {arcsch} z=\operatorname {Log} \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z^{2}}}+1}}\,\right)}$ .
It is defined when the arguments of the logarithm and the square root are not non-positive real numbers. The principal value of the square root is thus defined outside the interval [−i, i] of the imaginary line. If the argument of the logarithm is real, then z is a non-zero real number, and this implies that the argument of the logarithm is positive.

Thus, the principal value is defined by the above formula outside the branch cut, consisting of the interval [−i, i] of the imaginary line.

For z = 0, there is a singular point that is included in the branch cut.


£#h5#£Principal value of the inverse hyperbolic secant£#/h5#£
Here, as in the case of the inverse hyperbolic cosine, we have to factorize the square root. This gives the principal value

${\displaystyle \operatorname {arsech} z=\operatorname {Log} \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z}}+1}}\,{\sqrt {{\frac {1}{z}}-1}}\right).}$
If the argument of a square root is real, then z is real, and it follows that both principal values of square roots are defined, except if z is real and belongs to one of the intervals (−∞, 0] and [1, +∞). If the argument of the logarithm is real and negative, then z is also real and negative. It follows that the principal value of arsech is well defined, by the above formula outside two branch cuts, the real intervals (−∞, 0] and [1, +∞).

For z = 0, there is a singular point that is included in one of the branch cuts.


£#h5#£Graphical representation£#/h5#£
In the following graphical representation of the principal values of the inverse hyperbolic functions, the branch cuts appear as discontinuities of the color. The fact that the whole branch cuts appear as discontinuities, shows that these principal values may not be extended into analytic functions defined over larger domains. In other words, the above defined branch cuts are minimal.


£#h5#£See also£#/h5#£ £#ul#££#li#£Complex logarithm£#/li#£ £#li#£Hyperbolic secant distribution£#/li#£ £#li#£ISO 80000-2£#/li#£ £#li#£List of integrals of inverse hyperbolic functions£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£Bibliography£#/h5#£ £#ul#££#li#£Herbert Busemann and Paul J. Kelly (1953) Projective Geometry and Projective Metrics, page 207, Academic Press.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Inverse hyperbolic functions", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Trigonometric Functions £#/li#££#/ul#£




£#h3#£Area Element£#/h3#£

In mathematics, a volume form or top-dimensional form is a differential form of degree equal to the differentiable manifold dimension. Thus on a manifold ${\displaystyle M}$ of dimension ${\displaystyle n}$ , a volume form is an ${\displaystyle n}$ -form. It is an element of the space of sections of the line bundle ${\displaystyle \textstyle {\bigwedge }^{n}(T^{*}M)}$ , denoted as ${\displaystyle \Omega ^{n}(M)}$ . A manifold admits a nowhere-vanishing volume form if and only if it is orientable. An orientable manifold has infinitely many volume forms, since multiplying a volume form by a function yields another volume form. On non-orientable manifolds, one may instead define the weaker notion of a density.

A volume form provides a means to define the integral of a function on a differentiable manifold. In other words, a volume form gives rise to a measure with respect to which functions can be integrated by the appropriate Lebesgue integral. The absolute value of a volume form is a volume element, which is also known variously as a twisted volume form or pseudo-volume form. It also defines a measure, but exists on any differentiable manifold, orientable or not.

Kähler manifolds, being complex manifolds, are naturally oriented, and so possess a volume form. More generally, the ${\displaystyle n}$ th exterior power of the symplectic form on a symplectic manifold is a volume form. Many classes of manifolds have canonical volume forms: they have extra structure which allows the choice of a preferred volume form. Oriented pseudo-Riemannian manifolds have an associated canonical volume form.


£#h5#£Orientation£#/h5#£
The following will only be about orientability of differentiable manifolds (it's a more general notion defined on any topological manifold).

A manifold is orientable if it has a coordinate atlas all of whose transition functions have positive Jacobian determinants. A selection of a maximal such atlas is an orientation on ${\displaystyle M.}$ A volume form ${\displaystyle \omega }$ on ${\displaystyle M}$ gives rise to an orientation in a natural way as the atlas of coordinate charts on ${\displaystyle M}$ that send ${\displaystyle \omega }$ to a positive multiple of the Euclidean volume form ${\displaystyle dx^{1}\wedge \cdots \wedge dx^{n}.}$

A volume form also allows for the specification of a preferred class of frames on ${\displaystyle M.}$ Call a basis of tangent vectors ${\displaystyle (X_{1},\ldots ,X_{n})}$ right-handed if

The collection of all right-handed frames is acted upon by the group ${\displaystyle \mathrm {GL} ^{+}(n)}$ of general linear mappings in ${\displaystyle n}$ dimensions with positive determinant. They form a principal ${\displaystyle \mathrm {GL} ^{+}(n)}$ sub-bundle of the linear frame bundle of ${\displaystyle M,}$ and so the orientation associated to a volume form gives a canonical reduction of the frame bundle of ${\displaystyle M}$ to a sub-bundle with structure group ${\displaystyle \mathrm {GL} ^{+}(n).}$ That is to say that a volume form gives rise to ${\displaystyle \mathrm {GL} ^{+}(n)}$ -structure on ${\displaystyle M.}$ More reduction is clearly possible by considering frames that have

Thus a volume form gives rise to an ${\displaystyle \mathrm {SL} (n)}$ -structure as well. Conversely, given an ${\displaystyle \mathrm {SL} (n)}$ -structure, one can recover a volume form by imposing (1) for the special linear frames and then solving for the required ${\displaystyle n}$ -form ${\displaystyle \omega }$ by requiring homogeneity in its arguments.

A manifold is orientable if and only if it has a nowhere-vanishing volume form. Indeed, ${\displaystyle \mathrm {SL} (n)\to \mathrm {GL} ^{+}(n)}$ is a deformation retract since ${\displaystyle \mathrm {GL} ^{+}=\mathrm {SL} \times \mathbb {R} ^{+},}$ where the positive reals are embedded as scalar matrices. Thus every ${\displaystyle \mathrm {GL} ^{+}(n)}$ -structure is reducible to an ${\displaystyle \mathrm {SL} (n)}$ -structure, and ${\displaystyle \mathrm {GL} ^{+}(n)}$ -structures coincide with orientations on ${\displaystyle M.}$ More concretely, triviality of the determinant bundle ${\displaystyle \Omega ^{n}(M)}$ is equivalent to orientability, and a line bundle is trivial if and only if it has a nowhere-vanishing section. Thus, the existence of a volume form is equivalent to orientability.


£#h5#£Relation to measures£#/h5#£
Given a volume form ${\displaystyle \omega }$ on an oriented manifold, the density ${\displaystyle |\omega |}$ is a volume pseudo-form on the nonoriented manifold obtained by forgetting the orientation. Densities may also be defined more generally on non-orientable manifolds.

Any volume pseudo-form ${\displaystyle \omega }$ (and therefore also any volume form) defines a measure on the Borel sets by

The difference is that while a measure can be integrated over a (Borel) subset, a volume form can only be integrated over an oriented cell. In single variable calculus, writing ${\textstyle \int _{b}^{a}f\,dx=-\int _{a}^{b}f\,dx}$ considers ${\displaystyle dx}$ as a volume form, not simply a measure, and ${\textstyle \int _{b}^{a}}$ indicates "integrate over the cell ${\displaystyle [a,b]}$ with the opposite orientation, sometimes denoted ${\displaystyle {\overline {[a,b]}}}$ ".

Further, general measures need not be continuous or smooth: they need not be defined by a volume form, or more formally, their Radon–Nikodym derivative with respect to a given volume form need not be absolutely continuous.


£#h5#£Divergence£#/h5#£
Given a volume form ${\displaystyle \omega }$ on ${\displaystyle M,}$ one can define the divergence of a vector field ${\displaystyle X}$ as the unique scalar-valued function, denoted by ${\displaystyle \operatorname {div} X,}$ satisfying

where ${\displaystyle L_{X}}$ denotes the Lie derivative along ${\displaystyle X}$ and ${\displaystyle X\mathbin {\!\lrcorner } \omega }$ denotes the interior product or the left contraction of ${\displaystyle \omega }$ along ${\displaystyle X.}$ If ${\displaystyle X}$ is a compactly supported vector field and ${\displaystyle M}$ is a manifold with boundary, then Stokes' theorem implies which is a generalization of the divergence theorem.
The solenoidal vector fields are those with ${\displaystyle \operatorname {div} X=0.}$ It follows from the definition of the Lie derivative that the volume form is preserved under the flow of a solenoidal vector field. Thus solenoidal vector fields are precisely those that have volume-preserving flows. This fact is well-known, for instance, in fluid mechanics where the divergence of a velocity field measures the compressibility of a fluid, which in turn represents the extent to which volume is preserved along flows of the fluid.


£#h5#£Special cases£#/h5#£
£#h5#£Lie groups£#/h5#£
For any Lie group, a natural volume form may be defined by translation. That is, if ${\displaystyle \omega _{e}}$ is an element of ${\displaystyle {\textstyle \bigwedge }^{n}T_{e}^{*}G,}$ then a left-invariant form may be defined by ${\displaystyle \omega _{g}=L_{g^{-1}}^{*}\omega _{e},}$ where ${\displaystyle L_{g}}$ is left-translation. As a corollary, every Lie group is orientable. This volume form is unique up to a scalar, and the corresponding measure is known as the Haar measure.


£#h5#£Symplectic manifolds£#/h5#£
Any symplectic manifold (or indeed any almost symplectic manifold) has a natural volume form. If ${\displaystyle M}$ is a ${\displaystyle 2n}$ -dimensional manifold with symplectic form ${\displaystyle \omega ,}$ then ${\displaystyle \omega ^{n}}$ is nowhere zero as a consequence of the nondegeneracy of the symplectic form. As a corollary, any symplectic manifold is orientable (indeed, oriented). If the manifold is both symplectic and Riemannian, then the two volume forms agree if the manifold is Kähler.


£#h5#£Riemannian volume form£#/h5#£
Any oriented pseudo-Riemannian (including Riemannian) manifold has a natural volume form. In local coordinates, it can be expressed as

where the ${\displaystyle dx^{i}}$ are 1-forms that form a positively oriented basis for the cotangent bundle of the manifold. Here, ${\displaystyle |g|}$ is the absolute value of the determinant of the matrix representation of the metric tensor on the manifold.
The volume form is denoted variously by

Here, the ${\displaystyle {\star }}$ is the Hodge star, thus the last form, ${\displaystyle {\star }(1),}$ emphasizes that the volume form is the Hodge dual of the constant map on the manifold, which equals the Levi-Civita tensor ${\displaystyle \varepsilon .}$

Although the Greek letter ${\displaystyle \omega }$ is frequently used to denote the volume form, this notation is not universal; the symbol ${\displaystyle \omega }$ often carries many other meanings in differential geometry (such as a symplectic form).


£#h5#£Invariants of a volume form£#/h5#£
Volume forms are not unique; they form a torsor over non-vanishing functions on the manifold, as follows. Given a non-vanishing function ${\displaystyle f}$ on ${\displaystyle M,}$ and a volume form ${\displaystyle \omega ,}$ ${\displaystyle f\omega }$ is a volume form on ${\displaystyle M.}$ Conversely, given two volume forms ${\displaystyle \omega ,\omega ',}$ their ratio is a non-vanishing function (positive if they define the same orientation, negative if they define opposite orientations).

In coordinates, they are both simply a non-zero function times Lebesgue measure, and their ratio is the ratio of the functions, which is independent of choice of coordinates. Intrinsically, it is the Radon–Nikodym derivative of ${\displaystyle \omega '}$ with respect to ${\displaystyle \omega .}$ On an oriented manifold, the proportionality of any two volume forms can be thought of as a geometric form of the Radon–Nikodym theorem.


£#h5#£No local structure£#/h5#£
A volume form on a manifold has no local structure in the sense that it is not possible on small open sets to distinguish between the given volume form and the volume form on Euclidean space (Kobayashi 1972). That is, for every point ${\displaystyle p}$ in ${\displaystyle M,}$ there is an open neighborhood ${\displaystyle U}$ of ${\displaystyle p}$ and a diffeomorphism ${\displaystyle \varphi }$ of ${\displaystyle U}$ onto an open set in ${\displaystyle \mathbb {R} ^{n}}$ such that the volume form on ${\displaystyle U}$ is the pullback of ${\displaystyle dx^{1}\wedge \cdots \wedge dx^{n}}$ along ${\displaystyle \varphi .}$

As a corollary, if ${\displaystyle M}$ and ${\displaystyle N}$ are two manifolds, each with volume forms ${\displaystyle \omega _{M},\omega _{N},}$ then for any points ${\displaystyle m\in M,n\in N,}$ there are open neighborhoods ${\displaystyle U}$ of ${\displaystyle m}$ and ${\displaystyle V}$ of ${\displaystyle n}$ and a map ${\displaystyle f:U\to V}$ such that the volume form on ${\displaystyle N}$ restricted to the neighborhood ${\displaystyle V}$ pulls back to volume form on ${\displaystyle M}$ restricted to the neighborhood ${\displaystyle U}$ : ${\displaystyle f^{*}\omega _{N}\vert _{V}=\omega _{M}\vert _{U}.}$

In one dimension, one can prove it thus: given a volume form ${\displaystyle \omega }$ on ${\displaystyle \mathbb {R} ,}$ define

Then the standard Lebesgue measure ${\displaystyle dx}$ pulls back to ${\displaystyle \omega }$ under ${\displaystyle f}$ : ${\displaystyle \omega =f^{*}dx.}$ Concretely, ${\displaystyle \omega =f'\,dx.}$ In higher dimensions, given any point ${\displaystyle m\in M,}$ it has a neighborhood locally homeomorphic to ${\displaystyle \mathbb {R} \times \mathbb {R} ^{n-1},}$ and one can apply the same procedure.
£#h5#£Global structure: volume£#/h5#£
A volume form on a connected manifold ${\displaystyle M}$ has a single global invariant, namely the (overall) volume, denoted ${\displaystyle \mu (M),}$ which is invariant under volume-form preserving maps; this may be infinite, such as for Lebesgue measure on ${\displaystyle \mathbb {R} ^{n}.}$ On a disconnected manifold, the volume of each connected component is the invariant.

In symbols, if ${\displaystyle f:M\to N}$ is a homeomorphism of manifolds that pulls back ${\displaystyle \omega _{N}}$ to ${\displaystyle \omega _{M},}$ then

and the manifolds have the same volume.
Volume forms can also be pulled back under covering maps, in which case they multiply volume by the cardinality of the fiber (formally, by integration along the fiber). In the case of an infinite sheeted cover (such as ${\displaystyle \mathbb {R} \to S^{1}}$ ), a volume form on a finite volume manifold pulls back to a volume form on an infinite volume manifold.


£#h5#£See also£#/h5#£ £#ul#££#li#£Cylindrical coordinate system § Line and volume elements£#/li#£ £#li#£Measure (mathematics) – Generalization of mass, length, area and volume£#/li#£ £#li#£Poincaré metric provides a review of the volume form on the complex plane£#/li#£ £#li#£Spherical coordinate system § Integration and differentiation in spherical coordinates£#/li#££#/ul#£
£#h5#£References£#/h5#£ £#ul#££#li#£Kobayashi, S. (1972), Transformation Groups in Differential Geometry, Classics in Mathematics, Springer, ISBN 3-540-58659-8, OCLC 31374337.£#/li#£ £#li#£Spivak, Michael (1965), Calculus on Manifolds, Reading, Massachusetts: W.A. Benjamin, Inc., ISBN 0-8053-9021-9.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Gray, A. "The Intuitive Idea of Area on a Surface." §15.3 in Modern Differential Geometry of Curves and Surfaces with Mathematica, 2nd ed. Boca Raton, FL: CRC Press, pp. 351-353, 1997.£#/li#££#li#£ Gray, A. "The Intuitive Idea of Area on a Surface." §15.3 in Modern Differential Geometry of Curves and Surfaces with Mathematica, 2nd ed. Boca Raton, FL: CRC Press, pp. 351-353, 1997. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Differential Geometry > General Differential Geometry £#/li#££#/ul#£




£#h3#£Area Hyperbolic Cosecant£#/h3#£


In mathematics, the inverse hyperbolic functions are the inverse functions of the hyperbolic functions.

For a given value of a hyperbolic function, the corresponding inverse hyperbolic function provides the corresponding hyperbolic angle. The size of the hyperbolic angle is equal to the area of the corresponding hyperbolic sector of the hyperbola xy = 1, or twice the area of the corresponding sector of the unit hyperbola x2 − y2 = 1, just as a circular angle is twice the area of the circular sector of the unit circle. Some authors have called inverse hyperbolic functions "area functions" to realize the hyperbolic angles.

Hyperbolic functions occur in the calculations of angles and distances in hyperbolic geometry. It also occurs in the solutions of many linear differential equations (such as the equation defining a catenary), cubic equations, and Laplace's equation in Cartesian coordinates. Laplace's equations are important in many areas of physics, including electromagnetic theory, heat transfer, fluid dynamics, and special relativity.


£#h5#£Notation£#/h5#£
The ISO 80000-2 standard abbreviations consist of ar- followed by the abbreviation of the corresponding hyperbolic function (e.g., arsinh, arcosh). The prefix arc- followed by the corresponding hyperbolic function (e.g., arcsinh, arccosh) is also commonly seen, by analogy with the nomenclature for inverse trigonometric functions. These are misnomers, since the prefix arc is the abbreviation for arcus, while the prefix ar stands for area; the hyperbolic functions are not directly related to arcs.

Other authors prefer to use the notation argsinh, argcosh, argtanh, and so on, where the prefix arg is the abbreviation of the Latin argumentum. In computer science, this is often shortened to asinh.

The notation sinh−1(x), cosh−1(x), etc., is also used, despite the fact that care must be taken to avoid misinterpretations of the superscript −1 as a power, as opposed to a shorthand to denote the inverse function (e.g., cosh−1(x) versus cosh(x)−1).


£#h5#£Definitions in terms of logarithms£#/h5#£
Since the hyperbolic functions are rational functions of ex whose numerator and denominator are of degree at most two, these functions may be solved in terms of ex, by using the quadratic formula; then, taking the natural logarithm gives the following expressions for the inverse hyperbolic functions.

For complex arguments, the inverse hyperbolic functions, the square root and the logarithm are multi-valued functions, and the equalities of the next subsections may be viewed as equalities of multi-valued functions.

For all inverse hyperbolic functions (save the inverse hyperbolic cotangent and the inverse hyperbolic cosecant), the domain of the real function is connected.


£#h5#£Inverse hyperbolic sine£#/h5#£
Inverse hyperbolic sine (a.k.a. area hyperbolic sine) (Latin: Area sinus hyperbolicus):

${\displaystyle \operatorname {arsinh} x=\ln \left(x+{\sqrt {x^{2}+1}}\right)}$
The domain is the whole real line.


£#h5#£Inverse hyperbolic cosine£#/h5#£
Inverse hyperbolic cosine (a.k.a. area hyperbolic cosine) (Latin: Area cosinus hyperbolicus):

${\displaystyle \operatorname {arcosh} x=\ln \left(x+{\sqrt {x^{2}-1}}\right)}$
The domain is the closed interval [1, +∞ ).


£#h5#£Inverse hyperbolic tangent£#/h5#£
Inverse hyperbolic tangent (a.k.a. area hyperbolic tangent) (Latin: Area tangens hyperbolicus):

${\displaystyle \operatorname {artanh} x={\frac {1}{2}}\ln \left({\frac {1+x}{1-x}}\right)}$
The domain is the open interval (−1, 1).


£#h5#£Inverse hyperbolic cotangent£#/h5#£
Inverse hyperbolic cotangent (a.k.a., area hyperbolic cotangent) (Latin: Area cotangens hyperbolicus):

${\displaystyle \operatorname {arcoth} x={\frac {1}{2}}\ln \left({\frac {x+1}{x-1}}\right)}$
The domain is the union of the open intervals (−∞, −1) and (1, +∞).


£#h5#£Inverse hyperbolic secant£#/h5#£
Inverse hyperbolic secant (a.k.a., area hyperbolic secant) (Latin: Area secans hyperbolicus):

${\displaystyle \operatorname {arsech} x=\ln \left({\frac {1}{x}}+{\sqrt {{\frac {1}{x^{2}}}-1}}\right)=\ln \left({\frac {1+{\sqrt {1-x^{2}}}}{x}}\right)}$
The domain is the semi-open interval (0, 1].


£#h5#£Inverse hyperbolic cosecant£#/h5#£
Inverse hyperbolic cosecant (a.k.a., area hyperbolic cosecant) (Latin: Area cosecans hyperbolicus):

${\displaystyle \operatorname {arcsch} x=\ln \left({\frac {1}{x}}+{\sqrt {{\frac {1}{x^{2}}}+1}}\right)}$
The domain is the real line with 0 removed.


£#h5#£Addition formulae£#/h5#£
${\displaystyle \operatorname {arsinh} u\pm \operatorname {arsinh} v=\operatorname {arsinh} \left(u{\sqrt {1+v^{2}}}\pm v{\sqrt {1+u^{2}}}\right)}$
${\displaystyle \operatorname {arcosh} u\pm \operatorname {arcosh} v=\operatorname {arcosh} \left(uv\pm {\sqrt {(u^{2}-1)(v^{2}-1)}}\right)}$
${\displaystyle \operatorname {artanh} u\pm \operatorname {artanh} v=\operatorname {artanh} \left({\frac {u\pm v}{1\pm uv}}\right)}$
${\displaystyle \operatorname {arcoth} u\pm \operatorname {arcoth} v=\operatorname {arcoth} \left({\frac {1\pm uv}{u\pm v}}\right)}$
${\displaystyle {\begin{aligned}\operatorname {arsinh} u+\operatorname {arcosh} v&=\operatorname {arsinh} \left(uv+{\sqrt {(1+u^{2})(v^{2}-1)}}\right)\\&=\operatorname {arcosh} \left(v{\sqrt {1+u^{2}}}+u{\sqrt {v^{2}-1}}\right)\end{aligned}}}$

£#h5#£Other identities£#/h5#£
${\displaystyle {\begin{aligned}2\operatorname {arcosh} x&=\operatorname {arcosh} (2x^{2}-1)&\quad {\hbox{ for }}x\geq 1\\4\operatorname {arcosh} x&=\operatorname {arcosh} (8x^{4}-8x^{2}+1)&\quad {\hbox{ for }}x\geq 1\\2\operatorname {arsinh} x&=\operatorname {arcosh} (2x^{2}+1)&\quad {\hbox{ for }}x\geq 0\\4\operatorname {arsinh} x&=\operatorname {arcosh} (8x^{4}+8x^{2}+1)&\quad {\hbox{ for }}x\geq 0\end{aligned}}}$
${\displaystyle \ln(x)=\operatorname {arcosh} \left({\frac {x^{2}+1}{2x}}\right)=\operatorname {arsinh} \left({\frac {x^{2}-1}{2x}}\right)=\operatorname {artanh} \left({\frac {x^{2}-1}{x^{2}+1}}\right)}$

£#h5#£Composition of hyperbolic and inverse hyperbolic functions£#/h5#£
${\displaystyle {\begin{aligned}&\sinh(\operatorname {arcosh} x)={\sqrt {x^{2}-1}}\quad {\text{for}}\quad |x|>1\\&\sinh(\operatorname {artanh} x)={\frac {x}{\sqrt {1-x^{2}}}}\quad {\text{for}}\quad -1<x<1\\&\cosh(\operatorname {arsinh} x)={\sqrt {1+x^{2}}}\\&\cosh(\operatorname {artanh} x)={\frac {1}{\sqrt {1-x^{2}}}}\quad {\text{for}}\quad -1<x<1\\&\tanh(\operatorname {arsinh} x)={\frac {x}{\sqrt {1+x^{2}}}}\\&\tanh(\operatorname {arcosh} x)={\frac {\sqrt {x^{2}-1}}{x}}\quad {\text{for}}\quad |x|>1\end{aligned}}}$

£#h5#£Composition of inverse hyperbolic and trigonometric functions£#/h5#£
${\displaystyle \operatorname {arsinh} \left(\tan \alpha \right)=\operatorname {artanh} \left(\sin \alpha \right)=\ln \left({\frac {1+\sin \alpha }{\cos \alpha }}\right)=\pm \operatorname {arcosh} \left({\frac {1}{\cos \alpha }}\right)}$
${\displaystyle \ln \left(\left|\tan \alpha \right|\right)=-\operatorname {artanh} \left(\cos 2\alpha \right)}$

£#h5#£Conversions£#/h5#£
${\displaystyle \ln x=\operatorname {artanh} \left({\frac {x^{2}-1}{x^{2}+1}}\right)=\operatorname {arsinh} \left({\frac {x^{2}-1}{2x}}\right)=\pm \operatorname {arcosh} \left({\frac {x^{2}+1}{2x}}\right)}$
${\displaystyle \operatorname {artanh} x=\operatorname {arsinh} \left({\frac {x}{\sqrt {1-x^{2}}}}\right)=\pm \operatorname {arcosh} \left({\frac {1}{\sqrt {1-x^{2}}}}\right)}$
${\displaystyle \operatorname {arsinh} x=\operatorname {artanh} \left({\frac {x}{\sqrt {1+x^{2}}}}\right)=\pm \operatorname {arcosh} \left({\sqrt {1+x^{2}}}\right)}$
${\displaystyle \operatorname {arcosh} x=\left|\operatorname {arsinh} \left({\sqrt {x^{2}-1}}\right)\right|=\left|\operatorname {artanh} \left({\frac {\sqrt {x^{2}-1}}{x}}\right)\right|}$

£#h5#£Derivatives£#/h5#£
${\displaystyle {\begin{aligned}{\frac {d}{dx}}\operatorname {arsinh} x&{}={\frac {1}{\sqrt {x^{2}+1}}},{\text{ for all real }}x\\{\frac {d}{dx}}\operatorname {arcosh} x&{}={\frac {1}{\sqrt {x^{2}-1}}},{\text{ for all real }}x>1\\{\frac {d}{dx}}\operatorname {artanh} x&{}={\frac {1}{1-x^{2}}},{\text{ for all real }}|x|<1\\{\frac {d}{dx}}\operatorname {arcoth} x&{}={\frac {1}{1-x^{2}}},{\text{ for all real }}|x|>1\\{\frac {d}{dx}}\operatorname {arsech} x&{}={\frac {-1}{x{\sqrt {1-x^{2}}}}},{\text{ for all real }}x\in (0,1)\\{\frac {d}{dx}}\operatorname {arcsch} x&{}={\frac {-1}{|x|{\sqrt {1+x^{2}}}}},{\text{ for all real }}x{\text{, except }}0\\\end{aligned}}}$
For an example differentiation: let θ = arsinh x, so (where sinh2 θ = (sinh θ)2):

${\displaystyle {\frac {d\,\operatorname {arsinh} x}{dx}}={\frac {d\theta }{d\sinh \theta }}={\frac {1}{\cosh \theta }}={\frac {1}{\sqrt {1+\sinh ^{2}\theta }}}={\frac {1}{\sqrt {1+x^{2}}}}.}$

£#h5#£Series expansions£#/h5#£
Expansion series can be obtained for the above functions:

${\displaystyle {\begin{aligned}\operatorname {arsinh} x&=x-\left({\frac {1}{2}}\right){\frac {x^{3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{5}}{5}}-\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{7}}{7}}\pm \cdots \\&=\sum _{n=0}^{\infty }\left({\frac {(-1)^{n}(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{2n+1}}{2n+1}},\qquad \left|x\right|<1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcosh} x&=\ln(2x)-\left(\left({\frac {1}{2}}\right){\frac {x^{-2}}{2}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{-4}}{4}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{-6}}{6}}+\cdots \right)\\&=\ln(2x)-\sum _{n=1}^{\infty }\left({\frac {(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{-2n}}{2n}},\qquad \left|x\right|>1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {artanh} x&=x+{\frac {x^{3}}{3}}+{\frac {x^{5}}{5}}+{\frac {x^{7}}{7}}+\cdots \\&=\sum _{n=0}^{\infty }{\frac {x^{2n+1}}{2n+1}},\qquad \left|x\right|<1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcsch} x=\operatorname {arsinh} {\frac {1}{x}}&=x^{-1}-\left({\frac {1}{2}}\right){\frac {x^{-3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{-5}}{5}}-\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{-7}}{7}}\pm \cdots \\&=\sum _{n=0}^{\infty }\left({\frac {(-1)^{n}(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{-(2n+1)}}{2n+1}},\qquad \left|x\right|>1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arsech} x=\operatorname {arcosh} {\frac {1}{x}}&=\ln {\frac {2}{x}}-\left(\left({\frac {1}{2}}\right){\frac {x^{2}}{2}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{4}}{4}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{6}}{6}}+\cdots \right)\\&=\ln {\frac {2}{x}}-\sum _{n=1}^{\infty }\left({\frac {(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{2n}}{2n}},\qquad 0<x\leq 1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcoth} x=\operatorname {artanh} {\frac {1}{x}}&=x^{-1}+{\frac {x^{-3}}{3}}+{\frac {x^{-5}}{5}}+{\frac {x^{-7}}{7}}+\cdots \\&=\sum _{n=0}^{\infty }{\frac {x^{-(2n+1)}}{2n+1}},\qquad \left|x\right|>1\end{aligned}}}$
Asymptotic expansion for the arsinh x is given by

${\displaystyle \operatorname {arsinh} x=\ln(2x)+\sum \limits _{n=1}^{\infty }{\left({-1}\right)^{n-1}{\frac {\left({2n-1}\right)!!}{2n\left({2n}\right)!!}}}{\frac {1}{x^{2n}}}}$



£#h5#£Principal values in the complex plane£#/h5#£
As functions of a complex variable, inverse hyperbolic functions are multivalued functions that are analytic, except at a finite number of points. For such a function, it is common to define a principal value, which is a single valued analytic function which coincides with one specific branch of the multivalued function, over a domain consisting of the complex plane in which a finite number of arcs (usually half lines or line segments) have been removed. These arcs are called branch cuts. For specifying the branch, that is, defining which value of the multivalued function is considered at each point, one generally define it at a particular point, and deduce the value everywhere in the domain of definition of the principal value by analytic continuation. When possible, it is better to define the principal value directly—without referring to analytic continuation.

For example, for the square root, the principal value is defined as the square root that has a positive real part. This defines a single valued analytic function, which is defined everywhere, except for non-positive real values of the variables (where the two square roots have a zero real part). This principal value of the square root function is denoted ${\displaystyle {\sqrt {x}}}$ in what follows. Similarly, the principal value of the logarithm, denoted ${\displaystyle \operatorname {Log} }$ in what follows, is defined as the value for which the imaginary part has the smallest absolute value. It is defined everywhere except for non-positive real values of the variable, for which two different values of the logarithm reach the minimum.

For all inverse hyperbolic functions, the principal value may be defined in terms of principal values of the square root and the logarithm function. However, in some cases, the formulas of § Definitions in terms of logarithms do not give a correct principal value, as giving a domain of definition which is too small and, in one case non-connected.


£#h5#£Principal value of the inverse hyperbolic sine£#/h5#£
The principal value of the inverse hyperbolic sine is given by

${\displaystyle \operatorname {arsinh} z=\operatorname {Log} (z+{\sqrt {z^{2}+1}}\,)\,.}$
The argument of the square root is a non-positive real number, if and only if z belongs to one of the intervals [i, +i∞) and (−i∞, −i] of the imaginary axis. If the argument of the logarithm is real, then it is positive. Thus this formula defines a principal value for arsinh, with branch cuts [i, +i∞) and (−i∞, −i]. This is optimal, as the branch cuts must connect the singular points i and −i to the infinity.


£#h5#£Principal value of the inverse hyperbolic cosine£#/h5#£
The formula for the inverse hyperbolic cosine given in § Inverse hyperbolic cosine is not convenient, since similar to the principal values of the logarithm and the square root, the principal value of arcosh would not be defined for imaginary z. Thus the square root has to be factorized, leading to

${\displaystyle \operatorname {arcosh} z=\operatorname {Log} (z+{\sqrt {z+1}}{\sqrt {z-1}}\,)\,.}$
The principal values of the square roots are both defined, except if z belongs to the real interval (−∞, 1]. If the argument of the logarithm is real, then z is real and has the same sign. Thus, the above formula defines a principal value of arcosh outside the real interval (−∞, 1], which is thus the unique branch cut.


£#h5#£Principal values of the inverse hyperbolic tangent and cotangent£#/h5#£
The formulas given in § Definitions in terms of logarithms suggests

${\displaystyle {\begin{aligned}\operatorname {artanh} z&={\frac {1}{2}}\operatorname {Log} \left({\frac {1+z}{1-z}}\right)\\\operatorname {arcoth} z&={\frac {1}{2}}\operatorname {Log} \left({\frac {z+1}{z-1}}\right)\end{aligned}}}$
for the definition of the principal values of the inverse hyperbolic tangent and cotangent. In these formulas, the argument of the logarithm is real if and only if z is real. For artanh, this argument is in the real interval (−∞, 0], if z belongs either to (−∞, −1] or to [1, ∞). For arcoth, the argument of the logarithm is in (−∞, 0], if and only if z belongs to the real interval [−1, 1].

Therefore, these formulas define convenient principal values, for which the branch cuts are (−∞, −1] and [1, ∞) for the inverse hyperbolic tangent, and [−1, 1] for the inverse hyperbolic cotangent.

In view of a better numerical evaluation near the branch cuts, some authors use the following definitions of the principal values, although the second one introduces a removable singularity at z = 0. The two definitions of ${\displaystyle \operatorname {artanh} }$ differ for real values of ${\displaystyle z}$ with ${\displaystyle z>1}$ . The ones of ${\displaystyle \operatorname {arcoth} }$ differ for real values of ${\displaystyle z}$ with ${\displaystyle z\in [0,1)}$ .

${\displaystyle {\begin{aligned}\operatorname {artanh} z&={\tfrac {1}{2}}\operatorname {Log} \left({1+z}\right)-{\tfrac {1}{2}}\operatorname {Log} \left({1-z}\right)\\\operatorname {arcoth} z&={\tfrac {1}{2}}\operatorname {Log} \left({1+{\frac {1}{z}}}\right)-{\tfrac {1}{2}}\operatorname {Log} \left({1-{\frac {1}{z}}}\right)\end{aligned}}}$

£#h5#£Principal value of the inverse hyperbolic cosecant£#/h5#£
For the inverse hyperbolic cosecant, the principal value is defined as

${\displaystyle \operatorname {arcsch} z=\operatorname {Log} \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z^{2}}}+1}}\,\right)}$ .
It is defined when the arguments of the logarithm and the square root are not non-positive real numbers. The principal value of the square root is thus defined outside the interval [−i, i] of the imaginary line. If the argument of the logarithm is real, then z is a non-zero real number, and this implies that the argument of the logarithm is positive.

Thus, the principal value is defined by the above formula outside the branch cut, consisting of the interval [−i, i] of the imaginary line.

For z = 0, there is a singular point that is included in the branch cut.


£#h5#£Principal value of the inverse hyperbolic secant£#/h5#£
Here, as in the case of the inverse hyperbolic cosine, we have to factorize the square root. This gives the principal value

${\displaystyle \operatorname {arsech} z=\operatorname {Log} \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z}}+1}}\,{\sqrt {{\frac {1}{z}}-1}}\right).}$
If the argument of a square root is real, then z is real, and it follows that both principal values of square roots are defined, except if z is real and belongs to one of the intervals (−∞, 0] and [1, +∞). If the argument of the logarithm is real and negative, then z is also real and negative. It follows that the principal value of arsech is well defined, by the above formula outside two branch cuts, the real intervals (−∞, 0] and [1, +∞).

For z = 0, there is a singular point that is included in one of the branch cuts.


£#h5#£Graphical representation£#/h5#£
In the following graphical representation of the principal values of the inverse hyperbolic functions, the branch cuts appear as discontinuities of the color. The fact that the whole branch cuts appear as discontinuities, shows that these principal values may not be extended into analytic functions defined over larger domains. In other words, the above defined branch cuts are minimal.


£#h5#£See also£#/h5#£ £#ul#££#li#£Complex logarithm£#/li#£ £#li#£Hyperbolic secant distribution£#/li#£ £#li#£ISO 80000-2£#/li#£ £#li#£List of integrals of inverse hyperbolic functions£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£Bibliography£#/h5#£ £#ul#££#li#£Herbert Busemann and Paul J. Kelly (1953) Projective Geometry and Projective Metrics, page 207, Academic Press.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Inverse hyperbolic functions", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Hyperbolic Functions £#/li#££#/ul#£




£#h3#£Area Hyperbolic Cosine£#/h3#£


In mathematics, the inverse hyperbolic functions are the inverse functions of the hyperbolic functions.

For a given value of a hyperbolic function, the corresponding inverse hyperbolic function provides the corresponding hyperbolic angle. The size of the hyperbolic angle is equal to the area of the corresponding hyperbolic sector of the hyperbola xy = 1, or twice the area of the corresponding sector of the unit hyperbola x2 − y2 = 1, just as a circular angle is twice the area of the circular sector of the unit circle. Some authors have called inverse hyperbolic functions "area functions" to realize the hyperbolic angles.

Hyperbolic functions occur in the calculations of angles and distances in hyperbolic geometry. It also occurs in the solutions of many linear differential equations (such as the equation defining a catenary), cubic equations, and Laplace's equation in Cartesian coordinates. Laplace's equations are important in many areas of physics, including electromagnetic theory, heat transfer, fluid dynamics, and special relativity.


£#h5#£Notation£#/h5#£
The ISO 80000-2 standard abbreviations consist of ar- followed by the abbreviation of the corresponding hyperbolic function (e.g., arsinh, arcosh). The prefix arc- followed by the corresponding hyperbolic function (e.g., arcsinh, arccosh) is also commonly seen, by analogy with the nomenclature for inverse trigonometric functions. These are misnomers, since the prefix arc is the abbreviation for arcus, while the prefix ar stands for area; the hyperbolic functions are not directly related to arcs.

Other authors prefer to use the notation argsinh, argcosh, argtanh, and so on, where the prefix arg is the abbreviation of the Latin argumentum. In computer science, this is often shortened to asinh.

The notation sinh−1(x), cosh−1(x), etc., is also used, despite the fact that care must be taken to avoid misinterpretations of the superscript −1 as a power, as opposed to a shorthand to denote the inverse function (e.g., cosh−1(x) versus cosh(x)−1).


£#h5#£Definitions in terms of logarithms£#/h5#£
Since the hyperbolic functions are rational functions of ex whose numerator and denominator are of degree at most two, these functions may be solved in terms of ex, by using the quadratic formula; then, taking the natural logarithm gives the following expressions for the inverse hyperbolic functions.

For complex arguments, the inverse hyperbolic functions, the square root and the logarithm are multi-valued functions, and the equalities of the next subsections may be viewed as equalities of multi-valued functions.

For all inverse hyperbolic functions (save the inverse hyperbolic cotangent and the inverse hyperbolic cosecant), the domain of the real function is connected.


£#h5#£Inverse hyperbolic sine£#/h5#£
Inverse hyperbolic sine (a.k.a. area hyperbolic sine) (Latin: Area sinus hyperbolicus):

${\displaystyle \operatorname {arsinh} x=\ln \left(x+{\sqrt {x^{2}+1}}\right)}$
The domain is the whole real line.


£#h5#£Inverse hyperbolic cosine£#/h5#£
Inverse hyperbolic cosine (a.k.a. area hyperbolic cosine) (Latin: Area cosinus hyperbolicus):

${\displaystyle \operatorname {arcosh} x=\ln \left(x+{\sqrt {x^{2}-1}}\right)}$
The domain is the closed interval [1, +∞ ).


£#h5#£Inverse hyperbolic tangent£#/h5#£
Inverse hyperbolic tangent (a.k.a. area hyperbolic tangent) (Latin: Area tangens hyperbolicus):

${\displaystyle \operatorname {artanh} x={\frac {1}{2}}\ln \left({\frac {1+x}{1-x}}\right)}$
The domain is the open interval (−1, 1).


£#h5#£Inverse hyperbolic cotangent£#/h5#£
Inverse hyperbolic cotangent (a.k.a., area hyperbolic cotangent) (Latin: Area cotangens hyperbolicus):

${\displaystyle \operatorname {arcoth} x={\frac {1}{2}}\ln \left({\frac {x+1}{x-1}}\right)}$
The domain is the union of the open intervals (−∞, −1) and (1, +∞).


£#h5#£Inverse hyperbolic secant£#/h5#£
Inverse hyperbolic secant (a.k.a., area hyperbolic secant) (Latin: Area secans hyperbolicus):

${\displaystyle \operatorname {arsech} x=\ln \left({\frac {1}{x}}+{\sqrt {{\frac {1}{x^{2}}}-1}}\right)=\ln \left({\frac {1+{\sqrt {1-x^{2}}}}{x}}\right)}$
The domain is the semi-open interval (0, 1].


£#h5#£Inverse hyperbolic cosecant£#/h5#£
Inverse hyperbolic cosecant (a.k.a., area hyperbolic cosecant) (Latin: Area cosecans hyperbolicus):

${\displaystyle \operatorname {arcsch} x=\ln \left({\frac {1}{x}}+{\sqrt {{\frac {1}{x^{2}}}+1}}\right)}$
The domain is the real line with 0 removed.


£#h5#£Addition formulae£#/h5#£
${\displaystyle \operatorname {arsinh} u\pm \operatorname {arsinh} v=\operatorname {arsinh} \left(u{\sqrt {1+v^{2}}}\pm v{\sqrt {1+u^{2}}}\right)}$
${\displaystyle \operatorname {arcosh} u\pm \operatorname {arcosh} v=\operatorname {arcosh} \left(uv\pm {\sqrt {(u^{2}-1)(v^{2}-1)}}\right)}$
${\displaystyle \operatorname {artanh} u\pm \operatorname {artanh} v=\operatorname {artanh} \left({\frac {u\pm v}{1\pm uv}}\right)}$
${\displaystyle \operatorname {arcoth} u\pm \operatorname {arcoth} v=\operatorname {arcoth} \left({\frac {1\pm uv}{u\pm v}}\right)}$
${\displaystyle {\begin{aligned}\operatorname {arsinh} u+\operatorname {arcosh} v&=\operatorname {arsinh} \left(uv+{\sqrt {(1+u^{2})(v^{2}-1)}}\right)\\&=\operatorname {arcosh} \left(v{\sqrt {1+u^{2}}}+u{\sqrt {v^{2}-1}}\right)\end{aligned}}}$

£#h5#£Other identities£#/h5#£
${\displaystyle {\begin{aligned}2\operatorname {arcosh} x&=\operatorname {arcosh} (2x^{2}-1)&\quad {\hbox{ for }}x\geq 1\\4\operatorname {arcosh} x&=\operatorname {arcosh} (8x^{4}-8x^{2}+1)&\quad {\hbox{ for }}x\geq 1\\2\operatorname {arsinh} x&=\operatorname {arcosh} (2x^{2}+1)&\quad {\hbox{ for }}x\geq 0\\4\operatorname {arsinh} x&=\operatorname {arcosh} (8x^{4}+8x^{2}+1)&\quad {\hbox{ for }}x\geq 0\end{aligned}}}$
${\displaystyle \ln(x)=\operatorname {arcosh} \left({\frac {x^{2}+1}{2x}}\right)=\operatorname {arsinh} \left({\frac {x^{2}-1}{2x}}\right)=\operatorname {artanh} \left({\frac {x^{2}-1}{x^{2}+1}}\right)}$

£#h5#£Composition of hyperbolic and inverse hyperbolic functions£#/h5#£
${\displaystyle {\begin{aligned}&\sinh(\operatorname {arcosh} x)={\sqrt {x^{2}-1}}\quad {\text{for}}\quad |x|>1\\&\sinh(\operatorname {artanh} x)={\frac {x}{\sqrt {1-x^{2}}}}\quad {\text{for}}\quad -1<x<1\\&\cosh(\operatorname {arsinh} x)={\sqrt {1+x^{2}}}\\&\cosh(\operatorname {artanh} x)={\frac {1}{\sqrt {1-x^{2}}}}\quad {\text{for}}\quad -1<x<1\\&\tanh(\operatorname {arsinh} x)={\frac {x}{\sqrt {1+x^{2}}}}\\&\tanh(\operatorname {arcosh} x)={\frac {\sqrt {x^{2}-1}}{x}}\quad {\text{for}}\quad |x|>1\end{aligned}}}$

£#h5#£Composition of inverse hyperbolic and trigonometric functions£#/h5#£
${\displaystyle \operatorname {arsinh} \left(\tan \alpha \right)=\operatorname {artanh} \left(\sin \alpha \right)=\ln \left({\frac {1+\sin \alpha }{\cos \alpha }}\right)=\pm \operatorname {arcosh} \left({\frac {1}{\cos \alpha }}\right)}$
${\displaystyle \ln \left(\left|\tan \alpha \right|\right)=-\operatorname {artanh} \left(\cos 2\alpha \right)}$

£#h5#£Conversions£#/h5#£
${\displaystyle \ln x=\operatorname {artanh} \left({\frac {x^{2}-1}{x^{2}+1}}\right)=\operatorname {arsinh} \left({\frac {x^{2}-1}{2x}}\right)=\pm \operatorname {arcosh} \left({\frac {x^{2}+1}{2x}}\right)}$
${\displaystyle \operatorname {artanh} x=\operatorname {arsinh} \left({\frac {x}{\sqrt {1-x^{2}}}}\right)=\pm \operatorname {arcosh} \left({\frac {1}{\sqrt {1-x^{2}}}}\right)}$
${\displaystyle \operatorname {arsinh} x=\operatorname {artanh} \left({\frac {x}{\sqrt {1+x^{2}}}}\right)=\pm \operatorname {arcosh} \left({\sqrt {1+x^{2}}}\right)}$
${\displaystyle \operatorname {arcosh} x=\left|\operatorname {arsinh} \left({\sqrt {x^{2}-1}}\right)\right|=\left|\operatorname {artanh} \left({\frac {\sqrt {x^{2}-1}}{x}}\right)\right|}$

£#h5#£Derivatives£#/h5#£
${\displaystyle {\begin{aligned}{\frac {d}{dx}}\operatorname {arsinh} x&{}={\frac {1}{\sqrt {x^{2}+1}}},{\text{ for all real }}x\\{\frac {d}{dx}}\operatorname {arcosh} x&{}={\frac {1}{\sqrt {x^{2}-1}}},{\text{ for all real }}x>1\\{\frac {d}{dx}}\operatorname {artanh} x&{}={\frac {1}{1-x^{2}}},{\text{ for all real }}|x|<1\\{\frac {d}{dx}}\operatorname {arcoth} x&{}={\frac {1}{1-x^{2}}},{\text{ for all real }}|x|>1\\{\frac {d}{dx}}\operatorname {arsech} x&{}={\frac {-1}{x{\sqrt {1-x^{2}}}}},{\text{ for all real }}x\in (0,1)\\{\frac {d}{dx}}\operatorname {arcsch} x&{}={\frac {-1}{|x|{\sqrt {1+x^{2}}}}},{\text{ for all real }}x{\text{, except }}0\\\end{aligned}}}$
For an example differentiation: let θ = arsinh x, so (where sinh2 θ = (sinh θ)2):

${\displaystyle {\frac {d\,\operatorname {arsinh} x}{dx}}={\frac {d\theta }{d\sinh \theta }}={\frac {1}{\cosh \theta }}={\frac {1}{\sqrt {1+\sinh ^{2}\theta }}}={\frac {1}{\sqrt {1+x^{2}}}}.}$

£#h5#£Series expansions£#/h5#£
Expansion series can be obtained for the above functions:

${\displaystyle {\begin{aligned}\operatorname {arsinh} x&=x-\left({\frac {1}{2}}\right){\frac {x^{3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{5}}{5}}-\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{7}}{7}}\pm \cdots \\&=\sum _{n=0}^{\infty }\left({\frac {(-1)^{n}(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{2n+1}}{2n+1}},\qquad \left|x\right|<1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcosh} x&=\ln(2x)-\left(\left({\frac {1}{2}}\right){\frac {x^{-2}}{2}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{-4}}{4}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{-6}}{6}}+\cdots \right)\\&=\ln(2x)-\sum _{n=1}^{\infty }\left({\frac {(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{-2n}}{2n}},\qquad \left|x\right|>1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {artanh} x&=x+{\frac {x^{3}}{3}}+{\frac {x^{5}}{5}}+{\frac {x^{7}}{7}}+\cdots \\&=\sum _{n=0}^{\infty }{\frac {x^{2n+1}}{2n+1}},\qquad \left|x\right|<1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcsch} x=\operatorname {arsinh} {\frac {1}{x}}&=x^{-1}-\left({\frac {1}{2}}\right){\frac {x^{-3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{-5}}{5}}-\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{-7}}{7}}\pm \cdots \\&=\sum _{n=0}^{\infty }\left({\frac {(-1)^{n}(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{-(2n+1)}}{2n+1}},\qquad \left|x\right|>1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arsech} x=\operatorname {arcosh} {\frac {1}{x}}&=\ln {\frac {2}{x}}-\left(\left({\frac {1}{2}}\right){\frac {x^{2}}{2}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{4}}{4}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{6}}{6}}+\cdots \right)\\&=\ln {\frac {2}{x}}-\sum _{n=1}^{\infty }\left({\frac {(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{2n}}{2n}},\qquad 0<x\leq 1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcoth} x=\operatorname {artanh} {\frac {1}{x}}&=x^{-1}+{\frac {x^{-3}}{3}}+{\frac {x^{-5}}{5}}+{\frac {x^{-7}}{7}}+\cdots \\&=\sum _{n=0}^{\infty }{\frac {x^{-(2n+1)}}{2n+1}},\qquad \left|x\right|>1\end{aligned}}}$
Asymptotic expansion for the arsinh x is given by

${\displaystyle \operatorname {arsinh} x=\ln(2x)+\sum \limits _{n=1}^{\infty }{\left({-1}\right)^{n-1}{\frac {\left({2n-1}\right)!!}{2n\left({2n}\right)!!}}}{\frac {1}{x^{2n}}}}$



£#h5#£Principal values in the complex plane£#/h5#£
As functions of a complex variable, inverse hyperbolic functions are multivalued functions that are analytic, except at a finite number of points. For such a function, it is common to define a principal value, which is a single valued analytic function which coincides with one specific branch of the multivalued function, over a domain consisting of the complex plane in which a finite number of arcs (usually half lines or line segments) have been removed. These arcs are called branch cuts. For specifying the branch, that is, defining which value of the multivalued function is considered at each point, one generally define it at a particular point, and deduce the value everywhere in the domain of definition of the principal value by analytic continuation. When possible, it is better to define the principal value directly—without referring to analytic continuation.

For example, for the square root, the principal value is defined as the square root that has a positive real part. This defines a single valued analytic function, which is defined everywhere, except for non-positive real values of the variables (where the two square roots have a zero real part). This principal value of the square root function is denoted ${\displaystyle {\sqrt {x}}}$ in what follows. Similarly, the principal value of the logarithm, denoted ${\displaystyle \operatorname {Log} }$ in what follows, is defined as the value for which the imaginary part has the smallest absolute value. It is defined everywhere except for non-positive real values of the variable, for which two different values of the logarithm reach the minimum.

For all inverse hyperbolic functions, the principal value may be defined in terms of principal values of the square root and the logarithm function. However, in some cases, the formulas of § Definitions in terms of logarithms do not give a correct principal value, as giving a domain of definition which is too small and, in one case non-connected.


£#h5#£Principal value of the inverse hyperbolic sine£#/h5#£
The principal value of the inverse hyperbolic sine is given by

${\displaystyle \operatorname {arsinh} z=\operatorname {Log} (z+{\sqrt {z^{2}+1}}\,)\,.}$
The argument of the square root is a non-positive real number, if and only if z belongs to one of the intervals [i, +i∞) and (−i∞, −i] of the imaginary axis. If the argument of the logarithm is real, then it is positive. Thus this formula defines a principal value for arsinh, with branch cuts [i, +i∞) and (−i∞, −i]. This is optimal, as the branch cuts must connect the singular points i and −i to the infinity.


£#h5#£Principal value of the inverse hyperbolic cosine£#/h5#£
The formula for the inverse hyperbolic cosine given in § Inverse hyperbolic cosine is not convenient, since similar to the principal values of the logarithm and the square root, the principal value of arcosh would not be defined for imaginary z. Thus the square root has to be factorized, leading to

${\displaystyle \operatorname {arcosh} z=\operatorname {Log} (z+{\sqrt {z+1}}{\sqrt {z-1}}\,)\,.}$
The principal values of the square roots are both defined, except if z belongs to the real interval (−∞, 1]. If the argument of the logarithm is real, then z is real and has the same sign. Thus, the above formula defines a principal value of arcosh outside the real interval (−∞, 1], which is thus the unique branch cut.


£#h5#£Principal values of the inverse hyperbolic tangent and cotangent£#/h5#£
The formulas given in § Definitions in terms of logarithms suggests

${\displaystyle {\begin{aligned}\operatorname {artanh} z&={\frac {1}{2}}\operatorname {Log} \left({\frac {1+z}{1-z}}\right)\\\operatorname {arcoth} z&={\frac {1}{2}}\operatorname {Log} \left({\frac {z+1}{z-1}}\right)\end{aligned}}}$
for the definition of the principal values of the inverse hyperbolic tangent and cotangent. In these formulas, the argument of the logarithm is real if and only if z is real. For artanh, this argument is in the real interval (−∞, 0], if z belongs either to (−∞, −1] or to [1, ∞). For arcoth, the argument of the logarithm is in (−∞, 0], if and only if z belongs to the real interval [−1, 1].

Therefore, these formulas define convenient principal values, for which the branch cuts are (−∞, −1] and [1, ∞) for the inverse hyperbolic tangent, and [−1, 1] for the inverse hyperbolic cotangent.

In view of a better numerical evaluation near the branch cuts, some authors use the following definitions of the principal values, although the second one introduces a removable singularity at z = 0. The two definitions of ${\displaystyle \operatorname {artanh} }$ differ for real values of ${\displaystyle z}$ with ${\displaystyle z>1}$ . The ones of ${\displaystyle \operatorname {arcoth} }$ differ for real values of ${\displaystyle z}$ with ${\displaystyle z\in [0,1)}$ .

${\displaystyle {\begin{aligned}\operatorname {artanh} z&={\tfrac {1}{2}}\operatorname {Log} \left({1+z}\right)-{\tfrac {1}{2}}\operatorname {Log} \left({1-z}\right)\\\operatorname {arcoth} z&={\tfrac {1}{2}}\operatorname {Log} \left({1+{\frac {1}{z}}}\right)-{\tfrac {1}{2}}\operatorname {Log} \left({1-{\frac {1}{z}}}\right)\end{aligned}}}$

£#h5#£Principal value of the inverse hyperbolic cosecant£#/h5#£
For the inverse hyperbolic cosecant, the principal value is defined as

${\displaystyle \operatorname {arcsch} z=\operatorname {Log} \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z^{2}}}+1}}\,\right)}$ .
It is defined when the arguments of the logarithm and the square root are not non-positive real numbers. The principal value of the square root is thus defined outside the interval [−i, i] of the imaginary line. If the argument of the logarithm is real, then z is a non-zero real number, and this implies that the argument of the logarithm is positive.

Thus, the principal value is defined by the above formula outside the branch cut, consisting of the interval [−i, i] of the imaginary line.

For z = 0, there is a singular point that is included in the branch cut.


£#h5#£Principal value of the inverse hyperbolic secant£#/h5#£
Here, as in the case of the inverse hyperbolic cosine, we have to factorize the square root. This gives the principal value

${\displaystyle \operatorname {arsech} z=\operatorname {Log} \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z}}+1}}\,{\sqrt {{\frac {1}{z}}-1}}\right).}$
If the argument of a square root is real, then z is real, and it follows that both principal values of square roots are defined, except if z is real and belongs to one of the intervals (−∞, 0] and [1, +∞). If the argument of the logarithm is real and negative, then z is also real and negative. It follows that the principal value of arsech is well defined, by the above formula outside two branch cuts, the real intervals (−∞, 0] and [1, +∞).

For z = 0, there is a singular point that is included in one of the branch cuts.


£#h5#£Graphical representation£#/h5#£
In the following graphical representation of the principal values of the inverse hyperbolic functions, the branch cuts appear as discontinuities of the color. The fact that the whole branch cuts appear as discontinuities, shows that these principal values may not be extended into analytic functions defined over larger domains. In other words, the above defined branch cuts are minimal.


£#h5#£See also£#/h5#£ £#ul#££#li#£Complex logarithm£#/li#£ £#li#£Hyperbolic secant distribution£#/li#£ £#li#£ISO 80000-2£#/li#£ £#li#£List of integrals of inverse hyperbolic functions£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£Bibliography£#/h5#£ £#ul#££#li#£Herbert Busemann and Paul J. Kelly (1953) Projective Geometry and Projective Metrics, page 207, Academic Press.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Inverse hyperbolic functions", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Hyperbolic Functions £#/li#££#/ul#£




£#h3#£Area Hyperbolic Cotangent£#/h3#£


In mathematics, the inverse hyperbolic functions are the inverse functions of the hyperbolic functions.

For a given value of a hyperbolic function, the corresponding inverse hyperbolic function provides the corresponding hyperbolic angle. The size of the hyperbolic angle is equal to the area of the corresponding hyperbolic sector of the hyperbola xy = 1, or twice the area of the corresponding sector of the unit hyperbola x2 − y2 = 1, just as a circular angle is twice the area of the circular sector of the unit circle. Some authors have called inverse hyperbolic functions "area functions" to realize the hyperbolic angles.

Hyperbolic functions occur in the calculations of angles and distances in hyperbolic geometry. It also occurs in the solutions of many linear differential equations (such as the equation defining a catenary), cubic equations, and Laplace's equation in Cartesian coordinates. Laplace's equations are important in many areas of physics, including electromagnetic theory, heat transfer, fluid dynamics, and special relativity.


£#h5#£Notation£#/h5#£
The ISO 80000-2 standard abbreviations consist of ar- followed by the abbreviation of the corresponding hyperbolic function (e.g., arsinh, arcosh). The prefix arc- followed by the corresponding hyperbolic function (e.g., arcsinh, arccosh) is also commonly seen, by analogy with the nomenclature for inverse trigonometric functions. These are misnomers, since the prefix arc is the abbreviation for arcus, while the prefix ar stands for area; the hyperbolic functions are not directly related to arcs.

Other authors prefer to use the notation argsinh, argcosh, argtanh, and so on, where the prefix arg is the abbreviation of the Latin argumentum. In computer science, this is often shortened to asinh.

The notation sinh−1(x), cosh−1(x), etc., is also used, despite the fact that care must be taken to avoid misinterpretations of the superscript −1 as a power, as opposed to a shorthand to denote the inverse function (e.g., cosh−1(x) versus cosh(x)−1).


£#h5#£Definitions in terms of logarithms£#/h5#£
Since the hyperbolic functions are rational functions of ex whose numerator and denominator are of degree at most two, these functions may be solved in terms of ex, by using the quadratic formula; then, taking the natural logarithm gives the following expressions for the inverse hyperbolic functions.

For complex arguments, the inverse hyperbolic functions, the square root and the logarithm are multi-valued functions, and the equalities of the next subsections may be viewed as equalities of multi-valued functions.

For all inverse hyperbolic functions (save the inverse hyperbolic cotangent and the inverse hyperbolic cosecant), the domain of the real function is connected.


£#h5#£Inverse hyperbolic sine£#/h5#£
Inverse hyperbolic sine (a.k.a. area hyperbolic sine) (Latin: Area sinus hyperbolicus):

${\displaystyle \operatorname {arsinh} x=\ln \left(x+{\sqrt {x^{2}+1}}\right)}$
The domain is the whole real line.


£#h5#£Inverse hyperbolic cosine£#/h5#£
Inverse hyperbolic cosine (a.k.a. area hyperbolic cosine) (Latin: Area cosinus hyperbolicus):

${\displaystyle \operatorname {arcosh} x=\ln \left(x+{\sqrt {x^{2}-1}}\right)}$
The domain is the closed interval [1, +∞ ).


£#h5#£Inverse hyperbolic tangent£#/h5#£
Inverse hyperbolic tangent (a.k.a. area hyperbolic tangent) (Latin: Area tangens hyperbolicus):

${\displaystyle \operatorname {artanh} x={\frac {1}{2}}\ln \left({\frac {1+x}{1-x}}\right)}$
The domain is the open interval (−1, 1).


£#h5#£Inverse hyperbolic cotangent£#/h5#£
Inverse hyperbolic cotangent (a.k.a., area hyperbolic cotangent) (Latin: Area cotangens hyperbolicus):

${\displaystyle \operatorname {arcoth} x={\frac {1}{2}}\ln \left({\frac {x+1}{x-1}}\right)}$
The domain is the union of the open intervals (−∞, −1) and (1, +∞).


£#h5#£Inverse hyperbolic secant£#/h5#£
Inverse hyperbolic secant (a.k.a., area hyperbolic secant) (Latin: Area secans hyperbolicus):

${\displaystyle \operatorname {arsech} x=\ln \left({\frac {1}{x}}+{\sqrt {{\frac {1}{x^{2}}}-1}}\right)=\ln \left({\frac {1+{\sqrt {1-x^{2}}}}{x}}\right)}$
The domain is the semi-open interval (0, 1].


£#h5#£Inverse hyperbolic cosecant£#/h5#£
Inverse hyperbolic cosecant (a.k.a., area hyperbolic cosecant) (Latin: Area cosecans hyperbolicus):

${\displaystyle \operatorname {arcsch} x=\ln \left({\frac {1}{x}}+{\sqrt {{\frac {1}{x^{2}}}+1}}\right)}$
The domain is the real line with 0 removed.


£#h5#£Addition formulae£#/h5#£
${\displaystyle \operatorname {arsinh} u\pm \operatorname {arsinh} v=\operatorname {arsinh} \left(u{\sqrt {1+v^{2}}}\pm v{\sqrt {1+u^{2}}}\right)}$
${\displaystyle \operatorname {arcosh} u\pm \operatorname {arcosh} v=\operatorname {arcosh} \left(uv\pm {\sqrt {(u^{2}-1)(v^{2}-1)}}\right)}$
${\displaystyle \operatorname {artanh} u\pm \operatorname {artanh} v=\operatorname {artanh} \left({\frac {u\pm v}{1\pm uv}}\right)}$
${\displaystyle \operatorname {arcoth} u\pm \operatorname {arcoth} v=\operatorname {arcoth} \left({\frac {1\pm uv}{u\pm v}}\right)}$
${\displaystyle {\begin{aligned}\operatorname {arsinh} u+\operatorname {arcosh} v&=\operatorname {arsinh} \left(uv+{\sqrt {(1+u^{2})(v^{2}-1)}}\right)\\&=\operatorname {arcosh} \left(v{\sqrt {1+u^{2}}}+u{\sqrt {v^{2}-1}}\right)\end{aligned}}}$

£#h5#£Other identities£#/h5#£
${\displaystyle {\begin{aligned}2\operatorname {arcosh} x&=\operatorname {arcosh} (2x^{2}-1)&\quad {\hbox{ for }}x\geq 1\\4\operatorname {arcosh} x&=\operatorname {arcosh} (8x^{4}-8x^{2}+1)&\quad {\hbox{ for }}x\geq 1\\2\operatorname {arsinh} x&=\operatorname {arcosh} (2x^{2}+1)&\quad {\hbox{ for }}x\geq 0\\4\operatorname {arsinh} x&=\operatorname {arcosh} (8x^{4}+8x^{2}+1)&\quad {\hbox{ for }}x\geq 0\end{aligned}}}$
${\displaystyle \ln(x)=\operatorname {arcosh} \left({\frac {x^{2}+1}{2x}}\right)=\operatorname {arsinh} \left({\frac {x^{2}-1}{2x}}\right)=\operatorname {artanh} \left({\frac {x^{2}-1}{x^{2}+1}}\right)}$

£#h5#£Composition of hyperbolic and inverse hyperbolic functions£#/h5#£
${\displaystyle {\begin{aligned}&\sinh(\operatorname {arcosh} x)={\sqrt {x^{2}-1}}\quad {\text{for}}\quad |x|>1\\&\sinh(\operatorname {artanh} x)={\frac {x}{\sqrt {1-x^{2}}}}\quad {\text{for}}\quad -1<x<1\\&\cosh(\operatorname {arsinh} x)={\sqrt {1+x^{2}}}\\&\cosh(\operatorname {artanh} x)={\frac {1}{\sqrt {1-x^{2}}}}\quad {\text{for}}\quad -1<x<1\\&\tanh(\operatorname {arsinh} x)={\frac {x}{\sqrt {1+x^{2}}}}\\&\tanh(\operatorname {arcosh} x)={\frac {\sqrt {x^{2}-1}}{x}}\quad {\text{for}}\quad |x|>1\end{aligned}}}$

£#h5#£Composition of inverse hyperbolic and trigonometric functions£#/h5#£
${\displaystyle \operatorname {arsinh} \left(\tan \alpha \right)=\operatorname {artanh} \left(\sin \alpha \right)=\ln \left({\frac {1+\sin \alpha }{\cos \alpha }}\right)=\pm \operatorname {arcosh} \left({\frac {1}{\cos \alpha }}\right)}$
${\displaystyle \ln \left(\left|\tan \alpha \right|\right)=-\operatorname {artanh} \left(\cos 2\alpha \right)}$

£#h5#£Conversions£#/h5#£
${\displaystyle \ln x=\operatorname {artanh} \left({\frac {x^{2}-1}{x^{2}+1}}\right)=\operatorname {arsinh} \left({\frac {x^{2}-1}{2x}}\right)=\pm \operatorname {arcosh} \left({\frac {x^{2}+1}{2x}}\right)}$
${\displaystyle \operatorname {artanh} x=\operatorname {arsinh} \left({\frac {x}{\sqrt {1-x^{2}}}}\right)=\pm \operatorname {arcosh} \left({\frac {1}{\sqrt {1-x^{2}}}}\right)}$
${\displaystyle \operatorname {arsinh} x=\operatorname {artanh} \left({\frac {x}{\sqrt {1+x^{2}}}}\right)=\pm \operatorname {arcosh} \left({\sqrt {1+x^{2}}}\right)}$
${\displaystyle \operatorname {arcosh} x=\left|\operatorname {arsinh} \left({\sqrt {x^{2}-1}}\right)\right|=\left|\operatorname {artanh} \left({\frac {\sqrt {x^{2}-1}}{x}}\right)\right|}$

£#h5#£Derivatives£#/h5#£
${\displaystyle {\begin{aligned}{\frac {d}{dx}}\operatorname {arsinh} x&{}={\frac {1}{\sqrt {x^{2}+1}}},{\text{ for all real }}x\\{\frac {d}{dx}}\operatorname {arcosh} x&{}={\frac {1}{\sqrt {x^{2}-1}}},{\text{ for all real }}x>1\\{\frac {d}{dx}}\operatorname {artanh} x&{}={\frac {1}{1-x^{2}}},{\text{ for all real }}|x|<1\\{\frac {d}{dx}}\operatorname {arcoth} x&{}={\frac {1}{1-x^{2}}},{\text{ for all real }}|x|>1\\{\frac {d}{dx}}\operatorname {arsech} x&{}={\frac {-1}{x{\sqrt {1-x^{2}}}}},{\text{ for all real }}x\in (0,1)\\{\frac {d}{dx}}\operatorname {arcsch} x&{}={\frac {-1}{|x|{\sqrt {1+x^{2}}}}},{\text{ for all real }}x{\text{, except }}0\\\end{aligned}}}$
For an example differentiation: let θ = arsinh x, so (where sinh2 θ = (sinh θ)2):

${\displaystyle {\frac {d\,\operatorname {arsinh} x}{dx}}={\frac {d\theta }{d\sinh \theta }}={\frac {1}{\cosh \theta }}={\frac {1}{\sqrt {1+\sinh ^{2}\theta }}}={\frac {1}{\sqrt {1+x^{2}}}}.}$

£#h5#£Series expansions£#/h5#£
Expansion series can be obtained for the above functions:

${\displaystyle {\begin{aligned}\operatorname {arsinh} x&=x-\left({\frac {1}{2}}\right){\frac {x^{3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{5}}{5}}-\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{7}}{7}}\pm \cdots \\&=\sum _{n=0}^{\infty }\left({\frac {(-1)^{n}(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{2n+1}}{2n+1}},\qquad \left|x\right|<1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcosh} x&=\ln(2x)-\left(\left({\frac {1}{2}}\right){\frac {x^{-2}}{2}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{-4}}{4}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{-6}}{6}}+\cdots \right)\\&=\ln(2x)-\sum _{n=1}^{\infty }\left({\frac {(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{-2n}}{2n}},\qquad \left|x\right|>1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {artanh} x&=x+{\frac {x^{3}}{3}}+{\frac {x^{5}}{5}}+{\frac {x^{7}}{7}}+\cdots \\&=\sum _{n=0}^{\infty }{\frac {x^{2n+1}}{2n+1}},\qquad \left|x\right|<1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcsch} x=\operatorname {arsinh} {\frac {1}{x}}&=x^{-1}-\left({\frac {1}{2}}\right){\frac {x^{-3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{-5}}{5}}-\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{-7}}{7}}\pm \cdots \\&=\sum _{n=0}^{\infty }\left({\frac {(-1)^{n}(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{-(2n+1)}}{2n+1}},\qquad \left|x\right|>1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arsech} x=\operatorname {arcosh} {\frac {1}{x}}&=\ln {\frac {2}{x}}-\left(\left({\frac {1}{2}}\right){\frac {x^{2}}{2}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{4}}{4}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{6}}{6}}+\cdots \right)\\&=\ln {\frac {2}{x}}-\sum _{n=1}^{\infty }\left({\frac {(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{2n}}{2n}},\qquad 0<x\leq 1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcoth} x=\operatorname {artanh} {\frac {1}{x}}&=x^{-1}+{\frac {x^{-3}}{3}}+{\frac {x^{-5}}{5}}+{\frac {x^{-7}}{7}}+\cdots \\&=\sum _{n=0}^{\infty }{\frac {x^{-(2n+1)}}{2n+1}},\qquad \left|x\right|>1\end{aligned}}}$
Asymptotic expansion for the arsinh x is given by

${\displaystyle \operatorname {arsinh} x=\ln(2x)+\sum \limits _{n=1}^{\infty }{\left({-1}\right)^{n-1}{\frac {\left({2n-1}\right)!!}{2n\left({2n}\right)!!}}}{\frac {1}{x^{2n}}}}$



£#h5#£Principal values in the complex plane£#/h5#£
As functions of a complex variable, inverse hyperbolic functions are multivalued functions that are analytic, except at a finite number of points. For such a function, it is common to define a principal value, which is a single valued analytic function which coincides with one specific branch of the multivalued function, over a domain consisting of the complex plane in which a finite number of arcs (usually half lines or line segments) have been removed. These arcs are called branch cuts. For specifying the branch, that is, defining which value of the multivalued function is considered at each point, one generally define it at a particular point, and deduce the value everywhere in the domain of definition of the principal value by analytic continuation. When possible, it is better to define the principal value directly—without referring to analytic continuation.

For example, for the square root, the principal value is defined as the square root that has a positive real part. This defines a single valued analytic function, which is defined everywhere, except for non-positive real values of the variables (where the two square roots have a zero real part). This principal value of the square root function is denoted ${\displaystyle {\sqrt {x}}}$ in what follows. Similarly, the principal value of the logarithm, denoted ${\displaystyle \operatorname {Log} }$ in what follows, is defined as the value for which the imaginary part has the smallest absolute value. It is defined everywhere except for non-positive real values of the variable, for which two different values of the logarithm reach the minimum.

For all inverse hyperbolic functions, the principal value may be defined in terms of principal values of the square root and the logarithm function. However, in some cases, the formulas of § Definitions in terms of logarithms do not give a correct principal value, as giving a domain of definition which is too small and, in one case non-connected.


£#h5#£Principal value of the inverse hyperbolic sine£#/h5#£
The principal value of the inverse hyperbolic sine is given by

${\displaystyle \operatorname {arsinh} z=\operatorname {Log} (z+{\sqrt {z^{2}+1}}\,)\,.}$
The argument of the square root is a non-positive real number, if and only if z belongs to one of the intervals [i, +i∞) and (−i∞, −i] of the imaginary axis. If the argument of the logarithm is real, then it is positive. Thus this formula defines a principal value for arsinh, with branch cuts [i, +i∞) and (−i∞, −i]. This is optimal, as the branch cuts must connect the singular points i and −i to the infinity.


£#h5#£Principal value of the inverse hyperbolic cosine£#/h5#£
The formula for the inverse hyperbolic cosine given in § Inverse hyperbolic cosine is not convenient, since similar to the principal values of the logarithm and the square root, the principal value of arcosh would not be defined for imaginary z. Thus the square root has to be factorized, leading to

${\displaystyle \operatorname {arcosh} z=\operatorname {Log} (z+{\sqrt {z+1}}{\sqrt {z-1}}\,)\,.}$
The principal values of the square roots are both defined, except if z belongs to the real interval (−∞, 1]. If the argument of the logarithm is real, then z is real and has the same sign. Thus, the above formula defines a principal value of arcosh outside the real interval (−∞, 1], which is thus the unique branch cut.


£#h5#£Principal values of the inverse hyperbolic tangent and cotangent£#/h5#£
The formulas given in § Definitions in terms of logarithms suggests

${\displaystyle {\begin{aligned}\operatorname {artanh} z&={\frac {1}{2}}\operatorname {Log} \left({\frac {1+z}{1-z}}\right)\\\operatorname {arcoth} z&={\frac {1}{2}}\operatorname {Log} \left({\frac {z+1}{z-1}}\right)\end{aligned}}}$
for the definition of the principal values of the inverse hyperbolic tangent and cotangent. In these formulas, the argument of the logarithm is real if and only if z is real. For artanh, this argument is in the real interval (−∞, 0], if z belongs either to (−∞, −1] or to [1, ∞). For arcoth, the argument of the logarithm is in (−∞, 0], if and only if z belongs to the real interval [−1, 1].

Therefore, these formulas define convenient principal values, for which the branch cuts are (−∞, −1] and [1, ∞) for the inverse hyperbolic tangent, and [−1, 1] for the inverse hyperbolic cotangent.

In view of a better numerical evaluation near the branch cuts, some authors use the following definitions of the principal values, although the second one introduces a removable singularity at z = 0. The two definitions of ${\displaystyle \operatorname {artanh} }$ differ for real values of ${\displaystyle z}$ with ${\displaystyle z>1}$ . The ones of ${\displaystyle \operatorname {arcoth} }$ differ for real values of ${\displaystyle z}$ with ${\displaystyle z\in [0,1)}$ .

${\displaystyle {\begin{aligned}\operatorname {artanh} z&={\tfrac {1}{2}}\operatorname {Log} \left({1+z}\right)-{\tfrac {1}{2}}\operatorname {Log} \left({1-z}\right)\\\operatorname {arcoth} z&={\tfrac {1}{2}}\operatorname {Log} \left({1+{\frac {1}{z}}}\right)-{\tfrac {1}{2}}\operatorname {Log} \left({1-{\frac {1}{z}}}\right)\end{aligned}}}$

£#h5#£Principal value of the inverse hyperbolic cosecant£#/h5#£
For the inverse hyperbolic cosecant, the principal value is defined as

${\displaystyle \operatorname {arcsch} z=\operatorname {Log} \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z^{2}}}+1}}\,\right)}$ .
It is defined when the arguments of the logarithm and the square root are not non-positive real numbers. The principal value of the square root is thus defined outside the interval [−i, i] of the imaginary line. If the argument of the logarithm is real, then z is a non-zero real number, and this implies that the argument of the logarithm is positive.

Thus, the principal value is defined by the above formula outside the branch cut, consisting of the interval [−i, i] of the imaginary line.

For z = 0, there is a singular point that is included in the branch cut.


£#h5#£Principal value of the inverse hyperbolic secant£#/h5#£
Here, as in the case of the inverse hyperbolic cosine, we have to factorize the square root. This gives the principal value

${\displaystyle \operatorname {arsech} z=\operatorname {Log} \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z}}+1}}\,{\sqrt {{\frac {1}{z}}-1}}\right).}$
If the argument of a square root is real, then z is real, and it follows that both principal values of square roots are defined, except if z is real and belongs to one of the intervals (−∞, 0] and [1, +∞). If the argument of the logarithm is real and negative, then z is also real and negative. It follows that the principal value of arsech is well defined, by the above formula outside two branch cuts, the real intervals (−∞, 0] and [1, +∞).

For z = 0, there is a singular point that is included in one of the branch cuts.


£#h5#£Graphical representation£#/h5#£
In the following graphical representation of the principal values of the inverse hyperbolic functions, the branch cuts appear as discontinuities of the color. The fact that the whole branch cuts appear as discontinuities, shows that these principal values may not be extended into analytic functions defined over larger domains. In other words, the above defined branch cuts are minimal.


£#h5#£See also£#/h5#£ £#ul#££#li#£Complex logarithm£#/li#£ £#li#£Hyperbolic secant distribution£#/li#£ £#li#£ISO 80000-2£#/li#£ £#li#£List of integrals of inverse hyperbolic functions£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£Bibliography£#/h5#£ £#ul#££#li#£Herbert Busemann and Paul J. Kelly (1953) Projective Geometry and Projective Metrics, page 207, Academic Press.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Inverse hyperbolic functions", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Hyperbolic Functions £#/li#££#/ul#£




£#h3#£Area Hyperbolic Functions£#/h3#£


In mathematics, the inverse hyperbolic functions are the inverse functions of the hyperbolic functions.

For a given value of a hyperbolic function, the corresponding inverse hyperbolic function provides the corresponding hyperbolic angle. The size of the hyperbolic angle is equal to the area of the corresponding hyperbolic sector of the hyperbola xy = 1, or twice the area of the corresponding sector of the unit hyperbola x2 − y2 = 1, just as a circular angle is twice the area of the circular sector of the unit circle. Some authors have called inverse hyperbolic functions "area functions" to realize the hyperbolic angles.

Hyperbolic functions occur in the calculations of angles and distances in hyperbolic geometry. It also occurs in the solutions of many linear differential equations (such as the equation defining a catenary), cubic equations, and Laplace's equation in Cartesian coordinates. Laplace's equations are important in many areas of physics, including electromagnetic theory, heat transfer, fluid dynamics, and special relativity.


£#h5#£Notation£#/h5#£
The ISO 80000-2 standard abbreviations consist of ar- followed by the abbreviation of the corresponding hyperbolic function (e.g., arsinh, arcosh). The prefix arc- followed by the corresponding hyperbolic function (e.g., arcsinh, arccosh) is also commonly seen, by analogy with the nomenclature for inverse trigonometric functions. These are misnomers, since the prefix arc is the abbreviation for arcus, while the prefix ar stands for area; the hyperbolic functions are not directly related to arcs.

Other authors prefer to use the notation argsinh, argcosh, argtanh, and so on, where the prefix arg is the abbreviation of the Latin argumentum. In computer science, this is often shortened to asinh.

The notation sinh−1(x), cosh−1(x), etc., is also used, despite the fact that care must be taken to avoid misinterpretations of the superscript −1 as a power, as opposed to a shorthand to denote the inverse function (e.g., cosh−1(x) versus cosh(x)−1).


£#h5#£Definitions in terms of logarithms£#/h5#£
Since the hyperbolic functions are rational functions of ex whose numerator and denominator are of degree at most two, these functions may be solved in terms of ex, by using the quadratic formula; then, taking the natural logarithm gives the following expressions for the inverse hyperbolic functions.

For complex arguments, the inverse hyperbolic functions, the square root and the logarithm are multi-valued functions, and the equalities of the next subsections may be viewed as equalities of multi-valued functions.

For all inverse hyperbolic functions (save the inverse hyperbolic cotangent and the inverse hyperbolic cosecant), the domain of the real function is connected.


£#h5#£Inverse hyperbolic sine£#/h5#£
Inverse hyperbolic sine (a.k.a. area hyperbolic sine) (Latin: Area sinus hyperbolicus):

${\displaystyle \operatorname {arsinh} x=\ln \left(x+{\sqrt {x^{2}+1}}\right)}$
The domain is the whole real line.


£#h5#£Inverse hyperbolic cosine£#/h5#£
Inverse hyperbolic cosine (a.k.a. area hyperbolic cosine) (Latin: Area cosinus hyperbolicus):

${\displaystyle \operatorname {arcosh} x=\ln \left(x+{\sqrt {x^{2}-1}}\right)}$
The domain is the closed interval [1, +∞ ).


£#h5#£Inverse hyperbolic tangent£#/h5#£
Inverse hyperbolic tangent (a.k.a. area hyperbolic tangent) (Latin: Area tangens hyperbolicus):

${\displaystyle \operatorname {artanh} x={\frac {1}{2}}\ln \left({\frac {1+x}{1-x}}\right)}$
The domain is the open interval (−1, 1).


£#h5#£Inverse hyperbolic cotangent£#/h5#£
Inverse hyperbolic cotangent (a.k.a., area hyperbolic cotangent) (Latin: Area cotangens hyperbolicus):

${\displaystyle \operatorname {arcoth} x={\frac {1}{2}}\ln \left({\frac {x+1}{x-1}}\right)}$
The domain is the union of the open intervals (−∞, −1) and (1, +∞).


£#h5#£Inverse hyperbolic secant£#/h5#£
Inverse hyperbolic secant (a.k.a., area hyperbolic secant) (Latin: Area secans hyperbolicus):

${\displaystyle \operatorname {arsech} x=\ln \left({\frac {1}{x}}+{\sqrt {{\frac {1}{x^{2}}}-1}}\right)=\ln \left({\frac {1+{\sqrt {1-x^{2}}}}{x}}\right)}$
The domain is the semi-open interval (0, 1].


£#h5#£Inverse hyperbolic cosecant£#/h5#£
Inverse hyperbolic cosecant (a.k.a., area hyperbolic cosecant) (Latin: Area cosecans hyperbolicus):

${\displaystyle \operatorname {arcsch} x=\ln \left({\frac {1}{x}}+{\sqrt {{\frac {1}{x^{2}}}+1}}\right)}$
The domain is the real line with 0 removed.


£#h5#£Addition formulae£#/h5#£
${\displaystyle \operatorname {arsinh} u\pm \operatorname {arsinh} v=\operatorname {arsinh} \left(u{\sqrt {1+v^{2}}}\pm v{\sqrt {1+u^{2}}}\right)}$
${\displaystyle \operatorname {arcosh} u\pm \operatorname {arcosh} v=\operatorname {arcosh} \left(uv\pm {\sqrt {(u^{2}-1)(v^{2}-1)}}\right)}$
${\displaystyle \operatorname {artanh} u\pm \operatorname {artanh} v=\operatorname {artanh} \left({\frac {u\pm v}{1\pm uv}}\right)}$
${\displaystyle \operatorname {arcoth} u\pm \operatorname {arcoth} v=\operatorname {arcoth} \left({\frac {1\pm uv}{u\pm v}}\right)}$
${\displaystyle {\begin{aligned}\operatorname {arsinh} u+\operatorname {arcosh} v&=\operatorname {arsinh} \left(uv+{\sqrt {(1+u^{2})(v^{2}-1)}}\right)\\&=\operatorname {arcosh} \left(v{\sqrt {1+u^{2}}}+u{\sqrt {v^{2}-1}}\right)\end{aligned}}}$

£#h5#£Other identities£#/h5#£
${\displaystyle {\begin{aligned}2\operatorname {arcosh} x&=\operatorname {arcosh} (2x^{2}-1)&\quad {\hbox{ for }}x\geq 1\\4\operatorname {arcosh} x&=\operatorname {arcosh} (8x^{4}-8x^{2}+1)&\quad {\hbox{ for }}x\geq 1\\2\operatorname {arsinh} x&=\operatorname {arcosh} (2x^{2}+1)&\quad {\hbox{ for }}x\geq 0\\4\operatorname {arsinh} x&=\operatorname {arcosh} (8x^{4}+8x^{2}+1)&\quad {\hbox{ for }}x\geq 0\end{aligned}}}$
${\displaystyle \ln(x)=\operatorname {arcosh} \left({\frac {x^{2}+1}{2x}}\right)=\operatorname {arsinh} \left({\frac {x^{2}-1}{2x}}\right)=\operatorname {artanh} \left({\frac {x^{2}-1}{x^{2}+1}}\right)}$

£#h5#£Composition of hyperbolic and inverse hyperbolic functions£#/h5#£
${\displaystyle {\begin{aligned}&\sinh(\operatorname {arcosh} x)={\sqrt {x^{2}-1}}\quad {\text{for}}\quad |x|>1\\&\sinh(\operatorname {artanh} x)={\frac {x}{\sqrt {1-x^{2}}}}\quad {\text{for}}\quad -1<x<1\\&\cosh(\operatorname {arsinh} x)={\sqrt {1+x^{2}}}\\&\cosh(\operatorname {artanh} x)={\frac {1}{\sqrt {1-x^{2}}}}\quad {\text{for}}\quad -1<x<1\\&\tanh(\operatorname {arsinh} x)={\frac {x}{\sqrt {1+x^{2}}}}\\&\tanh(\operatorname {arcosh} x)={\frac {\sqrt {x^{2}-1}}{x}}\quad {\text{for}}\quad |x|>1\end{aligned}}}$

£#h5#£Composition of inverse hyperbolic and trigonometric functions£#/h5#£
${\displaystyle \operatorname {arsinh} \left(\tan \alpha \right)=\operatorname {artanh} \left(\sin \alpha \right)=\ln \left({\frac {1+\sin \alpha }{\cos \alpha }}\right)=\pm \operatorname {arcosh} \left({\frac {1}{\cos \alpha }}\right)}$
${\displaystyle \ln \left(\left|\tan \alpha \right|\right)=-\operatorname {artanh} \left(\cos 2\alpha \right)}$

£#h5#£Conversions£#/h5#£
${\displaystyle \ln x=\operatorname {artanh} \left({\frac {x^{2}-1}{x^{2}+1}}\right)=\operatorname {arsinh} \left({\frac {x^{2}-1}{2x}}\right)=\pm \operatorname {arcosh} \left({\frac {x^{2}+1}{2x}}\right)}$
${\displaystyle \operatorname {artanh} x=\operatorname {arsinh} \left({\frac {x}{\sqrt {1-x^{2}}}}\right)=\pm \operatorname {arcosh} \left({\frac {1}{\sqrt {1-x^{2}}}}\right)}$
${\displaystyle \operatorname {arsinh} x=\operatorname {artanh} \left({\frac {x}{\sqrt {1+x^{2}}}}\right)=\pm \operatorname {arcosh} \left({\sqrt {1+x^{2}}}\right)}$
${\displaystyle \operatorname {arcosh} x=\left|\operatorname {arsinh} \left({\sqrt {x^{2}-1}}\right)\right|=\left|\operatorname {artanh} \left({\frac {\sqrt {x^{2}-1}}{x}}\right)\right|}$

£#h5#£Derivatives£#/h5#£
${\displaystyle {\begin{aligned}{\frac {d}{dx}}\operatorname {arsinh} x&{}={\frac {1}{\sqrt {x^{2}+1}}},{\text{ for all real }}x\\{\frac {d}{dx}}\operatorname {arcosh} x&{}={\frac {1}{\sqrt {x^{2}-1}}},{\text{ for all real }}x>1\\{\frac {d}{dx}}\operatorname {artanh} x&{}={\frac {1}{1-x^{2}}},{\text{ for all real }}|x|<1\\{\frac {d}{dx}}\operatorname {arcoth} x&{}={\frac {1}{1-x^{2}}},{\text{ for all real }}|x|>1\\{\frac {d}{dx}}\operatorname {arsech} x&{}={\frac {-1}{x{\sqrt {1-x^{2}}}}},{\text{ for all real }}x\in (0,1)\\{\frac {d}{dx}}\operatorname {arcsch} x&{}={\frac {-1}{|x|{\sqrt {1+x^{2}}}}},{\text{ for all real }}x{\text{, except }}0\\\end{aligned}}}$
For an example differentiation: let θ = arsinh x, so (where sinh2 θ = (sinh θ)2):

${\displaystyle {\frac {d\,\operatorname {arsinh} x}{dx}}={\frac {d\theta }{d\sinh \theta }}={\frac {1}{\cosh \theta }}={\frac {1}{\sqrt {1+\sinh ^{2}\theta }}}={\frac {1}{\sqrt {1+x^{2}}}}.}$

£#h5#£Series expansions£#/h5#£
Expansion series can be obtained for the above functions:

${\displaystyle {\begin{aligned}\operatorname {arsinh} x&=x-\left({\frac {1}{2}}\right){\frac {x^{3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{5}}{5}}-\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{7}}{7}}\pm \cdots \\&=\sum _{n=0}^{\infty }\left({\frac {(-1)^{n}(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{2n+1}}{2n+1}},\qquad \left|x\right|<1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcosh} x&=\ln(2x)-\left(\left({\frac {1}{2}}\right){\frac {x^{-2}}{2}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{-4}}{4}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{-6}}{6}}+\cdots \right)\\&=\ln(2x)-\sum _{n=1}^{\infty }\left({\frac {(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{-2n}}{2n}},\qquad \left|x\right|>1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {artanh} x&=x+{\frac {x^{3}}{3}}+{\frac {x^{5}}{5}}+{\frac {x^{7}}{7}}+\cdots \\&=\sum _{n=0}^{\infty }{\frac {x^{2n+1}}{2n+1}},\qquad \left|x\right|<1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcsch} x=\operatorname {arsinh} {\frac {1}{x}}&=x^{-1}-\left({\frac {1}{2}}\right){\frac {x^{-3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{-5}}{5}}-\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{-7}}{7}}\pm \cdots \\&=\sum _{n=0}^{\infty }\left({\frac {(-1)^{n}(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{-(2n+1)}}{2n+1}},\qquad \left|x\right|>1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arsech} x=\operatorname {arcosh} {\frac {1}{x}}&=\ln {\frac {2}{x}}-\left(\left({\frac {1}{2}}\right){\frac {x^{2}}{2}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{4}}{4}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{6}}{6}}+\cdots \right)\\&=\ln {\frac {2}{x}}-\sum _{n=1}^{\infty }\left({\frac {(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{2n}}{2n}},\qquad 0<x\leq 1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcoth} x=\operatorname {artanh} {\frac {1}{x}}&=x^{-1}+{\frac {x^{-3}}{3}}+{\frac {x^{-5}}{5}}+{\frac {x^{-7}}{7}}+\cdots \\&=\sum _{n=0}^{\infty }{\frac {x^{-(2n+1)}}{2n+1}},\qquad \left|x\right|>1\end{aligned}}}$
Asymptotic expansion for the arsinh x is given by

${\displaystyle \operatorname {arsinh} x=\ln(2x)+\sum \limits _{n=1}^{\infty }{\left({-1}\right)^{n-1}{\frac {\left({2n-1}\right)!!}{2n\left({2n}\right)!!}}}{\frac {1}{x^{2n}}}}$



£#h5#£Principal values in the complex plane£#/h5#£
As functions of a complex variable, inverse hyperbolic functions are multivalued functions that are analytic, except at a finite number of points. For such a function, it is common to define a principal value, which is a single valued analytic function which coincides with one specific branch of the multivalued function, over a domain consisting of the complex plane in which a finite number of arcs (usually half lines or line segments) have been removed. These arcs are called branch cuts. For specifying the branch, that is, defining which value of the multivalued function is considered at each point, one generally define it at a particular point, and deduce the value everywhere in the domain of definition of the principal value by analytic continuation. When possible, it is better to define the principal value directly—without referring to analytic continuation.

For example, for the square root, the principal value is defined as the square root that has a positive real part. This defines a single valued analytic function, which is defined everywhere, except for non-positive real values of the variables (where the two square roots have a zero real part). This principal value of the square root function is denoted ${\displaystyle {\sqrt {x}}}$ in what follows. Similarly, the principal value of the logarithm, denoted ${\displaystyle \operatorname {Log} }$ in what follows, is defined as the value for which the imaginary part has the smallest absolute value. It is defined everywhere except for non-positive real values of the variable, for which two different values of the logarithm reach the minimum.

For all inverse hyperbolic functions, the principal value may be defined in terms of principal values of the square root and the logarithm function. However, in some cases, the formulas of § Definitions in terms of logarithms do not give a correct principal value, as giving a domain of definition which is too small and, in one case non-connected.


£#h5#£Principal value of the inverse hyperbolic sine£#/h5#£
The principal value of the inverse hyperbolic sine is given by

${\displaystyle \operatorname {arsinh} z=\operatorname {Log} (z+{\sqrt {z^{2}+1}}\,)\,.}$
The argument of the square root is a non-positive real number, if and only if z belongs to one of the intervals [i, +i∞) and (−i∞, −i] of the imaginary axis. If the argument of the logarithm is real, then it is positive. Thus this formula defines a principal value for arsinh, with branch cuts [i, +i∞) and (−i∞, −i]. This is optimal, as the branch cuts must connect the singular points i and −i to the infinity.


£#h5#£Principal value of the inverse hyperbolic cosine£#/h5#£
The formula for the inverse hyperbolic cosine given in § Inverse hyperbolic cosine is not convenient, since similar to the principal values of the logarithm and the square root, the principal value of arcosh would not be defined for imaginary z. Thus the square root has to be factorized, leading to

${\displaystyle \operatorname {arcosh} z=\operatorname {Log} (z+{\sqrt {z+1}}{\sqrt {z-1}}\,)\,.}$
The principal values of the square roots are both defined, except if z belongs to the real interval (−∞, 1]. If the argument of the logarithm is real, then z is real and has the same sign. Thus, the above formula defines a principal value of arcosh outside the real interval (−∞, 1], which is thus the unique branch cut.


£#h5#£Principal values of the inverse hyperbolic tangent and cotangent£#/h5#£
The formulas given in § Definitions in terms of logarithms suggests

${\displaystyle {\begin{aligned}\operatorname {artanh} z&={\frac {1}{2}}\operatorname {Log} \left({\frac {1+z}{1-z}}\right)\\\operatorname {arcoth} z&={\frac {1}{2}}\operatorname {Log} \left({\frac {z+1}{z-1}}\right)\end{aligned}}}$
for the definition of the principal values of the inverse hyperbolic tangent and cotangent. In these formulas, the argument of the logarithm is real if and only if z is real. For artanh, this argument is in the real interval (−∞, 0], if z belongs either to (−∞, −1] or to [1, ∞). For arcoth, the argument of the logarithm is in (−∞, 0], if and only if z belongs to the real interval [−1, 1].

Therefore, these formulas define convenient principal values, for which the branch cuts are (−∞, −1] and [1, ∞) for the inverse hyperbolic tangent, and [−1, 1] for the inverse hyperbolic cotangent.

In view of a better numerical evaluation near the branch cuts, some authors use the following definitions of the principal values, although the second one introduces a removable singularity at z = 0. The two definitions of ${\displaystyle \operatorname {artanh} }$ differ for real values of ${\displaystyle z}$ with ${\displaystyle z>1}$ . The ones of ${\displaystyle \operatorname {arcoth} }$ differ for real values of ${\displaystyle z}$ with ${\displaystyle z\in [0,1)}$ .

${\displaystyle {\begin{aligned}\operatorname {artanh} z&={\tfrac {1}{2}}\operatorname {Log} \left({1+z}\right)-{\tfrac {1}{2}}\operatorname {Log} \left({1-z}\right)\\\operatorname {arcoth} z&={\tfrac {1}{2}}\operatorname {Log} \left({1+{\frac {1}{z}}}\right)-{\tfrac {1}{2}}\operatorname {Log} \left({1-{\frac {1}{z}}}\right)\end{aligned}}}$

£#h5#£Principal value of the inverse hyperbolic cosecant£#/h5#£
For the inverse hyperbolic cosecant, the principal value is defined as

${\displaystyle \operatorname {arcsch} z=\operatorname {Log} \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z^{2}}}+1}}\,\right)}$ .
It is defined when the arguments of the logarithm and the square root are not non-positive real numbers. The principal value of the square root is thus defined outside the interval [−i, i] of the imaginary line. If the argument of the logarithm is real, then z is a non-zero real number, and this implies that the argument of the logarithm is positive.

Thus, the principal value is defined by the above formula outside the branch cut, consisting of the interval [−i, i] of the imaginary line.

For z = 0, there is a singular point that is included in the branch cut.


£#h5#£Principal value of the inverse hyperbolic secant£#/h5#£
Here, as in the case of the inverse hyperbolic cosine, we have to factorize the square root. This gives the principal value

${\displaystyle \operatorname {arsech} z=\operatorname {Log} \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z}}+1}}\,{\sqrt {{\frac {1}{z}}-1}}\right).}$
If the argument of a square root is real, then z is real, and it follows that both principal values of square roots are defined, except if z is real and belongs to one of the intervals (−∞, 0] and [1, +∞). If the argument of the logarithm is real and negative, then z is also real and negative. It follows that the principal value of arsech is well defined, by the above formula outside two branch cuts, the real intervals (−∞, 0] and [1, +∞).

For z = 0, there is a singular point that is included in one of the branch cuts.


£#h5#£Graphical representation£#/h5#£
In the following graphical representation of the principal values of the inverse hyperbolic functions, the branch cuts appear as discontinuities of the color. The fact that the whole branch cuts appear as discontinuities, shows that these principal values may not be extended into analytic functions defined over larger domains. In other words, the above defined branch cuts are minimal.


£#h5#£See also£#/h5#£ £#ul#££#li#£Complex logarithm£#/li#£ £#li#£Hyperbolic secant distribution£#/li#£ £#li#£ISO 80000-2£#/li#£ £#li#£List of integrals of inverse hyperbolic functions£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£Bibliography£#/h5#£ £#ul#££#li#£Herbert Busemann and Paul J. Kelly (1953) Projective Geometry and Projective Metrics, page 207, Academic Press.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Inverse hyperbolic functions", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Hyperbolic Functions £#/li#££#/ul#£




£#h3#£Area Hyperbolic Secant£#/h3#£


In mathematics, the inverse hyperbolic functions are the inverse functions of the hyperbolic functions.

For a given value of a hyperbolic function, the corresponding inverse hyperbolic function provides the corresponding hyperbolic angle. The size of the hyperbolic angle is equal to the area of the corresponding hyperbolic sector of the hyperbola xy = 1, or twice the area of the corresponding sector of the unit hyperbola x2 − y2 = 1, just as a circular angle is twice the area of the circular sector of the unit circle. Some authors have called inverse hyperbolic functions "area functions" to realize the hyperbolic angles.

Hyperbolic functions occur in the calculations of angles and distances in hyperbolic geometry. It also occurs in the solutions of many linear differential equations (such as the equation defining a catenary), cubic equations, and Laplace's equation in Cartesian coordinates. Laplace's equations are important in many areas of physics, including electromagnetic theory, heat transfer, fluid dynamics, and special relativity.


£#h5#£Notation£#/h5#£
The ISO 80000-2 standard abbreviations consist of ar- followed by the abbreviation of the corresponding hyperbolic function (e.g., arsinh, arcosh). The prefix arc- followed by the corresponding hyperbolic function (e.g., arcsinh, arccosh) is also commonly seen, by analogy with the nomenclature for inverse trigonometric functions. These are misnomers, since the prefix arc is the abbreviation for arcus, while the prefix ar stands for area; the hyperbolic functions are not directly related to arcs.

Other authors prefer to use the notation argsinh, argcosh, argtanh, and so on, where the prefix arg is the abbreviation of the Latin argumentum. In computer science, this is often shortened to asinh.

The notation sinh−1(x), cosh−1(x), etc., is also used, despite the fact that care must be taken to avoid misinterpretations of the superscript −1 as a power, as opposed to a shorthand to denote the inverse function (e.g., cosh−1(x) versus cosh(x)−1).


£#h5#£Definitions in terms of logarithms£#/h5#£
Since the hyperbolic functions are rational functions of ex whose numerator and denominator are of degree at most two, these functions may be solved in terms of ex, by using the quadratic formula; then, taking the natural logarithm gives the following expressions for the inverse hyperbolic functions.

For complex arguments, the inverse hyperbolic functions, the square root and the logarithm are multi-valued functions, and the equalities of the next subsections may be viewed as equalities of multi-valued functions.

For all inverse hyperbolic functions (save the inverse hyperbolic cotangent and the inverse hyperbolic cosecant), the domain of the real function is connected.


£#h5#£Inverse hyperbolic sine£#/h5#£
Inverse hyperbolic sine (a.k.a. area hyperbolic sine) (Latin: Area sinus hyperbolicus):

${\displaystyle \operatorname {arsinh} x=\ln \left(x+{\sqrt {x^{2}+1}}\right)}$
The domain is the whole real line.


£#h5#£Inverse hyperbolic cosine£#/h5#£
Inverse hyperbolic cosine (a.k.a. area hyperbolic cosine) (Latin: Area cosinus hyperbolicus):

${\displaystyle \operatorname {arcosh} x=\ln \left(x+{\sqrt {x^{2}-1}}\right)}$
The domain is the closed interval [1, +∞ ).


£#h5#£Inverse hyperbolic tangent£#/h5#£
Inverse hyperbolic tangent (a.k.a. area hyperbolic tangent) (Latin: Area tangens hyperbolicus):

${\displaystyle \operatorname {artanh} x={\frac {1}{2}}\ln \left({\frac {1+x}{1-x}}\right)}$
The domain is the open interval (−1, 1).


£#h5#£Inverse hyperbolic cotangent£#/h5#£
Inverse hyperbolic cotangent (a.k.a., area hyperbolic cotangent) (Latin: Area cotangens hyperbolicus):

${\displaystyle \operatorname {arcoth} x={\frac {1}{2}}\ln \left({\frac {x+1}{x-1}}\right)}$
The domain is the union of the open intervals (−∞, −1) and (1, +∞).


£#h5#£Inverse hyperbolic secant£#/h5#£
Inverse hyperbolic secant (a.k.a., area hyperbolic secant) (Latin: Area secans hyperbolicus):

${\displaystyle \operatorname {arsech} x=\ln \left({\frac {1}{x}}+{\sqrt {{\frac {1}{x^{2}}}-1}}\right)=\ln \left({\frac {1+{\sqrt {1-x^{2}}}}{x}}\right)}$
The domain is the semi-open interval (0, 1].


£#h5#£Inverse hyperbolic cosecant£#/h5#£
Inverse hyperbolic cosecant (a.k.a., area hyperbolic cosecant) (Latin: Area cosecans hyperbolicus):

${\displaystyle \operatorname {arcsch} x=\ln \left({\frac {1}{x}}+{\sqrt {{\frac {1}{x^{2}}}+1}}\right)}$
The domain is the real line with 0 removed.


£#h5#£Addition formulae£#/h5#£
${\displaystyle \operatorname {arsinh} u\pm \operatorname {arsinh} v=\operatorname {arsinh} \left(u{\sqrt {1+v^{2}}}\pm v{\sqrt {1+u^{2}}}\right)}$
${\displaystyle \operatorname {arcosh} u\pm \operatorname {arcosh} v=\operatorname {arcosh} \left(uv\pm {\sqrt {(u^{2}-1)(v^{2}-1)}}\right)}$
${\displaystyle \operatorname {artanh} u\pm \operatorname {artanh} v=\operatorname {artanh} \left({\frac {u\pm v}{1\pm uv}}\right)}$
${\displaystyle \operatorname {arcoth} u\pm \operatorname {arcoth} v=\operatorname {arcoth} \left({\frac {1\pm uv}{u\pm v}}\right)}$
${\displaystyle {\begin{aligned}\operatorname {arsinh} u+\operatorname {arcosh} v&=\operatorname {arsinh} \left(uv+{\sqrt {(1+u^{2})(v^{2}-1)}}\right)\\&=\operatorname {arcosh} \left(v{\sqrt {1+u^{2}}}+u{\sqrt {v^{2}-1}}\right)\end{aligned}}}$

£#h5#£Other identities£#/h5#£
${\displaystyle {\begin{aligned}2\operatorname {arcosh} x&=\operatorname {arcosh} (2x^{2}-1)&\quad {\hbox{ for }}x\geq 1\\4\operatorname {arcosh} x&=\operatorname {arcosh} (8x^{4}-8x^{2}+1)&\quad {\hbox{ for }}x\geq 1\\2\operatorname {arsinh} x&=\operatorname {arcosh} (2x^{2}+1)&\quad {\hbox{ for }}x\geq 0\\4\operatorname {arsinh} x&=\operatorname {arcosh} (8x^{4}+8x^{2}+1)&\quad {\hbox{ for }}x\geq 0\end{aligned}}}$
${\displaystyle \ln(x)=\operatorname {arcosh} \left({\frac {x^{2}+1}{2x}}\right)=\operatorname {arsinh} \left({\frac {x^{2}-1}{2x}}\right)=\operatorname {artanh} \left({\frac {x^{2}-1}{x^{2}+1}}\right)}$

£#h5#£Composition of hyperbolic and inverse hyperbolic functions£#/h5#£
${\displaystyle {\begin{aligned}&\sinh(\operatorname {arcosh} x)={\sqrt {x^{2}-1}}\quad {\text{for}}\quad |x|>1\\&\sinh(\operatorname {artanh} x)={\frac {x}{\sqrt {1-x^{2}}}}\quad {\text{for}}\quad -1<x<1\\&\cosh(\operatorname {arsinh} x)={\sqrt {1+x^{2}}}\\&\cosh(\operatorname {artanh} x)={\frac {1}{\sqrt {1-x^{2}}}}\quad {\text{for}}\quad -1<x<1\\&\tanh(\operatorname {arsinh} x)={\frac {x}{\sqrt {1+x^{2}}}}\\&\tanh(\operatorname {arcosh} x)={\frac {\sqrt {x^{2}-1}}{x}}\quad {\text{for}}\quad |x|>1\end{aligned}}}$

£#h5#£Composition of inverse hyperbolic and trigonometric functions£#/h5#£
${\displaystyle \operatorname {arsinh} \left(\tan \alpha \right)=\operatorname {artanh} \left(\sin \alpha \right)=\ln \left({\frac {1+\sin \alpha }{\cos \alpha }}\right)=\pm \operatorname {arcosh} \left({\frac {1}{\cos \alpha }}\right)}$
${\displaystyle \ln \left(\left|\tan \alpha \right|\right)=-\operatorname {artanh} \left(\cos 2\alpha \right)}$

£#h5#£Conversions£#/h5#£
${\displaystyle \ln x=\operatorname {artanh} \left({\frac {x^{2}-1}{x^{2}+1}}\right)=\operatorname {arsinh} \left({\frac {x^{2}-1}{2x}}\right)=\pm \operatorname {arcosh} \left({\frac {x^{2}+1}{2x}}\right)}$
${\displaystyle \operatorname {artanh} x=\operatorname {arsinh} \left({\frac {x}{\sqrt {1-x^{2}}}}\right)=\pm \operatorname {arcosh} \left({\frac {1}{\sqrt {1-x^{2}}}}\right)}$
${\displaystyle \operatorname {arsinh} x=\operatorname {artanh} \left({\frac {x}{\sqrt {1+x^{2}}}}\right)=\pm \operatorname {arcosh} \left({\sqrt {1+x^{2}}}\right)}$
${\displaystyle \operatorname {arcosh} x=\left|\operatorname {arsinh} \left({\sqrt {x^{2}-1}}\right)\right|=\left|\operatorname {artanh} \left({\frac {\sqrt {x^{2}-1}}{x}}\right)\right|}$

£#h5#£Derivatives£#/h5#£
${\displaystyle {\begin{aligned}{\frac {d}{dx}}\operatorname {arsinh} x&{}={\frac {1}{\sqrt {x^{2}+1}}},{\text{ for all real }}x\\{\frac {d}{dx}}\operatorname {arcosh} x&{}={\frac {1}{\sqrt {x^{2}-1}}},{\text{ for all real }}x>1\\{\frac {d}{dx}}\operatorname {artanh} x&{}={\frac {1}{1-x^{2}}},{\text{ for all real }}|x|<1\\{\frac {d}{dx}}\operatorname {arcoth} x&{}={\frac {1}{1-x^{2}}},{\text{ for all real }}|x|>1\\{\frac {d}{dx}}\operatorname {arsech} x&{}={\frac {-1}{x{\sqrt {1-x^{2}}}}},{\text{ for all real }}x\in (0,1)\\{\frac {d}{dx}}\operatorname {arcsch} x&{}={\frac {-1}{|x|{\sqrt {1+x^{2}}}}},{\text{ for all real }}x{\text{, except }}0\\\end{aligned}}}$
For an example differentiation: let θ = arsinh x, so (where sinh2 θ = (sinh θ)2):

${\displaystyle {\frac {d\,\operatorname {arsinh} x}{dx}}={\frac {d\theta }{d\sinh \theta }}={\frac {1}{\cosh \theta }}={\frac {1}{\sqrt {1+\sinh ^{2}\theta }}}={\frac {1}{\sqrt {1+x^{2}}}}.}$

£#h5#£Series expansions£#/h5#£
Expansion series can be obtained for the above functions:

${\displaystyle {\begin{aligned}\operatorname {arsinh} x&=x-\left({\frac {1}{2}}\right){\frac {x^{3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{5}}{5}}-\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{7}}{7}}\pm \cdots \\&=\sum _{n=0}^{\infty }\left({\frac {(-1)^{n}(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{2n+1}}{2n+1}},\qquad \left|x\right|<1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcosh} x&=\ln(2x)-\left(\left({\frac {1}{2}}\right){\frac {x^{-2}}{2}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{-4}}{4}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{-6}}{6}}+\cdots \right)\\&=\ln(2x)-\sum _{n=1}^{\infty }\left({\frac {(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{-2n}}{2n}},\qquad \left|x\right|>1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {artanh} x&=x+{\frac {x^{3}}{3}}+{\frac {x^{5}}{5}}+{\frac {x^{7}}{7}}+\cdots \\&=\sum _{n=0}^{\infty }{\frac {x^{2n+1}}{2n+1}},\qquad \left|x\right|<1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcsch} x=\operatorname {arsinh} {\frac {1}{x}}&=x^{-1}-\left({\frac {1}{2}}\right){\frac {x^{-3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{-5}}{5}}-\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{-7}}{7}}\pm \cdots \\&=\sum _{n=0}^{\infty }\left({\frac {(-1)^{n}(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{-(2n+1)}}{2n+1}},\qquad \left|x\right|>1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arsech} x=\operatorname {arcosh} {\frac {1}{x}}&=\ln {\frac {2}{x}}-\left(\left({\frac {1}{2}}\right){\frac {x^{2}}{2}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{4}}{4}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{6}}{6}}+\cdots \right)\\&=\ln {\frac {2}{x}}-\sum _{n=1}^{\infty }\left({\frac {(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{2n}}{2n}},\qquad 0<x\leq 1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcoth} x=\operatorname {artanh} {\frac {1}{x}}&=x^{-1}+{\frac {x^{-3}}{3}}+{\frac {x^{-5}}{5}}+{\frac {x^{-7}}{7}}+\cdots \\&=\sum _{n=0}^{\infty }{\frac {x^{-(2n+1)}}{2n+1}},\qquad \left|x\right|>1\end{aligned}}}$
Asymptotic expansion for the arsinh x is given by

${\displaystyle \operatorname {arsinh} x=\ln(2x)+\sum \limits _{n=1}^{\infty }{\left({-1}\right)^{n-1}{\frac {\left({2n-1}\right)!!}{2n\left({2n}\right)!!}}}{\frac {1}{x^{2n}}}}$



£#h5#£Principal values in the complex plane£#/h5#£
As functions of a complex variable, inverse hyperbolic functions are multivalued functions that are analytic, except at a finite number of points. For such a function, it is common to define a principal value, which is a single valued analytic function which coincides with one specific branch of the multivalued function, over a domain consisting of the complex plane in which a finite number of arcs (usually half lines or line segments) have been removed. These arcs are called branch cuts. For specifying the branch, that is, defining which value of the multivalued function is considered at each point, one generally define it at a particular point, and deduce the value everywhere in the domain of definition of the principal value by analytic continuation. When possible, it is better to define the principal value directly—without referring to analytic continuation.

For example, for the square root, the principal value is defined as the square root that has a positive real part. This defines a single valued analytic function, which is defined everywhere, except for non-positive real values of the variables (where the two square roots have a zero real part). This principal value of the square root function is denoted ${\displaystyle {\sqrt {x}}}$ in what follows. Similarly, the principal value of the logarithm, denoted ${\displaystyle \operatorname {Log} }$ in what follows, is defined as the value for which the imaginary part has the smallest absolute value. It is defined everywhere except for non-positive real values of the variable, for which two different values of the logarithm reach the minimum.

For all inverse hyperbolic functions, the principal value may be defined in terms of principal values of the square root and the logarithm function. However, in some cases, the formulas of § Definitions in terms of logarithms do not give a correct principal value, as giving a domain of definition which is too small and, in one case non-connected.


£#h5#£Principal value of the inverse hyperbolic sine£#/h5#£
The principal value of the inverse hyperbolic sine is given by

${\displaystyle \operatorname {arsinh} z=\operatorname {Log} (z+{\sqrt {z^{2}+1}}\,)\,.}$
The argument of the square root is a non-positive real number, if and only if z belongs to one of the intervals [i, +i∞) and (−i∞, −i] of the imaginary axis. If the argument of the logarithm is real, then it is positive. Thus this formula defines a principal value for arsinh, with branch cuts [i, +i∞) and (−i∞, −i]. This is optimal, as the branch cuts must connect the singular points i and −i to the infinity.


£#h5#£Principal value of the inverse hyperbolic cosine£#/h5#£
The formula for the inverse hyperbolic cosine given in § Inverse hyperbolic cosine is not convenient, since similar to the principal values of the logarithm and the square root, the principal value of arcosh would not be defined for imaginary z. Thus the square root has to be factorized, leading to

${\displaystyle \operatorname {arcosh} z=\operatorname {Log} (z+{\sqrt {z+1}}{\sqrt {z-1}}\,)\,.}$
The principal values of the square roots are both defined, except if z belongs to the real interval (−∞, 1]. If the argument of the logarithm is real, then z is real and has the same sign. Thus, the above formula defines a principal value of arcosh outside the real interval (−∞, 1], which is thus the unique branch cut.


£#h5#£Principal values of the inverse hyperbolic tangent and cotangent£#/h5#£
The formulas given in § Definitions in terms of logarithms suggests

${\displaystyle {\begin{aligned}\operatorname {artanh} z&={\frac {1}{2}}\operatorname {Log} \left({\frac {1+z}{1-z}}\right)\\\operatorname {arcoth} z&={\frac {1}{2}}\operatorname {Log} \left({\frac {z+1}{z-1}}\right)\end{aligned}}}$
for the definition of the principal values of the inverse hyperbolic tangent and cotangent. In these formulas, the argument of the logarithm is real if and only if z is real. For artanh, this argument is in the real interval (−∞, 0], if z belongs either to (−∞, −1] or to [1, ∞). For arcoth, the argument of the logarithm is in (−∞, 0], if and only if z belongs to the real interval [−1, 1].

Therefore, these formulas define convenient principal values, for which the branch cuts are (−∞, −1] and [1, ∞) for the inverse hyperbolic tangent, and [−1, 1] for the inverse hyperbolic cotangent.

In view of a better numerical evaluation near the branch cuts, some authors use the following definitions of the principal values, although the second one introduces a removable singularity at z = 0. The two definitions of ${\displaystyle \operatorname {artanh} }$ differ for real values of ${\displaystyle z}$ with ${\displaystyle z>1}$ . The ones of ${\displaystyle \operatorname {arcoth} }$ differ for real values of ${\displaystyle z}$ with ${\displaystyle z\in [0,1)}$ .

${\displaystyle {\begin{aligned}\operatorname {artanh} z&={\tfrac {1}{2}}\operatorname {Log} \left({1+z}\right)-{\tfrac {1}{2}}\operatorname {Log} \left({1-z}\right)\\\operatorname {arcoth} z&={\tfrac {1}{2}}\operatorname {Log} \left({1+{\frac {1}{z}}}\right)-{\tfrac {1}{2}}\operatorname {Log} \left({1-{\frac {1}{z}}}\right)\end{aligned}}}$

£#h5#£Principal value of the inverse hyperbolic cosecant£#/h5#£
For the inverse hyperbolic cosecant, the principal value is defined as

${\displaystyle \operatorname {arcsch} z=\operatorname {Log} \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z^{2}}}+1}}\,\right)}$ .
It is defined when the arguments of the logarithm and the square root are not non-positive real numbers. The principal value of the square root is thus defined outside the interval [−i, i] of the imaginary line. If the argument of the logarithm is real, then z is a non-zero real number, and this implies that the argument of the logarithm is positive.

Thus, the principal value is defined by the above formula outside the branch cut, consisting of the interval [−i, i] of the imaginary line.

For z = 0, there is a singular point that is included in the branch cut.


£#h5#£Principal value of the inverse hyperbolic secant£#/h5#£
Here, as in the case of the inverse hyperbolic cosine, we have to factorize the square root. This gives the principal value

${\displaystyle \operatorname {arsech} z=\operatorname {Log} \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z}}+1}}\,{\sqrt {{\frac {1}{z}}-1}}\right).}$
If the argument of a square root is real, then z is real, and it follows that both principal values of square roots are defined, except if z is real and belongs to one of the intervals (−∞, 0] and [1, +∞). If the argument of the logarithm is real and negative, then z is also real and negative. It follows that the principal value of arsech is well defined, by the above formula outside two branch cuts, the real intervals (−∞, 0] and [1, +∞).

For z = 0, there is a singular point that is included in one of the branch cuts.


£#h5#£Graphical representation£#/h5#£
In the following graphical representation of the principal values of the inverse hyperbolic functions, the branch cuts appear as discontinuities of the color. The fact that the whole branch cuts appear as discontinuities, shows that these principal values may not be extended into analytic functions defined over larger domains. In other words, the above defined branch cuts are minimal.


£#h5#£See also£#/h5#£ £#ul#££#li#£Complex logarithm£#/li#£ £#li#£Hyperbolic secant distribution£#/li#£ £#li#£ISO 80000-2£#/li#£ £#li#£List of integrals of inverse hyperbolic functions£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£Bibliography£#/h5#£ £#ul#££#li#£Herbert Busemann and Paul J. Kelly (1953) Projective Geometry and Projective Metrics, page 207, Academic Press.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Inverse hyperbolic functions", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Hyperbolic Functions £#/li#££#/ul#£




£#h3#£Area Hyperbolic Sine£#/h3#£


In mathematics, the inverse hyperbolic functions are the inverse functions of the hyperbolic functions.

For a given value of a hyperbolic function, the corresponding inverse hyperbolic function provides the corresponding hyperbolic angle. The size of the hyperbolic angle is equal to the area of the corresponding hyperbolic sector of the hyperbola xy = 1, or twice the area of the corresponding sector of the unit hyperbola x2 − y2 = 1, just as a circular angle is twice the area of the circular sector of the unit circle. Some authors have called inverse hyperbolic functions "area functions" to realize the hyperbolic angles.

Hyperbolic functions occur in the calculations of angles and distances in hyperbolic geometry. It also occurs in the solutions of many linear differential equations (such as the equation defining a catenary), cubic equations, and Laplace's equation in Cartesian coordinates. Laplace's equations are important in many areas of physics, including electromagnetic theory, heat transfer, fluid dynamics, and special relativity.


£#h5#£Notation£#/h5#£
The ISO 80000-2 standard abbreviations consist of ar- followed by the abbreviation of the corresponding hyperbolic function (e.g., arsinh, arcosh). The prefix arc- followed by the corresponding hyperbolic function (e.g., arcsinh, arccosh) is also commonly seen, by analogy with the nomenclature for inverse trigonometric functions. These are misnomers, since the prefix arc is the abbreviation for arcus, while the prefix ar stands for area; the hyperbolic functions are not directly related to arcs.

Other authors prefer to use the notation argsinh, argcosh, argtanh, and so on, where the prefix arg is the abbreviation of the Latin argumentum. In computer science, this is often shortened to asinh.

The notation sinh−1(x), cosh−1(x), etc., is also used, despite the fact that care must be taken to avoid misinterpretations of the superscript −1 as a power, as opposed to a shorthand to denote the inverse function (e.g., cosh−1(x) versus cosh(x)−1).


£#h5#£Definitions in terms of logarithms£#/h5#£
Since the hyperbolic functions are rational functions of ex whose numerator and denominator are of degree at most two, these functions may be solved in terms of ex, by using the quadratic formula; then, taking the natural logarithm gives the following expressions for the inverse hyperbolic functions.

For complex arguments, the inverse hyperbolic functions, the square root and the logarithm are multi-valued functions, and the equalities of the next subsections may be viewed as equalities of multi-valued functions.

For all inverse hyperbolic functions (save the inverse hyperbolic cotangent and the inverse hyperbolic cosecant), the domain of the real function is connected.


£#h5#£Inverse hyperbolic sine£#/h5#£
Inverse hyperbolic sine (a.k.a. area hyperbolic sine) (Latin: Area sinus hyperbolicus):

${\displaystyle \operatorname {arsinh} x=\ln \left(x+{\sqrt {x^{2}+1}}\right)}$
The domain is the whole real line.


£#h5#£Inverse hyperbolic cosine£#/h5#£
Inverse hyperbolic cosine (a.k.a. area hyperbolic cosine) (Latin: Area cosinus hyperbolicus):

${\displaystyle \operatorname {arcosh} x=\ln \left(x+{\sqrt {x^{2}-1}}\right)}$
The domain is the closed interval [1, +∞ ).


£#h5#£Inverse hyperbolic tangent£#/h5#£
Inverse hyperbolic tangent (a.k.a. area hyperbolic tangent) (Latin: Area tangens hyperbolicus):

${\displaystyle \operatorname {artanh} x={\frac {1}{2}}\ln \left({\frac {1+x}{1-x}}\right)}$
The domain is the open interval (−1, 1).


£#h5#£Inverse hyperbolic cotangent£#/h5#£
Inverse hyperbolic cotangent (a.k.a., area hyperbolic cotangent) (Latin: Area cotangens hyperbolicus):

${\displaystyle \operatorname {arcoth} x={\frac {1}{2}}\ln \left({\frac {x+1}{x-1}}\right)}$
The domain is the union of the open intervals (−∞, −1) and (1, +∞).


£#h5#£Inverse hyperbolic secant£#/h5#£
Inverse hyperbolic secant (a.k.a., area hyperbolic secant) (Latin: Area secans hyperbolicus):

${\displaystyle \operatorname {arsech} x=\ln \left({\frac {1}{x}}+{\sqrt {{\frac {1}{x^{2}}}-1}}\right)=\ln \left({\frac {1+{\sqrt {1-x^{2}}}}{x}}\right)}$
The domain is the semi-open interval (0, 1].


£#h5#£Inverse hyperbolic cosecant£#/h5#£
Inverse hyperbolic cosecant (a.k.a., area hyperbolic cosecant) (Latin: Area cosecans hyperbolicus):

${\displaystyle \operatorname {arcsch} x=\ln \left({\frac {1}{x}}+{\sqrt {{\frac {1}{x^{2}}}+1}}\right)}$
The domain is the real line with 0 removed.


£#h5#£Addition formulae£#/h5#£
${\displaystyle \operatorname {arsinh} u\pm \operatorname {arsinh} v=\operatorname {arsinh} \left(u{\sqrt {1+v^{2}}}\pm v{\sqrt {1+u^{2}}}\right)}$
${\displaystyle \operatorname {arcosh} u\pm \operatorname {arcosh} v=\operatorname {arcosh} \left(uv\pm {\sqrt {(u^{2}-1)(v^{2}-1)}}\right)}$
${\displaystyle \operatorname {artanh} u\pm \operatorname {artanh} v=\operatorname {artanh} \left({\frac {u\pm v}{1\pm uv}}\right)}$
${\displaystyle \operatorname {arcoth} u\pm \operatorname {arcoth} v=\operatorname {arcoth} \left({\frac {1\pm uv}{u\pm v}}\right)}$
${\displaystyle {\begin{aligned}\operatorname {arsinh} u+\operatorname {arcosh} v&=\operatorname {arsinh} \left(uv+{\sqrt {(1+u^{2})(v^{2}-1)}}\right)\\&=\operatorname {arcosh} \left(v{\sqrt {1+u^{2}}}+u{\sqrt {v^{2}-1}}\right)\end{aligned}}}$

£#h5#£Other identities£#/h5#£
${\displaystyle {\begin{aligned}2\operatorname {arcosh} x&=\operatorname {arcosh} (2x^{2}-1)&\quad {\hbox{ for }}x\geq 1\\4\operatorname {arcosh} x&=\operatorname {arcosh} (8x^{4}-8x^{2}+1)&\quad {\hbox{ for }}x\geq 1\\2\operatorname {arsinh} x&=\operatorname {arcosh} (2x^{2}+1)&\quad {\hbox{ for }}x\geq 0\\4\operatorname {arsinh} x&=\operatorname {arcosh} (8x^{4}+8x^{2}+1)&\quad {\hbox{ for }}x\geq 0\end{aligned}}}$
${\displaystyle \ln(x)=\operatorname {arcosh} \left({\frac {x^{2}+1}{2x}}\right)=\operatorname {arsinh} \left({\frac {x^{2}-1}{2x}}\right)=\operatorname {artanh} \left({\frac {x^{2}-1}{x^{2}+1}}\right)}$

£#h5#£Composition of hyperbolic and inverse hyperbolic functions£#/h5#£
${\displaystyle {\begin{aligned}&\sinh(\operatorname {arcosh} x)={\sqrt {x^{2}-1}}\quad {\text{for}}\quad |x|>1\\&\sinh(\operatorname {artanh} x)={\frac {x}{\sqrt {1-x^{2}}}}\quad {\text{for}}\quad -1<x<1\\&\cosh(\operatorname {arsinh} x)={\sqrt {1+x^{2}}}\\&\cosh(\operatorname {artanh} x)={\frac {1}{\sqrt {1-x^{2}}}}\quad {\text{for}}\quad -1<x<1\\&\tanh(\operatorname {arsinh} x)={\frac {x}{\sqrt {1+x^{2}}}}\\&\tanh(\operatorname {arcosh} x)={\frac {\sqrt {x^{2}-1}}{x}}\quad {\text{for}}\quad |x|>1\end{aligned}}}$

£#h5#£Composition of inverse hyperbolic and trigonometric functions£#/h5#£
${\displaystyle \operatorname {arsinh} \left(\tan \alpha \right)=\operatorname {artanh} \left(\sin \alpha \right)=\ln \left({\frac {1+\sin \alpha }{\cos \alpha }}\right)=\pm \operatorname {arcosh} \left({\frac {1}{\cos \alpha }}\right)}$
${\displaystyle \ln \left(\left|\tan \alpha \right|\right)=-\operatorname {artanh} \left(\cos 2\alpha \right)}$

£#h5#£Conversions£#/h5#£
${\displaystyle \ln x=\operatorname {artanh} \left({\frac {x^{2}-1}{x^{2}+1}}\right)=\operatorname {arsinh} \left({\frac {x^{2}-1}{2x}}\right)=\pm \operatorname {arcosh} \left({\frac {x^{2}+1}{2x}}\right)}$
${\displaystyle \operatorname {artanh} x=\operatorname {arsinh} \left({\frac {x}{\sqrt {1-x^{2}}}}\right)=\pm \operatorname {arcosh} \left({\frac {1}{\sqrt {1-x^{2}}}}\right)}$
${\displaystyle \operatorname {arsinh} x=\operatorname {artanh} \left({\frac {x}{\sqrt {1+x^{2}}}}\right)=\pm \operatorname {arcosh} \left({\sqrt {1+x^{2}}}\right)}$
${\displaystyle \operatorname {arcosh} x=\left|\operatorname {arsinh} \left({\sqrt {x^{2}-1}}\right)\right|=\left|\operatorname {artanh} \left({\frac {\sqrt {x^{2}-1}}{x}}\right)\right|}$

£#h5#£Derivatives£#/h5#£
${\displaystyle {\begin{aligned}{\frac {d}{dx}}\operatorname {arsinh} x&{}={\frac {1}{\sqrt {x^{2}+1}}},{\text{ for all real }}x\\{\frac {d}{dx}}\operatorname {arcosh} x&{}={\frac {1}{\sqrt {x^{2}-1}}},{\text{ for all real }}x>1\\{\frac {d}{dx}}\operatorname {artanh} x&{}={\frac {1}{1-x^{2}}},{\text{ for all real }}|x|<1\\{\frac {d}{dx}}\operatorname {arcoth} x&{}={\frac {1}{1-x^{2}}},{\text{ for all real }}|x|>1\\{\frac {d}{dx}}\operatorname {arsech} x&{}={\frac {-1}{x{\sqrt {1-x^{2}}}}},{\text{ for all real }}x\in (0,1)\\{\frac {d}{dx}}\operatorname {arcsch} x&{}={\frac {-1}{|x|{\sqrt {1+x^{2}}}}},{\text{ for all real }}x{\text{, except }}0\\\end{aligned}}}$
For an example differentiation: let θ = arsinh x, so (where sinh2 θ = (sinh θ)2):

${\displaystyle {\frac {d\,\operatorname {arsinh} x}{dx}}={\frac {d\theta }{d\sinh \theta }}={\frac {1}{\cosh \theta }}={\frac {1}{\sqrt {1+\sinh ^{2}\theta }}}={\frac {1}{\sqrt {1+x^{2}}}}.}$

£#h5#£Series expansions£#/h5#£
Expansion series can be obtained for the above functions:

${\displaystyle {\begin{aligned}\operatorname {arsinh} x&=x-\left({\frac {1}{2}}\right){\frac {x^{3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{5}}{5}}-\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{7}}{7}}\pm \cdots \\&=\sum _{n=0}^{\infty }\left({\frac {(-1)^{n}(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{2n+1}}{2n+1}},\qquad \left|x\right|<1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcosh} x&=\ln(2x)-\left(\left({\frac {1}{2}}\right){\frac {x^{-2}}{2}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{-4}}{4}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{-6}}{6}}+\cdots \right)\\&=\ln(2x)-\sum _{n=1}^{\infty }\left({\frac {(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{-2n}}{2n}},\qquad \left|x\right|>1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {artanh} x&=x+{\frac {x^{3}}{3}}+{\frac {x^{5}}{5}}+{\frac {x^{7}}{7}}+\cdots \\&=\sum _{n=0}^{\infty }{\frac {x^{2n+1}}{2n+1}},\qquad \left|x\right|<1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcsch} x=\operatorname {arsinh} {\frac {1}{x}}&=x^{-1}-\left({\frac {1}{2}}\right){\frac {x^{-3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{-5}}{5}}-\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{-7}}{7}}\pm \cdots \\&=\sum _{n=0}^{\infty }\left({\frac {(-1)^{n}(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{-(2n+1)}}{2n+1}},\qquad \left|x\right|>1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arsech} x=\operatorname {arcosh} {\frac {1}{x}}&=\ln {\frac {2}{x}}-\left(\left({\frac {1}{2}}\right){\frac {x^{2}}{2}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{4}}{4}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{6}}{6}}+\cdots \right)\\&=\ln {\frac {2}{x}}-\sum _{n=1}^{\infty }\left({\frac {(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{2n}}{2n}},\qquad 0<x\leq 1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcoth} x=\operatorname {artanh} {\frac {1}{x}}&=x^{-1}+{\frac {x^{-3}}{3}}+{\frac {x^{-5}}{5}}+{\frac {x^{-7}}{7}}+\cdots \\&=\sum _{n=0}^{\infty }{\frac {x^{-(2n+1)}}{2n+1}},\qquad \left|x\right|>1\end{aligned}}}$
Asymptotic expansion for the arsinh x is given by

${\displaystyle \operatorname {arsinh} x=\ln(2x)+\sum \limits _{n=1}^{\infty }{\left({-1}\right)^{n-1}{\frac {\left({2n-1}\right)!!}{2n\left({2n}\right)!!}}}{\frac {1}{x^{2n}}}}$



£#h5#£Principal values in the complex plane£#/h5#£
As functions of a complex variable, inverse hyperbolic functions are multivalued functions that are analytic, except at a finite number of points. For such a function, it is common to define a principal value, which is a single valued analytic function which coincides with one specific branch of the multivalued function, over a domain consisting of the complex plane in which a finite number of arcs (usually half lines or line segments) have been removed. These arcs are called branch cuts. For specifying the branch, that is, defining which value of the multivalued function is considered at each point, one generally define it at a particular point, and deduce the value everywhere in the domain of definition of the principal value by analytic continuation. When possible, it is better to define the principal value directly—without referring to analytic continuation.

For example, for the square root, the principal value is defined as the square root that has a positive real part. This defines a single valued analytic function, which is defined everywhere, except for non-positive real values of the variables (where the two square roots have a zero real part). This principal value of the square root function is denoted ${\displaystyle {\sqrt {x}}}$ in what follows. Similarly, the principal value of the logarithm, denoted ${\displaystyle \operatorname {Log} }$ in what follows, is defined as the value for which the imaginary part has the smallest absolute value. It is defined everywhere except for non-positive real values of the variable, for which two different values of the logarithm reach the minimum.

For all inverse hyperbolic functions, the principal value may be defined in terms of principal values of the square root and the logarithm function. However, in some cases, the formulas of § Definitions in terms of logarithms do not give a correct principal value, as giving a domain of definition which is too small and, in one case non-connected.


£#h5#£Principal value of the inverse hyperbolic sine£#/h5#£
The principal value of the inverse hyperbolic sine is given by

${\displaystyle \operatorname {arsinh} z=\operatorname {Log} (z+{\sqrt {z^{2}+1}}\,)\,.}$
The argument of the square root is a non-positive real number, if and only if z belongs to one of the intervals [i, +i∞) and (−i∞, −i] of the imaginary axis. If the argument of the logarithm is real, then it is positive. Thus this formula defines a principal value for arsinh, with branch cuts [i, +i∞) and (−i∞, −i]. This is optimal, as the branch cuts must connect the singular points i and −i to the infinity.


£#h5#£Principal value of the inverse hyperbolic cosine£#/h5#£
The formula for the inverse hyperbolic cosine given in § Inverse hyperbolic cosine is not convenient, since similar to the principal values of the logarithm and the square root, the principal value of arcosh would not be defined for imaginary z. Thus the square root has to be factorized, leading to

${\displaystyle \operatorname {arcosh} z=\operatorname {Log} (z+{\sqrt {z+1}}{\sqrt {z-1}}\,)\,.}$
The principal values of the square roots are both defined, except if z belongs to the real interval (−∞, 1]. If the argument of the logarithm is real, then z is real and has the same sign. Thus, the above formula defines a principal value of arcosh outside the real interval (−∞, 1], which is thus the unique branch cut.


£#h5#£Principal values of the inverse hyperbolic tangent and cotangent£#/h5#£
The formulas given in § Definitions in terms of logarithms suggests

${\displaystyle {\begin{aligned}\operatorname {artanh} z&={\frac {1}{2}}\operatorname {Log} \left({\frac {1+z}{1-z}}\right)\\\operatorname {arcoth} z&={\frac {1}{2}}\operatorname {Log} \left({\frac {z+1}{z-1}}\right)\end{aligned}}}$
for the definition of the principal values of the inverse hyperbolic tangent and cotangent. In these formulas, the argument of the logarithm is real if and only if z is real. For artanh, this argument is in the real interval (−∞, 0], if z belongs either to (−∞, −1] or to [1, ∞). For arcoth, the argument of the logarithm is in (−∞, 0], if and only if z belongs to the real interval [−1, 1].

Therefore, these formulas define convenient principal values, for which the branch cuts are (−∞, −1] and [1, ∞) for the inverse hyperbolic tangent, and [−1, 1] for the inverse hyperbolic cotangent.

In view of a better numerical evaluation near the branch cuts, some authors use the following definitions of the principal values, although the second one introduces a removable singularity at z = 0. The two definitions of ${\displaystyle \operatorname {artanh} }$ differ for real values of ${\displaystyle z}$ with ${\displaystyle z>1}$ . The ones of ${\displaystyle \operatorname {arcoth} }$ differ for real values of ${\displaystyle z}$ with ${\displaystyle z\in [0,1)}$ .

${\displaystyle {\begin{aligned}\operatorname {artanh} z&={\tfrac {1}{2}}\operatorname {Log} \left({1+z}\right)-{\tfrac {1}{2}}\operatorname {Log} \left({1-z}\right)\\\operatorname {arcoth} z&={\tfrac {1}{2}}\operatorname {Log} \left({1+{\frac {1}{z}}}\right)-{\tfrac {1}{2}}\operatorname {Log} \left({1-{\frac {1}{z}}}\right)\end{aligned}}}$

£#h5#£Principal value of the inverse hyperbolic cosecant£#/h5#£
For the inverse hyperbolic cosecant, the principal value is defined as

${\displaystyle \operatorname {arcsch} z=\operatorname {Log} \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z^{2}}}+1}}\,\right)}$ .
It is defined when the arguments of the logarithm and the square root are not non-positive real numbers. The principal value of the square root is thus defined outside the interval [−i, i] of the imaginary line. If the argument of the logarithm is real, then z is a non-zero real number, and this implies that the argument of the logarithm is positive.

Thus, the principal value is defined by the above formula outside the branch cut, consisting of the interval [−i, i] of the imaginary line.

For z = 0, there is a singular point that is included in the branch cut.


£#h5#£Principal value of the inverse hyperbolic secant£#/h5#£
Here, as in the case of the inverse hyperbolic cosine, we have to factorize the square root. This gives the principal value

${\displaystyle \operatorname {arsech} z=\operatorname {Log} \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z}}+1}}\,{\sqrt {{\frac {1}{z}}-1}}\right).}$
If the argument of a square root is real, then z is real, and it follows that both principal values of square roots are defined, except if z is real and belongs to one of the intervals (−∞, 0] and [1, +∞). If the argument of the logarithm is real and negative, then z is also real and negative. It follows that the principal value of arsech is well defined, by the above formula outside two branch cuts, the real intervals (−∞, 0] and [1, +∞).

For z = 0, there is a singular point that is included in one of the branch cuts.


£#h5#£Graphical representation£#/h5#£
In the following graphical representation of the principal values of the inverse hyperbolic functions, the branch cuts appear as discontinuities of the color. The fact that the whole branch cuts appear as discontinuities, shows that these principal values may not be extended into analytic functions defined over larger domains. In other words, the above defined branch cuts are minimal.


£#h5#£See also£#/h5#£ £#ul#££#li#£Complex logarithm£#/li#£ £#li#£Hyperbolic secant distribution£#/li#£ £#li#£ISO 80000-2£#/li#£ £#li#£List of integrals of inverse hyperbolic functions£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£Bibliography£#/h5#£ £#ul#££#li#£Herbert Busemann and Paul J. Kelly (1953) Projective Geometry and Projective Metrics, page 207, Academic Press.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Inverse hyperbolic functions", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Hyperbolic Functions £#/li#££#/ul#£




£#h3#£Area Hyperbolic Tangent£#/h3#£


In mathematics, the inverse hyperbolic functions are the inverse functions of the hyperbolic functions.

For a given value of a hyperbolic function, the corresponding inverse hyperbolic function provides the corresponding hyperbolic angle. The size of the hyperbolic angle is equal to the area of the corresponding hyperbolic sector of the hyperbola xy = 1, or twice the area of the corresponding sector of the unit hyperbola x2 − y2 = 1, just as a circular angle is twice the area of the circular sector of the unit circle. Some authors have called inverse hyperbolic functions "area functions" to realize the hyperbolic angles.

Hyperbolic functions occur in the calculations of angles and distances in hyperbolic geometry. It also occurs in the solutions of many linear differential equations (such as the equation defining a catenary), cubic equations, and Laplace's equation in Cartesian coordinates. Laplace's equations are important in many areas of physics, including electromagnetic theory, heat transfer, fluid dynamics, and special relativity.


£#h5#£Notation£#/h5#£
The ISO 80000-2 standard abbreviations consist of ar- followed by the abbreviation of the corresponding hyperbolic function (e.g., arsinh, arcosh). The prefix arc- followed by the corresponding hyperbolic function (e.g., arcsinh, arccosh) is also commonly seen, by analogy with the nomenclature for inverse trigonometric functions. These are misnomers, since the prefix arc is the abbreviation for arcus, while the prefix ar stands for area; the hyperbolic functions are not directly related to arcs.

Other authors prefer to use the notation argsinh, argcosh, argtanh, and so on, where the prefix arg is the abbreviation of the Latin argumentum. In computer science, this is often shortened to asinh.

The notation sinh−1(x), cosh−1(x), etc., is also used, despite the fact that care must be taken to avoid misinterpretations of the superscript −1 as a power, as opposed to a shorthand to denote the inverse function (e.g., cosh−1(x) versus cosh(x)−1).


£#h5#£Definitions in terms of logarithms£#/h5#£
Since the hyperbolic functions are rational functions of ex whose numerator and denominator are of degree at most two, these functions may be solved in terms of ex, by using the quadratic formula; then, taking the natural logarithm gives the following expressions for the inverse hyperbolic functions.

For complex arguments, the inverse hyperbolic functions, the square root and the logarithm are multi-valued functions, and the equalities of the next subsections may be viewed as equalities of multi-valued functions.

For all inverse hyperbolic functions (save the inverse hyperbolic cotangent and the inverse hyperbolic cosecant), the domain of the real function is connected.


£#h5#£Inverse hyperbolic sine£#/h5#£
Inverse hyperbolic sine (a.k.a. area hyperbolic sine) (Latin: Area sinus hyperbolicus):

${\displaystyle \operatorname {arsinh} x=\ln \left(x+{\sqrt {x^{2}+1}}\right)}$
The domain is the whole real line.


£#h5#£Inverse hyperbolic cosine£#/h5#£
Inverse hyperbolic cosine (a.k.a. area hyperbolic cosine) (Latin: Area cosinus hyperbolicus):

${\displaystyle \operatorname {arcosh} x=\ln \left(x+{\sqrt {x^{2}-1}}\right)}$
The domain is the closed interval [1, +∞ ).


£#h5#£Inverse hyperbolic tangent£#/h5#£
Inverse hyperbolic tangent (a.k.a. area hyperbolic tangent) (Latin: Area tangens hyperbolicus):

${\displaystyle \operatorname {artanh} x={\frac {1}{2}}\ln \left({\frac {1+x}{1-x}}\right)}$
The domain is the open interval (−1, 1).


£#h5#£Inverse hyperbolic cotangent£#/h5#£
Inverse hyperbolic cotangent (a.k.a., area hyperbolic cotangent) (Latin: Area cotangens hyperbolicus):

${\displaystyle \operatorname {arcoth} x={\frac {1}{2}}\ln \left({\frac {x+1}{x-1}}\right)}$
The domain is the union of the open intervals (−∞, −1) and (1, +∞).


£#h5#£Inverse hyperbolic secant£#/h5#£
Inverse hyperbolic secant (a.k.a., area hyperbolic secant) (Latin: Area secans hyperbolicus):

${\displaystyle \operatorname {arsech} x=\ln \left({\frac {1}{x}}+{\sqrt {{\frac {1}{x^{2}}}-1}}\right)=\ln \left({\frac {1+{\sqrt {1-x^{2}}}}{x}}\right)}$
The domain is the semi-open interval (0, 1].


£#h5#£Inverse hyperbolic cosecant£#/h5#£
Inverse hyperbolic cosecant (a.k.a., area hyperbolic cosecant) (Latin: Area cosecans hyperbolicus):

${\displaystyle \operatorname {arcsch} x=\ln \left({\frac {1}{x}}+{\sqrt {{\frac {1}{x^{2}}}+1}}\right)}$
The domain is the real line with 0 removed.


£#h5#£Addition formulae£#/h5#£
${\displaystyle \operatorname {arsinh} u\pm \operatorname {arsinh} v=\operatorname {arsinh} \left(u{\sqrt {1+v^{2}}}\pm v{\sqrt {1+u^{2}}}\right)}$
${\displaystyle \operatorname {arcosh} u\pm \operatorname {arcosh} v=\operatorname {arcosh} \left(uv\pm {\sqrt {(u^{2}-1)(v^{2}-1)}}\right)}$
${\displaystyle \operatorname {artanh} u\pm \operatorname {artanh} v=\operatorname {artanh} \left({\frac {u\pm v}{1\pm uv}}\right)}$
${\displaystyle \operatorname {arcoth} u\pm \operatorname {arcoth} v=\operatorname {arcoth} \left({\frac {1\pm uv}{u\pm v}}\right)}$
${\displaystyle {\begin{aligned}\operatorname {arsinh} u+\operatorname {arcosh} v&=\operatorname {arsinh} \left(uv+{\sqrt {(1+u^{2})(v^{2}-1)}}\right)\\&=\operatorname {arcosh} \left(v{\sqrt {1+u^{2}}}+u{\sqrt {v^{2}-1}}\right)\end{aligned}}}$

£#h5#£Other identities£#/h5#£
${\displaystyle {\begin{aligned}2\operatorname {arcosh} x&=\operatorname {arcosh} (2x^{2}-1)&\quad {\hbox{ for }}x\geq 1\\4\operatorname {arcosh} x&=\operatorname {arcosh} (8x^{4}-8x^{2}+1)&\quad {\hbox{ for }}x\geq 1\\2\operatorname {arsinh} x&=\operatorname {arcosh} (2x^{2}+1)&\quad {\hbox{ for }}x\geq 0\\4\operatorname {arsinh} x&=\operatorname {arcosh} (8x^{4}+8x^{2}+1)&\quad {\hbox{ for }}x\geq 0\end{aligned}}}$
${\displaystyle \ln(x)=\operatorname {arcosh} \left({\frac {x^{2}+1}{2x}}\right)=\operatorname {arsinh} \left({\frac {x^{2}-1}{2x}}\right)=\operatorname {artanh} \left({\frac {x^{2}-1}{x^{2}+1}}\right)}$

£#h5#£Composition of hyperbolic and inverse hyperbolic functions£#/h5#£
${\displaystyle {\begin{aligned}&\sinh(\operatorname {arcosh} x)={\sqrt {x^{2}-1}}\quad {\text{for}}\quad |x|>1\\&\sinh(\operatorname {artanh} x)={\frac {x}{\sqrt {1-x^{2}}}}\quad {\text{for}}\quad -1<x<1\\&\cosh(\operatorname {arsinh} x)={\sqrt {1+x^{2}}}\\&\cosh(\operatorname {artanh} x)={\frac {1}{\sqrt {1-x^{2}}}}\quad {\text{for}}\quad -1<x<1\\&\tanh(\operatorname {arsinh} x)={\frac {x}{\sqrt {1+x^{2}}}}\\&\tanh(\operatorname {arcosh} x)={\frac {\sqrt {x^{2}-1}}{x}}\quad {\text{for}}\quad |x|>1\end{aligned}}}$

£#h5#£Composition of inverse hyperbolic and trigonometric functions£#/h5#£
${\displaystyle \operatorname {arsinh} \left(\tan \alpha \right)=\operatorname {artanh} \left(\sin \alpha \right)=\ln \left({\frac {1+\sin \alpha }{\cos \alpha }}\right)=\pm \operatorname {arcosh} \left({\frac {1}{\cos \alpha }}\right)}$
${\displaystyle \ln \left(\left|\tan \alpha \right|\right)=-\operatorname {artanh} \left(\cos 2\alpha \right)}$

£#h5#£Conversions£#/h5#£
${\displaystyle \ln x=\operatorname {artanh} \left({\frac {x^{2}-1}{x^{2}+1}}\right)=\operatorname {arsinh} \left({\frac {x^{2}-1}{2x}}\right)=\pm \operatorname {arcosh} \left({\frac {x^{2}+1}{2x}}\right)}$
${\displaystyle \operatorname {artanh} x=\operatorname {arsinh} \left({\frac {x}{\sqrt {1-x^{2}}}}\right)=\pm \operatorname {arcosh} \left({\frac {1}{\sqrt {1-x^{2}}}}\right)}$
${\displaystyle \operatorname {arsinh} x=\operatorname {artanh} \left({\frac {x}{\sqrt {1+x^{2}}}}\right)=\pm \operatorname {arcosh} \left({\sqrt {1+x^{2}}}\right)}$
${\displaystyle \operatorname {arcosh} x=\left|\operatorname {arsinh} \left({\sqrt {x^{2}-1}}\right)\right|=\left|\operatorname {artanh} \left({\frac {\sqrt {x^{2}-1}}{x}}\right)\right|}$

£#h5#£Derivatives£#/h5#£
${\displaystyle {\begin{aligned}{\frac {d}{dx}}\operatorname {arsinh} x&{}={\frac {1}{\sqrt {x^{2}+1}}},{\text{ for all real }}x\\{\frac {d}{dx}}\operatorname {arcosh} x&{}={\frac {1}{\sqrt {x^{2}-1}}},{\text{ for all real }}x>1\\{\frac {d}{dx}}\operatorname {artanh} x&{}={\frac {1}{1-x^{2}}},{\text{ for all real }}|x|<1\\{\frac {d}{dx}}\operatorname {arcoth} x&{}={\frac {1}{1-x^{2}}},{\text{ for all real }}|x|>1\\{\frac {d}{dx}}\operatorname {arsech} x&{}={\frac {-1}{x{\sqrt {1-x^{2}}}}},{\text{ for all real }}x\in (0,1)\\{\frac {d}{dx}}\operatorname {arcsch} x&{}={\frac {-1}{|x|{\sqrt {1+x^{2}}}}},{\text{ for all real }}x{\text{, except }}0\\\end{aligned}}}$
For an example differentiation: let θ = arsinh x, so (where sinh2 θ = (sinh θ)2):

${\displaystyle {\frac {d\,\operatorname {arsinh} x}{dx}}={\frac {d\theta }{d\sinh \theta }}={\frac {1}{\cosh \theta }}={\frac {1}{\sqrt {1+\sinh ^{2}\theta }}}={\frac {1}{\sqrt {1+x^{2}}}}.}$

£#h5#£Series expansions£#/h5#£
Expansion series can be obtained for the above functions:

${\displaystyle {\begin{aligned}\operatorname {arsinh} x&=x-\left({\frac {1}{2}}\right){\frac {x^{3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{5}}{5}}-\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{7}}{7}}\pm \cdots \\&=\sum _{n=0}^{\infty }\left({\frac {(-1)^{n}(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{2n+1}}{2n+1}},\qquad \left|x\right|<1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcosh} x&=\ln(2x)-\left(\left({\frac {1}{2}}\right){\frac {x^{-2}}{2}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{-4}}{4}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{-6}}{6}}+\cdots \right)\\&=\ln(2x)-\sum _{n=1}^{\infty }\left({\frac {(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{-2n}}{2n}},\qquad \left|x\right|>1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {artanh} x&=x+{\frac {x^{3}}{3}}+{\frac {x^{5}}{5}}+{\frac {x^{7}}{7}}+\cdots \\&=\sum _{n=0}^{\infty }{\frac {x^{2n+1}}{2n+1}},\qquad \left|x\right|<1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcsch} x=\operatorname {arsinh} {\frac {1}{x}}&=x^{-1}-\left({\frac {1}{2}}\right){\frac {x^{-3}}{3}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{-5}}{5}}-\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{-7}}{7}}\pm \cdots \\&=\sum _{n=0}^{\infty }\left({\frac {(-1)^{n}(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{-(2n+1)}}{2n+1}},\qquad \left|x\right|>1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arsech} x=\operatorname {arcosh} {\frac {1}{x}}&=\ln {\frac {2}{x}}-\left(\left({\frac {1}{2}}\right){\frac {x^{2}}{2}}+\left({\frac {1\cdot 3}{2\cdot 4}}\right){\frac {x^{4}}{4}}+\left({\frac {1\cdot 3\cdot 5}{2\cdot 4\cdot 6}}\right){\frac {x^{6}}{6}}+\cdots \right)\\&=\ln {\frac {2}{x}}-\sum _{n=1}^{\infty }\left({\frac {(2n)!}{2^{2n}(n!)^{2}}}\right){\frac {x^{2n}}{2n}},\qquad 0<x\leq 1\end{aligned}}}$
${\displaystyle {\begin{aligned}\operatorname {arcoth} x=\operatorname {artanh} {\frac {1}{x}}&=x^{-1}+{\frac {x^{-3}}{3}}+{\frac {x^{-5}}{5}}+{\frac {x^{-7}}{7}}+\cdots \\&=\sum _{n=0}^{\infty }{\frac {x^{-(2n+1)}}{2n+1}},\qquad \left|x\right|>1\end{aligned}}}$
Asymptotic expansion for the arsinh x is given by

${\displaystyle \operatorname {arsinh} x=\ln(2x)+\sum \limits _{n=1}^{\infty }{\left({-1}\right)^{n-1}{\frac {\left({2n-1}\right)!!}{2n\left({2n}\right)!!}}}{\frac {1}{x^{2n}}}}$



£#h5#£Principal values in the complex plane£#/h5#£
As functions of a complex variable, inverse hyperbolic functions are multivalued functions that are analytic, except at a finite number of points. For such a function, it is common to define a principal value, which is a single valued analytic function which coincides with one specific branch of the multivalued function, over a domain consisting of the complex plane in which a finite number of arcs (usually half lines or line segments) have been removed. These arcs are called branch cuts. For specifying the branch, that is, defining which value of the multivalued function is considered at each point, one generally define it at a particular point, and deduce the value everywhere in the domain of definition of the principal value by analytic continuation. When possible, it is better to define the principal value directly—without referring to analytic continuation.

For example, for the square root, the principal value is defined as the square root that has a positive real part. This defines a single valued analytic function, which is defined everywhere, except for non-positive real values of the variables (where the two square roots have a zero real part). This principal value of the square root function is denoted ${\displaystyle {\sqrt {x}}}$ in what follows. Similarly, the principal value of the logarithm, denoted ${\displaystyle \operatorname {Log} }$ in what follows, is defined as the value for which the imaginary part has the smallest absolute value. It is defined everywhere except for non-positive real values of the variable, for which two different values of the logarithm reach the minimum.

For all inverse hyperbolic functions, the principal value may be defined in terms of principal values of the square root and the logarithm function. However, in some cases, the formulas of § Definitions in terms of logarithms do not give a correct principal value, as giving a domain of definition which is too small and, in one case non-connected.


£#h5#£Principal value of the inverse hyperbolic sine£#/h5#£
The principal value of the inverse hyperbolic sine is given by

${\displaystyle \operatorname {arsinh} z=\operatorname {Log} (z+{\sqrt {z^{2}+1}}\,)\,.}$
The argument of the square root is a non-positive real number, if and only if z belongs to one of the intervals [i, +i∞) and (−i∞, −i] of the imaginary axis. If the argument of the logarithm is real, then it is positive. Thus this formula defines a principal value for arsinh, with branch cuts [i, +i∞) and (−i∞, −i]. This is optimal, as the branch cuts must connect the singular points i and −i to the infinity.


£#h5#£Principal value of the inverse hyperbolic cosine£#/h5#£
The formula for the inverse hyperbolic cosine given in § Inverse hyperbolic cosine is not convenient, since similar to the principal values of the logarithm and the square root, the principal value of arcosh would not be defined for imaginary z. Thus the square root has to be factorized, leading to

${\displaystyle \operatorname {arcosh} z=\operatorname {Log} (z+{\sqrt {z+1}}{\sqrt {z-1}}\,)\,.}$
The principal values of the square roots are both defined, except if z belongs to the real interval (−∞, 1]. If the argument of the logarithm is real, then z is real and has the same sign. Thus, the above formula defines a principal value of arcosh outside the real interval (−∞, 1], which is thus the unique branch cut.


£#h5#£Principal values of the inverse hyperbolic tangent and cotangent£#/h5#£
The formulas given in § Definitions in terms of logarithms suggests

${\displaystyle {\begin{aligned}\operatorname {artanh} z&={\frac {1}{2}}\operatorname {Log} \left({\frac {1+z}{1-z}}\right)\\\operatorname {arcoth} z&={\frac {1}{2}}\operatorname {Log} \left({\frac {z+1}{z-1}}\right)\end{aligned}}}$
for the definition of the principal values of the inverse hyperbolic tangent and cotangent. In these formulas, the argument of the logarithm is real if and only if z is real. For artanh, this argument is in the real interval (−∞, 0], if z belongs either to (−∞, −1] or to [1, ∞). For arcoth, the argument of the logarithm is in (−∞, 0], if and only if z belongs to the real interval [−1, 1].

Therefore, these formulas define convenient principal values, for which the branch cuts are (−∞, −1] and [1, ∞) for the inverse hyperbolic tangent, and [−1, 1] for the inverse hyperbolic cotangent.

In view of a better numerical evaluation near the branch cuts, some authors use the following definitions of the principal values, although the second one introduces a removable singularity at z = 0. The two definitions of ${\displaystyle \operatorname {artanh} }$ differ for real values of ${\displaystyle z}$ with ${\displaystyle z>1}$ . The ones of ${\displaystyle \operatorname {arcoth} }$ differ for real values of ${\displaystyle z}$ with ${\displaystyle z\in [0,1)}$ .

${\displaystyle {\begin{aligned}\operatorname {artanh} z&={\tfrac {1}{2}}\operatorname {Log} \left({1+z}\right)-{\tfrac {1}{2}}\operatorname {Log} \left({1-z}\right)\\\operatorname {arcoth} z&={\tfrac {1}{2}}\operatorname {Log} \left({1+{\frac {1}{z}}}\right)-{\tfrac {1}{2}}\operatorname {Log} \left({1-{\frac {1}{z}}}\right)\end{aligned}}}$

£#h5#£Principal value of the inverse hyperbolic cosecant£#/h5#£
For the inverse hyperbolic cosecant, the principal value is defined as

${\displaystyle \operatorname {arcsch} z=\operatorname {Log} \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z^{2}}}+1}}\,\right)}$ .
It is defined when the arguments of the logarithm and the square root are not non-positive real numbers. The principal value of the square root is thus defined outside the interval [−i, i] of the imaginary line. If the argument of the logarithm is real, then z is a non-zero real number, and this implies that the argument of the logarithm is positive.

Thus, the principal value is defined by the above formula outside the branch cut, consisting of the interval [−i, i] of the imaginary line.

For z = 0, there is a singular point that is included in the branch cut.


£#h5#£Principal value of the inverse hyperbolic secant£#/h5#£
Here, as in the case of the inverse hyperbolic cosine, we have to factorize the square root. This gives the principal value

${\displaystyle \operatorname {arsech} z=\operatorname {Log} \left({\frac {1}{z}}+{\sqrt {{\frac {1}{z}}+1}}\,{\sqrt {{\frac {1}{z}}-1}}\right).}$
If the argument of a square root is real, then z is real, and it follows that both principal values of square roots are defined, except if z is real and belongs to one of the intervals (−∞, 0] and [1, +∞). If the argument of the logarithm is real and negative, then z is also real and negative. It follows that the principal value of arsech is well defined, by the above formula outside two branch cuts, the real intervals (−∞, 0] and [1, +∞).

For z = 0, there is a singular point that is included in one of the branch cuts.


£#h5#£Graphical representation£#/h5#£
In the following graphical representation of the principal values of the inverse hyperbolic functions, the branch cuts appear as discontinuities of the color. The fact that the whole branch cuts appear as discontinuities, shows that these principal values may not be extended into analytic functions defined over larger domains. In other words, the above defined branch cuts are minimal.


£#h5#£See also£#/h5#£ £#ul#££#li#£Complex logarithm£#/li#£ £#li#£Hyperbolic secant distribution£#/li#£ £#li#£ISO 80000-2£#/li#£ £#li#£List of integrals of inverse hyperbolic functions£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£Bibliography£#/h5#£ £#ul#££#li#£Herbert Busemann and Paul J. Kelly (1953) Projective Geometry and Projective Metrics, page 207, Academic Press.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Inverse hyperbolic functions", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Hyperbolic Functions £#/li#££#/ul#£




£#h3#£Area Integral£#/h3#£

In mathematics (specifically multivariable calculus), a multiple integral is a definite integral of a function of several real variables, for instance, f(x, y) or f(x, y, z). Integrals of a function of two variables over a region in ${\displaystyle \mathbb {R} ^{2}}$ (the real-number plane) are called double integrals, and integrals of a function of three variables over a region in ${\displaystyle \mathbb {R} ^{3}}$ (real-number 3D space) are called triple integrals. For multiple integrals of a single-variable function, see the Cauchy formula for repeated integration.


£#h5#£Introduction£#/h5#£
Just as the definite integral of a positive function of one variable represents the area of the region between the graph of the function and the x-axis, the double integral of a positive function of two variables represents the volume of the region between the surface defined by the function (on the three-dimensional Cartesian plane where z = f(x, y)) and the plane which contains its domain. If there are more variables, a multiple integral will yield hypervolumes of multidimensional functions.

Multiple integration of a function in n variables: f(x1, x2, ..., xn) over a domain D is most commonly represented by nested integral signs in the reverse order of execution (the leftmost integral sign is computed last), followed by the function and integrand arguments in proper order (the integral with respect to the rightmost argument is computed last). The domain of integration is either represented symbolically for every argument over each integral sign, or is abbreviated by a variable at the rightmost integral sign:

${\displaystyle \int \cdots \int _{\mathbf {D} }\,f(x_{1},x_{2},\ldots ,x_{n})\,dx_{1}\!\cdots dx_{n}}$
Since the concept of an antiderivative is only defined for functions of a single real variable, the usual definition of the indefinite integral does not immediately extend to the multiple integral.


£#h5#£Mathematical definition£#/h5#£
For n > 1, consider a so-called "half-open" n-dimensional hyperrectangular domain T, defined as:

${\displaystyle T=[a_{1},b_{1})\times [a_{2},b_{2})\times \cdots \times [a_{n},b_{n})\subseteq \mathbb {R} ^{n}.}$
Partition each interval [aj, bj) into a finite family Ij of non-overlapping subintervals ijα, with each subinterval closed at the left end, and open at the right end.

Then the finite family of subrectangles C given by

${\displaystyle C=I_{1}\times I_{2}\times \cdots \times I_{n}}$
is a partition of T; that is, the subrectangles Ck are non-overlapping and their union is T.

Let f : T → R be a function defined on T. Consider a partition C of T as defined above, such that C is a family of m subrectangles Cm and

${\displaystyle T=C_{1}\cup C_{2}\cup \cdots \cup C_{m}}$
We can approximate the total (n + 1)-dimensional volume bounded below by the n-dimensional hyperrectangle T and above by the n-dimensional graph of f with the following Riemann sum:

${\displaystyle \sum _{k=1}^{m}f(P_{k})\,\operatorname {m} (C_{k})}$
where Pk is a point in Ck and m(Ck) is the product of the lengths of the intervals whose Cartesian product is Ck, also known as the measure of Ck.

The diameter of a subrectangle Ck is the largest of the lengths of the intervals whose Cartesian product is Ck. The diameter of a given partition of T is defined as the largest of the diameters of the subrectangles in the partition. Intuitively, as the diameter of the partition C is restricted smaller and smaller, the number of subrectangles m gets larger, and the measure m(Ck) of each subrectangle grows smaller. The function f is said to be Riemann integrable if the limit

${\displaystyle S=\lim _{\delta \to 0}\sum _{k=1}^{m}f(P_{k})\,\operatorname {m} (C_{k})}$
exists, where the limit is taken over all possible partitions of T of diameter at most δ.

If f is Riemann integrable, S is called the Riemann integral of f over T and is denoted

${\displaystyle \int \cdots \int _{T}\,f(x_{1},x_{2},\ldots ,x_{n})\,dx_{1}\!\cdots dx_{n}}$
Frequently this notation is abbreviated as

${\displaystyle \int _{T}\!f(\mathbf {x} )\,d^{n}\mathbf {x} .}$
where x represents the n-tuple (x1, …, xn) and dnx is the n-dimensional volume differential.

The Riemann integral of a function defined over an arbitrary bounded n-dimensional set can be defined by extending that function to a function defined over a half-open rectangle whose values are zero outside the domain of the original function. Then the integral of the original function over the original domain is defined to be the integral of the extended function over its rectangular domain, if it exists.

In what follows the Riemann integral in n dimensions will be called the multiple integral.


£#h5#£Properties£#/h5#£
Multiple integrals have many properties common to those of integrals of functions of one variable (linearity, commutativity, monotonicity, and so on). One important property of multiple integrals is that the value of an integral is independent of the order of integrands under certain conditions. This property is popularly known as Fubini's theorem.


£#h5#£Particular cases£#/h5#£
In the case of ${\displaystyle T\subseteq \mathbb {R} ^{2}}$ , the integral

${\displaystyle l=\iint _{T}f(x,y)\,dx\,dy}$
is the double integral of f on T, and if ${\displaystyle T\subseteq \mathbb {R} ^{3}}$ the integral

${\displaystyle l=\iiint _{T}f(x,y,z)\,dx\,dy\,dz}$
is the triple integral of f on T.

Notice that, by convention, the double integral has two integral signs, and the triple integral has three; this is a notational convention which is convenient when computing a multiple integral as an iterated integral, as shown later in this article.


£#h5#£Methods of integration£#/h5#£
The resolution of problems with multiple integrals consists, in most cases, of finding a way to reduce the multiple integral to an iterated integral, a series of integrals of one variable, each being directly solvable. For continuous functions, this is justified by Fubini's theorem. Sometimes, it is possible to obtain the result of the integration by direct examination without any calculations.

The following are some simple methods of integration:


£#h5#£Integrating constant functions£#/h5#£
When the integrand is a constant function c, the integral is equal to the product of c and the measure of the domain of integration. If c = 1 and the domain is a subregion of R2, the integral gives the area of the region, while if the domain is a subregion of R3, the integral gives the volume of the region.

Example. Let f(x, y) = 2 and

${\displaystyle D=\left\{(x,y)\in \mathbb {R} ^{2}\ :\ 2\leq x\leq 4\ ;\ 3\leq y\leq 6\right\}}$
in which case

${\displaystyle \int _{3}^{6}\int _{2}^{4}\ 2\ dx\,dy=2\int _{3}^{6}\int _{2}^{4}\ 1\ dx\,dy=2\cdot \operatorname {area} (D)=2\cdot (2\cdot 3)=12,}$
since by definition we have:

${\displaystyle \int _{3}^{6}\int _{2}^{4}\ 1\ dx\,dy=\operatorname {area} (D).}$

£#h5#£Use of symmetry£#/h5#£
When the domain of integration is symmetric about the origin with respect to at least one of the variables of integration and the integrand is odd with respect to this variable, the integral is equal to zero, as the integrals over the two halves of the domain have the same absolute value but opposite signs. When the integrand is even with respect to this variable, the integral is equal to twice the integral over one half of the domain, as the integrals over the two halves of the domain are equal.

Example 1. Consider the function f(x,y) = 2 sin(x) − 3y3 + 5 integrated over the domain

${\displaystyle T=\left\{(x,y)\in \mathbb {R} ^{2}\ :\ x^{2}+y^{2}\leq 1\right\},}$
a disc with radius 1 centered at the origin with the boundary included.

Using the linearity property, the integral can be decomposed into three pieces:

${\displaystyle \iint _{T}\left(2\sin x-3y^{3}+5\right)\,dx\,dy=\iint _{T}2\sin x\,dx\,dy-\iint _{T}3y^{3}\,dx\,dy+\iint _{T}5\,dx\,dy}$
The function 2 sin(x) is an odd function in the variable x and the disc T is symmetric with respect to the y-axis, so the value of the first integral is 0. Similarly, the function 3y3 is an odd function of y, and T is symmetric with respect to the x-axis, and so the only contribution to the final result is that of the third integral. Therefore the original integral is equal to the area of the disk times 5, or 5π.

Example 2. Consider the function f(x, y, z) = x exp(y2 + z2) and as integration region the ball with radius 2 centered at the origin,

${\displaystyle T=\left\{(x,y,z)\in \mathbb {R} ^{3}\ :\ x^{2}+y^{2}+z^{2}\leq 4\right\}.}$
The "ball" is symmetric about all three axes, but it is sufficient to integrate with respect to x-axis to show that the integral is 0, because the function is an odd function of that variable.


£#h5#£Normal domains on R2£#/h5#£
This method is applicable to any domain D for which:

£#ul#££#li#£the projection of D onto either the x-axis or the y-axis is bounded by the two values, a and b£#/li#£ £#li#£any line perpendicular to this axis that passes between these two values intersects the domain in an interval whose endpoints are given by the graphs of two functions, α and β.£#/li#££#/ul#£
Such a domain will be here called a normal domain. Elsewhere in the literature, normal domains are sometimes called type I or type II domains, depending on which axis the domain is fibred over. In all cases, the function to be integrated must be Riemann integrable on the domain, which is true (for instance) if the function is continuous.


£#h5#£x-axis£#/h5#£
If the domain D is normal with respect to the x-axis, and f : D → R is a continuous function; then α(x) and β(x) (both of which are defined on the interval [a, b]) are the two functions that determine D. Then, by Fubini's theorem:

${\displaystyle \iint _{D}f(x,y)\,dx\,dy=\int _{a}^{b}dx\int _{\alpha (x)}^{\beta (x)}f(x,y)\,dy.}$

£#h5#£y-axis£#/h5#£
If D is normal with respect to the y-axis and f : D → R is a continuous function; then α(y) and β(y) (both of which are defined on the interval [a, b]) are the two functions that determine D. Again, by Fubini's theorem:

${\displaystyle \iint _{D}f(x,y)\,dx\,dy=\int _{a}^{b}dy\int _{\alpha (y)}^{\beta (y)}f(x,y)\,dx.}$

£#h5#£Normal domains on R3£#/h5#£
If T is a domain that is normal with respect to the xy-plane and determined by the functions α(x, y) and β(x, y), then

${\displaystyle \iiint _{T}f(x,y,z)\,dx\,dy\,dz=\iint _{D}\int _{\alpha (x,y)}^{\beta (x,y)}f(x,y,z)\,dz\,dx\,dy}$
This definition is the same for the other five normality cases on R3. It can be generalized in a straightforward way to domains in Rn.


£#h5#£Change of variables£#/h5#£
The limits of integration are often not easily interchangeable (without normality or with complex formulae to integrate). One makes a change of variables to rewrite the integral in a more "comfortable" region, which can be described in simpler formulae. To do so, the function must be adapted to the new coordinates.

Example 1a. The function is f(x, y) = (x − 1)2 + √y; if one adopts the substitution u = x − 1, v = y therefore x = u + 1, y = v one obtains the new function f2(u, v) = (u)2 + √v.

£#ul#££#li#£Similarly for the domain because it is delimited by the original variables that were transformed before (x and y in example).£#/li#£ £#li#£the differentials dx and dy transform via the absolute value of the determinant of the Jacobian matrix containing the partial derivatives of the transformations regarding the new variable (consider, as an example, the differential transformation in polar coordinates).£#/li#££#/ul#£
There exist three main "kinds" of changes of variable (one in R2, two in R3); however, more general substitutions can be made using the same principle.


£#h5#£Polar coordinates£#/h5#£
In R2 if the domain has a circular symmetry and the function has some particular characteristics one can apply the transformation to polar coordinates (see the example in the picture) which means that the generic points P(x, y) in Cartesian coordinates switch to their respective points in polar coordinates. That allows one to change the shape of the domain and simplify the operations.

The fundamental relation to make the transformation is the following:

${\displaystyle f(x,y)\rightarrow f(\rho \cos \varphi ,\rho \sin \varphi ).}$
Example 2a. The function is f(x, y) = x + y and applying the transformation one obtains

${\displaystyle f(x,y)=f(\rho \cos \varphi ,\rho \sin \varphi )=\rho \cos \varphi +\rho \sin \varphi =\rho (\cos \varphi +\sin \varphi ).}$
Example 2b. The function is f(x, y) = x2 + y2, in this case one has:

${\displaystyle f(x,y)=\rho ^{2}\left(\cos ^{2}\varphi +\sin ^{2}\varphi \right)=\rho ^{2}}$
using the Pythagorean trigonometric identity (very useful to simplify this operation).

The transformation of the domain is made by defining the radius' crown length and the amplitude of the described angle to define the ρ, φ intervals starting from x, y.

Example 2c. The domain is D = {x2 + y2 ≤ 4}, that is a circumference of radius 2; it's evident that the covered angle is the circle angle, so φ varies from 0 to 2π, while the crown radius varies from 0 to 2 (the crown with the inside radius null is just a circle).

Example 2d. The domain is D = {x2 + y2 ≤ 9, x2 + y2 ≥ 4, y ≥ 0}, that is the circular crown in the positive y half-plane (please see the picture in the example); φ describes a plane angle while ρ varies from 2 to 3. Therefore the transformed domain will be the following rectangle:

${\displaystyle T=\{2\leq \rho \leq 3,\ 0\leq \varphi \leq \pi \}.}$
The Jacobian determinant of that transformation is the following:

${\displaystyle {\frac {\partial (x,y)}{\partial (\rho ,\varphi )}}={\begin{vmatrix}\cos \varphi &-\rho \sin \varphi \\\sin \varphi &\rho \cos \varphi \end{vmatrix}}=\rho }$
which has been obtained by inserting the partial derivatives of x = ρ cos(φ), y = ρ sin(φ) in the first column respect to ρ and in the second respect to φ, so the dx dy differentials in this transformation become ρ dρ dφ.

Once the function is transformed and the domain evaluated, it is possible to define the formula for the change of variables in polar coordinates:

${\displaystyle \iint _{D}f(x,y)\,dx\,dy=\iint _{T}f(\rho \cos \varphi ,\rho \sin \varphi )\rho \,d\rho \,d\varphi .}$
φ is valid in the [0, 2π] interval while ρ, which is a measure of a length, can only have positive values.

Example 2e. The function is f(x, y) = x and the domain is the same as in Example 2d. From the previous analysis of D we know the intervals of ρ (from 2 to 3) and of φ (from 0 to π). Now we change the function:

${\displaystyle f(x,y)=x\longrightarrow f(\rho ,\varphi )=\rho \cos \varphi .}$
finally let's apply the integration formula:

${\displaystyle \iint _{D}x\,dx\,dy=\iint _{T}\rho \cos \varphi \rho \,d\rho \,d\varphi .}$
Once the intervals are known, you have

${\displaystyle \int _{0}^{\pi }\int _{2}^{3}\rho ^{2}\cos \varphi \,d\rho \,d\varphi =\int _{0}^{\pi }\cos \varphi \ d\varphi \left[{\frac {\rho ^{3}}{3}}\right]_{2}^{3}={\Big [}\sin \varphi {\Big ]}_{0}^{\pi }\ \left(9-{\frac {8}{3}}\right)=0.}$

£#h5#£Cylindrical coordinates£#/h5#£
In R3 the integration on domains with a circular base can be made by the passage to cylindrical coordinates; the transformation of the function is made by the following relation:

${\displaystyle f(x,y,z)\rightarrow f(\rho \cos \varphi ,\rho \sin \varphi ,z)}$
The domain transformation can be graphically attained, because only the shape of the base varies, while the height follows the shape of the starting region.

Example 3a. The region is D = {x2 + y2 ≤ 9, x2 + y2 ≥ 4, 0 ≤ z ≤ 5} (that is the "tube" whose base is the circular crown of Example 2d and whose height is 5); if the transformation is applied, this region is obtained:

${\displaystyle T=\{2\leq \rho \leq 3,\ 0\leq \varphi \leq 2\pi ,\ 0\leq z\leq 5\}}$
(that is, the parallelepiped whose base is similar to the rectangle in Example 2d and whose height is 5).

Because the z component is unvaried during the transformation, the dx dy dz differentials vary as in the passage to polar coordinates: therefore, they become ρ dρ dφ dz.

Finally, it is possible to apply the final formula to cylindrical coordinates:

${\displaystyle \iiint _{D}f(x,y,z)\,dx\,dy\,dz=\iiint _{T}f(\rho \cos \varphi ,\rho \sin \varphi ,z)\rho \,d\rho \,d\varphi \,dz.}$
This method is convenient in case of cylindrical or conical domains or in regions where it is easy to individuate the z interval and even transform the circular base and the function.

Example 3b. The function is f(x, y, z) = x2 + y2 + z and as integration domain this cylinder: D = {x2 + y2 ≤ 9, −5 ≤ z ≤ 5}. The transformation of D in cylindrical coordinates is the following:

${\displaystyle T=\{0\leq \rho \leq 3,\ 0\leq \varphi \leq 2\pi ,\ -5\leq z\leq 5\}.}$
while the function becomes

${\displaystyle f(\rho \cos \varphi ,\rho \sin \varphi ,z)=\rho ^{2}+z}$
Finally one can apply the integration formula:

${\displaystyle \iiint _{D}\left(x^{2}+y^{2}+z\right)\,dx\,dy\,dz=\iiint _{T}\left(\rho ^{2}+z\right)\rho \,d\rho \,d\varphi \,dz;}$
developing the formula you have

${\displaystyle \int _{-5}^{5}dz\int _{0}^{2\pi }d\varphi \int _{0}^{3}\left(\rho ^{3}+\rho z\right)\,d\rho =2\pi \int _{-5}^{5}\left[{\frac {\rho ^{4}}{4}}+{\frac {\rho ^{2}z}{2}}\right]_{0}^{3}\,dz=2\pi \int _{-5}^{5}\left({\frac {81}{4}}+{\frac {9}{2}}z\right)\,dz=\cdots =405\pi .}$

£#h5#£Spherical coordinates£#/h5#£
In R3 some domains have a spherical symmetry, so it's possible to specify the coordinates of every point of the integration region by two angles and one distance. It's possible to use therefore the passage to spherical coordinates; the function is transformed by this relation:

${\displaystyle f(x,y,z)\longrightarrow f(\rho \cos \theta \sin \varphi ,\rho \sin \theta \sin \varphi ,\rho \cos \varphi )}$
Points on the z-axis do not have a precise characterization in spherical coordinates, so θ can vary between 0 and 2π.

The better integration domain for this passage is the sphere.

Example 4a. The domain is D = x2 + y2 + z2 ≤ 16 (sphere with radius 4 and center at the origin); applying the transformation you get the region

${\displaystyle T=\{0\leq \rho \leq 4,\ 0\leq \varphi \leq \pi ,\ 0\leq \theta \leq 2\pi \}.}$
The Jacobian determinant of this transformation is the following:

${\displaystyle {\frac {\partial (x,y,z)}{\partial (\rho ,\theta ,\varphi )}}={\begin{vmatrix}\cos \theta \sin \varphi &-\rho \sin \theta \sin \varphi &\rho \cos \theta \cos \varphi \\\sin \theta \sin \varphi &\rho \cos \theta \sin \varphi &\rho \sin \theta \cos \varphi \\\cos \varphi &0&-\rho \sin \varphi \end{vmatrix}}=\rho ^{2}\sin \varphi }$
The dx dy dz differentials therefore are transformed to ρ2 sin(φ) dρ dθ dφ.

This yields the final integration formula:

${\displaystyle \iiint _{D}f(x,y,z)\,dx\,dy\,dz=\iiint _{T}f(\rho \sin \varphi \cos \theta ,\rho \sin \varphi \sin \theta ,\rho \cos \varphi )\rho ^{2}\sin \varphi \,d\rho \,d\theta \,d\varphi .}$
It is better to use this method in case of spherical domains and in case of functions that can be easily simplified by the first fundamental relation of trigonometry extended to R3 (see Example 4b); in other cases it can be better to use cylindrical coordinates (see Example 4c).

${\displaystyle \iiint _{T}f(a,b,c)\rho ^{2}\sin \varphi \,d\rho \,d\theta \,d\varphi .}$
The extra ρ2 and sin φ come from the Jacobian.

In the following examples the roles of φ and θ have been reversed.

Example 4b. D is the same region as in Example 4a and f(x, y, z) = x2 + y2 + z2 is the function to integrate. Its transformation is very easy:

${\displaystyle f(\rho \sin \varphi \cos \theta ,\rho \sin \varphi \sin \theta ,\rho \cos \varphi )=\rho ^{2},}$
while we know the intervals of the transformed region T from D:

${\displaystyle T=\{0\leq \rho \leq 4,\ 0\leq \varphi \leq \pi ,\ 0\leq \theta \leq 2\pi \}.}$
We therefore apply the integration formula:

${\displaystyle \iiint _{D}\left(x^{2}+y^{2}+z^{2}\right)\,dx\,dy\,dz=\iiint _{T}\rho ^{2}\,\rho ^{2}\sin \theta \,d\rho \,d\theta \,d\varphi ,}$
and, developing, we get

${\displaystyle \iiint _{T}\rho ^{4}\sin \theta \,d\rho \,d\theta \,d\varphi =\int _{0}^{\pi }\sin \varphi \,d\varphi \int _{0}^{4}\rho ^{4}d\rho \int _{0}^{2\pi }d\theta =2\pi \int _{0}^{\pi }\sin \varphi \left[{\frac {\rho ^{5}}{5}}\right]_{0}^{4}\,d\varphi =2\pi \left[{\frac {\rho ^{5}}{5}}\right]_{0}^{4}{\Big [}-\cos \varphi {\Big ]}_{0}^{\pi }={\frac {4096\pi }{5}}.}$
Example 4c. The domain D is the ball with center at the origin and radius 3a,

${\displaystyle D=\left\{x^{2}+y^{2}+z^{2}\leq 9a^{2}\right\}}$
and f(x, y, z) = x2 + y2 is the function to integrate.

Looking at the domain, it seems convenient to adopt the passage to spherical coordinates, in fact, the intervals of the variables that delimit the new T region are obviously:

${\displaystyle T=\{0\leq \rho \leq 3a,\ 0\leq \varphi \leq 2\pi ,\ 0\leq \theta \leq \pi \}.}$
However, applying the transformation, we get

${\displaystyle f(x,y,z)=x^{2}+y^{2}\longrightarrow \rho ^{2}\sin ^{2}\theta \cos ^{2}\varphi +\rho ^{2}\sin ^{2}\theta \sin ^{2}\varphi =\rho ^{2}\sin ^{2}\theta .}$
Applying the formula for integration we obtain:

${\displaystyle \iiint _{T}\rho ^{2}\sin ^{2}\theta \rho ^{2}\sin \theta \,d\rho \,d\theta \,d\varphi =\iiint _{T}\rho ^{4}\sin ^{3}\theta \,d\rho \,d\theta \,d\varphi }$
which can be solved by turning it into an iterated integral.


${\displaystyle \iiint _{T}\rho ^{4}\sin ^{3}\theta \,d\rho \,d\theta \,d\varphi =\underbrace {\int _{0}^{3a}\rho ^{4}d\rho } _{I}\,\underbrace {\int _{0}^{\pi }\sin ^{3}\theta \,d\theta } _{II}\,\underbrace {\int _{0}^{2\pi }d\varphi } _{III}}$ .

${\displaystyle I=\left.\int _{0}^{3a}\rho ^{4}d\rho ={\frac {\rho ^{5}}{5}}\right\vert _{0}^{3a}={\frac {243}{5}}a^{5}}$ ,

${\displaystyle II=\int _{0}^{\pi }\sin ^{3}\theta \,d\theta =-\int _{0}^{\pi }\sin ^{2}\theta \,d(\cos \theta )=\int _{0}^{\pi }(\cos ^{2}\theta -1)\,d(\cos \theta )=\left.{\frac {\cos ^{3}\theta }{3}}\right|_{0}^{\pi }-\left.\cos \theta \right|_{0}^{\pi }={\frac {4}{3}}}$ ,

${\displaystyle III=\int _{0}^{2\pi }d\varphi =2\pi }$ .


Collecting all parts,

${\displaystyle \iiint _{T}\rho ^{4}\sin ^{3}\theta \,d\rho \,d\theta \,d\varphi =I\cdot II\cdot III={\frac {243}{5}}a^{5}\cdot {\frac {4}{3}}\cdot 2\pi ={\frac {648}{5}}\pi a^{5}}$ .


Alternatively, this problem can be solved by using the passage to cylindrical coordinates. The new T intervals are

${\displaystyle T=\left\{0\leq \rho \leq 3a,\ 0\leq \varphi \leq 2\pi ,\ -{\sqrt {9a^{2}-\rho ^{2}}}\leq z\leq {\sqrt {9a^{2}-\rho ^{2}}}\right\};}$
the z interval has been obtained by dividing the ball into two hemispheres simply by solving the inequality from the formula of D (and then directly transforming x2 + y2 into ρ2). The new function is simply ρ2. Applying the integration formula

${\displaystyle \iiint _{T}\rho ^{2}\rho \,d\rho \,d\varphi \,dz.}$
Then we get

${\displaystyle {\begin{aligned}\int _{0}^{2\pi }d\varphi \int _{0}^{3a}\rho ^{3}d\rho \int _{-{\sqrt {9a^{2}-\rho ^{2}}}}^{\sqrt {9a^{2}-\rho ^{2}}}\,dz&=2\pi \int _{0}^{3a}2\rho ^{3}{\sqrt {9a^{2}-\rho ^{2}}}\,d\rho \\&=-2\pi \int _{9a^{2}}^{0}(9a^{2}-t){\sqrt {t}}\,dt&&t=9a^{2}-\rho ^{2}\\&=2\pi \int _{0}^{9a^{2}}\left(9a^{2}{\sqrt {t}}-t{\sqrt {t}}\right)\,dt\\&=2\pi \left(\int _{0}^{9a^{2}}9a^{2}{\sqrt {t}}\,dt-\int _{0}^{9a^{2}}t{\sqrt {t}}\,dt\right)\\&=2\pi \left[9a^{2}{\frac {2}{3}}t^{\frac {3}{2}}-{\frac {2}{5}}t^{\frac {5}{2}}\right]_{0}^{9a^{2}}\\&=2\cdot 27\pi a^{5}\left(6-{\frac {18}{5}}\right)\\&={\frac {648\pi }{5}}a^{5}.\end{aligned}}}$
Thanks to the passage to cylindrical coordinates it was possible to reduce the triple integral to an easier one-variable integral.

See also the differential volume entry in nabla in cylindrical and spherical coordinates.


£#h5#£Examples£#/h5#£
£#h5#£Double integral over a rectangle£#/h5#£
Let us assume that we wish to integrate a multivariable function f over a region A:

${\displaystyle A=\left\{(x,y)\in \mathbf {R} ^{2}\ :\ 11\leq x\leq 14\ ;\ 7\leq y\leq 10\right\}{\mbox{ and }}f(x,y)=x^{2}+4y\,}$
From this we formulate the iterated integral

${\displaystyle \int _{7}^{10}\int _{11}^{14}(x^{2}+4y)\,dx\,dy}$
The inner integral is performed first, integrating with respect to x and taking y as a constant, as it is not the variable of integration. The result of this integral, which is a function depending only on y, is then integrated with respect to y.

${\displaystyle {\begin{aligned}\int _{11}^{14}\left(x^{2}+4y\right)\,dx&=\left[{\frac {1}{3}}x^{3}+4yx\right]_{x=11}^{x=14}\\&={\frac {1}{3}}(14)^{3}+4y(14)-{\frac {1}{3}}(11)^{3}-4y(11)\\&=471+12y\end{aligned}}}$
We then integrate the result with respect to y.

${\displaystyle {\begin{aligned}\int _{7}^{10}(471+12y)\ dy&={\Big [}471y+6y^{2}{\Big ]}_{y=7}^{y=10}\\&=471(10)+6(10)^{2}-471(7)-6(7)^{2}\\&=1719\end{aligned}}}$
In cases where the double integral of the absolute value of the function is finite, the order of integration is interchangeable, that is, integrating with respect to x first and integrating with respect to y first produce the same result. That is Fubini's theorem. For example, doing the previous calculation with order reversed gives the same result:

${\displaystyle {\begin{aligned}\int _{11}^{14}\int _{7}^{10}\,\left(x^{2}+4y\right)\,dy\,dx&=\int _{11}^{14}{\Big [}x^{2}y+2y^{2}{\Big ]}_{y=7}^{y=10}\,dx\\&=\int _{11}^{14}\,(3x^{2}+102)\,dx\\&={\Big [}x^{3}+102x{\Big ]}_{x=11}^{x=14}\\&=1719.\end{aligned}}}$

£#h5#£Double integral over a normal domain£#/h5#£
Consider the region (please see the graphic in the example):

${\displaystyle D=\{(x,y)\in \mathbf {R} ^{2}\ :\ x\geq 0,y\leq 1,y\geq x^{2}\}}$
Calculate

${\displaystyle \iint _{D}(x+y)\,dx\,dy.}$
This domain is normal with respect to both the x- and y-axes. To apply the formulae it is required to find the functions that determine D and the intervals over which these functions are defined. In this case the two functions are:

${\displaystyle \alpha (x)=x^{2}{\text{ and }}\beta (x)=1}$
while the interval is given by the intersections of the functions with x = 0, so the interval is [a, b] = [0, 1] (normality has been chosen with respect to the x-axis for a better visual understanding).

It is now possible to apply the formula:

${\displaystyle \iint _{D}(x+y)\,dx\,dy=\int _{0}^{1}dx\int _{x^{2}}^{1}(x+y)\,dy=\int _{0}^{1}dx\ \left[xy+{\frac {y^{2}}{2}}\right]_{x^{2}}^{1}}$
(at first the second integral is calculated considering x as a constant). The remaining operations consist of applying the basic techniques of integration:

${\displaystyle \int _{0}^{1}\left[xy+{\frac {y^{2}}{2}}\right]_{x^{2}}^{1}\,dx=\int _{0}^{1}\left(x+{\frac {1}{2}}-x^{3}-{\frac {x^{4}}{2}}\right)dx=\cdots ={\frac {13}{20}}.}$
If we choose normality with respect to the y-axis we could calculate

${\displaystyle \int _{0}^{1}dy\int _{0}^{\sqrt {y}}(x+y)\,dx.}$
and obtain the same value.


£#h5#£Calculating volume£#/h5#£
Using the methods previously described, it is possible to calculate the volumes of some common solids.

£#ul#££#li#£Cylinder: The volume of a cylinder with height h and circular base of radius R can be calculated by integrating the constant function h over the circular base, using polar coordinates.£#/li#££#/ul#£
${\displaystyle \mathrm {Volume} =\int _{0}^{2\pi }d\varphi \,\int _{0}^{R}h\rho \,d\rho =2\pi h\left[{\frac {\rho ^{2}}{2}}\right]_{0}^{R}=\pi R^{2}h}$
This is in agreement with the formula for the volume of a prism

${\displaystyle \mathrm {Volume} ={\text{base area}}\times {\text{height}}.}$
£#ul#££#li#£Sphere: The volume of a sphere with radius R can be calculated by integrating the constant function 1 over the sphere, using spherical coordinates.£#/li#££#/ul#£
${\displaystyle {\begin{aligned}{\text{Volume}}&=\iiint _{D}f(x,y,z)\,dx\,dy\,dz\\&=\iiint _{D}1\,dV\\&=\iiint _{S}\rho ^{2}\sin \varphi \,d\rho \,d\theta \,d\varphi \\&=\int _{0}^{2\pi }\,d\theta \int _{0}^{\pi }\sin \varphi \,d\varphi \int _{0}^{R}\rho ^{2}\,d\rho \\&=2\pi \int _{0}^{\pi }\sin \varphi \,d\varphi \int _{0}^{R}\rho ^{2}\,d\rho \\&=2\pi \int _{0}^{\pi }\sin \varphi {\frac {R^{3}}{3}}\,d\varphi \\&={\frac {2}{3}}\pi R^{3}{\Big [}-\cos \varphi {\Big ]}_{0}^{\pi }={\frac {4}{3}}\pi R^{3}.\end{aligned}}}$
£#ul#££#li#£Tetrahedron (triangular pyramid or 3-simplex): The volume of a tetrahedron with its apex at the origin and edges of length ℓ along the x-, y- and z-axes can be calculated by integrating the constant function 1 over the tetrahedron.£#/li#££#/ul#£
${\displaystyle {\begin{aligned}{\text{Volume}}&=\int _{0}^{\ell }dx\int _{0}^{\ell -x}\,dy\int _{0}^{\ell -x-y}\,dz\\&=\int _{0}^{\ell }dx\int _{0}^{\ell -x}(\ell -x-y)\,dy\\&=\int _{0}^{\ell }\left(l^{2}-2\ell x+x^{2}-{\frac {(\ell -x)^{2}}{2}}\right)\,dx\\&=\ell ^{3}-\ell \ell ^{2}+{\frac {\ell ^{3}}{3}}-\left[{\frac {\ell ^{2}x}{2}}-{\frac {\ell x^{2}}{2}}+{\frac {x^{3}}{6}}\right]_{0}^{\ell }\\&={\frac {\ell ^{3}}{3}}-{\frac {\ell ^{3}}{6}}={\frac {\ell ^{3}}{6}}\end{aligned}}}$
This is in agreement with the formula for the volume of a pyramid
${\displaystyle \mathrm {Volume} ={\frac {1}{3}}\times {\text{base area}}\times {\text{height}}={\frac {1}{3}}\times {\frac {\ell ^{2}}{2}}\times \ell ={\frac {\ell ^{3}}{6}}.}$

£#h5#£Multiple improper integral£#/h5#£
In case of unbounded domains or functions not bounded near the boundary of the domain, we have to introduce the double improper integral or the triple improper integral.


£#h5#£Multiple integrals and iterated integrals£#/h5#£
Fubini's theorem states that if

${\displaystyle \iint _{A\times B}\left|f(x,y)\right|\,d(x,y)<\infty ,}$
that is, if the integral is absolutely convergent, then the multiple integral will give the same result as either of the two iterated integrals:

${\displaystyle \iint _{A\times B}f(x,y)\,d(x,y)=\int _{A}\left(\int _{B}f(x,y)\,dy\right)\,dx=\int _{B}\left(\int _{A}f(x,y)\,dx\right)\,dy.}$
In particular this will occur if |f(x, y)| is a bounded function and A and B are bounded sets.

If the integral is not absolutely convergent, care is needed not to confuse the concepts of multiple integral and iterated integral, especially since the same notation is often used for either concept. The notation

${\displaystyle \int _{0}^{1}\int _{0}^{1}f(x,y)\,dy\,dx}$
means, in some cases, an iterated integral rather than a true double integral. In an iterated integral, the outer integral

${\displaystyle \int _{0}^{1}\cdots \,dx}$
is the integral with respect to x of the following function of x:

${\displaystyle g(x)=\int _{0}^{1}f(x,y)\,dy.}$
A double integral, on the other hand, is defined with respect to area in the xy-plane. If the double integral exists, then it is equal to each of the two iterated integrals (either "dy dx" or "dx dy") and one often computes it by computing either of the iterated integrals. But sometimes the two iterated integrals exist when the double integral does not, and in some such cases the two iterated integrals are different numbers, i.e., one has

${\displaystyle \int _{0}^{1}\int _{0}^{1}f(x,y)\,dy\,dx\neq \int _{0}^{1}\int _{0}^{1}f(x,y)\,dx\,dy.}$
This is an instance of rearrangement of a conditionally convergent integral.

On the other hand, some conditions ensure that the two iterated integrals are equal even though the double integral need not exist. By the Fichtenholz–Lichtenstein theorem, if f is bounded on [0, 1] × [0, 1] and both iterated integrals exist, then they are equal. Moreover, existence of the inner integrals ensures existence of the outer integrals. The double integral need not exist in this case even as Lebesgue integral, according to Sierpiński.

The notation

${\displaystyle \int _{[0,1]\times [0,1]}f(x,y)\,dx\,dy}$
may be used if one wishes to be emphatic about intending a double integral rather than an iterated integral.


£#h5#£Some practical applications£#/h5#£
Quite generally, just as in one variable, one can use the multiple integral to find the average of a function over a given set. Given a set D ⊆ Rn and an integrable function f over D, the average value of f over its domain is given by

${\displaystyle {\bar {f}}={\frac {1}{m(D)}}\int _{D}f(x)\,dx,}$
where m(D) is the measure of D.

Additionally, multiple integrals are used in many applications in physics. The examples below also show some variations in the notation.

In mechanics, the moment of inertia is calculated as the volume integral (triple integral) of the density weighed with the square of the distance from the axis:

${\displaystyle I_{z}=\iiint _{V}\rho r^{2}\,dV.}$
The gravitational potential associated with a mass distribution given by a mass measure dm on three-dimensional Euclidean space R3 is

${\displaystyle V(\mathbf {x} )=-\iiint _{\mathbf {R} ^{3}}{\frac {G}{|\mathbf {x} -\mathbf {y} |}}\,dm(\mathbf {y} ).}$
If there is a continuous function ρ(x) representing the density of the distribution at x, so that dm(x) = ρ(x)d3x, where d3x is the Euclidean volume element, then the gravitational potential is

${\displaystyle V(\mathbf {x} )=-\iiint _{\mathbf {R} ^{3}}{\frac {G}{|\mathbf {x} -\mathbf {y} |}}\,\rho (\mathbf {y} )\,d^{3}\mathbf {y} .}$
In electromagnetism, Maxwell's equations can be written using multiple integrals to calculate the total magnetic and electric fields. In the following example, the electric field produced by a distribution of charges given by the volume charge density ρ( r→ ) is obtained by a triple integral of a vector function:

${\displaystyle {\vec {E}}={\frac {1}{4\pi \varepsilon _{0}}}\iiint {\frac {{\vec {r}}-{\vec {r}}'}{\left\|{\vec {r}}-{\vec {r}}'\right\|^{3}}}\rho ({\vec {r}}')\,d^{3}r'.}$
This can also be written as an integral with respect to a signed measure representing the charge distribution.


£#h5#£See also£#/h5#£ £#ul#££#li#£Main analysis theorems that relate multiple integrals: £#ul#££#li#£Divergence theorem£#/li#£ £#li#£Stokes' theorem£#/li#£ £#li#£Green's theorem£#/li#££#/ul#££#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£Further reading£#/h5#£ £#ul#££#li#£Adams, Robert A. (2003). Calculus: A Complete Course (5th ed.). ISBN 0-201-79131-5.£#/li#£ £#li#£Jain, R. K.; Iyengar, S. R. K. (2009). Advanced Engineering Mathematics (3rd ed.). Narosa Publishing House. ISBN 978-81-7319-730-7.£#/li#£ £#li#£Herman, Edwin “Jed” & Strang, Gilbert (2016): Calculus : Volume 3 : OpenStax, Rice University, Houston, Texas, USA. ISBN 978-1-50669-805-2. (PDF)£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Multiple Integral". MathWorld.£#/li#£ £#li#£L.D. Kudryavtsev (2001) [1994], "Multiple integral", Encyclopedia of Mathematics, EMS Press£#/li#£ £#li#£Mathematical Assistant on Web online evaluation of double integrals in Cartesian coordinates and polar coordinates (includes intermediate steps in the solution, powered by Maxima (software))£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Calculus > Integrals > Definite Integrals £#/li#££#/ul#£




£#h3#£Area Principle£#/h3#£

£#h5#£ References £#/h5#£ £#ul#££#li#£Grünbaum, B. and Shepard, G. C. "Ceva, Menelaus, and the Area Principle." Math. Mag. 68, 254-268, 1995.£#/li#££#li#£Krantz, S. G. "Schlicht Functions." §12.1.1 in Handbook of Complex Variables. Boston, MA: Birkhäuser, p. 149, 1999.£#/li#££#li#£ Grünbaum, B. and Shepard, G. C. "Ceva, Menelaus, and the Area Principle." Math. Mag. 68, 254-268, 1995. £#/li#££#li#£ Krantz, S. G. "Schlicht Functions." §12.1.1 in Handbook of Complex Variables. Boston, MA: Birkhäuser, p. 149, 1999. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Geometry > Plane Geometry > Triangles > Triangle Properties £#/li#££#li#£ Calculus and Analysis > Complex Analysis > General Complex Analysis £#/li#££#/ul#£




£#h3#£Argand Diagram£#/h3#£

In mathematics, a complex number is an element of a number system that extends the real numbers with a specific element denoted i, called the imaginary unit and satisfying the equation i2 = −1; every complex number can be expressed in the form a + bi, where a and b are real numbers. Because no real number satisfies the above equation, i was called an imaginary number by René Descartes. For the complex number a + bi, a is called the real part and b is called the imaginary part. The set of complex numbers is denoted by either of the symbols $${\displaystyle \mathbb {C} }$$ or C. Despite the historical nomenclature "imaginary", complex numbers are regarded in the mathematical sciences as just as "real" as the real numbers and are fundamental in many aspects of the scientific description of the natural world.

Complex numbers allow solutions to all polynomial equations, even those that have no solutions in real numbers. More precisely, the fundamental theorem of algebra asserts that every non-constant polynomial equation with real or complex coefficients has a solution which is a complex number. For example, the equation ${\displaystyle (x+1)^{2}=-9}$ has no real solution, since the square of a real number cannot be negative, but has the two nonreal complex solutions −1 + 3i and −1 − 3i.

Addition, subtraction and multiplication of complex numbers can be naturally defined by using the rule i2 = −1 combined with the associative, commutative and distributive laws. Every nonzero complex number has a multiplicative inverse. This makes the complex numbers a field that has the real numbers as a subfield. The complex numbers also form a real vector space of dimension two, with {1, i} as a standard basis.

This standard basis makes the complex numbers a Cartesian plane, called the complex plane. This allows a geometric interpretation of the complex numbers and their operations, and conversely expressing in terms of complex numbers some geometric properties and constructions. For example, the real numbers form the real line which is identified to the horizontal axis of the complex plane. The complex numbers of absolute value one form the unit circle. The addition of a complex number is a translation in the complex plane, and the multiplication by a complex number is a similarity centered at the origin. The complex conjugation is the reflection symmetry with respect to the real axis. The complex absolute value is a Euclidean norm.

In summary, the complex numbers form a rich structure that is simultaneously an algebraically closed field, a commutative algebra over the reals, and a Euclidean vector space of dimension two.


£#h5#£Definition£#/h5#£
A complex number is a number of the form a + bi, where a and b are real numbers, and i is an indeterminate satisfying i2 = −1. For example, 2 + 3i is a complex number.

This way, a complex number is defined as a polynomial with real coefficients in the single indeterminate i, for which the relation i2 + 1 = 0 is imposed. Based on this definition, complex numbers can be added and multiplied, using the addition and multiplication for polynomials. The relation i2 + 1 = 0 induces the equalities i4k = 1, i4k+1 = i, i4k+2 = −1, and i4k+3 = −i, which hold for all integers k; these allow the reduction of any polynomial that results from the addition and multiplication of complex numbers to a linear polynomial in i, again of the form a + bi with real coefficients a, b.

The real number a is called the real part of the complex number a + bi; the real number b is called its imaginary part. To emphasize, the imaginary part does not include a factor i; that is, the imaginary part is b, not bi.

Formally, the complex numbers are defined as the quotient ring of the polynomial ring in the indeterminate i, by the ideal generated by the polynomial i2 + 1 (see below).


£#h5#£Notation£#/h5#£
A real number a can be regarded as a complex number a + 0i, whose imaginary part is 0. A purely imaginary number bi is a complex number 0 + bi, whose real part is zero. As with polynomials, it is common to write a for a + 0i and bi for 0 + bi. Moreover, when the imaginary part is negative, that is, b = −|b| < 0, it is common to write a − |b|i instead of a + (−|b|)i; for example, for b = −4, 3 − 4i can be written instead of 3 + (−4)i.

Since the multiplication of the indeterminate i and a real is commutative in polynomials with real coefficients, the polynomial a + bi may be written as a + ib. This is often expedient for imaginary parts denoted by expressions, for example, when b is a radical.

The real part of a complex number z is denoted by Re(z), ${\displaystyle {\mathcal {Re}}(z)}$ , or ${\displaystyle {\mathfrak {R}}(z)}$ ; the imaginary part of a complex number z is denoted by Im(z), ${\displaystyle {\mathcal {Im}}(z)}$ , or ${\displaystyle {\mathfrak {I}}(z).}$ For example,

The set of all complex numbers is denoted by $${\displaystyle \mathbb {C} }$$ (blackboard bold) or C (upright bold).

In some disciplines, particularly in electromagnetism and electrical engineering, j is used instead of i as i is frequently used to represent electric current. In these cases, complex numbers are written as a + bj, or a + jb.


£#h5#£Visualization£#/h5#£
A complex number z can thus be identified with an ordered pair ${\displaystyle (\Re (z),\Im (z))}$ of real numbers, which in turn may be interpreted as coordinates of a point in a two-dimensional space. The most immediate space is the Euclidean plane with suitable coordinates, which is then called complex plane or Argand diagram, named after Jean-Robert Argand. Another prominent space on which the coordinates may be projected is the two-dimensional surface of a sphere, which is then called Riemann sphere.


£#h5#£Cartesian complex plane£#/h5#£
The definition of the complex numbers involving two arbitrary real values immediately suggests the use of Cartesian coordinates in the complex plane. The horizontal (real) axis is generally used to display the real part, with increasing values to the right, and the imaginary part marks the vertical (imaginary) axis, with increasing values upwards.

A charted number may be viewed either as the coordinatized point or as a position vector from the origin to this point. The coordinate values of a complex number z can hence be expressed in its Cartesian, rectangular, or algebraic form.

Notably, the operations of addition and multiplication take on a very natural geometric character, when complex numbers are viewed as position vectors: addition corresponds to vector addition, while multiplication (see below) corresponds to multiplying their magnitudes and adding the angles they make with the real axis. Viewed in this way, the multiplication of a complex number by i corresponds to rotating the position vector counterclockwise by a quarter turn (90°) about the origin—a fact which can be expressed algebraically as follows:


£#h5#£Polar complex plane £#/h5#£
£#h5#£Modulus and argument£#/h5#£
An alternative option for coordinates in the complex plane is the polar coordinate system that uses the distance of the point z from the origin (O), and the angle subtended between the positive real axis and the line segment Oz in a counterclockwise sense. This leads to the polar form

${\displaystyle z=re^{i\varphi }=r(\cos \varphi +i\sin \varphi )}$
of a complex number, where r is the absolute value of z, and ${\displaystyle \varphi }$ is the argument of z.

The absolute value (or modulus or magnitude) of a complex number z = x + yi is

If z is a real number (that is, if y = 0), then r = |x|. That is, the absolute value of a real number equals its absolute value as a complex number.
By Pythagoras' theorem, the absolute value of a complex number is the distance to the origin of the point representing the complex number in the complex plane.

The argument of z (in many applications referred to as the "phase" φ) is the angle of the radius Oz with the positive real axis, and is written as arg z. As with the modulus, the argument can be found from the rectangular form x + yi—by applying the inverse tangent to the quotient of imaginary-by-real parts. By using a half-angle identity, a single branch of the arctan suffices to cover the range (−π, π] of the arg-function, and avoids a more subtle case-by-case analysis

Normally, as given above, the principal value in the interval (−π, π] is chosen. If the arg value is negative, values in the range (−π, π] or [0, 2π) can be obtained by adding 2π. The value of φ is expressed in radians in this article. It can increase by any integer multiple of 2π and still give the same angle, viewed as subtended by the rays of the positive real axis and from the origin through z. Hence, the arg function is sometimes considered as multivalued. The polar angle for the complex number 0 is indeterminate, but arbitrary choice of the polar angle 0 is common.

The value of φ equals the result of atan2:

Together, r and φ give another way of representing complex numbers, the polar form, as the combination of modulus and argument fully specify the position of a point on the plane. Recovering the original rectangular co-ordinates from the polar form is done by the formula called trigonometric form

Using Euler's formula this can be written as

Using the cis function, this is sometimes abbreviated to

In angle notation, often used in electronics to represent a phasor with amplitude r and phase φ, it is written as


£#h5#£Complex graphs£#/h5#£
When visualizing complex functions, both a complex input and output are needed. Because each complex number is represented in two dimensions, visually graphing a complex function would require the perception of a four dimensional space, which is possible only in projections. Because of this, other ways of visualizing complex functions have been designed.

In domain coloring the output dimensions are represented by color and brightness, respectively. Each point in the complex plane as domain is ornated, typically with color representing the argument of the complex number, and brightness representing the magnitude. Dark spots mark moduli near zero, brighter spots are farther away from the origin, the gradation may be discontinuous, but is assumed as monotonous. The colors often vary in steps of π/3 for 0 to 2π from red, yellow, green, cyan, blue, to magenta. These plots are called color wheel graphs. This provides a simple way to visualize the functions without losing information. The picture shows zeros for ±1, (2 + i) and poles at ${\displaystyle \pm {\sqrt {-2-2i}}.}$


£#h5#£History£#/h5#£
The solution in radicals (without trigonometric functions) of a general cubic equation, when all three of its roots are real numbers, contains the square roots of negative numbers, a situation that cannot be rectified by factoring aided by the rational root test, if the cubic is irreducible; this is the so-called casus irreducibilis ("irreducible case"). This conundrum led Italian mathematician Gerolamo Cardano to conceive of complex numbers in around 1545 in his Ars Magna, though his understanding was rudimentary; moreover he later dismissed complex numbers as "subtle as they are useless".

Work on the problem of general polynomials ultimately led to the fundamental theorem of algebra, which shows that with complex numbers, a solution exists to every polynomial equation of degree one or higher. Complex numbers thus form an algebraically closed field, where any polynomial equation has a root.

Many mathematicians contributed to the development of complex numbers. The rules for addition, subtraction, multiplication, and root extraction of complex numbers were developed by the Italian mathematician Rafael Bombelli. A more abstract formalism for the complex numbers was further developed by the Irish mathematician William Rowan Hamilton, who extended this abstraction to the theory of quaternions.

The earliest fleeting reference to square roots of negative numbers can perhaps be said to occur in the work of the Greek mathematician Hero of Alexandria in the 1st century AD, where in his Stereometrica he considered, apparently in error, the volume of an impossible frustum of a pyramid to arrive at the term ${\displaystyle {\sqrt {81-144}}}$ in his calculations, which today would simplify to ${\displaystyle {\sqrt {-63}}}$ . Negative quantities were not conceived of in Hellenistic mathematics and Hero merely replaced it by its positive ${\displaystyle {\sqrt {144-81}}=3{\sqrt {7}}.}$

The impetus to study complex numbers as a topic in itself first arose in the 16th century when algebraic solutions for the roots of cubic and quartic polynomials were discovered by Italian mathematicians (see Niccolò Fontana Tartaglia, Gerolamo Cardano). It was soon realized (but proved much later) that these formulas, even if one was interested only in real solutions, sometimes required the manipulation of square roots of negative numbers. As an example, Tartaglia's formula for a cubic equation of the form x3 = px + q gives the solution to the equation x3 = x as

At first glance this looks like nonsense. However, formal calculations with complex numbers show that the equation z3 = i has three solutions: ${\displaystyle -i,{\frac {{\sqrt {3}}+i}{2}},{\frac {-{\sqrt {3}}+i}{2}}.}$ Substituting these in turn for ${\displaystyle {\sqrt {-1}}^{1/3}}$ in Tartaglia's cubic formula and simplifying, one gets 0, 1 and −1 as the solutions of x3 − x = 0. Of course this particular equation can be solved at sight but it does illustrate that when general formulas are used to solve cubic equations with real roots then, as later mathematicians showed rigorously, the use of complex numbers is unavoidable. Rafael Bombelli was the first to address explicitly these seemingly paradoxical solutions of cubic equations and developed the rules for complex arithmetic trying to resolve these issues.

The term "imaginary" for these quantities was coined by René Descartes in 1637, who was at pains to stress their unreal nature

... sometimes only imaginary, that is one can imagine as many as I said in each equation, but sometimes there exists no quantity that matches that which we imagine.
[... quelquefois seulement imaginaires c'est-à-dire que l'on peut toujours en imaginer autant que j'ai dit en chaque équation, mais qu'il n'y a quelquefois aucune quantité qui corresponde à celle qu'on imagine.]

A further source of confusion was that the equation ${\displaystyle {\sqrt {-1}}^{2}={\sqrt {-1}}{\sqrt {-1}}=-1}$ seemed to be capriciously inconsistent with the algebraic identity ${\displaystyle {\sqrt {a}}{\sqrt {b}}={\sqrt {ab}}}$ , which is valid for non-negative real numbers a and b, and which was also used in complex number calculations with one of a, b positive and the other negative. The incorrect use of this identity (and the related identity ${\textstyle {\frac {1}{\sqrt {a}}}={\sqrt {\frac {1}{a}}}}$ ) in the case when both a and b are negative even bedeviled Leonhard Euler. This difficulty eventually led to the convention of using the special symbol i in place of ${\displaystyle {\sqrt {-1}}}$ to guard against this mistake. Even so, Euler considered it natural to introduce students to complex numbers much earlier than we do today. In his elementary algebra text book, Elements of Algebra, he introduces these numbers almost at once and then uses them in a natural way throughout.

In the 18th century complex numbers gained wider use, as it was noticed that formal manipulation of complex expressions could be used to simplify calculations involving trigonometric functions. For instance, in 1730 Abraham de Moivre noted that the complicated identities relating trigonometric functions of an integer multiple of an angle to powers of trigonometric functions of that angle could be simply re-expressed by the following well-known formula which bears his name, de Moivre's formula:

In 1748, Euler went further and obtained Euler's formula of complex analysis:

by formally manipulating complex power series and observed that this formula could be used to reduce any trigonometric identity to much simpler exponential identities.

The idea of a complex number as a point in the complex plane (above) was first described by Danish–Norwegian mathematician Caspar Wessel in 1799, although it had been anticipated as early as 1685 in Wallis's A Treatise of Algebra.

Wessel's memoir appeared in the Proceedings of the Copenhagen Academy but went largely unnoticed. In 1806 Jean-Robert Argand independently issued a pamphlet on complex numbers and provided a rigorous proof of the fundamental theorem of algebra. Carl Friedrich Gauss had earlier published an essentially topological proof of the theorem in 1797 but expressed his doubts at the time about "the true metaphysics of the square root of −1". It was not until 1831 that he overcame these doubts and published his treatise on complex numbers as points in the plane, largely establishing modern notation and terminology:

If one formerly contemplated this subject from a false point of view and therefore found a mysterious darkness, this is in large part attributable to clumsy terminology. Had one not called +1, -1, ${\displaystyle {\sqrt {-1}}}$ positive, negative, or imaginary (or even impossible) units, but instead, say, direct, inverse, or lateral units, then there could scarcely have been talk of such darkness.

In the beginning of the 19th century, other mathematicians discovered independently the geometrical representation of the complex numbers: Buée, Mourey, Warren, Français and his brother, Bellavitis.

The English mathematician G.H. Hardy remarked that Gauss was the first mathematician to use complex numbers in 'a really confident and scientific way' although mathematicians such as Norwegian Niels Henrik Abel and Carl Gustav Jacob Jacobi were necessarily using them routinely before Gauss published his 1831 treatise.

Augustin-Louis Cauchy and Bernhard Riemann together brought the fundamental ideas of complex analysis to a high state of completion, commencing around 1825 in Cauchy's case.

The common terms used in the theory are chiefly due to the founders. Argand called cos φ + i sin φ the direction factor, and ${\displaystyle r={\sqrt {a^{2}+b^{2}}}}$ the modulus; Cauchy (1821) called cos φ + i sin φ the reduced form (l'expression réduite) and apparently introduced the term argument; Gauss used i for ${\displaystyle {\sqrt {-1}}}$ , introduced the term complex number for a + bi, and called a2 + b2 the norm. The expression direction coefficient, often used for cos φ + i sin φ, is due to Hankel (1867), and absolute value, for modulus, is due to Weierstrass.

Later classical writers on the general theory include Richard Dedekind, Otto Hölder, Felix Klein, Henri Poincaré, Hermann Schwarz, Karl Weierstrass and many others. Important work (including a systematization) in complex multivariate calculus has been started at beginning of the 20th century. Important results have been achieved by Wilhelm Wirtinger in 1927.


£#h5#£Relations and operations£#/h5#£
£#h5#£Equality£#/h5#£
Complex numbers have a similar definition of equality to real numbers; two complex numbers a1 + b1i and a2 + b2i are equal if and only if both their real and imaginary parts are equal, that is, if a1 = a2 and b1 = b2. Nonzero complex numbers written in polar form are equal if and only if they have the same magnitude and their arguments differ by an integer multiple of 2π.


£#h5#£Ordering£#/h5#£
Unlike the real numbers, there is no natural ordering of the complex numbers. In particular, there is no linear ordering on the complex numbers that is compatible with addition and multiplication. Hence, the complex numbers do not have the structure of an ordered field. One explanation for this is that every non-trivial sum of squares in an ordered field is nonzero, and i2 + 12 = 0 is a non-trivial sum of squares. Thus, complex numbers are naturally thought of as existing on a two-dimensional plane.


£#h5#£Conjugate£#/h5#£
The complex conjugate of the complex number z = x + yi is given by x − yi. It is denoted by either z or z*. This unary operation on complex numbers cannot be expressed by applying only their basic operations addition, subtraction, multiplication and division.

Geometrically, z is the "reflection" of z about the real axis. Conjugating twice gives the original complex number

which makes this operation an involution. The reflection leaves both the real part and the magnitude of z unchanged, that is

and ${\displaystyle \quad |{\overline {z}}|=|z|.}$
The imaginary part and the argument of a complex number z change their sign under conjugation

For details on argument and magnitude, see the section on Polar form.

The product of a complex number z = x + yi and its conjugate is known as the absolute square. It is always a non-negative real number and equals the square of the magnitude of each:

This property can be used to convert a fraction with a complex denominator to an equivalent fraction with a real denominator by expanding both numerator and denominator of the fraction by the conjugate of the given denominator. This process is sometimes called "rationalization" of the denominator (although the denominator in the final expression might be an irrational real number), because it resembles the method to remove roots from simple expressions in a denominator.

The real and imaginary parts of a complex number z can be extracted using the conjugation:

Moreover, a complex number is real if and only if it equals its own conjugate.
Conjugation distributes over the basic complex arithmetic operations:

Conjugation is also employed in inversive geometry, a branch of geometry studying reflections more general than ones about a line. In the network analysis of electrical circuits, the complex conjugate is used in finding the equivalent impedance when the maximum power transfer theorem is looked for.


£#h5#£Addition and subtraction£#/h5#£
Two complex numbers ${\displaystyle a=x+yi}$ and ${\displaystyle b=u+vi}$ are most easily added by separately adding their real and imaginary parts. That is to say:

Similarly, subtraction can be performed as
Multiplication of a complex number ${\displaystyle a=x+yi}$ and a real number r can be done similarly by multiplying separately r and the real and imaginary parts of a:

In particular, subtraction can be done by negating the subtrahend (that is multiplying it with –1) and adding the result to the minuend:
Using the visualization of complex numbers in the complex plane, addition has the following geometric interpretation: the sum of two complex numbers a and b, interpreted as points in the complex plane, is the point obtained by building a parallelogram from the three vertices O, and the points of the arrows labeled a and b (provided that they are not on a line). Equivalently, calling these points A, B, respectively and the fourth point of the parallelogram X the triangles OAB and XBA are congruent.


£#h5#£Multiplication and square£#/h5#£
The rules of the distributive property, the commutative properties (of addition and multiplication), and the defining property i2 = −1 apply to complex numbers. It follows that

In particular,


£#h5#£Reciprocal and division£#/h5#£
Using the conjugation, the reciprocal of a nonzero complex number z = x + yi can always be broken down to

since non-zero implies that x2 + y2 is greater than zero.

This can be used to express a division of an arbitrary complex number w = u + vi by a non-zero complex number z as


£#h5#£Multiplication and division in polar form£#/h5#£
Formulas for multiplication, division and exponentiation are simpler in polar form than the corresponding formulas in Cartesian coordinates. Given two complex numbers z1 = r1(cos φ1 + i sin φ1) and z2 = r2(cos φ2 + i sin φ2), because of the trigonometric identities

we may derive

In other words, the absolute values are multiplied and the arguments are added to yield the polar form of the product. For example, multiplying by i corresponds to a quarter-turn counter-clockwise, which gives back i2 = −1. The picture at the right illustrates the multiplication of Since the real and imaginary part of 5 + 5i are equal, the argument of that number is 45 degrees, or π/4 (in radian). On the other hand, it is also the sum of the angles at the origin of the red and blue triangles are arctan(1/3) and arctan(1/2), respectively. Thus, the formula holds. As the arctan function can be approximated highly efficiently, formulas like this – known as Machin-like formulas – are used for high-precision approximations of π.
Similarly, division is given by


£#h5#£Square root£#/h5#£
The square roots of a + bi (with b ≠ 0) are ${\displaystyle \pm (\gamma +\delta i)}$ , where

and

where sgn is the signum function. This can be seen by squaring ${\displaystyle \pm (\gamma +\delta i)}$ to obtain a + bi. Here ${\displaystyle {\sqrt {a^{2}+b^{2}}}}$ is called the modulus of a + bi, and the square root sign indicates the square root with non-negative real part, called the principal square root; also ${\displaystyle {\sqrt {a^{2}+b^{2}}}={\sqrt {z{\overline {z}}}},}$ where z = a + bi.


£#h5#£Exponential function£#/h5#£
The exponential function ${\displaystyle \exp \colon \mathbb {C} \to \mathbb {C} ;z\mapsto \exp z}$ can be defined for every complex number z by the power series

which has an infinite radius of convergence.
The value at 1 of the exponential function is Euler's number

If z is real, one has ${\displaystyle \exp z=e^{z}.}$ Analytic continuation allows extending this equality for every complex value of z, and thus to define the complex exponentiation with base e as
£#h5#£Functional equation£#/h5#£
The exponential function satisfies the functional equation ${\displaystyle e^{z+t}=e^{z}e^{t}.}$ This can be proved either by comparing the power series expansion of both members or by applying analytic continuation from the restriction of the equation to real arguments.


£#h5#£Euler's formula£#/h5#£
Euler's formula states that, for any real number y,

The functional equation implies thus that, if x and y are real, one has

which is the decomposition of the exponential function into its real and imaginary parts.
£#h5#£Complex logarithm£#/h5#£
In the real case, the natural logarithm can be defined as the inverse ${\displaystyle \ln \colon \mathbb {R} ^{+}\to \mathbb {R} ;x\mapsto \ln x}$ of the exponential function. For extending this to the complex domain, one can start from Euler's formula. It implies that, if a complex number ${\displaystyle z\in \mathbb {C} ^{\times }}$ is written in polar form

with ${\displaystyle r,\varphi \in \mathbb {R} ,}$ then with as complex logarithm one has a proper inverse:
However, because cosine and sine are periodic functions, the addition of an integer multiple of 2π to φ does not change z. For example, eiπ = e3iπ = −1 , so both iπ and 3iπ are possible values for the natural logarithm of −1.

Therefore, if the complex logarithm is not to be defined as a multivalued function

one has to use a branch cut and to restrict the codomain, resulting in the bijective function
If ${\displaystyle z\in \mathbb {C} \setminus \left(-\mathbb {R} _{\geq 0}\right)}$ is not a non-positive real number (a positive or a non-real number), the resulting principal value of the complex logarithm is obtained with −π < φ < π. It is an analytic function outside the negative real numbers, but it cannot be prolongated to a function that is continuous at any negative real number ${\displaystyle z\in -\mathbb {R} ^{+}}$ , where the principal value is ln z = ln(−z) + iπ.


£#h5#£Exponentiation£#/h5#£
If x > 0 is real and z complex, the exponentiation is defined as

where ln denotes the natural logarithm.
It seems natural to extend this formula to complex values of x, but there are some difficulties resulting from the fact that the complex logarithm is not really a function, but a multivalued function.

It follows that if z is as above, and if t is another complex number, then the exponentiation is the multivalued function


£#h5#£Integer and fractional exponents£#/h5#£
If, in the preceding formula, t is an integer, then the sine and the cosine are independent of k. Thus, if the exponent n is an integer, then zn is well defined, and the exponentiation formula simplifies to de Moivre's formula:

The n nth roots of a complex number z are given by

for 0 ≤ k ≤ n − 1. (Here ${\displaystyle {\sqrt[{n}]{r}}}$ is the usual (positive) nth root of the positive real number r.) Because sine and cosine are periodic, other integer values of k do not give other values.
While the nth root of a positive real number r is chosen to be the positive real number c satisfying cn = r, there is no natural way of distinguishing one particular complex nth root of a complex number. Therefore, the nth root is a n-valued function of z. This implies that, contrary to the case of positive real numbers, one has

since the left-hand side consists of n values, and the right-hand side is a single value.
£#h5#£Properties£#/h5#£
£#h5#£Field structure£#/h5#£
The set $${\displaystyle \mathbb {C} }$$ of complex numbers is a field. Briefly, this means that the following facts hold: first, any two complex numbers can be added and multiplied to yield another complex number. Second, for any complex number z, its additive inverse –z is also a complex number; and third, every nonzero complex number has a reciprocal complex number. Moreover, these operations satisfy a number of laws, for example the law of commutativity of addition and multiplication for any two complex numbers z1 and z2:

These two laws and the other requirements on a field can be proven by the formulas given above, using the fact that the real numbers themselves form a field.
Unlike the reals, $${\displaystyle \mathbb {C} }$$ is not an ordered field, that is to say, it is not possible to define a relation z1 < z2 that is compatible with the addition and multiplication. In fact, in any ordered field, the square of any element is necessarily positive, so i2 = −1 precludes the existence of an ordering on ${\displaystyle \mathbb {C} .}$

When the underlying field for a mathematical topic or construct is the field of complex numbers, the topic's name is usually modified to reflect that fact. For example: complex analysis, complex matrix, complex polynomial, and complex Lie algebra.


£#h5#£Solutions of polynomial equations£#/h5#£
Given any complex numbers (called coefficients) a0, ..., an, the equation

has at least one complex solution z, provided that at least one of the higher coefficients a1, ..., an is nonzero. This is the statement of the fundamental theorem of algebra, of Carl Friedrich Gauss and Jean le Rond d'Alembert. Because of this fact, $${\displaystyle \mathbb {C} }$$ is called an algebraically closed field. This property does not hold for the field of rational numbers ${\displaystyle \mathbb {Q} }$ (the polynomial x2 − 2 does not have a rational root, since √2 is not a rational number) nor the real numbers ${\displaystyle \mathbb {R} }$ (the polynomial x2 + a does not have a real root for a > 0, since the square of x is positive for any real number x).
There are various proofs of this theorem, by either analytic methods such as Liouville's theorem, or topological ones such as the winding number, or a proof combining Galois theory and the fact that any real polynomial of odd degree has at least one real root.

Because of this fact, theorems that hold for any algebraically closed field apply to ${\displaystyle \mathbb {C} .}$ For example, any non-empty complex square matrix has at least one (complex) eigenvalue.


£#h5#£Algebraic characterization£#/h5#£
The field $${\displaystyle \mathbb {C} }$$ has the following three properties:

£#ul#££#li#£First, it has characteristic 0. This means that 1 + 1 + ⋯ + 1 ≠ 0 for any number of summands (all of which equal one).£#/li#£ £#li#£Second, its transcendence degree over ${\displaystyle \mathbb {Q} }$ , the prime field of ${\displaystyle \mathbb {C} ,}$ is the cardinality of the continuum.£#/li#£ £#li#£Third, it is algebraically closed (see above).£#/li#££#/ul#£
It can be shown that any field having these properties is isomorphic (as a field) to ${\displaystyle \mathbb {C} .}$ For example, the algebraic closure of the field ${\displaystyle \mathbb {Q} _{p}}$ of the p-adic number also satisfies these three properties, so these two fields are isomorphic (as fields, but not as topological fields). Also, $${\displaystyle \mathbb {C} }$$ is isomorphic to the field of complex Puiseux series. However, specifying an isomorphism requires the axiom of choice. Another consequence of this algebraic characterization is that $${\displaystyle \mathbb {C} }$$ contains many proper subfields that are isomorphic to $${\displaystyle \mathbb {C} }$$ .


£#h5#£Characterization as a topological field£#/h5#£
The preceding characterization of $${\displaystyle \mathbb {C} }$$ describes only the algebraic aspects of ${\displaystyle \mathbb {C} .}$ That is to say, the properties of nearness and continuity, which matter in areas such as analysis and topology, are not dealt with. The following description of $${\displaystyle \mathbb {C} }$$ as a topological field (that is, a field that is equipped with a topology, which allows the notion of convergence) does take into account the topological properties. $${\displaystyle \mathbb {C} }$$ contains a subset P (namely the set of positive real numbers) of nonzero elements satisfying the following three conditions:

£#ul#££#li#£P is closed under addition, multiplication and taking inverses.£#/li#£ £#li#£If x and y are distinct elements of P, then either x − y or y − x is in P.£#/li#£ £#li#£If S is any nonempty subset of P, then S + P = x + P for some x in ${\displaystyle \mathbb {C} .}$ £#/li#££#/ul#£
Moreover, $${\displaystyle \mathbb {C} }$$ has a nontrivial involutive automorphism x ↦ x* (namely the complex conjugation), such that x x* is in P for any nonzero x in ${\displaystyle \mathbb {C} .}$

Any field F with these properties can be endowed with a topology by taking the sets B(x, p) = { y | p − (y − x)(y − x)* ∈ P }  as a base, where x ranges over the field and p ranges over P. With this topology F is isomorphic as a topological field to ${\displaystyle \mathbb {C} .}$

The only connected locally compact topological fields are ${\displaystyle \mathbb {R} }$ and ${\displaystyle \mathbb {C} .}$ This gives another characterization of $${\displaystyle \mathbb {C} }$$ as a topological field, since $${\displaystyle \mathbb {C} }$$ can be distinguished from ${\displaystyle \mathbb {R} }$ because the nonzero complex numbers are connected, while the nonzero real numbers are not.


£#h5#£Formal construction£#/h5#£
£#h5#£Construction as ordered pairs£#/h5#£
William Rowan Hamilton introduced the approach to define the set $${\displaystyle \mathbb {C} }$$ of complex numbers as the set ${\displaystyle \mathbb {R} ^{2}}$ of ordered pairs (a, b) of real numbers, in which the following rules for addition and multiplication are imposed:

It is then just a matter of notation to express (a, b) as a + bi.


£#h5#£Construction as a quotient field£#/h5#£
Though this low-level construction does accurately describe the structure of the complex numbers, the following equivalent definition reveals the algebraic nature of $${\displaystyle \mathbb {C} }$$ more immediately. This characterization relies on the notion of fields and polynomials. A field is a set endowed with addition, subtraction, multiplication and division operations that behave as is familiar from, say, rational numbers. For example, the distributive law

must hold for any three elements x, y and z of a field. The set ${\displaystyle \mathbb {R} }$ of real numbers does form a field. A polynomial p(X) with real coefficients is an expression of the form where the a0, ..., an are real numbers. The usual addition and multiplication of polynomials endows the set ${\displaystyle \mathbb {R} [X]}$ of all such polynomials with a ring structure. This ring is called the polynomial ring over the real numbers.
The set of complex numbers is defined as the quotient ring ${\displaystyle \mathbb {R} [X]/(X^{2}+1).}$ This extension field contains two square roots of −1, namely (the cosets of) X and −X, respectively. (The cosets of) 1 and X form a basis of ${\displaystyle \mathbb {R} [X]/(X^{2}+1)}$ as a real vector space, which means that each element of the extension field can be uniquely written as a linear combination in these two elements. Equivalently, elements of the extension field can be written as ordered pairs (a, b) of real numbers. The quotient ring is a field, because X2 + 1 is irreducible over ${\displaystyle \mathbb {R} ,}$ so the ideal it generates is maximal.

The formulas for addition and multiplication in the ring ${\displaystyle \mathbb {R} [X],}$ modulo the relation X2 = −1, correspond to the formulas for addition and multiplication of complex numbers defined as ordered pairs. So the two definitions of the field $${\displaystyle \mathbb {C} }$$ are isomorphic (as fields).

Accepting that $${\displaystyle \mathbb {C} }$$ is algebraically closed, since it is an algebraic extension of ${\displaystyle \mathbb {R} }$ in this approach, $${\displaystyle \mathbb {C} }$$ is therefore the algebraic closure of ${\displaystyle \mathbb {R} .}$


£#h5#£Matrix representation of complex numbers£#/h5#£
Complex numbers a + bi can also be represented by 2 × 2 matrices that have the form:

Here the entries a and b are real numbers. As the sum and product of two such matrices is again of this form, these matrices form a subring of the ring 2 × 2 matrices.
A simple computation shows that the map:

is a ring isomorphism from the field of complex numbers to the ring of these matrices. This isomorphism associates the square of the absolute value of a complex number with the determinant of the corresponding matrix, and the conjugate of a complex number with the transpose of the matrix.
The geometric description of the multiplication of complex numbers can also be expressed in terms of rotation matrices by using this correspondence between complex numbers and such matrices. The action of the matrix on a vector (x, y) corresponds to the multiplication of x + iy by a + ib. In particular, if the determinant is 1, there is a real number t such that the matrix has the form:

In this case, the action of the matrix on vectors and the multiplication by the complex number ${\displaystyle \cos t+i\sin t}$ are both the rotation of the angle t.
£#h5#£Complex analysis£#/h5#£
The study of functions of a complex variable is known as complex analysis and has enormous practical use in applied mathematics as well as in other branches of mathematics. Often, the most natural proofs for statements in real analysis or even number theory employ techniques from complex analysis (see prime number theorem for an example). Unlike real functions, which are commonly represented as two-dimensional graphs, complex functions have four-dimensional graphs and may usefully be illustrated by color-coding a three-dimensional graph to suggest four dimensions, or by animating the complex function's dynamic transformation of the complex plane.


£#h5#£Complex exponential and related functions£#/h5#£
The notions of convergent series and continuous functions in (real) analysis have natural analogs in complex analysis. A sequence of complex numbers is said to converge if and only if its real and imaginary parts do. This is equivalent to the (ε, δ)-definition of limits, where the absolute value of real numbers is replaced by the one of complex numbers. From a more abstract point of view, $${\displaystyle \mathbb {C} }$$ , endowed with the metric

is a complete metric space, which notably includes the triangle inequality for any two complex numbers z1 and z2.
Like in real analysis, this notion of convergence is used to construct a number of elementary functions: the exponential function exp z, also written ez, is defined as the infinite series

The series defining the real trigonometric functions sine and cosine, as well as the hyperbolic functions sinh and cosh, also carry over to complex arguments without change. For the other trigonometric and hyperbolic functions, such as tangent, things are slightly more complicated, as the defining series do not converge for all complex values. Therefore, one must define them either in terms of sine, cosine and exponential, or, equivalently, by using the method of analytic continuation.

Euler's formula states:

for any real number φ, in particular , which is Euler's identity. Unlike in the situation of real numbers, there is an infinitude of complex solutions z of the equation for any complex number w ≠ 0. It can be shown that any such solution z – called complex logarithm of w – satisfies where arg is the argument defined above, and ln the (real) natural logarithm. As arg is a multivalued function, unique only up to a multiple of 2π, log is also multivalued. The principal value of log is often taken by restricting the imaginary part to the interval (−π, π].
Complex exponentiation zω is defined as

and is multi-valued, except when ω is an integer. For ω = 1 / n, for some natural number n, this recovers the non-uniqueness of nth roots mentioned above.
Complex numbers, unlike real numbers, do not in general satisfy the unmodified power and logarithm identities, particularly when naïvely treated as single-valued functions; see failure of power and logarithm identities. For example, they do not satisfy

Both sides of the equation are multivalued by the definition of complex exponentiation given here, and the values on the left are a subset of those on the right.
£#h5#£Holomorphic functions£#/h5#£
A function f: $${\displaystyle \mathbb {C} }$$ → $${\displaystyle \mathbb {C} }$$ is called holomorphic if it satisfies the Cauchy–Riemann equations. For example, any ${\displaystyle \mathbb {R} }$ -linear map $${\displaystyle \mathbb {C} }$$ → $${\displaystyle \mathbb {C} }$$ can be written in the form

with complex coefficients a and b. This map is holomorphic if and only if b = 0. The second summand ${\displaystyle b{\overline {z}}}$ is real-differentiable, but does not satisfy the Cauchy–Riemann equations.
Complex analysis shows some features not apparent in real analysis. For example, any two holomorphic functions f and g that agree on an arbitrarily small open subset of $${\displaystyle \mathbb {C} }$$ necessarily agree everywhere. Meromorphic functions, functions that can locally be written as f(z)/(z − z0)n with a holomorphic function f, still share some of the features of holomorphic functions. Other functions have essential singularities, such as sin(1/z) at z = 0.


£#h5#£Applications£#/h5#£
Complex numbers have applications in many scientific areas, including signal processing, control theory, electromagnetism, fluid dynamics, quantum mechanics, cartography, and vibration analysis. Some of these applications are described below.


£#h5#£Geometry£#/h5#£
£#h5#£Shapes£#/h5#£
Three non-collinear points ${\displaystyle u,v,w}$ in the plane determine the shape of the triangle ${\displaystyle \{u,v,w\}}$ . Locating the points in the complex plane, this shape of a triangle may be expressed by complex arithmetic as

The shape ${\displaystyle S}$ of a triangle will remain the same, when the complex plane is transformed by translation or dilation (by an affine transformation), corresponding to the intuitive notion of shape, and describing similarity. Thus each triangle ${\displaystyle \{u,v,w\}}$ is in a similarity class of triangles with the same shape.
£#h5#£Fractal geometry£#/h5#£
The Mandelbrot set is a popular example of a fractal formed on the complex plane. It is defined by plotting every location ${\displaystyle c}$ where iterating the sequence ${\displaystyle f_{c}(z)=z^{2}+c}$ does not diverge when iterated infinitely. Similarly, Julia sets have the same rules, except where ${\displaystyle c}$ remains constant.


£#h5#£Triangles£#/h5#£
Every triangle has a unique Steiner inellipse – an ellipse inside the triangle and tangent to the midpoints of the three sides of the triangle. The foci of a triangle's Steiner inellipse can be found as follows, according to Marden's theorem: Denote the triangle's vertices in the complex plane as a = xA + yAi, b = xB + yBi, and c = xC + yCi. Write the cubic equation ${\displaystyle (x-a)(x-b)(x-c)=0}$ , take its derivative, and equate the (quadratic) derivative to zero. Marden's Theorem says that the solutions of this equation are the complex numbers denoting the locations of the two foci of the Steiner inellipse.


£#h5#£Algebraic number theory£#/h5#£
As mentioned above, any nonconstant polynomial equation (in complex coefficients) has a solution in $${\displaystyle \mathbb {C} }$$ . A fortiori, the same is true if the equation has rational coefficients. The roots of such equations are called algebraic numbers – they are a principal object of study in algebraic number theory. Compared to ${\displaystyle {\overline {\mathbb {Q} }}}$ , the algebraic closure of ${\displaystyle \mathbb {Q} }$ , which also contains all algebraic numbers, $${\displaystyle \mathbb {C} }$$ has the advantage of being easily understandable in geometric terms. In this way, algebraic methods can be used to study geometric questions and vice versa. With algebraic methods, more specifically applying the machinery of field theory to the number field containing roots of unity, it can be shown that it is not possible to construct a regular nonagon using only compass and straightedge – a purely geometric problem.

Another example is the Gaussian integers; that is, numbers of the form x + iy, where x and y are integers, which can be used to classify sums of squares.


£#h5#£Analytic number theory£#/h5#£
Analytic number theory studies numbers, often integers or rationals, by taking advantage of the fact that they can be regarded as complex numbers, in which analytic methods can be used. This is done by encoding number-theoretic information in complex-valued functions. For example, the Riemann zeta function ζ(s) is related to the distribution of prime numbers.


£#h5#£Improper integrals£#/h5#£
In applied fields, complex numbers are often used to compute certain real-valued improper integrals, by means of complex-valued functions. Several methods exist to do this; see methods of contour integration.


£#h5#£Dynamic equations£#/h5#£
In differential equations, it is common to first find all complex roots r of the characteristic equation of a linear differential equation or equation system and then attempt to solve the system in terms of base functions of the form f(t) = ert. Likewise, in difference equations, the complex roots r of the characteristic equation of the difference equation system are used, to attempt to solve the system in terms of base functions of the form f(t) = rt.


£#h5#£Linear Algebra£#/h5#£
Eigendecomposition is a useful tool for computing matrix powers and matrix exponentials. However, it often requires the use of complex numbers, even if the matrix is real (for example, a rotation matrix).

Complex numbers often generalize concepts originally conceived in the real numbers. For example, the conjugate transpose generalizes the transpose, hermitian matrices generalize symmetric matrices, and unitary matrices generalize orthogonal matrices.


£#h5#£In applied mathematics£#/h5#£
£#h5#£Control theory£#/h5#£
In control theory, systems are often transformed from the time domain to the complex frequency domain using the Laplace transform. The system's zeros and poles are then analyzed in the complex plane. The root locus, Nyquist plot, and Nichols plot techniques all make use of the complex plane.

In the root locus method, it is important whether zeros and poles are in the left or right half planes, that is, have real part greater than or less than zero. If a linear, time-invariant (LTI) system has poles that are

£#ul#££#li#£in the right half plane, it will be unstable,£#/li#£ £#li#£all in the left half plane, it will be stable,£#/li#£ £#li#£on the imaginary axis, it will have marginal stability.£#/li#££#/ul#£
If a system has zeros in the right half plane, it is a nonminimum phase system.


£#h5#£Signal analysis£#/h5#£
Complex numbers are used in signal analysis and other fields for a convenient description for periodically varying signals. For given real functions representing actual physical quantities, often in terms of sines and cosines, corresponding complex functions are considered of which the real parts are the original quantities. For a sine wave of a given frequency, the absolute value |z| of the corresponding z is the amplitude and the argument arg z is the phase.

If Fourier analysis is employed to write a given real-valued signal as a sum of periodic functions, these periodic functions are often written as complex-valued functions of the form

and

where ω represents the angular frequency and the complex number A encodes the phase and amplitude as explained above.

This use is also extended into digital signal processing and digital image processing, which utilize digital versions of Fourier analysis (and wavelet analysis) to transmit, compress, restore, and otherwise process digital audio signals, still images, and video signals.

Another example, relevant to the two side bands of amplitude modulation of AM radio, is:


£#h5#£In physics£#/h5#£
£#h5#£Electromagnetism and electrical engineering£#/h5#£
In electrical engineering, the Fourier transform is used to analyze varying voltages and currents. The treatment of resistors, capacitors, and inductors can then be unified by introducing imaginary, frequency-dependent resistances for the latter two and combining all three in a single complex number called the impedance. This approach is called phasor calculus.

In electrical engineering, the imaginary unit is denoted by j, to avoid confusion with I, which is generally in use to denote electric current, or, more particularly, i, which is generally in use to denote instantaneous electric current.

Since the voltage in an AC circuit is oscillating, it can be represented as

To obtain the measurable quantity, the real part is taken:

The complex-valued signal V(t) is called the analytic representation of the real-valued, measurable signal v(t).


£#h5#£Fluid dynamics£#/h5#£
In fluid dynamics, complex functions are used to describe potential flow in two dimensions.


£#h5#£Quantum mechanics£#/h5#£
The complex number field is intrinsic to the mathematical formulations of quantum mechanics, where complex Hilbert spaces provide the context for one such formulation that is convenient and perhaps most standard. The original foundation formulas of quantum mechanics – the Schrödinger equation and Heisenberg's matrix mechanics – make use of complex numbers.


£#h5#£Relativity£#/h5#£
In special and general relativity, some formulas for the metric on spacetime become simpler if one takes the time component of the spacetime continuum to be imaginary. (This approach is no longer standard in classical relativity, but is used in an essential way in quantum field theory.) Complex numbers are essential to spinors, which are a generalization of the tensors used in relativity.


£#h5#£Generalizations and related notions£#/h5#£
The process of extending the field ${\displaystyle \mathbb {R} }$ of reals to $${\displaystyle \mathbb {C} }$$ is known as the Cayley–Dickson construction. It can be carried further to higher dimensions, yielding the quaternions ${\displaystyle \mathbb {H} }$ and octonions ${\displaystyle \mathbb {O} }$ which (as a real vector space) are of dimension 4 and 8, respectively. In this context the complex numbers have been called the binarions.

Just as by applying the construction to reals the property of ordering is lost, properties familiar from real and complex numbers vanish with each extension. The quaternions lose commutativity, that is, x·y ≠ y·x for some quaternions x, y, and the multiplication of octonions, additionally to not being commutative, fails to be associative: (x·y)·z ≠ x·(y·z) for some octonions x, y, z.

Reals, complex numbers, quaternions and octonions are all normed division algebras over ${\displaystyle \mathbb {R} }$ . By Hurwitz's theorem they are the only ones; the sedenions, the next step in the Cayley–Dickson construction, fail to have this structure.

The Cayley–Dickson construction is closely related to the regular representation of ${\displaystyle \mathbb {C} ,}$ thought of as an ${\displaystyle \mathbb {R} }$ -algebra (an ${\displaystyle \mathbb {R} }$ -vector space with a multiplication), with respect to the basis (1, i). This means the following: the ${\displaystyle \mathbb {R} }$ -linear map

for some fixed complex number w can be represented by a 2 × 2 matrix (once a basis has been chosen). With respect to the basis (1, i), this matrix is that is, the one mentioned in the section on matrix representation of complex numbers above. While this is a linear representation of $${\displaystyle \mathbb {C} }$$ in the 2 × 2 real matrices, it is not the only one. Any matrix has the property that its square is the negative of the identity matrix: J2 = −I. Then is also isomorphic to the field ${\displaystyle \mathbb {C} ,}$ and gives an alternative complex structure on ${\displaystyle \mathbb {R} ^{2}.}$ This is generalized by the notion of a linear complex structure.
Hypercomplex numbers also generalize ${\displaystyle \mathbb {R} ,}$ ${\displaystyle \mathbb {C} ,}$ ${\displaystyle \mathbb {H} ,}$ and ${\displaystyle \mathbb {O} .}$ For example, this notion contains the split-complex numbers, which are elements of the ring ${\displaystyle \mathbb {R} [x]/(x^{2}-1)}$ (as opposed to ${\displaystyle \mathbb {R} [x]/(x^{2}+1)}$ for complex numbers). In this ring, the equation a2 = 1 has four solutions.

The field ${\displaystyle \mathbb {R} }$ is the completion of ${\displaystyle \mathbb {Q} ,}$ the field of rational numbers, with respect to the usual absolute value metric. Other choices of metrics on ${\displaystyle \mathbb {Q} }$ lead to the fields ${\displaystyle \mathbb {Q} _{p}}$ of p-adic numbers (for any prime number p), which are thereby analogous to ${\displaystyle \mathbb {R} }$ . There are no other nontrivial ways of completing ${\displaystyle \mathbb {Q} }$ than ${\displaystyle \mathbb {R} }$ and ${\displaystyle \mathbb {Q} _{p},}$ by Ostrowski's theorem. The algebraic closures ${\displaystyle {\overline {\mathbb {Q} _{p}}}}$ of ${\displaystyle \mathbb {Q} _{p}}$ still carry a norm, but (unlike $${\displaystyle \mathbb {C} }$$ ) are not complete with respect to it. The completion ${\displaystyle \mathbb {C} _{p}}$ of ${\displaystyle {\overline {\mathbb {Q} _{p}}}}$ turns out to be algebraically closed. By analogy, the field is called p-adic complex numbers.

The fields ${\displaystyle \mathbb {R} ,}$ ${\displaystyle \mathbb {Q} _{p},}$ and their finite field extensions, including ${\displaystyle \mathbb {C} ,}$ are called local fields.


£#h5#£See also£#/h5#£ £#ul#££#li#£Algebraic surface£#/li#£ £#li#£Circular motion using complex numbers£#/li#£ £#li#£Complex-base system£#/li#£ £#li#£Complex geometry£#/li#£ £#li#£Dual-complex number£#/li#£ £#li#£Eisenstein integer£#/li#£ £#li#£Euler's identity£#/li#£ £#li#£Geometric algebra (which includes the complex plane as the 2-dimensional spinor subspace ${\displaystyle {\mathcal {G}}_{2}^{+}}$ )£#/li#£ £#li#£Unit complex number£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£
£#h5#£Works cited£#/h5#£
£#h5#£Further reading£#/h5#£ £#ul#££#li#£Penrose, Roger (2005). The Road to Reality: A complete guide to the laws of the universe. Alfred A. Knopf. ISBN 978-0-679-45443-4.£#/li#£ £#li#£Derbyshire, John (2006). Unknown Quantity: A real and imaginary history of algebra. Joseph Henry Press. ISBN 978-0-309-09657-7.£#/li#£ £#li#£Needham, Tristan (1997). Visual Complex Analysis. Clarendon Press. ISBN 978-0-19-853447-1.£#/li#££#/ul#£
£#h5#£Mathematical£#/h5#£
£#h5#£Historical£#/h5#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Argand, R. Essai sur une manière de représenter les quantités imaginaires dans les constructions géométriques. Paris: Albert Blanchard, 1971. Reprint of the 2nd ed., published by G. J. Hoel in 1874. First edition published Paris, 1806.£#/li#££#li#£Mazur, B. Imagining Numbers (Particularly the Square Root of Minus Fifteen). Farrar, Straus and Giroux, 2003.£#/li#££#li#£Wells, D. The Penguin Dictionary of Curious and Interesting Numbers. Middlesex, England: Penguin Books, p. 23, 1986.£#/li#££#li#£ Argand, R. Essai sur une manière de représenter les quantités imaginaires dans les constructions géométriques. Paris: Albert Blanchard, 1971. Reprint of the 2nd ed., published by G. J. Hoel in 1874. First edition published Paris, 1806. £#/li#££#li#£ Mazur, B. Imagining Numbers (Particularly the Square Root of Minus Fifteen). Farrar, Straus and Giroux, 2003. £#/li#££#li#£ Wells, D. The Penguin Dictionary of Curious and Interesting Numbers. Middlesex, England: Penguin Books, p. 23, 1986. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Complex Analysis > Complex Numbers £#/li#££#/ul#£




£#h3#£Argand Plane£#/h3#£

In mathematics, the complex plane is the plane formed by the complex numbers, with a Cartesian coordinate system such that the x-axis, called real axis, is formed by the real numbers, and the y-axis, called imaginary axis, is formed by the imaginary numbers.

The complex plane allows a geometric interpretation of complex numbers. Under addition, they add like vectors. The multiplication of two complex numbers can be expressed more easily in polar coordinates—the magnitude or modulus of the product is the product of the two absolute values, or moduli, and the angle or argument of the product is the sum of the two angles, or arguments. In particular, multiplication by a complex number of modulus 1 acts as a rotation.

The complex plane is sometimes known as the Argand plane or Gauss plane.


£#h5#£Notational conventions£#/h5#£
£#h5#£Complex numbers£#/h5#£
In complex analysis, the complex numbers are customarily represented by the symbol z, which can be separated into its real (x) and imaginary (y) parts:

for example: z = 4 + 5i, where x and y are real numbers, and i is the imaginary unit. In this customary notation the complex number z corresponds to the point (x, y) in the Cartesian plane.

In the Cartesian plane the point (x, y) can also be represented in polar coordinates as

In the Cartesian plane it may be assumed that the arctangent takes values from −π/2 to π/2 (in radians), and some care must be taken to define the more complete arctangent function for points (x, y) when x ≤ 0. In the complex plane these polar coordinates take the form

where

Here |z| is the absolute value or modulus of the complex number z; θ, the argument of z, is usually taken on the interval 0 ≤ θ < 2π; and the last equality (to |z|eiθ) is taken from Euler's formula. Without the constraint on the range of θ, the argument of z is multi-valued, because the complex exponential function is periodic, with period 2π i. Thus, if θ is one value of arg(z), the other values are given by arg(z) = θ + 2nπ, where n is any non-zero integer.

While seldom used explicitly, the geometric view of the complex numbers is implicitly based on its structure of a Euclidean vector space of dimension 2, where the inner product of complex numbers w and z is given by ${\displaystyle \Re (w{\overline {z}})}$ ; then for a complex number z its absolute value |z| coincides with its Euclidean norm, and its argument arg(z) with the angle turning from 1 to z.

The theory of contour integration comprises a major part of complex analysis. In this context, the direction of travel around a closed curve is important – reversing the direction in which the curve is traversed multiplies the value of the integral by −1. By convention the positive direction is counterclockwise. For example, the unit circle is traversed in the positive direction when we start at the point z = 1, then travel up and to the left through the point z = i, then down and to the left through −1, then down and to the right through −i, and finally up and to the right to z = 1, where we started.

Almost all of complex analysis is concerned with complex functions – that is, with functions that map some subset of the complex plane into some other (possibly overlapping, or even identical) subset of the complex plane. Here it is customary to speak of the domain of f(z) as lying in the z-plane, while referring to the range of f(z) as a set of points in the w-plane. In symbols we write

and often think of the function f as a transformation from the z-plane (with coordinates (x, y)) into the w-plane (with coordinates (u, v)).


£#h5#£Complex plane notation£#/h5#£
Complex plane is denoted as ${\displaystyle \mathbb {C} }$ .


£#h5#£Argand diagram£#/h5#£
Argand diagram refers to a geometric plot of complex numbers as points z = x + iy using the x-axis as the real axis and y-axis as the imaginary axis. Such plots are named after Jean-Robert Argand (1768–1822), although they were first described by Norwegian–Danish land surveyor and mathematician Caspar Wessel (1745–1818). Argand diagrams are frequently used to plot the positions of the zeros and poles of a function in the complex plane.


£#h5#£Stereographic projections£#/h5#£
It can be useful to think of the complex plane as if it occupied the surface of a sphere. Given a sphere of unit radius, place its center at the origin of the complex plane, oriented so that the equator on the sphere coincides with the unit circle in the plane, and the north pole is "above" the plane.

We can establish a one-to-one correspondence between the points on the surface of the sphere minus the north pole and the points in the complex plane as follows. Given a point in the plane, draw a straight line connecting it with the north pole on the sphere. That line will intersect the surface of the sphere in exactly one other point. The point z = 0 will be projected onto the south pole of the sphere. Since the interior of the unit circle lies inside the sphere, that entire region (|z| < 1) will be mapped onto the southern hemisphere. The unit circle itself (|z| = 1) will be mapped onto the equator, and the exterior of the unit circle (|z| > 1) will be mapped onto the northern hemisphere, minus the north pole. Clearly this procedure is reversible – given any point on the surface of the sphere that is not the north pole, we can draw a straight line connecting that point to the north pole and intersecting the flat plane in exactly one point.

Under this stereographic projection the north pole itself is not associated with any point in the complex plane. We perfect the one-to-one correspondence by adding one more point to the complex plane – the so-called point at infinity – and identifying it with the north pole on the sphere. This topological space, the complex plane plus the point at infinity, is known as the extended complex plane. We speak of a single "point at infinity" when discussing complex analysis. There are two points at infinity (positive, and negative) on the real number line, but there is only one point at infinity (the north pole) in the extended complex plane.

Imagine for a moment what will happen to the lines of latitude and longitude when they are projected from the sphere onto the flat plane. The lines of latitude are all parallel to the equator, so they will become perfect circles centered on the origin z = 0. And the lines of longitude will become straight lines passing through the origin (and also through the "point at infinity", since they pass through both the north and south poles on the sphere).

This is not the only possible yet plausible stereographic situation of the projection of a sphere onto a plane consisting of two or more values. For instance, the north pole of the sphere might be placed on top of the origin z = −1 in a plane that is tangent to the circle. The details don't really matter. Any stereographic projection of a sphere onto a plane will produce one "point at infinity", and it will map the lines of latitude and longitude on the sphere into circles and straight lines, respectively, in the plane.


£#h5#£Cutting the plane£#/h5#£
When discussing functions of a complex variable it is often convenient to think of a cut in the complex plane. This idea arises naturally in several different contexts.


£#h5#£Multi-valued relationships and branch points£#/h5#£
Consider the simple two-valued relationship

Before we can treat this relationship as a single-valued function, the range of the resulting value must be restricted somehow. When dealing with the square roots of non-negative real numbers this is easily done. For instance, we can just define

to be the non-negative real number y such that y2 = x. This idea doesn't work so well in the two-dimensional complex plane. To see why, let's think about the way the value of f(z) varies as the point z moves around the unit circle. We can write

Evidently, as z moves all the way around the circle, w only traces out one-half of the circle. So one continuous motion in the complex plane has transformed the positive square root e0 = 1 into the negative square root eiπ = −1.

This problem arises because the point z = 0 has just one square root, while every other complex number z ≠ 0 has exactly two square roots. On the real number line we could circumvent this problem by erecting a "barrier" at the single point x = 0. A bigger barrier is needed in the complex plane, to prevent any closed contour from completely encircling the branch point z = 0. This is commonly done by introducing a branch cut; in this case the "cut" might extend from the point z = 0 along the positive real axis to the point at infinity, so that the argument of the variable z in the cut plane is restricted to the range 0 ≤ arg(z) < 2π.

We can now give a complete description of w = z1⁄2. To do so we need two copies of the z-plane, each of them cut along the real axis. On one copy we define the square root of 1 to be e0 = 1, and on the other we define the square root of 1 to be eiπ = −1. We call these two copies of the complete cut plane sheets. By making a continuity argument we see that the (now single-valued) function w = z1⁄2 maps the first sheet into the upper half of the w-plane, where 0 ≤ arg(w) < π, while mapping the second sheet into the lower half of the w-plane (where π ≤ arg(w) < 2π).

The branch cut in this example doesn't have to lie along the real axis. It doesn't even have to be a straight line. Any continuous curve connecting the origin z = 0 with the point at infinity would work. In some cases the branch cut doesn't even have to pass through the point at infinity. For example, consider the relationship

Here the polynomial z2 − 1 vanishes when z = ±1, so g evidently has two branch points. We can "cut" the plane along the real axis, from −1 to 1, and obtain a sheet on which g(z) is a single-valued function. Alternatively, the cut can run from z = 1 along the positive real axis through the point at infinity, then continue "up" the negative real axis to the other branch point, z = −1.

This situation is most easily visualized by using the stereographic projection described above. On the sphere one of these cuts runs longitudinally through the southern hemisphere, connecting a point on the equator (z = −1) with another point on the equator (z = 1), and passing through the south pole (the origin, z = 0) on the way. The second version of the cut runs longitudinally through the northern hemisphere and connects the same two equatorial points by passing through the north pole (that is, the point at infinity).


£#h5#£Restricting the domain of meromorphic functions£#/h5#£
A meromorphic function is a complex function that is holomorphic and therefore analytic everywhere in its domain except at a finite, or countably infinite, number of points. The points at which such a function cannot be defined are called the poles of the meromorphic function. Sometimes all of these poles lie in a straight line. In that case mathematicians may say that the function is "holomorphic on the cut plane". Here's a simple example.

The gamma function, defined by

where γ is the Euler–Mascheroni constant, and has simple poles at 0, −1, −2, −3, ... because exactly one denominator in the infinite product vanishes when z is zero, or a negative integer. Since all its poles lie on the negative real axis, from z = 0 to the point at infinity, this function might be described as "holomorphic on the cut plane, the cut extending along the negative real axis, from 0 (inclusive) to the point at infinity."

Alternatively, Γ(z) might be described as "holomorphic in the cut plane with −π < arg(z) < π and excluding the point z = 0."

This cut is slightly different from the branch cut we've already encountered, because it actually excludes the negative real axis from the cut plane. The branch cut left the real axis connected with the cut plane on one side (0 ≤ θ), but severed it from the cut plane along the other side (θ < 2π).

Of course, it's not actually necessary to exclude the entire line segment from z = 0 to −∞ to construct a domain in which Γ(z) is holomorphic. All we really have to do is puncture the plane at a countably infinite set of points {0, −1, −2, −3, ...}. But a closed contour in the punctured plane might encircle one or more of the poles of Γ(z), giving a contour integral that is not necessarily zero, by the residue theorem. By cutting the complex plane we ensure not only that Γ(z) is holomorphic in this restricted domain – we also ensure that the contour integral of Γ over any closed curve lying in the cut plane is identically equal to zero.


£#h5#£Specifying convergence regions£#/h5#£
Many complex functions are defined by infinite series, or by continued fractions. A fundamental consideration in the analysis of these infinitely long expressions is identifying the portion of the complex plane in which they converge to a finite value. A cut in the plane may facilitate this process, as the following examples show.

Consider the function defined by the infinite series

Since z2 = (−z)2 for every complex number z, it's clear that f(z) is an even function of z, so the analysis can be restricted to one half of the complex plane. And since the series is undefined when

it makes sense to cut the plane along the entire imaginary axis and establish the convergence of this series where the real part of z is not zero before undertaking the more arduous task of examining f(z) when z is a pure imaginary number.

In this example the cut is a mere convenience, because the points at which the infinite sum is undefined are isolated, and the cut plane can be replaced with a suitably punctured plane. In some contexts the cut is necessary, and not just convenient. Consider the infinite periodic continued fraction

It can be shown that f(z) converges to a finite value if and only if z is not a negative real number such that z < −1⁄4. In other words, the convergence region for this continued fraction is the cut plane, where the cut runs along the negative real axis, from −1⁄4 to the point at infinity.


£#h5#£Gluing the cut plane back together£#/h5#£
We have already seen how the relationship

can be made into a single-valued function by splitting the domain of f into two disconnected sheets. It is also possible to "glue" those two sheets back together to form a single Riemann surface on which f(z) = z1/2 can be defined as a holomorphic function whose image is the entire w-plane (except for the point w = 0). Here's how that works.

Imagine two copies of the cut complex plane, the cuts extending along the positive real axis from z = 0 to the point at infinity. On one sheet define 0 ≤ arg(z) < 2π, so that 11/2 = e0 = 1, by definition. On the second sheet define 2π ≤ arg(z) < 4π, so that 11/2 = eiπ = −1, again by definition. Now flip the second sheet upside down, so the imaginary axis points in the opposite direction of the imaginary axis on the first sheet, with both real axes pointing in the same direction, and "glue" the two sheets together (so that the edge on the first sheet labeled "θ = 0" is connected to the edge labeled "θ < 4π" on the second sheet, and the edge on the second sheet labeled "θ = 2π" is connected to the edge labeled "θ < 2π" on the first sheet). The result is the Riemann surface domain on which f(z) = z1/2 is single-valued and holomorphic (except when z = 0).

To understand why f is single-valued in this domain, imagine a circuit around the unit circle, starting with z = 1 on the first sheet. When 0 ≤ θ < 2π we are still on the first sheet. When θ = 2π we have crossed over onto the second sheet, and are obliged to make a second complete circuit around the branch point z = 0 before returning to our starting point, where θ = 4π is equivalent to θ = 0, because of the way we glued the two sheets together. In other words, as the variable z makes two complete turns around the branch point, the image of z in the w-plane traces out just one complete circle.

Formal differentiation shows that

from which we can conclude that the derivative of f exists and is finite everywhere on the Riemann surface, except when z = 0 (that is, f is holomorphic, except when z = 0).

How can the Riemann surface for the function

also discussed above, be constructed? Once again we begin with two copies of the z-plane, but this time each one is cut along the real line segment extending from z = −1 to z = 1 – these are the two branch points of g(z). We flip one of these upside down, so the two imaginary axes point in opposite directions, and glue the corresponding edges of the two cut sheets together. We can verify that g is a single-valued function on this surface by tracing a circuit around a circle of unit radius centered at z = 1. Commencing at the point z = 2 on the first sheet we turn halfway around the circle before encountering the cut at z = 0. The cut forces us onto the second sheet, so that when z has traced out one full turn around the branch point z = 1, w has taken just one-half of a full turn, the sign of w has been reversed (since eiπ = −1), and our path has taken us to the point z = 2 on the second sheet of the surface. Continuing on through another half turn we encounter the other side of the cut, where z = 0, and finally reach our starting point (z = 2 on the first sheet) after making two full turns around the branch point.

The natural way to label θ = arg(z) in this example is to set −π < θ ≤ π on the first sheet, with π < θ ≤ 3π on the second. The imaginary axes on the two sheets point in opposite directions so that the counterclockwise sense of positive rotation is preserved as a closed contour moves from one sheet to the other (remember, the second sheet is upside down). Imagine this surface embedded in a three-dimensional space, with both sheets parallel to the xy-plane. Then there appears to be a vertical hole in the surface, where the two cuts are joined together. What if the cut is made from z = −1 down the real axis to the point at infinity, and from z = 1, up the real axis until the cut meets itself? Again a Riemann surface can be constructed, but this time the "hole" is horizontal. Topologically speaking, both versions of this Riemann surface are equivalent – they are orientable two-dimensional surfaces of genus one.


£#h5#£Use in control theory£#/h5#£
In control theory, one use of the complex plane is known as the s-plane. It is used to visualise the roots of the equation describing a system's behaviour (the characteristic equation) graphically. The equation is normally expressed as a polynomial in the parameter 's' of the Laplace transform, hence the name 's' plane. Points in the s-plane take the form ${\displaystyle s=\sigma +j\omega }$ , where 'j' is used instead of the usual 'i' to represent the imaginary component.

Another related use of the complex plane is with the Nyquist stability criterion. This is a geometric principle which allows the stability of a closed-loop feedback system to be determined by inspecting a Nyquist plot of its open-loop magnitude and phase response as a function of frequency (or loop transfer function) in the complex plane.

The z-plane is a discrete-time version of the s-plane, where z-transforms are used instead of the Laplace transformation.


£#h5#£Quadratic spaces£#/h5#£
The complex plane is associated with two distinct quadratic spaces. For a point z = x + iy in the complex plane, the squaring function z2 and the norm-squared ${\displaystyle x^{2}+y^{2}}$ are both quadratic forms. The former is frequently neglected in the wake of the latter's use in setting a metric on the complex plane. These distinct faces of the complex plane as a quadratic space arise in the construction of algebras over a field with the Cayley–Dickson process. That procedure can be applied to any field, and different results occur for the fields R and C: when R is the take-off field, then C is constructed with the quadratic form ${\displaystyle x^{2}+y^{2},}$ but the process can also begin with C and z2, and that case generates algebras that differ from those derived from R. In any case, the algebras generated are composition algebras; in this case the complex plane is the point set for two distinct composition algebras.


£#h5#£Other meanings of "complex plane"£#/h5#£
The preceding sections of this article deal with the complex plane in terms of a geometric representation of the complex numbers. Although this usage of the term "complex plane" has a long and mathematically rich history, it is by no means the only mathematical concept that can be characterized as "the complex plane". There are at least three additional possibilities.

£#li#£Two-dimensional complex vector space, a "complex plane" in the sense that it is a two-dimensional vector space whose coordinates are complex numbers. See also: Complex affine space § Two dimensions.£#/li#£ £#li#£(1 + 1)-dimensional Minkowski space, also known as the split-complex plane, is a "complex plane" in the sense that the algebraic split-complex numbers can be separated into two real components that are easily associated with the point (x, y) in the Cartesian plane.£#/li#£ £#li#£The set of dual numbers over the reals can also be placed into one-to-one correspondence with the points (x, y) of the Cartesian plane, and represent another example of a "complex plane".£#/li#£

£#h5#£See also£#/h5#£ £#ul#££#li#£Constellation diagram£#/li#£ £#li#£Riemann sphere£#/li#£ £#li#£s-plane£#/li#£ £#li#£In-phase and quadrature components£#/li#£ £#li#£Real line£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£
£#h5#£Works Cited£#/h5#£ £#ul#££#li#£Flanigan, Francis J. (1983). Complex Variables: Harmonic and Analytic Functions. Dover. ISBN 0-486-61388-7.£#/li#£ £#li#£Moretti, Gino (1964). Functions of a Complex Variable. Prentice-Hall.£#/li#£ £#li#£Wall, H. S. (1948). Analytic Theory of Continued Fractions. D. Van Nostrand Company. Reprinted (1973) by Chelsea Publishing Company ISBN 0-8284-0207-8.£#/li#£ £#li#£Whittaker, E. T.; Watson, G. N. (1927). A Course in Modern Analysis (Fourth ed.). Cambridge University Press.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Argand Diagram". MathWorld.£#/li#£ £#li#£Jean-Robert Argand, "Essai sur une manière de représenter des quantités imaginaires dans les constructions géométriques", 1806, online and analyzed on BibNum [for English version, click 'à télécharger']£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Complex Analysis > Complex Numbers £#/li#££#/ul#£




£#h3#£Argument£#/h3#£

An argument is a statement or group of statements called premises intended to determine the degree of truth or acceptability of another statement called conclusion. Arguments can be studied from three main perspectives: the logical, the dialectical and the rhetorical perspective.

In logic, an argument is usually expressed not in natural language but in a symbolic formal language, and it can be defined as any group of propositions of which one is claimed to follow from the others through deductively valid inferences that preserve truth from the premises to the conclusion. This logical perspective on argument is relevant for scientific fields such as math and computer science. Logic is the study of the forms of reasoning in arguments and the development of standards and criteria to evaluate arguments. Deductive arguments can be valid, and the valid ones can be sound: in a valid argument, premisses necessitate the conclusion, even if one or more of the premises is false and the conclusion is false; in a sound argument, true premises necessitate a true conclusion. Inductive arguments, by contrast, can have different degrees of logical strength: the stronger or more cogent the argument, the greater the probability that the conclusion is true, the weaker the argument, the lesser that probability. The standards for evaluating non-deductive arguments may rest on different or additional criteria than truth—for example, the persuasiveness of so-called "indispensability claims" in transcendental arguments, the quality of hypotheses in retroduction, or even the disclosure of new possibilities for thinking and acting.

In dialectics, and also in a more colloquial sense, an argument can be conceived as a social and verbal means of trying to resolve, or at least contend with, a conflict or difference of opinion that has arisen or exists between two or more parties. For the rhetorical perspective, the argument is constitutively linked with the context, in particular with the time and place in which the argument is located. From this perspective, the argument is evaluated not just by two parties (as in a dialectical approach) but also by an audience. In both dialectic and rhetoric, arguments are used not through a formal but through natural language. Since classical antiquity, philosophers and rhetoricians have developed lists of argument types in which premises and conclusions are connected in informal and defeasible ways.


£#h5#£Etymology£#/h5#£
The Latin root arguere (to make bright, enlighten, make known, prove, etc.) is from Proto-Indo-European argu-yo-, suffixed form of arg- (to shine; white).


£#h5#£Formal and informal£#/h5#£
Informal arguments as studied in informal logic, are presented in ordinary language and are intended for everyday discourse. Formal arguments are studied in formal logic (historically called symbolic logic, more commonly referred to as mathematical logic today) and are expressed in a formal language. Informal logic emphasizes the study of argumentation; formal logic emphasizes implication and inference. Informal arguments are sometimes implicit. The rational structure – the relationship of claims, premises, warrants, relations of implication, and conclusion – is not always spelled out and immediately visible and must be made explicit by analysis.


£#h5#£Standard logical account of argument types£#/h5#£
There are several kinds of arguments in logic, the best-known of which are "deductive" and "inductive." An argument has one or more premises but only one conclusion. Each premise and the conclusion are truth bearers or "truth-candidates", each capable of being either true or false (but not both). These truth values bear on the terminology used with arguments.


£#h5#£Deductive arguments£#/h5#£
A deductive argument asserts that the truth of the conclusion is a logical consequence of the premises: if the premises are true, the conclusion must be true. It would be self-contradictory to assert the premises and deny the conclusion, because negation of the conclusion is contradictory to the truth of the premises. Based on the premises, the conclusion follows necessarily (with certainty). Given premises that A=B and B=C, then the conclusion follows necessarily that A=C. Deductive arguments are sometimes referred to as "truth-preserving" arguments. For example, consider the argument that because bats can fly (premise=true), and all flying creatures are birds (premise=false), therefore bats are birds (conclusion=false). If we assume the premises are true, the conclusion follows necessarily, and it is a valid argument.


£#h5#£Validity£#/h5#£
Deductive arguments may be either valid or invalid. If valid, it has a conclusion that is entailed by its premises; if its premises are true, the conclusion must be true. An argument is formally valid if and only if the denial of the conclusion is incompatible with accepting all the premises.

The validity of an argument depends not on the actual truth or falsity of its premises and conclusion, but on whether the argument has a valid logical form. The validity of an argument is not a guarantee of the truth of its conclusion. A valid argument may have false premises that render it inconclusive: the conclusion of a valid argument with one or more false premises may be true or false.

Logic seeks to discover the forms that make arguments valid. A form of argument is valid if and only if the conclusion is true under all interpretations of that argument in which the premises are true. Since the validity of an argument depends on its form, an argument can be shown invalid by showing that its form is invalid. This can be done by a counter example of the same form of argument with premises that are true under a given interpretation, but a conclusion that is false under that interpretation. In informal logic this is called a counter argument.

The form of an argument can be shown by the use of symbols. For each argument form, there is a corresponding statement form, called a corresponding conditional, and an argument form is valid if and only if its corresponding conditional is a logical truth. A statement form which is logically true is also said to be a valid statement form. A statement form is a logical truth if it is true under all interpretations. A statement form can be shown to be a logical truth by either (a) showing that it is a tautology or (b) by means of a proof procedure.

The corresponding conditional of a valid argument is a necessary truth (true in all possible worlds) and so the conclusion necessarily follows from the premises, or follows of logical necessity. The conclusion of a valid argument is not necessarily true, it depends on whether the premises are true. If the conclusion, itself, is a necessary truth, it is without regard to the premises.

Some examples:

£#ul#££#li#£All Greeks are human and all humans are mortal; therefore, all Greeks are mortal. : Valid argument; if the premises are true the conclusion must be true.£#/li#£ £#li#£Some Greeks are logicians and some logicians are tiresome; therefore, some Greeks are tiresome. Invalid argument: the tiresome logicians might all be Romans (for example).£#/li#£ £#li#£Either we are all doomed or we are all saved; we are not all saved; therefore, we are all doomed. Valid argument; the premises entail the conclusion. (This does not mean the conclusion has to be true; it is only true if the premises are true, which they may not be!)£#/li#£ £#li#£Some men are hawkers. Some hawkers are rich. Therefore, some men are rich. Invalid argument. This can be easier seen by giving a counter-example with the same argument form: £#ul#££#li#£Some people are herbivores. Some herbivores are zebras. Therefore, some people are zebras. Invalid argument, as it is possible that the premises be true and the conclusion false.£#/li#££#/ul#££#/li#££#/ul#£
In the above second to last case (Some men are hawkers...), the counter-example follows the same logical form as the previous argument, (Premise 1: "Some X are Y." Premise 2: "Some Y are Z." Conclusion: "Some X are Z.") in order to demonstrate that whatever hawkers may be, they may or may not be rich, in consideration of the premises as such. (See also: Existential import).

The forms of argument that render deductions valid are well-established, however some invalid arguments can also be persuasive depending on their construction (inductive arguments, for example). (See also: Formal fallacy and Informal fallacy).


£#h5#£Soundness£#/h5#£
A sound argument is a valid argument whose conclusion follows from its premise(s), and the premise(s) of which is/are true.


£#h5#£Inductive arguments£#/h5#£
An inductive argument asserts that the truth of the conclusion is supported by the probability of the premises. For example, given that the U.S. military budget is the largest in the world (premise=true), then it is probable that it will remain so for the next 10 years (conclusion=true). Arguments that involve predictions are inductive since the future is uncertain. An inductive argument is said to be strong or weak. If the premises of an inductive argument are assumed true, is it probable the conclusion is also true? If yes, the argument is strong. If no, it is weak. A strong argument is said to be cogent if it has all true premises. Otherwise, the argument is uncogent. The military budget argument example is a strong, cogent argument.

Non-deductive logic is reasoning using arguments in which the premises support the conclusion but do not entail it. Forms of non-deductive logic include the statistical syllogism, which argues from generalizations true for the most part, and induction, a form of reasoning that makes generalizations based on individual instances. An inductive argument is said to be cogent if and only if the truth of the argument's premises would render the truth of the conclusion probable (i.e., the argument is strong), and the argument's premises are, in fact, true. Cogency can be considered inductive logic's analogue to deductive logic's "soundness". Despite its name, mathematical induction is not a form of inductive reasoning. The lack of deductive validity is known as the problem of induction.


£#h5#£Defeasible arguments and argumentation schemes£#/h5#£
In modern argumentation theories, arguments are regarded as defeasible passages from premises to a conclusion. Defeasibility means that when additional information (new evidence or contrary arguments) is provided, the premises may be no longer lead to the conclusion (non-monotonic reasoning). This type of reasoning is referred to as defeasible reasoning. For instance we consider the famous Tweety example:

Tweety is a bird.
Birds generally fly.
Therefore, Tweety (probably) flies.
This argument is reasonable and the premises support the conclusion unless additional information indicating that the case is an exception comes in. If Tweety is a penguin, the inference is no longer justified by the premise. Defeasible arguments are based on generalizations that hold only in the majority of cases, but are subject to exceptions and defaults.

In order to represent and assess defeasible reasoning, it is necessary to combine the logical rules (governing the acceptance of a conclusion based on the acceptance of its premises) with rules of material inference, governing how a premise can support a given conclusion (whether it is reasonable or not to draw a specific conclusion from a specific description of a state of affairs).

Argumentation schemes have been developed to describe and assess the acceptability or the fallaciousness of defeasible arguments. Argumentation schemes are stereotypical patterns of inference, combining semantic-ontological relations with types of reasoning and logical axioms and representing the abstract structure of the most common types of natural arguments. A typical example is the argument from expert opinion, shown below, which has two premises and a conclusion.

Each scheme may be associated with a set of critical questions, namely criteria for assessing dialectically the reasonableness and acceptability of an argument. The matching critical questions are the standard ways of casting the argument into doubt.


£#h5#£By analogy£#/h5#£
Argument by analogy may be thought of as argument from the particular to particular. An argument by analogy may use a particular truth in a premise to argue towards a similar particular truth in the conclusion. For example, if A. Plato was mortal, and B. Socrates was like Plato in other respects, then asserting that C. Socrates was mortal is an example of argument by analogy because the reasoning employed in it proceeds from a particular truth in a premise (Plato was mortal) to a similar particular truth in the conclusion, namely that Socrates was mortal.


£#h5#£Other kinds£#/h5#£
Other kinds of arguments may have different or additional standards of validity or justification. For example, philosopher Charles Taylor said that so-called transcendental arguments are made up of a "chain of indispensability claims" that attempt to show why something is necessarily true based on its connection to our experience, while Nikolas Kompridis has suggested that there are two types of "fallible" arguments: one based on truth claims, and the other based on the time-responsive disclosure of possibility (world disclosure). Kompridis said that the French philosopher Michel Foucault was a prominent advocate of this latter form of philosophical argument.


£#h5#£World-disclosing£#/h5#£
World-disclosing arguments are a group of philosophical arguments that according to Nikolas Kompridis employ a disclosive approach, to reveal features of a wider ontological or cultural-linguistic understanding – a "world", in a specifically ontological sense – in order to clarify or transform the background of meaning (tacit knowledge) and what Kompridis has called the "logical space" on which an argument implicitly depends.


£#h5#£Explanations£#/h5#£
While arguments attempt to show that something was, is, will be, or should be the case, explanations try to show why or how something is or will be. If Fred and Joe address the issue of whether or not Fred's cat has fleas, Joe may state: "Fred, your cat has fleas. Observe, the cat is scratching right now." Joe has made an argument that the cat has fleas. However, if Joe asks Fred, "Why is your cat scratching itself?" the explanation, "...because it has fleas." provides understanding.

Both the above argument and explanation require knowing the generalities that a) fleas often cause itching, and b) that one often scratches to relieve itching. The difference is in the intent: an argument attempts to settle whether or not some claim is true, and an explanation attempts to provide understanding of the event. Note, that by subsuming the specific event (of Fred's cat scratching) as an instance of the general rule that "animals scratch themselves when they have fleas", Joe will no longer wonder why Fred's cat is scratching itself. Arguments address problems of belief, explanations address problems of understanding. Also note that in the argument above, the statement, "Fred's cat has fleas" is up for debate (i.e. is a claim), but in the explanation, the statement, "Fred's cat has fleas" is assumed to be true (unquestioned at this time) and just needs explaining.

Arguments and explanations largely resemble each other in rhetorical use. This is the cause of much difficulty in thinking critically about claims. There are several reasons for this difficulty.

£#ul#££#li#£People often are not themselves clear on whether they are arguing for or explaining something.£#/li#£ £#li#£The same types of words and phrases are used in presenting explanations and arguments.£#/li#£ £#li#£The terms 'explain' or 'explanation,' et cetera are frequently used in arguments.£#/li#£ £#li#£Explanations are often used within arguments and presented so as to serve as arguments.£#/li#£ £#li#£Likewise, "...arguments are essential to the process of justifying the validity of any explanation as there are often multiple explanations for any given phenomenon."£#/li#££#/ul#£
Explanations and arguments are often studied in the field of Information Systems to help explain user acceptance of knowledge-based systems. Certain argument types may fit better with personality traits to enhance acceptance by individuals.


£#h5#£Fallacies and non-arguments£#/h5#£
Fallacies are types of argument or expressions which are held to be of an invalid form or contain errors in reasoning.

One type of fallacy occurs when a word frequently used to indicate a conclusion is used as a transition (conjunctive adverb) between independent clauses. In English the words therefore, so, because and hence typically separate the premises from the conclusion of an argument. Thus: Socrates is a man, all men are mortal therefore Socrates is mortal is an argument because the assertion Socrates is mortal follows from the preceding statements. However, I was thirsty and therefore I drank is not an argument, despite its appearance. It is not being claimed that I drank is logically entailed by I was thirsty. The therefore in this sentence indicates for that reason not it follows that.


£#h5#£Elliptical or ethymematic arguments£#/h5#£
Often an argument is invalid or weak because there is a missing premise—the supply of which would make it valid or strong. This is referred to as an elliptical or ethymematic argument (see also Enthymeme § Syllogism with an unstated premise). Speakers and writers will often leave out a necessary premise in their reasoning if it is widely accepted and the writer does not wish to state the blindingly obvious. Example: All metals expand when heated, therefore iron will expand when heated. The missing premise is: Iron is a metal. On the other hand, a seemingly valid argument may be found to lack a premise – a "hidden assumption" – which, if highlighted, can show a fault in reasoning. Example: A witness reasoned: Nobody came out the front door except the milkman; therefore the murderer must have left by the back door. The hidden assumptions are: (1) the milkman was not the murderer and (2) the murderer has left by the front or back door.


£#h5#£Argument mining£#/h5#£
The goal of argument mining is the automatic extraction and identification of argumentative structures from natural language text with the aid of computer programs.  Such argumentative structures include the premise, conclusions, the argument scheme and the relationship between the main and subsidiary argument, or the main and counter-argument within discourse.


£#h5#£See also£#/h5#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Shaw, Warren Choate (1922). The Art of Debate. Allyn and Bacon. p. 74. argument by analogy.£#/li#£ £#li#£Robert Audi, Epistemology, Routledge, 1998. Particularly relevant is Chapter 6, which explores the relationship between knowledge, inference and argument.£#/li#£ £#li#£J. L. Austin How to Do Things With Words, Oxford University Press, 1976.£#/li#£ £#li#£H. P. Grice, Logic and Conversation in The Logic of Grammar, Dickenson, 1975.£#/li#£ £#li#£Vincent F. Hendricks, Thought 2 Talk: A Crash Course in Reflection and Expression, New York: Automatic Press / VIP, 2005, ISBN 87-991013-7-8£#/li#£ £#li#£R. A. DeMillo, R. J. Lipton and A. J. Perlis, Social Processes and Proofs of Theorems and Programs, Communications of the ACM, Vol. 22, No. 5, 1979. A classic article on the social process of acceptance of proofs in mathematics.£#/li#£ £#li#£Yu. Manin, A Course in Mathematical Logic, Springer Verlag, 1977. A mathematical view of logic. This book is different from most books on mathematical logic in that it emphasizes the mathematics of logic, as opposed to the formal structure of logic.£#/li#£ £#li#£Ch. Perelman and L. Olbrechts-Tyteca, The New Rhetoric, Notre Dame, 1970. This classic was originally published in French in 1958.£#/li#£ £#li#£Henri Poincaré, Science and Hypothesis, Dover Publications, 1952£#/li#£ £#li#£Frans van Eemeren and Rob Grootendorst, Speech Acts in Argumentative Discussions, Foris Publications, 1984.£#/li#£ £#li#£K. R. Popper Objective Knowledge; An Evolutionary Approach, Oxford: Clarendon Press, 1972.£#/li#£ £#li#£L. S. Stebbing, A Modern Introduction to Logic, Methuen and Co., 1948. An account of logic that covers the classic topics of logic and argument while carefully considering modern developments in logic.£#/li#£ £#li#£Douglas N. Walton, Informal Logic: A Handbook for Critical Argumentation, Cambridge, 1998.£#/li#£ £#li#£Walton, Douglas; Christopher Reed; Fabrizio Macagno, Argumentation Schemes, New York: Cambridge University Press, 2008.£#/li#£ £#li#£Carlos Chesñevar, Ana Maguitman and Ronald Loui, Logical Models of Argument, ACM Computing Surveys, vol. 32, num. 4, pp. 337–383, 2000.£#/li#£ £#li#£T. Edward Damer. Attacking Faulty Reasoning, 5th Edition, Wadsworth, 2005. ISBN 0-534-60516-8£#/li#£ £#li#£Charles Arthur Willard, A Theory of Argumentation. 1989.£#/li#£ £#li#£Charles Arthur Willard, Argumentation and the Social Grounds of Knowledge. 1982.£#/li#££#/ul#£
£#h5#£Further reading£#/h5#£ £#ul#££#li#£Salmon, Wesley C. Logic. New Jersey: Prentice-Hall (1963). Library of Congress Catalog Card no. 63–10528.£#/li#£ £#li#£Aristotle, Prior and Posterior Analytics. Ed. and trans. John Warrington. London: Dent (1964)£#/li#£ £#li#£Mates, Benson. Elementary Logic. New York: OUP (1972). Library of Congress Catalog Card no. 74–166004.£#/li#£ £#li#£Mendelson, Elliot. Introduction to Mathematical Logic. New York: Van Nostran Reinholds Company (1964).£#/li#£ £#li#£Frege, Gottlob. The Foundations of Arithmetic. Evanston, IL: Northwestern University Press (1980).£#/li#£ £#li#£Martin, Brian. The Controversy Manual (Sparsnäs, Sweden: Irene Publishing, 2014).£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Argument at PhilPapers£#/li#£ £#li#£Argument at the Indiana Philosophy Ontology Project£#/li#£ £#li#£Dutilh Novaes, Catarina. "Argument and Argumentation". In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.£#/li#£ £#li#£McKeon, Matthew. "Argument". Internet Encyclopedia of Philosophy.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Derbyshire, J. Prime Obsession: Bernhard Riemann and the Greatest Unsolved Problem in Mathematics. New York: Penguin, p. 36, 2004.£#/li#££#li#£ Derbyshire, J. Prime Obsession: Bernhard Riemann and the Greatest Unsolved Problem in Mathematics. New York: Penguin, p. 36, 2004. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > General Analysis £#/li#££#/ul#£




£#h3#£Argument Addition Relation£#/h3#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > General Analysis £#/li#££#/ul#£




£#h3#£Argument Multiplication Relation£#/h3#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > General Analysis £#/li#££#/ul#£




£#h3#£Argument Principle£#/h3#£

In complex analysis, the argument principle (or Cauchy's argument principle) relates the difference between the number of zeros and poles of a meromorphic function to a contour integral of the function's logarithmic derivative.

Specifically, if f(z) is a meromorphic function inside and on some closed contour C, and f has no zeros or poles on C, then

${\displaystyle {\frac {1}{2\pi i}}\oint _{C}{f'(z) \over f(z)}\,dz=Z-P}$
where Z and P denote respectively the number of zeros and poles of f(z) inside the contour C, with each zero and pole counted as many times as its multiplicity and order, respectively, indicate. This statement of the theorem assumes that the contour C is simple, that is, without self-intersections, and that it is oriented counter-clockwise.

More generally, suppose that f(z) is a meromorphic function on an open set Ω in the complex plane and that C is a closed curve in Ω which avoids all zeros and poles of f and is contractible to a point inside Ω. For each point z ∈ Ω, let n(C,z) be the winding number of C around z. Then

${\displaystyle {\frac {1}{2\pi i}}\oint _{C}{\frac {f'(z)}{f(z)}}\,dz=\sum _{a}n(C,a)-\sum _{b}n(C,b)\,}$
where the first summation is over all zeros a of f counted with their multiplicities, and the second summation is over the poles b of f counted with their orders.


£#h5#£Interpretation of the contour integral£#/h5#£
The contour integral ${\displaystyle \oint _{C}{\frac {f'(z)}{f(z)}}\,dz}$ can be interpreted as 2πi times the winding number of the path f(C) around the origin, using the substitution w = f(z):

${\displaystyle \oint _{C}{\frac {f'(z)}{f(z)}}\,dz=\oint _{f(C)}{\frac {1}{w}}\,dw}$
That is, it is i times the total change in the argument of f(z) as z travels around C, explaining the name of the theorem; this follows from

${\displaystyle {\frac {d}{dz}}\log(f(z))={\frac {f'(z)}{f(z)}}}$
and the relation between arguments and logarithms.


£#h5#£Proof of the argument principle£#/h5#£
Let zZ be a zero of f. We can write f(z) = (z − zZ)kg(z) where k is the multiplicity of the zero, and thus g(zZ) ≠ 0. We get

${\displaystyle f'(z)=k(z-z_{Z})^{k-1}g(z)+(z-z_{Z})^{k}g'(z)\,\!}$
and

${\displaystyle {f'(z) \over f(z)}={k \over z-z_{Z}}+{g'(z) \over g(z)}.}$
Since g(zZ) ≠ 0, it follows that g' (z)/g(z) has no singularities at zZ, and thus is analytic at zZ, which implies that the residue of f′(z)/f(z) at zZ is k.

Let zP be a pole of f. We can write f(z) = (z − zP)−mh(z) where m is the order of the pole, and h(zP) ≠ 0. Then,

${\displaystyle f'(z)=-m(z-z_{P})^{-m-1}h(z)+(z-z_{P})^{-m}h'(z)\,\!.}$
and

${\displaystyle {f'(z) \over f(z)}={-m \over z-z_{P}}+{h'(z) \over h(z)}}$
similarly as above. It follows that h′(z)/h(z) has no singularities at zP since h(zP) ≠ 0 and thus it is analytic at zP. We find that the residue of f′(z)/f(z) at zP is −m.

Putting these together, each zero zZ of multiplicity k of f creates a simple pole for f′(z)/f(z) with the residue being k, and each pole zP of order m of f creates a simple pole for f′(z)/f(z) with the residue being −m. (Here, by a simple pole we mean a pole of order one.) In addition, it can be shown that f′(z)/f(z) has no other poles, and so no other residues.

By the residue theorem we have that the integral about C is the product of 2πi and the sum of the residues. Together, the sum of the k's for each zero zZ is the number of zeros counting multiplicities of the zeros, and likewise for the poles, and so we have our result.


£#h5#£Applications and consequences£#/h5#£
The argument principle can be used to efficiently locate zeros or poles of meromorphic functions on a computer. Even with rounding errors, the expression ${\displaystyle {1 \over 2\pi i}\oint _{C}{f'(z) \over f(z)}\,dz}$ will yield results close to an integer; by determining these integers for different contours C one can obtain information about the location of the zeros and poles. Numerical tests of the Riemann hypothesis use this technique to get an upper bound for the number of zeros of Riemann's ${\displaystyle \xi (s)}$ function inside a rectangle intersecting the critical line.

The proof of Rouché's theorem uses the argument principle.

Modern books on feedback control theory quite frequently use the argument principle to serve as the theoretical basis of the Nyquist stability criterion.

A consequence of the more general formulation of the argument principle is that, under the same hypothesis, if g is an analytic function in Ω, then

${\displaystyle {\frac {1}{2\pi i}}\oint _{C}g(z){\frac {f'(z)}{f(z)}}\,dz=\sum _{a}n(C,a)g(a)-\sum _{b}n(C,b)g(b).}$
For example, if f is a polynomial having zeros z1, ..., zp inside a simple contour C, and g(z) = zk, then

${\displaystyle {\frac {1}{2\pi i}}\oint _{C}z^{k}{\frac {f'(z)}{f(z)}}\,dz=z_{1}^{k}+z_{2}^{k}+\cdots +z_{p}^{k},}$
is power sum symmetric polynomial of the roots of f.

Another consequence is if we compute the complex integral:

${\displaystyle \oint _{C}f(z){g'(z) \over g(z)}\,dz}$
for an appropriate choice of g and f we have the Abel–Plana formula:

${\displaystyle \sum _{n=0}^{\infty }f(n)-\int _{0}^{\infty }f(x)\,dx=f(0)/2+i\int _{0}^{\infty }{\frac {f(it)-f(-it)}{e^{2\pi t}-1}}\,dt\,}$
which expresses the relationship between a discrete sum and its integral.


£#h5#£Generalized argument principle£#/h5#£
There is an immediate generalization of the argument principle. Suppose that g is analytic in the region ${\displaystyle \Omega }$ . Then

${\displaystyle {\frac {1}{2\pi i}}\oint _{C}{f'(z) \over f(z)}g(z)\,dz=\sum _{a}g(a)n(C,a)-\sum _{b}g(b)n(C,b)\,}$
where the first summation is again over all zeros a of f counted with their multiplicities, and the second summation is again over the poles b of f counted with their orders.


£#h5#£History£#/h5#£
According to the book by Frank Smithies (Cauchy and the Creation of Complex Function Theory, Cambridge University Press, 1997, p. 177), Augustin-Louis Cauchy presented a theorem similar to the above on 27 November 1831, during his self-imposed exile in Turin (then capital of the Kingdom of Piedmont-Sardinia) away from France. However, according to this book, only zeroes were mentioned, not poles. This theorem by Cauchy was only published many years later in 1874 in a hand-written form and so is quite difficult to read. Cauchy published a paper with a discussion on both zeroes and poles in 1855, two years before his death.


£#h5#£See also£#/h5#£ £#ul#££#li#£Logarithmic derivative£#/li#£ £#li#£Nyquist stability criterion£#/li#££#/ul#£
£#h5#£References£#/h5#£ £#ul#££#li#£Rudin, Walter (1986). Real and Complex Analysis (International Series in Pure and Applied Mathematics). McGraw-Hill. ISBN 978-0-07-054234-1.£#/li#£ £#li#£Ahlfors, Lars (1979). Complex analysis: an introduction to the theory of analytic functions of one complex variable. McGraw-Hill. ISBN 978-0-07-000657-7.£#/li#£ £#li#£Churchill, Ruel Vance; Brown, James Ward (1989). Complex Variables and Applications. McGraw-Hill. ISBN 978-0-07-010905-6.£#/li#£ £#li#£Backlund, R.-J. (1914) Sur les zéros de la fonction zeta(s) de Riemann, C. R. Acad. Sci. Paris 158, 1979–1982.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Argument, principle of the", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Duren, P.; Hengartner, W.; and Laugessen, R. S. "The Argument Principle for Harmonic Functions." Amer. Math. Monthly 103, 411-415, 1996.£#/li#££#li#£Knopp, K. Theory of Functions, Parts I and II. New York: Dover, pp. 132-134, 1996.£#/li#££#li#£Krantz, S. G. "The Argument Principle." Ch. 5 in Handbook of Complex Variables. Boston, MA: Birkhäuser, pp. 69-78, 1999.£#/li#££#li#£ Duren, P.; Hengartner, W.; and Laugessen, R. S. "The Argument Principle for Harmonic Functions." Amer. Math. Monthly 103, 411-415, 1996. £#/li#££#li#£ Knopp, K. Theory of Functions, Parts I and II. New York: Dover, pp. 132-134, 1996. £#/li#££#li#£ Krantz, S. G. "The Argument Principle." Ch. 5 in Handbook of Complex Variables. Boston, MA: Birkhäuser, pp. 69-78, 1999. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Complex Analysis > Contours £#/li#££#/ul#£




£#h3#£Argument Variation£#/h3#£

In complex analysis, the argument principle (or Cauchy's argument principle) relates the difference between the number of zeros and poles of a meromorphic function to a contour integral of the function's logarithmic derivative.

Specifically, if f(z) is a meromorphic function inside and on some closed contour C, and f has no zeros or poles on C, then

${\displaystyle {\frac {1}{2\pi i}}\oint _{C}{f'(z) \over f(z)}\,dz=Z-P}$
where Z and P denote respectively the number of zeros and poles of f(z) inside the contour C, with each zero and pole counted as many times as its multiplicity and order, respectively, indicate. This statement of the theorem assumes that the contour C is simple, that is, without self-intersections, and that it is oriented counter-clockwise.

More generally, suppose that f(z) is a meromorphic function on an open set Ω in the complex plane and that C is a closed curve in Ω which avoids all zeros and poles of f and is contractible to a point inside Ω. For each point z ∈ Ω, let n(C,z) be the winding number of C around z. Then

${\displaystyle {\frac {1}{2\pi i}}\oint _{C}{\frac {f'(z)}{f(z)}}\,dz=\sum _{a}n(C,a)-\sum _{b}n(C,b)\,}$
where the first summation is over all zeros a of f counted with their multiplicities, and the second summation is over the poles b of f counted with their orders.


£#h5#£Interpretation of the contour integral£#/h5#£
The contour integral ${\displaystyle \oint _{C}{\frac {f'(z)}{f(z)}}\,dz}$ can be interpreted as 2πi times the winding number of the path f(C) around the origin, using the substitution w = f(z):

${\displaystyle \oint _{C}{\frac {f'(z)}{f(z)}}\,dz=\oint _{f(C)}{\frac {1}{w}}\,dw}$
That is, it is i times the total change in the argument of f(z) as z travels around C, explaining the name of the theorem; this follows from

${\displaystyle {\frac {d}{dz}}\log(f(z))={\frac {f'(z)}{f(z)}}}$
and the relation between arguments and logarithms.


£#h5#£Proof of the argument principle£#/h5#£
Let zZ be a zero of f. We can write f(z) = (z − zZ)kg(z) where k is the multiplicity of the zero, and thus g(zZ) ≠ 0. We get

${\displaystyle f'(z)=k(z-z_{Z})^{k-1}g(z)+(z-z_{Z})^{k}g'(z)\,\!}$
and

${\displaystyle {f'(z) \over f(z)}={k \over z-z_{Z}}+{g'(z) \over g(z)}.}$
Since g(zZ) ≠ 0, it follows that g' (z)/g(z) has no singularities at zZ, and thus is analytic at zZ, which implies that the residue of f′(z)/f(z) at zZ is k.

Let zP be a pole of f. We can write f(z) = (z − zP)−mh(z) where m is the order of the pole, and h(zP) ≠ 0. Then,

${\displaystyle f'(z)=-m(z-z_{P})^{-m-1}h(z)+(z-z_{P})^{-m}h'(z)\,\!.}$
and

${\displaystyle {f'(z) \over f(z)}={-m \over z-z_{P}}+{h'(z) \over h(z)}}$
similarly as above. It follows that h′(z)/h(z) has no singularities at zP since h(zP) ≠ 0 and thus it is analytic at zP. We find that the residue of f′(z)/f(z) at zP is −m.

Putting these together, each zero zZ of multiplicity k of f creates a simple pole for f′(z)/f(z) with the residue being k, and each pole zP of order m of f creates a simple pole for f′(z)/f(z) with the residue being −m. (Here, by a simple pole we mean a pole of order one.) In addition, it can be shown that f′(z)/f(z) has no other poles, and so no other residues.

By the residue theorem we have that the integral about C is the product of 2πi and the sum of the residues. Together, the sum of the k's for each zero zZ is the number of zeros counting multiplicities of the zeros, and likewise for the poles, and so we have our result.


£#h5#£Applications and consequences£#/h5#£
The argument principle can be used to efficiently locate zeros or poles of meromorphic functions on a computer. Even with rounding errors, the expression ${\displaystyle {1 \over 2\pi i}\oint _{C}{f'(z) \over f(z)}\,dz}$ will yield results close to an integer; by determining these integers for different contours C one can obtain information about the location of the zeros and poles. Numerical tests of the Riemann hypothesis use this technique to get an upper bound for the number of zeros of Riemann's ${\displaystyle \xi (s)}$ function inside a rectangle intersecting the critical line.

The proof of Rouché's theorem uses the argument principle.

Modern books on feedback control theory quite frequently use the argument principle to serve as the theoretical basis of the Nyquist stability criterion.

A consequence of the more general formulation of the argument principle is that, under the same hypothesis, if g is an analytic function in Ω, then

${\displaystyle {\frac {1}{2\pi i}}\oint _{C}g(z){\frac {f'(z)}{f(z)}}\,dz=\sum _{a}n(C,a)g(a)-\sum _{b}n(C,b)g(b).}$
For example, if f is a polynomial having zeros z1, ..., zp inside a simple contour C, and g(z) = zk, then

${\displaystyle {\frac {1}{2\pi i}}\oint _{C}z^{k}{\frac {f'(z)}{f(z)}}\,dz=z_{1}^{k}+z_{2}^{k}+\cdots +z_{p}^{k},}$
is power sum symmetric polynomial of the roots of f.

Another consequence is if we compute the complex integral:

${\displaystyle \oint _{C}f(z){g'(z) \over g(z)}\,dz}$
for an appropriate choice of g and f we have the Abel–Plana formula:

${\displaystyle \sum _{n=0}^{\infty }f(n)-\int _{0}^{\infty }f(x)\,dx=f(0)/2+i\int _{0}^{\infty }{\frac {f(it)-f(-it)}{e^{2\pi t}-1}}\,dt\,}$
which expresses the relationship between a discrete sum and its integral.


£#h5#£Generalized argument principle£#/h5#£
There is an immediate generalization of the argument principle. Suppose that g is analytic in the region ${\displaystyle \Omega }$ . Then

${\displaystyle {\frac {1}{2\pi i}}\oint _{C}{f'(z) \over f(z)}g(z)\,dz=\sum _{a}g(a)n(C,a)-\sum _{b}g(b)n(C,b)\,}$
where the first summation is again over all zeros a of f counted with their multiplicities, and the second summation is again over the poles b of f counted with their orders.


£#h5#£History£#/h5#£
According to the book by Frank Smithies (Cauchy and the Creation of Complex Function Theory, Cambridge University Press, 1997, p. 177), Augustin-Louis Cauchy presented a theorem similar to the above on 27 November 1831, during his self-imposed exile in Turin (then capital of the Kingdom of Piedmont-Sardinia) away from France. However, according to this book, only zeroes were mentioned, not poles. This theorem by Cauchy was only published many years later in 1874 in a hand-written form and so is quite difficult to read. Cauchy published a paper with a discussion on both zeroes and poles in 1855, two years before his death.


£#h5#£See also£#/h5#£ £#ul#££#li#£Logarithmic derivative£#/li#£ £#li#£Nyquist stability criterion£#/li#££#/ul#£
£#h5#£References£#/h5#£ £#ul#££#li#£Rudin, Walter (1986). Real and Complex Analysis (International Series in Pure and Applied Mathematics). McGraw-Hill. ISBN 978-0-07-054234-1.£#/li#£ £#li#£Ahlfors, Lars (1979). Complex analysis: an introduction to the theory of analytic functions of one complex variable. McGraw-Hill. ISBN 978-0-07-000657-7.£#/li#£ £#li#£Churchill, Ruel Vance; Brown, James Ward (1989). Complex Variables and Applications. McGraw-Hill. ISBN 978-0-07-010905-6.£#/li#£ £#li#£Backlund, R.-J. (1914) Sur les zéros de la fonction zeta(s) de Riemann, C. R. Acad. Sci. Paris 158, 1979–1982.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Argument, principle of the", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Complex Analysis > Contours £#/li#££#/ul#£




£#h3#£Arithmetic-Geometric Mean£#/h3#£

In mathematics, the inequality of arithmetic and geometric means, or more briefly the AM–GM inequality, states that the arithmetic mean of a list of non-negative real numbers is greater than or equal to the geometric mean of the same list; and further, that the two means are equal if and only if every number in the list is the same (in which case they are both that number).

The simplest non-trivial case – i.e., with more than one variable – for two non-negative numbers x and y, is the statement that

${\displaystyle {\frac {x+y}{2}}\geq {\sqrt {xy}}}$
with equality if and only if x = y. This case can be seen from the fact that the square of a real number is always non-negative (greater than or equal to zero) and from the elementary case (a ± b)2 = a2 ± 2ab + b2 of the binomial formula:

${\displaystyle {\begin{aligned}0&\leq (x-y)^{2}\\&=x^{2}-2xy+y^{2}\\&=x^{2}+2xy+y^{2}-4xy\\&=(x+y)^{2}-4xy.\end{aligned}}}$
Hence (x + y)2 ≥ 4xy, with equality precisely when (x − y)2 = 0, i.e. x = y. The AM–GM inequality then follows from taking the positive square root of both sides and then dividing both sides by 2.

For a geometrical interpretation, consider a rectangle with sides of length x and y, hence it has perimeter 2x + 2y and area xy. Similarly, a square with all sides of length √xy has the perimeter 4√xy and the same area as the rectangle. The simplest non-trivial case of the AM–GM inequality implies for the perimeters that 2x + 2y ≥ 4√xy and that only the square has the smallest perimeter amongst all rectangles of equal area.

Extensions of the AM–GM inequality are available to include weights or generalized means.


£#h5#£Background£#/h5#£
The arithmetic mean, or less precisely the average, of a list of n numbers x1, x2, . . . , xn is the sum of the numbers divided by n:

${\displaystyle {\frac {x_{1}+x_{2}+\cdots +x_{n}}{n}}.}$
The geometric mean is similar, except that it is only defined for a list of nonnegative real numbers, and uses multiplication and a root in place of addition and division:

${\displaystyle {\sqrt[{n}]{x_{1}\cdot x_{2}\cdots x_{n}}}.}$
If x1, x2, . . . , xn > 0, this is equal to the exponential of the arithmetic mean of the natural logarithms of the numbers:

${\displaystyle \exp \left({\frac {\ln {x_{1}}+\ln {x_{2}}+\cdots +\ln {x_{n}}}{n}}\right).}$

£#h5#£The inequality£#/h5#£
Restating the inequality using mathematical notation, we have that for any list of n nonnegative real numbers x1, x2, . . . , xn,

${\displaystyle {\frac {x_{1}+x_{2}+\cdots +x_{n}}{n}}\geq {\sqrt[{n}]{x_{1}\cdot x_{2}\cdots x_{n}}}\,,}$
and that equality holds if and only if x1 = x2 = · · · = xn.


£#h5#£Geometric interpretation£#/h5#£
In two dimensions, 2x1 + 2x2 is the perimeter of a rectangle with sides of length x1 and x2. Similarly, 4√x1x2 is the perimeter of a square with the same area, x1x2, as that rectangle. Thus for n = 2 the AM–GM inequality states that a rectangle of a given area has the smallest perimeter if that rectangle is also a square.

The full inequality is an extension of this idea to n dimensions. Every vertex of an n-dimensional box is connected to n edges. If these edges' lengths are x1, x2, . . . , xn, then x1 + x2 + · · · + xn is the total length of edges incident to the vertex. There are 2n vertices, so we multiply this by 2n; since each edge, however, meets two vertices, every edge is counted twice. Therefore, we divide by 2 and conclude that there are 2n−1n edges. There are equally many edges of each length and n lengths; hence there are 2n−1 edges of each length and the total of all edge lengths is 2n−1(x1 + x2 + · · · + xn). On the other hand,

${\displaystyle 2^{n-1}(x_{1}+\ldots +x_{n})=2^{n-1}n{\sqrt[{n}]{x_{1}x_{2}\cdots x_{n}}}}$
is the total length of edges connected to a vertex on an n-dimensional cube of equal volume, since in this case x1=...=xn. Since the inequality says

${\displaystyle {x_{1}+x_{2}+\cdots +x_{n} \over n}\geq {\sqrt[{n}]{x_{1}x_{2}\cdots x_{n}}},}$
it can be restated by multiplying through by n2n–1 to obtain

${\displaystyle 2^{n-1}(x_{1}+x_{2}+\cdots +x_{n})\geq 2^{n-1}n{\sqrt[{n}]{x_{1}x_{2}\cdots x_{n}}}}$
with equality if and only if x1 = x2 = · · · = xn.

Thus the AM–GM inequality states that only the n-cube has the smallest sum of lengths of edges connected to each vertex amongst all n-dimensional boxes with the same volume.


£#h5#£Examples£#/h5#£
£#h5#£Example 1£#/h5#£
If ${\displaystyle a,b,c>0}$ , then the A.M.-G.M. tells us that

${\displaystyle (1+a)(1+b)(1+c)\geq 8{\sqrt {abc}}}$

£#h5#£Example 2£#/h5#£
A simple upper bound for ${\displaystyle n!}$ can be found. AM-GM tells us

${\displaystyle 1+2+\dots +n\geq n{\sqrt[{n}]{n!}}}$
${\displaystyle {\frac {n(n+1)}{2}}\geq n{\sqrt[{n}]{n!}}}$
and so

${\displaystyle \left({\frac {n+1}{2}}\right)^{n}\geq n!}$
with equality at ${\displaystyle n=1}$ .

Equivalently,

${\displaystyle (n+1)^{n}\geq 2^{n}n!}$

£#h5#£Example 3£#/h5#£
Consider the function

${\displaystyle f(x,y,z)={\frac {x}{y}}+{\sqrt {\frac {y}{z}}}+{\sqrt[{3}]{\frac {z}{x}}}}$
for all positive real numbers x, y and z. Suppose we wish to find the minimal value of this function. It can be rewritten as:

${\displaystyle {\begin{aligned}f(x,y,z)&=6\cdot {\frac {{\frac {x}{y}}+{\frac {1}{2}}{\sqrt {\frac {y}{z}}}+{\frac {1}{2}}{\sqrt {\frac {y}{z}}}+{\frac {1}{3}}{\sqrt[{3}]{\frac {z}{x}}}+{\frac {1}{3}}{\sqrt[{3}]{\frac {z}{x}}}+{\frac {1}{3}}{\sqrt[{3}]{\frac {z}{x}}}}{6}}\\&=6\cdot {\frac {x_{1}+x_{2}+x_{3}+x_{4}+x_{5}+x_{6}}{6}}\end{aligned}}}$
with

${\displaystyle x_{1}={\frac {x}{y}},\qquad x_{2}=x_{3}={\frac {1}{2}}{\sqrt {\frac {y}{z}}},\qquad x_{4}=x_{5}=x_{6}={\frac {1}{3}}{\sqrt[{3}]{\frac {z}{x}}}.}$
Applying the AM–GM inequality for n = 6, we get

${\displaystyle {\begin{aligned}f(x,y,z)&\geq 6\cdot {\sqrt[{6}]{{\frac {x}{y}}\cdot {\frac {1}{2}}{\sqrt {\frac {y}{z}}}\cdot {\frac {1}{2}}{\sqrt {\frac {y}{z}}}\cdot {\frac {1}{3}}{\sqrt[{3}]{\frac {z}{x}}}\cdot {\frac {1}{3}}{\sqrt[{3}]{\frac {z}{x}}}\cdot {\frac {1}{3}}{\sqrt[{3}]{\frac {z}{x}}}}}\\&=6\cdot {\sqrt[{6}]{{\frac {1}{2\cdot 2\cdot 3\cdot 3\cdot 3}}{\frac {x}{y}}{\frac {y}{z}}{\frac {z}{x}}}}\\&=2^{2/3}\cdot 3^{1/2}.\end{aligned}}}$
Further, we know that the two sides are equal exactly when all the terms of the mean are equal:

${\displaystyle f(x,y,z)=2^{2/3}\cdot 3^{1/2}\quad {\mbox{when}}\quad {\frac {x}{y}}={\frac {1}{2}}{\sqrt {\frac {y}{z}}}={\frac {1}{3}}{\sqrt[{3}]{\frac {z}{x}}}.}$
All the points (x, y, z) satisfying these conditions lie on a half-line starting at the origin and are given by

${\displaystyle (x,y,z)={\biggr (}t,{\sqrt[{3}]{2}}{\sqrt {3}}\,t,{\frac {3{\sqrt {3}}}{2}}\,t{\biggr )}\quad {\mbox{with}}\quad t>0.}$

£#h5#£Practical applications£#/h5#£
An important practical application in financial mathematics is to computing the rate of return: the annualized return, computed via the geometric mean, is less than the average annual return, computed by the arithmetic mean (or equal if all returns are equal). This is important in analyzing investments, as the average return overstates the cumulative effect.


£#h5#£Proofs of the AM–GM inequality£#/h5#£
£#h5#£Proof using Jensen's inequality£#/h5#£
Jensen's inequality states that the value of a concave function of an arithmetic mean is greater than or equal to the arithmetic mean of the function's values. Since the logarithm function is concave, we have

${\displaystyle \log \left({\frac {\sum _{i}x_{i}}{n}}\right)\geq \sum {\frac {1}{n}}\log x_{i}=\sum \left(\log x_{i}^{1/n}\right)=\log \left(\prod x_{i}^{1/n}\right).}$
Taking antilogs of the far left and far right sides, we have the AM–GM inequality.


£#h5#£Proof by successive replacement of elements£#/h5#£
We have to show that

${\displaystyle \alpha ={\frac {x_{1}+x_{2}+\cdots +x_{n}}{n}}\geq {\sqrt[{n}]{x_{1}x_{2}\cdots x_{n}}}=\beta }$
with equality only when all numbers are equal.

If not all numbers are equal, then there exist ${\displaystyle x_{i},x_{j}}$ such that ${\displaystyle x_{i}<\alpha <x_{j}}$ . Replacing xi by ${\displaystyle \alpha }$ and xj by ${\displaystyle (x_{i}+x_{j}-\alpha )}$ will leave the arithmetic mean of the numbers unchanged, but will increase the geometric mean because

${\displaystyle \alpha (x_{j}+x_{i}-\alpha )-x_{i}x_{j}=(\alpha -x_{i})(x_{j}-\alpha )>0}$
If the numbers are still not equal, we continue replacing numbers as above. After at most ${\displaystyle (n-1)}$ such replacement steps all the numbers will have been replaced with ${\displaystyle \alpha }$ while the geometric mean strictly increases at each step. After the last step, the geometric mean will be ${\displaystyle {\sqrt[{n}]{\alpha \alpha \cdots \alpha }}=\alpha }$ , proving the inequality.

It may be noted that the replacement strategy works just as well from the right hand side. If any of the numbers is 0 then so will the geometric mean thus proving the inequality trivially. Therefore we may suppose that all the numbers are positive. If they are not all equal, then there exist ${\displaystyle x_{i},x_{j}}$ such that ${\displaystyle 0<x_{i}<\beta <x_{j}}$ . Replacing ${\displaystyle x_{i}}$ by ${\displaystyle \beta }$ and ${\displaystyle x_{j}}$ by ${\displaystyle {\frac {x_{i}x_{j}}{\beta }}}$ leaves the geometric mean unchanged but strictly decreases the arithmetic mean since

${\displaystyle x_{i}+x_{j}-\beta -{\frac {x_{i}x_{j}}{\beta }}={\frac {(\beta -x_{i})(x_{j}-\beta )}{\beta }}>0}$ . The proof then follows along similar lines as in the earlier replacement.

£#h5#£Induction Proofs£#/h5#£
£#h5#£Proof by induction #1£#/h5#£
Of the non-negative real numbers x1, . . . , xn, the AM–GM statement is equivalent to

${\displaystyle \alpha ^{n}\geq x_{1}x_{2}\cdots x_{n}}$
with equality if and only if α = xi for all i ∈ {1, . . . , n}.

For the following proof we apply mathematical induction and only well-known rules of arithmetic.

Induction basis: For n = 1 the statement is true with equality.

Induction hypothesis: Suppose that the AM–GM statement holds for all choices of n non-negative real numbers.

Induction step: Consider n + 1 non-negative real numbers x1, . . . , xn+1, . Their arithmetic mean α satisfies

${\displaystyle (n+1)\alpha =\ x_{1}+\cdots +x_{n}+x_{n+1}.}$
If all the xi are equal to α, then we have equality in the AM–GM statement and we are done. In the case where some are not equal to α, there must exist one number that is greater than the arithmetic mean α, and one that is smaller than α. Without loss of generality, we can reorder our xi in order to place these two particular elements at the end: xn > α and xn+1 < α. Then

${\displaystyle x_{n}-\alpha >0\qquad \alpha -x_{n+1}>0}$
${\displaystyle \implies (x_{n}-\alpha )(\alpha -x_{n+1})>0\,.\qquad (*)}$
Now define y with

${\displaystyle y:=x_{n}+x_{n+1}-\alpha \geq x_{n}-\alpha >0\,,}$
and consider the n numbers x1, . . . , xn–1, y which are all non-negative. Since

${\displaystyle (n+1)\alpha =x_{1}+\cdots +x_{n-1}+x_{n}+x_{n+1}}$
${\displaystyle n\alpha =x_{1}+\cdots +x_{n-1}+\underbrace {x_{n}+x_{n+1}-\alpha } _{=\,y},}$
Thus, α is also the arithmetic mean of n numbers x1, . . . , xn–1, y and the induction hypothesis implies

${\displaystyle \alpha ^{n+1}=\alpha ^{n}\cdot \alpha \geq x_{1}x_{2}\cdots x_{n-1}y\cdot \alpha .\qquad (**)}$
Due to (*) we know that

${\displaystyle (\underbrace {x_{n}+x_{n+1}-\alpha } _{=\,y})\alpha -x_{n}x_{n+1}=(x_{n}-\alpha )(\alpha -x_{n+1})>0,}$
hence

${\displaystyle y\alpha >x_{n}x_{n+1}\,,\qquad ({*}{*}{*})}$
in particular α > 0. Therefore, if at least one of the numbers x1, . . . , xn–1 is zero, then we already have strict inequality in (**). Otherwise the right-hand side of (**) is positive and strict inequality is obtained by using the estimate (***) to get a lower bound of the right-hand side of (**). Thus, in both cases we can substitute (***) into (**) to get

${\displaystyle \alpha ^{n+1}>x_{1}x_{2}\cdots x_{n-1}x_{n}x_{n+1}\,,}$
which completes the proof.


£#h5#£Proof by induction #2£#/h5#£
First of all we shall prove that for real numbers x1 < 1 and x2 > 1 there follows

${\displaystyle x_{1}+x_{2}>x_{1}x_{2}+1.}$
Indeed, multiplying both sides of the inequality x2 > 1 by 1 – x1, gives

${\displaystyle x_{2}-x_{1}x_{2}>1-x_{1},}$
whence the required inequality is obtained immediately.

Now, we are going to prove that for positive real numbers x1, . . . , xn satisfying x1 . . . xn = 1, there holds

${\displaystyle x_{1}+\cdots +x_{n}\geq n.}$
The equality holds only if x1 = ... = xn = 1.

Induction basis: For n = 2 the statement is true because of the above property.

Induction hypothesis: Suppose that the statement is true for all natural numbers up to n – 1.

Induction step: Consider natural number n, i.e. for positive real numbers x1, . . . , xn, there holds x1 . . . xn = 1. There exists at least one xk < 1, so there must be at least one xj > 1. Without loss of generality, we let k =n – 1 and j = n.

Further, the equality x1 . . . xn = 1 we shall write in the form of (x1 . . . xn–2) (xn–1 xn) = 1. Then, the induction hypothesis implies

${\displaystyle (x_{1}+\cdots +x_{n-2})+(x_{n-1}x_{n})>n-1.}$
However, taking into account the induction basis, we have

${\displaystyle {\begin{aligned}x_{1}+\cdots +x_{n-2}+x_{n-1}+x_{n}&=(x_{1}+\cdots +x_{n-2})+(x_{n-1}+x_{n})\\&>(x_{1}+\cdots +x_{n-2})+x_{n-1}x_{n}+1\\&>n,\end{aligned}}}$
which completes the proof.

For positive real numbers a1, . . . , an, let's denote

${\displaystyle x_{1}={\frac {a_{1}}{\sqrt[{n}]{a_{1}\cdots a_{n}}}},...,x_{n}={\frac {a_{n}}{\sqrt[{n}]{a_{1}\cdots a_{n}}}}.}$
The numbers x1, . . . , xn satisfy the condition x1 . . . xn = 1. So we have

${\displaystyle {\frac {a_{1}}{\sqrt[{n}]{a_{1}\cdots a_{n}}}}+\cdots +{\frac {a_{n}}{\sqrt[{n}]{a_{1}\cdots a_{n}}}}\geq n,}$
whence we obtain

${\displaystyle {\frac {a_{1}+\cdots +a_{n}}{n}}\geq {\sqrt[{n}]{a_{1}\cdots a_{n}}},}$
with the equality holding only for a1 = ... = an.


£#h5#£Proof by Cauchy using forward–backward induction£#/h5#£
The following proof by cases relies directly on well-known rules of arithmetic but employs the rarely used technique of forward-backward-induction. It is essentially from Augustin Louis Cauchy and can be found in his Cours d'analyse.


£#h5#£The case where all the terms are equal£#/h5#£
If all the terms are equal:

${\displaystyle x_{1}=x_{2}=\cdots =x_{n},}$
then their sum is nx1, so their arithmetic mean is x1; and their product is x1n, so their geometric mean is x1; therefore, the arithmetic mean and geometric mean are equal, as desired.


£#h5#£The case where not all the terms are equal£#/h5#£
It remains to show that if not all the terms are equal, then the arithmetic mean is greater than the geometric mean. Clearly, this is only possible when n > 1.

This case is significantly more complex, and we divide it into subcases.

The subcase where n = 2
If n = 2, then we have two terms, x1 and x2, and since (by our assumption) not all terms are equal, we have:

${\displaystyle {\begin{aligned}{\Bigl (}{\frac {x_{1}+x_{2}}{2}}{\Bigr )}^{2}-x_{1}x_{2}&={\frac {1}{4}}(x_{1}^{2}+2x_{1}x_{2}+x_{2}^{2})-x_{1}x_{2}\\&={\frac {1}{4}}(x_{1}^{2}-2x_{1}x_{2}+x_{2}^{2})\\&={\Bigl (}{\frac {x_{1}-x_{2}}{2}}{\Bigr )}^{2}>0,\end{aligned}}}$
hence

${\displaystyle {\frac {x_{1}+x_{2}}{2}}\geq {\sqrt {x_{1}x_{2}}}}$
as desired.

The subcase where n = 2k
Consider the case where n = 2k, where k is a positive integer. We proceed by mathematical induction.

In the base case, k = 1, so n = 2. We have already shown that the inequality holds when n = 2, so we are done.

Now, suppose that for a given k > 1, we have already shown that the inequality holds for n = 2k−1, and we wish to show that it holds for n = 2k. To do so, we apply the inequality twice for 2k-1 numbers and once for 2 numbers to obtain:

${\displaystyle {\begin{aligned}{\frac {x_{1}+x_{2}+\cdots +x_{2^{k}}}{2^{k}}}&{}={\frac {{\frac {x_{1}+x_{2}+\cdots +x_{2^{k-1}}}{2^{k-1}}}+{\frac {x_{2^{k-1}+1}+x_{2^{k-1}+2}+\cdots +x_{2^{k}}}{2^{k-1}}}}{2}}\\[7pt]&\geq {\frac {{\sqrt[{2^{k-1}}]{x_{1}x_{2}\cdots x_{2^{k-1}}}}+{\sqrt[{2^{k-1}}]{x_{2^{k-1}+1}x_{2^{k-1}+2}\cdots x_{2^{k}}}}}{2}}\\[7pt]&\geq {\sqrt {{\sqrt[{2^{k-1}}]{x_{1}x_{2}\cdots x_{2^{k-1}}}}{\sqrt[{2^{k-1}}]{x_{2^{k-1}+1}x_{2^{k-1}+2}\cdots x_{2^{k}}}}}}\\[7pt]&={\sqrt[{2^{k}}]{x_{1}x_{2}\cdots x_{2^{k}}}}\end{aligned}}}$
where in the first inequality, the two sides are equal only if

${\displaystyle x_{1}=x_{2}=\cdots =x_{2^{k-1}}}$
and

${\displaystyle x_{2^{k-1}+1}=x_{2^{k-1}+2}=\cdots =x_{2^{k}}}$
(in which case the first arithmetic mean and first geometric mean are both equal to x1, and similarly with the second arithmetic mean and second geometric mean); and in the second inequality, the two sides are only equal if the two geometric means are equal. Since not all 2k numbers are equal, it is not possible for both inequalities to be equalities, so we know that:

${\displaystyle {\frac {x_{1}+x_{2}+\cdots +x_{2^{k}}}{2^{k}}}\geq {\sqrt[{2^{k}}]{x_{1}x_{2}\cdots x_{2^{k}}}}}$
as desired.

The subcase where n < 2k
If n is not a natural power of 2, then it is certainly less than some natural power of 2, since the sequence 2, 4, 8, . . . , 2k, . . . is unbounded above. Therefore, without loss of generality, let m be some natural power of 2 that is greater than n.

So, if we have n terms, then let us denote their arithmetic mean by α, and expand our list of terms thus:

${\displaystyle x_{n+1}=x_{n+2}=\cdots =x_{m}=\alpha .}$
We then have:

${\displaystyle {\begin{aligned}\alpha &={\frac {x_{1}+x_{2}+\cdots +x_{n}}{n}}\\[6pt]&={\frac {{\frac {m}{n}}\left(x_{1}+x_{2}+\cdots +x_{n}\right)}{m}}\\[6pt]&={\frac {x_{1}+x_{2}+\cdots +x_{n}+{\frac {(m-n)}{n}}\left(x_{1}+x_{2}+\cdots +x_{n}\right)}{m}}\\[6pt]&={\frac {x_{1}+x_{2}+\cdots +x_{n}+\left(m-n\right)\alpha }{m}}\\[6pt]&={\frac {x_{1}+x_{2}+\cdots +x_{n}+x_{n+1}+\cdots +x_{m}}{m}}\\[6pt]&\geq {\sqrt[{m}]{x_{1}x_{2}\cdots x_{n}x_{n+1}\cdots x_{m}}}\\[6pt]&={\sqrt[{m}]{x_{1}x_{2}\cdots x_{n}\alpha ^{m-n}}}\,,\end{aligned}}}$
so

${\displaystyle \alpha ^{m}\geq x_{1}x_{2}\cdots x_{n}\alpha ^{m-n}}$
and

${\displaystyle \alpha \geq {\sqrt[{n}]{x_{1}x_{2}\cdots x_{n}}}}$
as desired.


£#h5#£Proof by induction using basic calculus£#/h5#£
The following proof uses mathematical induction and some basic differential calculus.

Induction basis: For n = 1 the statement is true with equality.

Induction hypothesis: Suppose that the AM–GM statement holds for all choices of n non-negative real numbers.

Induction step: In order to prove the statement for n + 1 non-negative real numbers x1, . . . , xn, xn+1, we need to prove that

${\displaystyle {\frac {x_{1}+\cdots +x_{n}+x_{n+1}}{n+1}}-({x_{1}\cdots x_{n}x_{n+1}})^{\frac {1}{n+1}}\geq 0}$
with equality only if all the n + 1 numbers are equal.

If all numbers are zero, the inequality holds with equality. If some but not all numbers are zero, we have strict inequality. Therefore, we may assume in the following, that all n + 1 numbers are positive.

We consider the last number xn+1 as a variable and define the function

${\displaystyle f(t)={\frac {x_{1}+\cdots +x_{n}+t}{n+1}}-({x_{1}\cdots x_{n}t})^{\frac {1}{n+1}},\qquad t>0.}$
Proving the induction step is equivalent to showing that f(t) ≥ 0 for all t > 0, with f(t) = 0 only if x1, . . . , xn and t are all equal. This can be done by analyzing the critical points of f using some basic calculus.

The first derivative of f is given by

${\displaystyle f'(t)={\frac {1}{n+1}}-{\frac {1}{n+1}}({x_{1}\cdots x_{n}})^{\frac {1}{n+1}}t^{-{\frac {n}{n+1}}},\qquad t>0.}$
A critical point t0 has to satisfy f′(t0) = 0, which means

${\displaystyle ({x_{1}\cdots x_{n}})^{\frac {1}{n+1}}t_{0}^{-{\frac {n}{n+1}}}=1.}$
After a small rearrangement we get

${\displaystyle t_{0}^{\frac {n}{n+1}}=({x_{1}\cdots x_{n}})^{\frac {1}{n+1}},}$
and finally

${\displaystyle t_{0}=({x_{1}\cdots x_{n}})^{\frac {1}{n}},}$
which is the geometric mean of x1, . . . , xn. This is the only critical point of f. Since f′′(t) > 0 for all t > 0, the function f is strictly convex and has a strict global minimum at t0. Next we compute the value of the function at this global minimum:

${\displaystyle {\begin{aligned}f(t_{0})&={\frac {x_{1}+\cdots +x_{n}+({x_{1}\cdots x_{n}})^{1/n}}{n+1}}-({x_{1}\cdots x_{n}})^{\frac {1}{n+1}}({x_{1}\cdots x_{n}})^{\frac {1}{n(n+1)}}\\&={\frac {x_{1}+\cdots +x_{n}}{n+1}}+{\frac {1}{n+1}}({x_{1}\cdots x_{n}})^{\frac {1}{n}}-({x_{1}\cdots x_{n}})^{\frac {1}{n}}\\&={\frac {x_{1}+\cdots +x_{n}}{n+1}}-{\frac {n}{n+1}}({x_{1}\cdots x_{n}})^{\frac {1}{n}}\\&={\frac {n}{n+1}}{\Bigl (}{\frac {x_{1}+\cdots +x_{n}}{n}}-({x_{1}\cdots x_{n}})^{\frac {1}{n}}{\Bigr )}\\&\geq 0,\end{aligned}}}$
where the final inequality holds due to the induction hypothesis. The hypothesis also says that we can have equality only when x1, . . . , xn are all equal. In this case, their geometric mean  t0 has the same value, Hence, unless x1, . . . , xn, xn+1 are all equal, we have f(xn+1) > 0. This completes the proof.

This technique can be used in the same manner to prove the generalized AM–GM inequality and Cauchy–Schwarz inequality in Euclidean space Rn.


£#h5#£Proof by Pólya using the exponential function£#/h5#£
George Pólya provided a proof similar to what follows. Let f(x) = ex–1 – x for all real x, with first derivative f′(x) = ex–1 – 1 and second derivative f′′(x) = ex–1. Observe that f(1) = 0, f′(1) = 0 and f′′(x) > 0 for all real x, hence f is strictly convex with the absolute minimum at x = 1. Hence x ≤ ex–1 for all real x with equality only for x = 1.

Consider a list of non-negative real numbers x1, x2, . . . , xn. If they are all zero, then the AM–GM inequality holds with equality. Hence we may assume in the following for their arithmetic mean α > 0. By n-fold application of the above inequality, we obtain that

${\displaystyle {\begin{aligned}{{\frac {x_{1}}{\alpha }}{\frac {x_{2}}{\alpha }}\cdots {\frac {x_{n}}{\alpha }}}&\leq {e^{{\frac {x_{1}}{\alpha }}-1}e^{{\frac {x_{2}}{\alpha }}-1}\cdots e^{{\frac {x_{n}}{\alpha }}-1}}\\&=\exp {\Bigl (}{\frac {x_{1}}{\alpha }}-1+{\frac {x_{2}}{\alpha }}-1+\cdots +{\frac {x_{n}}{\alpha }}-1{\Bigr )},\qquad (*)\end{aligned}}}$
with equality if and only if xi = α for every i ∈ {1, . . . , n}. The argument of the exponential function can be simplified:

${\displaystyle {\begin{aligned}{\frac {x_{1}}{\alpha }}-1+{\frac {x_{2}}{\alpha }}-1+\cdots +{\frac {x_{n}}{\alpha }}-1&={\frac {x_{1}+x_{2}+\cdots +x_{n}}{\alpha }}-n\\&={\frac {n\alpha }{\alpha }}-n\\&=0.\end{aligned}}}$
Returning to (*),

${\displaystyle {\frac {x_{1}x_{2}\cdots x_{n}}{\alpha ^{n}}}\leq e^{0}=1,}$
which produces x1 x2 · · · xn ≤ αn, hence the result

${\displaystyle {\sqrt[{n}]{x_{1}x_{2}\cdots x_{n}}}\leq \alpha .}$

£#h5#£Proof by Lagrangian Multipliers£#/h5#£
If any of the ${\displaystyle x_{i}}$ are ${\displaystyle 0}$ , then there is nothing to prove. So we may assume all the ${\displaystyle x_{i}}$ are strictly positive.

Because the arithmetic and geometric means are homogeneous of degree 1, without loss of generality assume that ${\displaystyle \prod _{i=1}^{n}x_{i}=1}$ . Set ${\displaystyle G(x_{1},x_{2},\ldots ,x_{n})=\prod _{i=1}^{n}x_{i}}$ , and ${\displaystyle F(x_{1},x_{2},\ldots ,x_{n})={\frac {1}{n}}\sum _{i=1}^{n}x_{i}}$ . The inequality will be proved (together with the equality case) if we can show that the minimum of ${\displaystyle F(x_{1},x_{2},...,x_{n}),}$ subject to the constraint ${\displaystyle G(x_{1},x_{2},\ldots ,x_{n})=1,}$ is equal to ${\displaystyle 1}$ , and the minimum is only achieved when ${\displaystyle x_{1}=x_{2}=\cdots =x_{n}=1}$ . Let us first show that the constrained minimization problem has a global minimum.

Set ${\displaystyle K=\{(x_{1},x_{2},\ldots ,x_{n})\colon 0\leq x_{1},x_{2},\ldots ,x_{n}\leq n\}}$ . Since the intersection ${\displaystyle K\cap \{G=1\}}$ is compact, the extreme value theorem guarantees that the minimum of ${\displaystyle F(x_{1},x_{2},...,x_{n})}$ subject to the constraints ${\displaystyle G(x_{1},x_{2},\ldots ,x_{n})=1}$ and ${\displaystyle (x_{1},x_{2},\ldots ,x_{n})\in K}$ is attained at some point inside ${\displaystyle K}$ . On the other hand, observe that if any of the ${\displaystyle x_{i}>n}$ , then ${\displaystyle F(x_{1},x_{2},\ldots ,x_{n})>1}$ , while ${\displaystyle F(1,1,\ldots ,1)=1}$ , and ${\displaystyle (1,1,\ldots ,1)\in K\cap \{G=1\}}$ . This means that the minimum inside ${\displaystyle K\cap \{G=1\}}$ is in fact a global minimum, since the value of ${\displaystyle F}$ at any point inside ${\displaystyle K\cap \{G=1\}}$ is certainly no smaller than the minimum, and the value of ${\displaystyle F}$ at any point ${\displaystyle (y_{1},y_{2},\ldots ,y_{n})}$ not inside ${\displaystyle K}$ is strictly bigger than the value at ${\displaystyle (1,1,\ldots ,1)}$ , which is no smaller than the minimum.

The method of Lagrange multipliers says that the global minimum is attained at a point ${\displaystyle (x_{1},x_{2},\ldots ,x_{n})}$ where the gradient of ${\displaystyle F(x_{1},x_{2},\ldots ,x_{n})}$ is ${\displaystyle \lambda }$ times the gradient of ${\displaystyle G(x_{1},x_{2},\ldots ,x_{n})}$ , for some ${\displaystyle \lambda }$ . We will show that the only point at which this happens is when ${\displaystyle x_{1}=x_{2}=\cdots =x_{n}=1}$ and ${\displaystyle F(x_{1},x_{2},...,x_{n})=1.}$

Compute ${\displaystyle {\frac {\partial F}{\partial x_{i}}}={\frac {1}{n}}}$ and

${\displaystyle {\frac {\partial G}{\partial x_{i}}}=\prod _{j\neq i}x_{j}={\frac {G(x_{1},x_{2},\ldots ,x_{n})}{x_{i}}}={\frac {1}{x_{i}}}}$

along the constraint. Setting the gradients proportional to one another therefore gives for each ${\displaystyle i}$ that ${\displaystyle {\frac {1}{n}}={\frac {\lambda }{x_{i}}},}$ and so ${\displaystyle n\lambda =x_{i}.}$ Since the left-hand side does not depend on ${\displaystyle i}$ , it follows that ${\displaystyle x_{1}=x_{2}=\cdots =x_{n}}$ , and since ${\displaystyle G(x_{1},x_{2},\ldots ,x_{n})=1}$ , it follows that ${\displaystyle x_{1}=x_{2}=\cdots =x_{n}=1}$ and ${\displaystyle F(x_{1},x_{2},\ldots ,x_{n})=1}$ , as desired.


£#h5#£Generalizations£#/h5#£
£#h5#£Weighted AM–GM inequality£#/h5#£
There is a similar inequality for the weighted arithmetic mean and weighted geometric mean. Specifically, let the nonnegative numbers x1, x2, . . . , xn and the nonnegative weights w1, w2, . . . , wn be given. Set w = w1 + w2 + · · · + wn. If w > 0, then the inequality

${\displaystyle {\frac {w_{1}x_{1}+w_{2}x_{2}+\cdots +w_{n}x_{n}}{w}}\geq {\sqrt[{w}]{x_{1}^{w_{1}}x_{2}^{w_{2}}\cdots x_{n}^{w_{n}}}}}$
holds with equality if and only if all the xk with wk > 0 are equal. Here the convention 00 = 1 is used.

If all wk = 1, this reduces to the above inequality of arithmetic and geometric means.


£#h5#£Proof using Jensen's inequality£#/h5#£
Using the finite form of Jensen's inequality for the natural logarithm, we can prove the inequality between the weighted arithmetic mean and the weighted geometric mean stated above.

Since an xk with weight wk = 0 has no influence on the inequality, we may assume in the following that all weights are positive. If all xk are equal, then equality holds. Therefore, it remains to prove strict inequality if they are not all equal, which we will assume in the following, too. If at least one xk is zero (but not all), then the weighted geometric mean is zero, while the weighted arithmetic mean is positive, hence strict inequality holds. Therefore, we may assume also that all xk are positive.

Since the natural logarithm is strictly concave, the finite form of Jensen's inequality and the functional equations of the natural logarithm imply

${\displaystyle {\begin{aligned}\ln {\Bigl (}{\frac {w_{1}x_{1}+\cdots +w_{n}x_{n}}{w}}{\Bigr )}&>{\frac {w_{1}}{w}}\ln x_{1}+\cdots +{\frac {w_{n}}{w}}\ln x_{n}\\&=\ln {\sqrt[{w}]{x_{1}^{w_{1}}x_{2}^{w_{2}}\cdots x_{n}^{w_{n}}}}.\end{aligned}}}$
Since the natural logarithm is strictly increasing,

${\displaystyle {\frac {w_{1}x_{1}+\cdots +w_{n}x_{n}}{w}}>{\sqrt[{w}]{x_{1}^{w_{1}}x_{2}^{w_{2}}\cdots x_{n}^{w_{n}}}}.}$

£#h5#£Matrix Arithmetic Geometric Mean Inequality£#/h5#£
Most matrix generalizations of the arithmetic geometric mean inequality apply on the level of unitarily invariant norms, owing to the fact that even if the matrices ${\displaystyle A}$ and ${\displaystyle B}$ are positive semi-definite the matrix ${\displaystyle AB}$ may not be positive semi-definite and hence may not have a canonical square root. In Bhatia and Kittaneh proved that for any unitarily invariant norm ${\displaystyle |||\cdot |||}$ and positive semi-definite matrices ${\displaystyle A}$ and ${\displaystyle B}$ it is the case that

${\displaystyle |||AB|||\leq {\frac {1}{2}}|||A^{2}+B^{2}|||}$
Later, in the same authors proved the stronger inequality that

${\displaystyle |||AB|||\leq {\frac {1}{4}}|||(A+B)^{2}|||}$
Finally, it is known for dimension ${\displaystyle n=2}$ that the following strongest possible matrix generalization of the arithmetic-geometric mean inequality holds, and it is conjectured to hold for all ${\displaystyle n}$

${\displaystyle |||(AB)^{\frac {1}{2}}|||\leq {\frac {1}{2}}|||A+B|||}$
This conjectured inequality was shown by Stephen Drury in 2012. Indeed, he proved

${\displaystyle {\sqrt {\sigma _{j}(AB)}}\leq {\frac {1}{2}}\lambda _{j}(A+B),\ j=1,\ldots ,n.}$
S.W. Drury, On a question of Bhatia and Kittaneh, Linear Algebra Appl. 437 (2012) 1955–1960.


£#h5#£Other generalizations£#/h5#£
Other generalizations of the inequality of arithmetic and geometric means include:

£#ul#££#li#£Muirhead's inequality,£#/li#£ £#li#£Maclaurin's inequality,£#/li#£ £#li#£Generalized mean inequality,£#/li#£ £#li#£Means of complex numbers.£#/li#££#/ul#£
£#h5#£See also£#/h5#£ £#ul#££#li#£Hoffman's packing puzzle£#/li#£ £#li#£Ky Fan inequality£#/li#£ £#li#£Young's inequality for products£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Arthur Lohwater (1982). "Introduction to Inequalities". Online e-book in PDF format.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Abramowitz, M. and Stegun, I. A. (Eds.). "The Process of the Arithmetic-Geometric Mean." §17.6 in Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing. New York: Dover, pp. 571 and 598-599, 1972.£#/li#££#li#£Borwein, J. and Bailey, D. Mathematics by Experiment: Plausible Reasoning in the 21st Century. Wellesley, MA: A K Peters, 2003.£#/li#££#li#£Borwein, J. M. Problem 10281. "A Cubic Relative of the AGM." Amer. Math. Monthly 103, 181-183, 1996.£#/li#££#li#£Borwein, J. M. and Borwein, P. B. Pi & the AGM: A Study in Analytic Number Theory and Computational Complexity. New York: Wiley, 1987.£#/li#££#li#£Borwein, J. M. and Borwein, P. B. "A Remarkable Cubic Iteration." In Computational Method & Function Theory: Proc. Conference Held in Valparaiso, Chile, March 13-18, 1989 (Ed. A. Dold, B. Eckmann, F. Takens, E. B. Saff, S. Ruscheweyh, L. C. Salinas, and R. S. Varga). New York: Springer-Verlag, 1990.£#/li#££#li#£Borwein, J. M. and Borwein, P. B. "A Cubic Counterpart of Jacobi's Identity and the AGM." Trans. Amer. Math. Soc. 323, 691-701, 1991.£#/li#££#li#£Press, W. H.; Flannery, B. P.; Teukolsky, S. A.; and Vetterling, W. T. Numerical Recipes in FORTRAN: The Art of Scientific Computing, 2nd ed. Cambridge, England: Cambridge University Press, pp. 906-907, 1992.£#/li#££#li#£Sloane, N. J. A. Sequences A014549, A068521, A084895, A084896, and A084897 in "The On-Line Encyclopedia of Integer Sequences."£#/li#££#li#£ Abramowitz, M. and Stegun, I. A. (Eds.). "The Process of the Arithmetic-Geometric Mean." §17.6 in Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing. New York: Dover, pp. 571 and 598-599, 1972. £#/li#££#li#£ Borwein, J. and Bailey, D. Mathematics by Experiment: Plausible Reasoning in the 21st Century. Wellesley, MA: A K Peters, 2003. £#/li#££#li#£ Borwein, J. M. Problem 10281. "A Cubic Relative of the AGM." Amer. Math. Monthly 103, 181-183, 1996. £#/li#££#li#£ Borwein, J. M. and Borwein, P. B. Pi & the AGM: A Study in Analytic Number Theory and Computational Complexity. New York: Wiley, 1987. £#/li#££#li#£ Borwein, J. M. and Borwein, P. B. "A Remarkable Cubic Iteration." In Computational Method & Function Theory: Proc. Conference Held in Valparaiso, Chile, March 13-18, 1989 (Ed. A. Dold, B. Eckmann, F. Takens, E. B. Saff, S. Ruscheweyh, L. C. Salinas, and R. S. Varga). New York: Springer-Verlag, 1990. £#/li#££#li#£ Borwein, J. M. and Borwein, P. B. "A Cubic Counterpart of Jacobi's Identity and the AGM." Trans. Amer. Math. Soc. 323, 691-701, 1991. £#/li#££#li#£ Press, W. H.; Flannery, B. P.; Teukolsky, S. A.; and Vetterling, W. T. Numerical Recipes in FORTRAN: The Art of Scientific Computing, 2nd ed. Cambridge, England: Cambridge University Press, pp. 906-907, 1992. £#/li#££#li#£ Sloane, N. J. A. Sequences A014549, A068521, A084895, A084896, and A084897 in "The On-Line Encyclopedia of Integer Sequences." £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Means £#/li#££#li#£ Calculus and Analysis > Special Functions > Arithmetic-Geometric Mean £#/li#££#/ul#£




£#h3#£Arithmetic-Harmonic Mean£#/h3#£

In mathematics, the geometric mean is a mean or average, which indicates the central tendency or typical value of a set of numbers by using the product of their values (as opposed to the arithmetic mean which uses their sum). The geometric mean is defined as the nth root of the product of n numbers, i.e., for a set of numbers x1, x2, ..., xn, the geometric mean is defined as

${\displaystyle \left(\prod _{i=1}^{n}x_{i}\right)^{\frac {1}{n}}={\sqrt[{n}]{x_{1}x_{2}\cdots x_{n}}}}$
or, equivalently, as the arithmetic mean in logscale:

${\displaystyle \exp {\left({{\frac {1}{n}}\sum \limits _{i=1}^{n}\ln a_{i}}\right)}}$
For instance, the geometric mean of two numbers, say 2 and 8, is just the square root of their product, that is, ${\displaystyle {\sqrt {2\cdot 8}}=4}$ . As another example, the geometric mean of the three numbers 4, 1, and 1/32 is the cube root of their product (1/8), which is 1/2, that is, ${\displaystyle {\sqrt[{3}]{4\cdot 1\cdot 1/32}}=1/2}$ . The geometric mean applies only to positive numbers.

The geometric mean is often used for a set of numbers whose values are meant to be multiplied together or are exponential in nature, such as a set of growth figures: values of the human population or interest rates of a financial investment over time. It also applies to benchmarking, where it is particularly useful for computing means of speedup ratios: since the mean of 0.5x (half as fast) and 2x (twice as fast) will be 1 (i.e., no speedup overall).

The geometric mean can be understood in terms of geometry. The geometric mean of two numbers, ${\displaystyle a}$ and ${\displaystyle b}$ , is the length of one side of a square whose area is equal to the area of a rectangle with sides of lengths ${\displaystyle a}$ and ${\displaystyle b}$ . Similarly, the geometric mean of three numbers, ${\displaystyle a}$ , ${\displaystyle b}$ , and ${\displaystyle c}$ , is the length of one edge of a cube whose volume is the same as that of a cuboid with sides whose lengths are equal to the three given numbers.

The geometric mean is one of the three classical Pythagorean means, together with the arithmetic mean and the harmonic mean. For all positive data sets containing at least one pair of unequal values, the harmonic mean is always the least of the three means, while the arithmetic mean is always the greatest of the three and the geometric mean is always in between (see Inequality of arithmetic and geometric means.)


£#h5#£Calculation£#/h5#£
The geometric mean of a data set ${\textstyle \left\{a_{1},a_{2},\,\ldots ,\,a_{n}\right\}}$ is given by:

${\displaystyle \left(\prod _{i=1}^{n}a_{i}\right)^{\frac {1}{n}}={\sqrt[{n}]{a_{1}a_{2}\cdots a_{n}}}.}$
The above figure uses capital pi notation to show a series of multiplications. Each side of the equal sign shows that a set of values is multiplied in succession (the number of values is represented by "n") to give a total product of the set, and then the nth root of the total product is taken to give the geometric mean of the original set. For example, in a set of four numbers ${\textstyle \{1,2,3,4\}}$ , the product of ${\textstyle 1\times 2\times 3\times 4}$ is ${\textstyle 24}$ , and the geometric mean is the fourth root of 24, or ~ 2.213. The exponent ${\textstyle {\frac {1}{n}}}$ on the left side is equivalent to the taking nth root. For example, ${\textstyle 24^{\frac {1}{4}}={\sqrt[{4}]{24}}}$ .


£#h5#£Iterative means£#/h5#£
The geometric mean of a data set is less than the data set's arithmetic mean unless all members of the data set are equal, in which case the geometric and arithmetic means are equal. This allows the definition of the arithmetic-geometric mean, an intersection of the two which always lies in between.

The geometric mean is also the arithmetic-harmonic mean in the sense that if two sequences ( ${\textstyle a_{n}}$ ) and ( ${\textstyle h_{n}}$ ) are defined:

${\displaystyle a_{n+1}={\frac {a_{n}+h_{n}}{2}},\quad a_{0}=x}$
and

${\displaystyle h_{n+1}={\frac {2}{{\frac {1}{a_{n}}}+{\frac {1}{h_{n}}}}},\quad h_{0}=y}$
where ${\textstyle h_{n+1}}$ is the harmonic mean of the previous values of the two sequences, then ${\textstyle a_{n}}$ and ${\textstyle h_{n}}$ will converge to the geometric mean of ${\textstyle x}$ and ${\textstyle y}$ . The sequences converge to a common limit, and the geometric mean is preserved:

${\displaystyle {\sqrt {a_{i}h_{i}}}={\sqrt {\frac {a_{i}+h_{i}}{\frac {a_{i}+h_{i}}{h_{i}a_{i}}}}}={\sqrt {\frac {a_{i}+h_{i}}{{\frac {1}{a_{i}}}+{\frac {1}{h_{i}}}}}}={\sqrt {a_{i+1}h_{i+1}}}}$
Replacing the arithmetic and harmonic mean by a pair of generalized means of opposite, finite exponents yields the same result.


£#h5#£Relationship with logarithms£#/h5#£
The geometric mean can also be expressed as the exponential of the arithmetic mean of logarithms. By using logarithmic identities to transform the formula, the multiplications can be expressed as a sum and the power as a multiplication:

When ${\displaystyle a_{1},a_{2},\dots ,a_{n}>0}$

${\displaystyle \left(\prod _{i=1}^{n}a_{i}\right)^{\frac {1}{n}}=\exp \left[{\frac {1}{n}}\sum _{i=1}^{n}\ln a_{i}\right];}$
As:
${\displaystyle {\begin{aligned}\left(\prod _{i=1}^{n}a_{i}\right)^{\frac {1}{n}}&={\sqrt[{n}]{a_{1}a_{2}\cdots a_{n}}}\\&=e^{\ln(a_{1}a_{2}\cdots a_{n})^{1/n}}\\&=e^{{\frac {1}{n}}\left(\ln a_{1}+\ln a_{2}+\cdots +\ln a_{n}\right)}\\&=e^{{\frac {1}{n}}\sum _{i=1}^{n}\ln a_{i}}\\{\text{geometric mean(}}a{\text{)}}&=e^{{\text{arithmetic mean(ln(}}a{\text{))}}}\end{aligned}}}$
alternatively, use any positive real number base, for both the logarithms and the number you are raising to the power of the arithmetic mean of the individual logarithms at that same base.
additionally, if negative values of the ${\displaystyle a_{i}}$ are allowed,

${\displaystyle \left(\prod _{i=1}^{n}a_{i}\right)^{\frac {1}{n}}=\left(\left(-1\right)^{m}\right)^{\frac {1}{n}}\exp \left[{\frac {1}{n}}\sum _{i=1}^{n}\ln \left|a_{i}\right|\right],}$
where m is the number of negative numbers.

This is sometimes called the log-average (not to be confused with the logarithmic average). It is simply computing the arithmetic mean of the logarithm-transformed values of ${\displaystyle a_{i}}$ (i.e., the arithmetic mean on the log scale) and then using the exponentiation to return the computation to the original scale, i.e., it is the generalised f-mean with ${\displaystyle f(x)=\log x}$ . For example, the geometric mean of 2 and 8 can be calculated as the following, where ${\displaystyle b}$ is any base of a logarithm (commonly 2, ${\displaystyle e}$ or 10):

${\displaystyle b^{{\frac {1}{2}}\left[\log _{b}(2)+\log _{b}(8)\right]}=4}$
Related to the above, it can be seen that for a given sample of points ${\displaystyle a_{1},\ldots ,a_{n}}$ , the geometric mean is the minimizer of

${\displaystyle f(a)=\sum _{i=1}^{n}(\log(a_{i})-\log(a))^{2}=\sum _{i=1}^{n}(\log(a_{i}/a))^{2}}$ ,
whereas the arithmetic mean is the minimizer of

${\displaystyle f(a)=\sum _{i=1}^{n}(a_{i}-a)^{2}}$ .
Thus, the geometric mean provides a summary of the samples whose exponent best matches the exponents of the samples (in the least squares sense).

The log form of the geometric mean is generally the preferred alternative for implementation in computer languages because calculating the product of many numbers can lead to an arithmetic overflow or arithmetic underflow. This is less likely to occur with the sum of the logarithms for each number.


£#h5#£Comparison to arithmetic mean£#/h5#£
The geometric mean of a non-empty data set of (positive) numbers is always at most their arithmetic mean. Equality is only obtained when all numbers in the data set are equal; otherwise, the geometric mean is smaller. For example, the geometric mean of 2 and 3 is 2.45, while their arithmetic mean is 2.5. In particular, this means that when a set of non-identical numbers is subjected to a mean-preserving spread — that is, the elements of the set are "spread apart" more from each other while leaving the arithmetic mean unchanged — their geometric mean decreases.


£#h5#£Average growth rate£#/h5#£
In many cases the geometric mean is the best measure to determine the average growth rate of some quantity. (For example, if in one year sales increases by 80% and the next year by 25%, the end result is the same as that of a constant growth rate of 50%, since the geometric mean of 1.80 and 1.25 is 1.50.) In order to determine the average growth rate, it is not necessary to take the product of the measured growth rates at every step. Let the quantity be given as the sequence ${\displaystyle a_{0},a_{1},...,a_{n}}$ , where ${\displaystyle n}$ is the number of steps from the initial to final state. The growth rate between successive measurements ${\displaystyle a_{k}}$ and ${\displaystyle a_{k+1}}$ is ${\displaystyle a_{k+1}/a_{k}}$ . The geometric mean of these growth rates is then just:

${\displaystyle \left({\frac {a_{1}}{a_{0}}}{\frac {a_{2}}{a_{1}}}\cdots {\frac {a_{n}}{a_{n-1}}}\right)^{\frac {1}{n}}=\left({\frac {a_{n}}{a_{0}}}\right)^{\frac {1}{n}}.}$

£#h5#£Application to normalized values£#/h5#£
The fundamental property of the geometric mean, which does not hold for any other mean, is that for two sequences ${\displaystyle X}$ and ${\displaystyle Y}$ of equal length,

${\displaystyle \operatorname {GM} \left({\frac {X_{i}}{Y_{i}}}\right)={\frac {\operatorname {GM} (X_{i})}{\operatorname {GM} (Y_{i})}}}$
This makes the geometric mean the only correct mean when averaging normalized results; that is, results that are presented as ratios to reference values. This is the case when presenting computer performance with respect to a reference computer, or when computing a single average index from several heterogeneous sources (for example, life expectancy, education years, and infant mortality). In this scenario, using the arithmetic or harmonic mean would change the ranking of the results depending on what is used as a reference. For example, take the following comparison of execution time of computer programs:

Table 1

The arithmetic and geometric means "agree" that computer C is the fastest. However, by presenting appropriately normalized values and using the arithmetic mean, we can show either of the other two computers to be the fastest. Normalizing by A's result gives A as the fastest computer according to the arithmetic mean:

Table 2

while normalizing by B's result gives B as the fastest computer according to the arithmetic mean but A as the fastest according to the harmonic mean:

Table 3

and normalizing by C's result gives C as the fastest computer according to the arithmetic mean but A as the fastest according to the harmonic mean:

Table 4

In all cases, the ranking given by the geometric mean stays the same as the one obtained with unnormalized values.

However, this reasoning has been questioned. Giving consistent results is not always equal to giving the correct results. In general, it is more rigorous to assign weights to each of the programs, calculate the average weighted execution time (using the arithmetic mean), and then normalize that result to one of the computers. The three tables above just give a different weight to each of the programs, explaining the inconsistent results of the arithmetic and harmonic means (Table 4 gives equal weight to both programs, the Table 2 gives a weight of 1/1000 to the second program, and the Table 3 gives a weight of 1/100 to the second program and 1/10 to the first one). The use of the geometric mean for aggregating performance numbers should be avoided if possible, because multiplying execution times has no physical meaning, in contrast to adding times as in the arithmetic mean. Metrics that are inversely proportional to time (speedup, IPC) should be averaged using the harmonic mean.

The geometric mean can be derived from the generalized mean as its limit as ${\displaystyle p}$ goes to zero. Similarly, this is possible for the weighted geometric mean.


£#h5#£Geometric mean of a continuous function£#/h5#£
If ${\displaystyle f:[a,b]\to (0,\infty )}$ is a positive continuous real-valued function, its geometric mean over this interval is

${\displaystyle {\text{GM}}[f]=\exp \left({\frac {1}{b-a}}\int _{a}^{b}\ln f(x)dx\right)}$
For instance, taking the identity function ${\displaystyle f(x)=x}$ over the unit interval shows that the geometric mean of the positive numbers between 0 and 1 is equal to ${\displaystyle {\frac {1}{e}}}$ .


£#h5#£Applications£#/h5#£
£#h5#£Proportional growth£#/h5#£
The geometric mean is more appropriate than the arithmetic mean for describing proportional growth, both exponential growth (constant proportional growth) and varying growth; in business the geometric mean of growth rates is known as the compound annual growth rate (CAGR). The geometric mean of growth over periods yields the equivalent constant growth rate that would yield the same final amount.

Suppose an orange tree yields 100 oranges one year and then 180, 210 and 300 the following years, so the growth is 80%, 16.6666% and 42.8571% for each year respectively. Using the arithmetic mean calculates a (linear) average growth of 46.5079% (80% + 16.6666% + 42.8571%, that sum then divided by 3). However, if we start with 100 oranges and let it grow 46.5079% each year, the result is 314 oranges, not 300, so the linear average over-states the year-on-year growth.

Instead, we can use the geometric mean. Growing with 80% corresponds to multiplying with 1.80, so we take the geometric mean of 1.80, 1.166666 and 1.428571, i.e. ${\displaystyle {\sqrt[{3}]{1.80\times 1.166666\times 1.428571}}\approx 1.442249}$ ; thus the "average" growth per year is 44.2249%. If we start with 100 oranges and let the number grow with 44.2249% each year, the result is 300 oranges.


£#h5#£Financial£#/h5#£
The geometric mean has from time to time been used to calculate financial indices (the averaging is over the components of the index). For example, in the past the FT 30 index used a geometric mean. It is also used in the recently introduced "RPIJ" measure of inflation in the United Kingdom and in the European Union.

This has the effect of understating movements in the index compared to using the arithmetic mean.


£#h5#£Applications in the social sciences£#/h5#£
Although the geometric mean has been relatively rare in computing social statistics, starting from 2010 the United Nations Human Development Index did switch to this mode of calculation, on the grounds that it better reflected the non-substitutable nature of the statistics being compiled and compared:

The geometric mean decreases the level of substitutability between dimensions [being compared] and at the same time ensures that a 1 percent decline in say life expectancy at birth has the same impact on the HDI as a 1 percent decline in education or income. Thus, as a basis for comparisons of achievements, this method is also more respectful of the intrinsic differences across the dimensions than a simple average.
Not all values used to compute the HDI (Human Development Index) are normalized; some of them instead have the form ${\displaystyle \left(X-X_{\text{min}}\right)/\left(X_{\text{norm}}-X_{\text{min}}\right)}$ . This makes the choice of the geometric mean less obvious than one would expect from the "Properties" section above.

The equally distributed welfare equivalent income associated with an Atkinson Index with an inequality aversion parameter of 1.0 is simply the geometric mean of incomes. For values other than one, the equivalent value is an Lp norm divided by the number of elements, with p equal to one minus the inequality aversion parameter.


£#h5#£Geometry£#/h5#£
In the case of a right triangle, its altitude is the length of a line extending perpendicularly from the hypotenuse to its 90° vertex. Imagining that this line splits the hypotenuse into two segments, the geometric mean of these segment lengths is the length of the altitude. This property is known as the geometric mean theorem.

In an ellipse, the semi-minor axis is the geometric mean of the maximum and minimum distances of the ellipse from a focus; it is also the geometric mean of the semi-major axis and the semi-latus rectum. The semi-major axis of an ellipse is the geometric mean of the distance from the center to either focus and the distance from the center to either directrix.

Another way to think about it is as follows:

Consider a circle with radius ${\displaystyle r}$ . Now take two diametrically opposite points on the circle and apply pressure from both ends to deform it into an ellipse with semi-major and semi-minor axes of lengths ${\displaystyle a}$ and ${\displaystyle b}$ .

Since the area of the circle and the ellipse stays the same, we have:

${\displaystyle {\begin{aligned}\pi r^{2}&=\pi ab\\r^{2}&=ab\\r&={\sqrt {ab}}\end{aligned}}}$

The radius of the circle is the geometric mean of the semi-major and the semi-minor axes of the ellipse formed by deforming the circle.

Distance to the horizon of a sphere is approximately equal to the geometric mean of the distance to the closest point of the sphere and the distance to the farthest point of the sphere when the distance to the closest point of the sphere is small.

Both in the approximation of squaring the circle according to S.A. Ramanujan (1914) and in the construction of the Heptadecagon according to "sent by T. P. Stowell, credited to Leybourn's Math. Repository, 1818", the geometric mean is employed.


£#h5#£Aspect ratios£#/h5#£
The geometric mean has been used in choosing a compromise aspect ratio in film and video: given two aspect ratios, the geometric mean of them provides a compromise between them, distorting or cropping both in some sense equally. Concretely, two equal area rectangles (with the same center and parallel sides) of different aspect ratios intersect in a rectangle whose aspect ratio is the geometric mean, and their hull (smallest rectangle which contains both of them) likewise has the aspect ratio of their geometric mean.

In the choice of 16:9 aspect ratio by the SMPTE, balancing 2.35 and 4:3, the geometric mean is ${\textstyle {\sqrt {2.35\times {\frac {4}{3}}}}\approx 1.7701}$ , and thus ${\textstyle 16:9=1.77{\overline {7}}}$ ... was chosen. This was discovered empirically by Kerns Powers, who cut out rectangles with equal areas and shaped them to match each of the popular aspect ratios. When overlapped with their center points aligned, he found that all of those aspect ratio rectangles fit within an outer rectangle with an aspect ratio of 1.77:1 and all of them also covered a smaller common inner rectangle with the same aspect ratio 1.77:1. The value found by Powers is exactly the geometric mean of the extreme aspect ratios, 4:3 (1.33:1) and CinemaScope (2.35:1), which is coincidentally close to ${\textstyle 16:9}$ ( ${\textstyle 1.77{\overline {7}}:1}$ ). The intermediate ratios have no effect on the result, only the two extreme ratios.

Applying the same geometric mean technique to 16:9 and 4:3 approximately yields the 14:9 ( ${\textstyle 1.55{\overline {5}}}$ ...) aspect ratio, which is likewise used as a compromise between these ratios. In this case 14:9 is exactly the arithmetic mean of ${\textstyle 16:9}$ and ${\textstyle 4:3=12:9}$ , since 14 is the average of 16 and 12, while the precise geometric mean is ${\textstyle {\sqrt {{\frac {16}{9}}\times {\frac {4}{3}}}}\approx 1.5396\approx 13.8:9,}$ but the two different means, arithmetic and geometric, are approximately equal because both numbers are sufficiently close to each other (a difference of less than 2%).


£#h5#£Paper formats£#/h5#£
The geometric mean is also used to calculate B and C series paper formats. The ${\displaystyle B_{n}}$ format has an area which is the geometric mean of the areas of ${\displaystyle A_{n}}$ and ${\displaystyle A_{n-1}}$ . For example, the area of a B1 paper is ${\displaystyle {\frac {\sqrt {2}}{2}}\mathrm {m} ^{2}}$ , because it is the geometric mean of the areas of an A0 ( ${\displaystyle 1\mathrm {m} ^{2}}$ ) and an A1 ( ${\displaystyle {\frac {1}{2}}\mathrm {m} ^{2}}$ ) paper ( ${\displaystyle {\sqrt {1\mathrm {m} ^{2}\cdot {\frac {1}{2}}\mathrm {m} ^{2}}}={\sqrt {{\frac {1}{2}}\mathrm {m} ^{4}}}={\frac {1}{\sqrt {2}}}\mathrm {m} ^{2}={\frac {\sqrt {2}}{2}}\mathrm {m} ^{2}}$ ).

The same principle applies with the C series, whose area is the geometric mean of the A and B series. For example, the C4 format has an area which is the geometric mean of the areas of A4 and B4.

An advantage that comes from this relationship is that an A4 paper fits inside a C4 envelope, and both fit inside a B4 envelope.


£#h5#£Other applications£#/h5#£ £#ul#££#li#£Spectral flatness: in signal processing, spectral flatness, a measure of how flat or spiky a spectrum is, is defined as the ratio of the geometric mean of the power spectrum to its arithmetic mean.£#/li#£ £#li#£Anti-reflective coatings: In optical coatings, where reflection needs to be minimised between two media of refractive indices n0 and n2, the optimum refractive index n1 of the anti-reflective coating is given by the geometric mean: ${\displaystyle n_{1}={\sqrt {n_{0}n_{2}}}}$ .£#/li#£ £#li#£Subtractive color mixing: The spectral reflectance curve for paint mixtures (of equal tinting strength, opacity and dilution) is approximately the geometric mean of the paints' individual reflectance curves computed at each wavelength of their spectra.£#/li#£ £#li#£Image processing: The geometric mean filter is used as a noise filter in image processing.£#/li#££#/ul#£
£#h5#£See also£#/h5#£
£#h5#£Notes and references£#/h5#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Calculation of the geometric mean of two numbers in comparison to the arithmetic solution£#/li#£ £#li#£Arithmetic and geometric means£#/li#£ £#li#£When to use the geometric mean£#/li#£ £#li#£Practical solutions for calculating geometric mean with different kinds of data Archived 2010-11-12 at the Wayback Machine£#/li#£ £#li#£Geometric Mean on MathWorld£#/li#£ £#li#£Geometric Meaning of the Geometric Mean£#/li#£ £#li#£Geometric Mean Calculator for larger data sets£#/li#£ £#li#£Computing Congressional apportionment using Geometric Mean £#/li#£ £#li#£Non-Newtonian calculus website£#/li#£ £#li#£Geometric Mean Definition and Formula£#/li#£ £#li#£The Distribution of the Geometric Mean£#/li#£ £#li#£The geometric mean?£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Means £#/li#££#/ul#£




£#h3#£Arithmetic-Logarithmic-Geometric Mean Inequality£#/h3#£

In mathematics, the logarithmic mean is a function of two non-negative numbers which is equal to their difference divided by the logarithm of their quotient. This calculation is applicable in engineering problems involving heat and mass transfer.


£#h5#£Definition£#/h5#£
The logarithmic mean is defined as:

${\displaystyle {\begin{aligned}M_{\text{lm}}(x,y)&=\lim _{(\xi ,\eta )\to (x,y)}{\frac {\eta -\xi }{\ln(\eta )-\ln(\xi )}}\\[6pt]&={\begin{cases}x&{\text{if }}x=y,\\{\frac {y-x}{\ln(y)-\ln(x)}}&{\text{otherwise,}}\end{cases}}\end{aligned}}}$
for the positive numbers ${\displaystyle x,y}$ .


£#h5#£Inequalities£#/h5#£
The logarithmic mean of two numbers is smaller than the arithmetic mean and the generalized mean with exponent one third but larger than the geometric mean, unless the numbers are the same, in which case all three means are equal to the numbers.

${\displaystyle {\sqrt {xy}}\leq M_{\text{lm}}(x,y)\leq \left({\frac {x^{1/3}+y^{1/3}}{2}}\right)^{3}\leq {\frac {x+y}{2}}\qquad {\text{ for all }}x>0{\text{ and }}y>0.}$

£#h5#£Derivation£#/h5#£
£#h5#£Mean value theorem of differential calculus£#/h5#£
From the mean value theorem, there exists a value ${\displaystyle \xi }$ in the interval between x and y where the derivative ${\displaystyle f'}$ equals the slope of the secant line:

${\displaystyle \exists \xi \in (x,y):\ f'(\xi )={\frac {f(x)-f(y)}{x-y}}}$
The logarithmic mean is obtained as the value of ${\displaystyle \xi }$ by substituting ${\displaystyle \ln }$ for ${\displaystyle f}$ and similarly for its corresponding derivative:

${\displaystyle {\frac {1}{\xi }}={\frac {\ln(x)-\ln(y)}{x-y}}}$
and solving for ${\displaystyle \xi }$ :

${\displaystyle \xi ={\frac {x-y}{\ln(x)-\ln(y)}}}$

£#h5#£Integration£#/h5#£
The logarithmic mean can also be interpreted as the area under an exponential curve.

${\displaystyle {\begin{aligned}L(x,y)={}&\int _{0}^{1}x^{1-t}y^{t}\ \mathrm {d} t={}\int _{0}^{1}\left({\frac {y}{x}}\right)^{t}x\ \mathrm {d} t={}x\int _{0}^{1}\left({\frac {y}{x}}\right)^{t}\mathrm {d} t\\[3pt]={}&\left.{\frac {x}{\ln \left({\frac {y}{x}}\right)}}\left({\frac {y}{x}}\right)^{t}\right|_{t=0}^{1}={}{\frac {x}{\ln \left({\frac {y}{x}}\right)}}\left({\frac {y}{x}}-1\right)={}{\frac {y-x}{\ln \left({\frac {y}{x}}\right)}}\\[3pt]={}&{\frac {y-x}{\ln \left(y\right)-\ln \left(x\right)}}\end{aligned}}}$
The area interpretation allows the easy derivation of some basic properties of the logarithmic mean. Since the exponential function is monotonic, the integral over an interval of length 1 is bounded by ${\displaystyle x}$ and ${\displaystyle y}$ . The homogeneity of the integral operator is transferred to the mean operator, that is ${\displaystyle L(cx,cy)=cL(x,y)}$ .

Two other useful integral representations are

and
£#h5#£Generalization£#/h5#£
£#h5#£Mean value theorem of differential calculus£#/h5#£
One can generalize the mean to ${\displaystyle n+1}$ variables by considering the mean value theorem for divided differences for the ${\displaystyle n}$ th derivative of the logarithm.

We obtain

${\displaystyle L_{\text{MV}}(x_{0},\,\dots ,\,x_{n})={\sqrt[{-n}]{(-1)^{(n+1)}n\ln \left(\left[x_{0},\,\dots ,\,x_{n}\right]\right)}}}$
where ${\displaystyle \ln \left(\left[x_{0},\,\dots ,\,x_{n}\right]\right)}$ denotes a divided difference of the logarithm.

For ${\displaystyle n=2}$ this leads to

${\displaystyle L_{\text{MV}}(x,y,z)={\sqrt {\frac {(x-y)\left(y-z\right)\left(z-x\right)}{2\left(\left(y-z\right)\ln \left(x\right)+\left(z-x\right)\ln \left(y\right)+\left(x-y\right)\ln \left(z\right)\right)}}}}$ .

£#h5#£Integral£#/h5#£
The integral interpretation can also be generalized to more variables, but it leads to a different result. Given the simplex ${\textstyle S}$ with ${\textstyle S=\{\left(\alpha _{0},\,\dots ,\,\alpha _{n}\right):\left(\alpha _{0}+\dots +\alpha _{n}=1\right)\land \left(\alpha _{0}\geq 0\right)\land \dots \land \left(\alpha _{n}\geq 0\right)\}}$ and an appropriate measure ${\textstyle \mathrm {d} \alpha }$ which assigns the simplex a volume of 1, we obtain

${\displaystyle L_{\text{I}}\left(x_{0},\,\dots ,\,x_{n}\right)=\int _{S}x_{0}^{\alpha _{0}}\cdot \,\cdots \,\cdot x_{n}^{\alpha _{n}}\ \mathrm {d} \alpha }$
This can be simplified using divided differences of the exponential function to

${\displaystyle L_{\text{I}}\left(x_{0},\,\dots ,\,x_{n}\right)=n!\exp \left[\ln \left(x_{0}\right),\,\dots ,\,\ln \left(x_{n}\right)\right]}$ .
Example ${\textstyle n=2}$

${\displaystyle L_{\text{I}}(x,y,z)=-2{\frac {x\left(\ln \left(y\right)-\ln \left(z\right)\right)+y\left(\ln \left(z\right)-\ln \left(x\right)\right)+z\left(\ln \left(x\right)-\ln \left(y\right)\right)}{\left(\ln \left(x\right)-\ln \left(y\right)\right)\left(\ln \left(y\right)-\ln \left(z\right)\right)\left(\ln \left(z\right)-\ln \left(x\right)\right)}}}$ .

£#h5#£Connection to other means£#/h5#£ £#ul#££#li#£Arithmetic mean: ${\displaystyle {\frac {L\left(x^{2},y^{2}\right)}{L(x,y)}}={\frac {x+y}{2}}}$ £#/li#££#/ul#££#ul#££#li#£Geometric mean: ${\displaystyle {\sqrt {\frac {L\left(x,y\right)}{L\left({\frac {1}{x}},{\frac {1}{y}}\right)}}}={\sqrt {xy}}}$ £#/li#££#/ul#££#ul#££#li#£Harmonic mean: ${\displaystyle {\frac {L\left({\frac {1}{x}},{\frac {1}{y}}\right)}{L\left({\frac {1}{x^{2}}},{\frac {1}{y^{2}}}\right)}}={\frac {2}{{\frac {1}{x}}+{\frac {1}{y}}}}}$ £#/li#££#/ul#£
£#h5#£See also£#/h5#£ £#ul#££#li#£A different mean which is related to logarithms is the geometric mean.£#/li#£ £#li#£The logarithmic mean is a special case of the Stolarsky mean.£#/li#£ £#li#£Logarithmic mean temperature difference£#/li#£ £#li#£Log semiring£#/li#££#/ul#£
£#h5#£References£#/h5#£
Citations
Bibliography
£#ul#££#li#£Oilfield Glossary: Term 'logarithmic mean'£#/li#£ £#li#£Weisstein, Eric W. "Arithmetic-Logarithmic-Geometric-Mean Inequality". MathWorld.£#/li#£ £#li#£Stolarsky, Kenneth B.: Generalizations of the logarithmic mean, Mathematics Magazine, Vol. 48, No. 2, Mar., 1975, pp 87–92£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Nelson, R. B. "Proof without Words: The Arithmetic-Logarithmic-Geometric Mean Inequality." Math. Mag. 68, 305, 1995.£#/li#££#li#£ Nelson, R. B. "Proof without Words: The Arithmetic-Logarithmic-Geometric Mean Inequality." Math. Mag. 68, 305, 1995. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Means £#/li#££#li#£ Calculus and Analysis > Inequalities £#/li#££#/ul#£




£#h3#£Arithmetical Function£#/h3#£

In number theory, an arithmetic, arithmetical, or number-theoretic function is for most authors any function f(n) whose domain is the positive integers and whose range is a subset of the complex numbers. Hardy & Wright include in their definition the requirement that an arithmetical function "expresses some arithmetical property of n".

An example of an arithmetic function is the divisor function whose value at a positive integer n is equal to the number of divisors of n.

There is a larger class of number-theoretic functions that do not fit the above definition, for example, the prime-counting functions. This article provides links to functions of both classes.

Arithmetic functions are often extremely irregular (see table), but some of them have series expansions in terms of Ramanujan's sum.


£#h5#£Multiplicative and additive functions£#/h5#£
An arithmetic function a is

£#ul#££#li#£completely additive if a(mn) = a(m) + a(n) for all natural numbers m and n;£#/li#£ £#li#£completely multiplicative if a(mn) = a(m)a(n) for all natural numbers m and n;£#/li#££#/ul#£
Two whole numbers m and n are called coprime if their greatest common divisor is 1, that is, if there is no prime number that divides both of them.

Then an arithmetic function a is

£#ul#££#li#£additive if a(mn) = a(m) + a(n) for all coprime natural numbers m and n;£#/li#£ £#li#£multiplicative if a(mn) = a(m)a(n) for all coprime natural numbers m and n.£#/li#££#/ul#£
£#h5#£Notation£#/h5#£
${\textstyle \sum _{p}f(p)}$ and ${\textstyle \prod _{p}f(p)}$ mean that the sum or product is over all prime numbers:

Similarly, ${\textstyle \sum _{p^{k}}f(p^{k})}$ and ${\textstyle \prod _{p^{k}}f(p^{k})}$ mean that the sum or product is over all prime powers with strictly positive exponent (so k = 0 is not included):

${\textstyle \sum _{d\mid n}f(d)}$ and ${\textstyle \prod _{d\mid n}f(d)}$ mean that the sum or product is over all positive divisors of n, including 1 and n. For example, if n = 12,

The notations can be combined: ${\textstyle \sum _{p\mid n}f(p)}$ and ${\textstyle \prod _{p\mid n}f(p)}$ mean that the sum or product is over all prime divisors of n. For example, if n = 18,

and similarly ${\textstyle \sum _{p^{k}\mid n}f(p^{k})}$ and ${\textstyle \prod _{p^{k}\mid n}f(p^{k})}$ mean that the sum or product is over all prime powers dividing n. For example, if n = 24,
£#h5#£Ω(n), ω(n), νp(n) – prime power decomposition£#/h5#£
The fundamental theorem of arithmetic states that any positive integer n can be represented uniquely as a product of powers of primes: ${\displaystyle n=p_{1}^{a_{1}}\cdots p_{k}^{a_{k}}}$ where p1 < p2 < ... < pk are primes and the aj are positive integers. (1 is given by the empty product.)

It is often convenient to write this as an infinite product over all the primes, where all but a finite number have a zero exponent. Define the p-adic valuation νp(n) to be the exponent of the highest power of the prime p that divides n. That is, if p is one of the pi then νp(n) = ai, otherwise it is zero. Then

In terms of the above the prime omega functions ω and Ω are defined by

To avoid repetition, whenever possible formulas for the functions listed in this article are given in terms of n and the corresponding pi, ai, ω, and Ω.


£#h5#£Multiplicative functions£#/h5#£
£#h5#£σk(n), τ(n), d(n) – divisor sums£#/h5#£
σk(n) is the sum of the kth powers of the positive divisors of n, including 1 and n, where k is a complex number.

σ1(n), the sum of the (positive) divisors of n, is usually denoted by σ(n).

Since a positive number to the zero power is one, σ0(n) is therefore the number of (positive) divisors of n; it is usually denoted by d(n) or τ(n) (for the German Teiler = divisors).

Setting k = 0 in the second product gives


£#h5#£φ(n) – Euler totient function£#/h5#£
φ(n), the Euler totient function, is the number of positive integers not greater than n that are coprime to n.


£#h5#£Jk(n) – Jordan totient function£#/h5#£
Jk(n), the Jordan totient function, is the number of k-tuples of positive integers all less than or equal to n that form a coprime (k + 1)-tuple together with n. It is a generalization of Euler's totient, φ(n) = J1(n).


£#h5#£μ(n) – Möbius function£#/h5#£
μ(n), the Möbius function, is important because of the Möbius inversion formula. See Dirichlet convolution, below.

This implies that μ(1) = 1. (Because Ω(1) = ω(1) = 0.)


£#h5#£τ(n) – Ramanujan tau function£#/h5#£
τ(n), the Ramanujan tau function, is defined by its generating function identity:

Although it is hard to say exactly what "arithmetical property of n" it "expresses", (τ(n) is (2π)−12 times the nth Fourier coefficient in the q-expansion of the modular discriminant function) it is included among the arithmetical functions because it is multiplicative and it occurs in identities involving certain σk(n) and rk(n) functions (because these are also coefficients in the expansion of modular forms).


£#h5#£cq(n) – Ramanujan's sum£#/h5#£
cq(n), Ramanujan's sum, is the sum of the nth powers of the primitive qth roots of unity:

Even though it is defined as a sum of complex numbers (irrational for most values of q), it is an integer. For a fixed value of n it is multiplicative in q:

If q and r are coprime, then ${\displaystyle c_{q}(n)c_{r}(n)=c_{qr}(n).}$

£#h5#£ψ(n) - Dedekind psi function£#/h5#£
The Dedekind psi function, used in the theory of modular functions, is defined by the formula


£#h5#£Completely multiplicative functions£#/h5#£
£#h5#£λ(n) – Liouville function£#/h5#£
λ(n), the Liouville function, is defined by


£#h5#£χ(n) – characters£#/h5#£
All Dirichlet characters χ(n) are completely multiplicative. Two characters have special notations:

The principal character (mod n) is denoted by χ0(a) (or χ1(a)). It is defined as

The quadratic character (mod n) is denoted by the Jacobi symbol for odd n (it is not defined for even n):

In this formula ${\displaystyle ({\tfrac {a}{p}})}$ is the Legendre symbol, defined for all integers a and all odd primes p by

Following the normal convention for the empty product, ${\displaystyle \left({\frac {a}{1}}\right)=1.}$


£#h5#£Additive functions£#/h5#£
£#h5#£ω(n) – distinct prime divisors£#/h5#£
ω(n), defined above as the number of distinct primes dividing n, is additive (see Prime omega function).


£#h5#£Completely additive functions£#/h5#£
£#h5#£Ω(n) – prime divisors£#/h5#£
Ω(n), defined above as the number of prime factors of n counted with multiplicities, is completely additive (see Prime omega function).


£#h5#£νp(n) – p-adic valuation of an integer n£#/h5#£
For a fixed prime p, νp(n), defined above as the exponent of the largest power of p dividing n, is completely additive.


£#h5#£Logarithmic derivative£#/h5#£
${\displaystyle \operatorname {ld} (n)={\frac {D(n)}{n}}=\sum _{\stackrel {p\mid n}{p{\text{ prime}}}}{\frac {v_{p}(n)}{p}}}$ , where ${\displaystyle D(n)}$ is the arithmetic derivative.


£#h5#£Neither multiplicative nor additive£#/h5#£
£#h5#£π(x), Π(x), θ(x), ψ(x) – prime-counting functions£#/h5#£
These important functions (which are not arithmetic functions) are defined for non-negative real arguments, and are used in the various statements and proofs of the prime number theorem. They are summation functions (see the main section just below) of arithmetic functions which are neither multiplicative nor additive.

π(x), the prime-counting function, is the number of primes not exceeding x. It is the summation function of the characteristic function of the prime numbers.

A related function counts prime powers with weight 1 for primes, 1/2 for their squares, 1/3 for cubes, ... It is the summation function of the arithmetic function which takes the value 1/k on integers which are the k-th power of some prime number, and the value 0 on other integers.

θ(x) and ψ(x), the Chebyshev functions, are defined as sums of the natural logarithms of the primes not exceeding x.

The Chebyshev function ψ(x) is the summation function of the von Mangoldt function just below.


£#h5#£Λ(n) – von Mangoldt function£#/h5#£
Λ(n), the von Mangoldt function, is 0 unless the argument n is a prime power pk, in which case it is the natural log of the prime p:


£#h5#£p(n) – partition function£#/h5#£
p(n), the partition function, is the number of ways of representing n as a sum of positive integers, where two representations with the same summands in a different order are not counted as being different:


£#h5#£λ(n) – Carmichael function£#/h5#£
λ(n), the Carmichael function, is the smallest positive number such that ${\displaystyle a^{\lambda (n)}\equiv 1{\pmod {n}}}$   for all a coprime to n. Equivalently, it is the least common multiple of the orders of the elements of the multiplicative group of integers modulo n.

For powers of odd primes and for 2 and 4, λ(n) is equal to the Euler totient function of n; for powers of 2 greater than 4 it is equal to one half of the Euler totient function of n:

and for general n it is the least common multiple of λ of each of the prime power factors of n:
£#h5#£h(n) – Class number£#/h5#£
h(n), the class number function, is the order of the ideal class group of an algebraic extension of the rationals with discriminant n. The notation is ambiguous, as there are in general many extensions with the same discriminant. See quadratic field and cyclotomic field for classical examples.


£#h5#£rk(n) – Sum of k squares£#/h5#£
rk(n) is the number of ways n can be represented as the sum of k squares, where representations that differ only in the order of the summands or in the signs of the square roots are counted as different.


£#h5#£D(n) – Arithmetic derivative£#/h5#£
Using the Heaviside notation for the derivative, D(n) is a function such that

£#ul#££#li#£ ${\displaystyle D(n)=1}$ if n prime, and£#/li#£ £#li#£ ${\displaystyle D(mn)=mD(n)+D(m)n}$ (Product rule)£#/li#££#/ul#£
£#h5#£Summation functions£#/h5#£
Given an arithmetic function a(n), its summation function A(x) is defined by

A can be regarded as a function of a real variable. Given a positive integer m, A is constant along open intervals m < x < m + 1, and has a jump discontinuity at each integer for which a(m) ≠ 0.
Since such functions are often represented by series and integrals, to achieve pointwise convergence it is usual to define the value at the discontinuities as the average of the values to the left and right:

Individual values of arithmetic functions may fluctuate wildly – as in most of the above examples. Summation functions "smooth out" these fluctuations. In some cases it may be possible to find asymptotic behaviour for the summation function for large x.

A classical example of this phenomenon is given by the divisor summatory function, the summation function of d(n), the number of divisors of n:

An average order of an arithmetic function is some simpler or better-understood function which has the same summation function asymptotically, and hence takes the same values "on average". We say that g is an average order of f if

as x tends to infinity. The example above shows that d(n) has the average order log(n).


£#h5#£Dirichlet convolution£#/h5#£
Given an arithmetic function a(n), let Fa(s), for complex s, be the function defined by the corresponding Dirichlet series (where it converges):

Fa(s) is called a generating function of a(n). The simplest such series, corresponding to the constant function a(n) = 1 for all n, is ς(s) the Riemann zeta function.
The generating function of the Möbius function is the inverse of the zeta function:

Consider two arithmetic functions a and b and their respective generating functions Fa(s) and Fb(s). The product Fa(s)Fb(s) can be computed as follows:

It is a straightforward exercise to show that if c(n) is defined by

then
This function c is called the Dirichlet convolution of a and b, and is denoted by ${\displaystyle a*b}$ .

A particularly important case is convolution with the constant function a(n) = 1 for all n, corresponding to multiplying the generating function by the zeta function:

Multiplying by the inverse of the zeta function gives the Möbius inversion formula:

If f is multiplicative, then so is g. If f is completely multiplicative, then g is multiplicative, but may or may not be completely multiplicative.


£#h5#£Relations among the functions£#/h5#£
There are a great many formulas connecting arithmetical functions with each other and with the functions of analysis, especially powers, roots, and the exponential and log functions. The page divisor sum identities contains many more generalized and related examples of identities involving arithmetic functions.

Here are a few examples:


£#h5#£Dirichlet convolutions£#/h5#£
${\displaystyle \sum _{\delta \mid n}\mu (\delta )=\sum _{\delta \mid n}\lambda \left({\frac {n}{\delta }}\right)|\mu (\delta )|={\begin{cases}1&{\text{if }}n=1\\0&{\text{if }}n\neq 1\end{cases}}}$     where λ is the Liouville function.
${\displaystyle \sum _{\delta \mid n}\varphi (\delta )=n.}$      
${\displaystyle \varphi (n)=\sum _{\delta \mid n}\mu \left({\frac {n}{\delta }}\right)\delta =n\sum _{\delta \mid n}{\frac {\mu (\delta )}{\delta }}.}$       Möbius inversion
${\displaystyle \sum _{d\mid n}J_{k}(d)=n^{k}.}$      
${\displaystyle J_{k}(n)=\sum _{\delta \mid n}\mu \left({\frac {n}{\delta }}\right)\delta ^{k}=n^{k}\sum _{\delta \mid n}{\frac {\mu (\delta )}{\delta ^{k}}}.}$       Möbius inversion
${\displaystyle \sum _{\delta \mid n}\delta ^{s}J_{r}(\delta )J_{s}\left({\frac {n}{\delta }}\right)=J_{r+s}(n)}$      
${\displaystyle \sum _{\delta \mid n}\varphi (\delta )d\left({\frac {n}{\delta }}\right)=\sigma (n).}$      
${\displaystyle \sum _{\delta \mid n}|\mu (\delta )|=2^{\omega (n)}.}$      
${\displaystyle |\mu (n)|=\sum _{\delta \mid n}\mu \left({\frac {n}{\delta }}\right)2^{\omega (\delta )}.}$       Möbius inversion
${\displaystyle \sum _{\delta \mid n}2^{\omega (\delta )}=d(n^{2}).}$      
${\displaystyle 2^{\omega (n)}=\sum _{\delta \mid n}\mu \left({\frac {n}{\delta }}\right)d(\delta ^{2}).}$       Möbius inversion
${\displaystyle \sum _{\delta \mid n}d(\delta ^{2})=d^{2}(n).}$      
${\displaystyle d(n^{2})=\sum _{\delta \mid n}\mu \left({\frac {n}{\delta }}\right)d^{2}(\delta ).}$       Möbius inversion
${\displaystyle \sum _{\delta \mid n}d\left({\frac {n}{\delta }}\right)2^{\omega (\delta )}=d^{2}(n).}$      
${\displaystyle \sum _{\delta \mid n}\lambda (\delta )={\begin{cases}&1{\text{ if }}n{\text{ is a square }}\\&0{\text{ if }}n{\text{ is not square.}}\end{cases}}}$     where λ is the Liouville function.
${\displaystyle \sum _{\delta \mid n}\Lambda (\delta )=\log n.}$      
${\displaystyle \Lambda (n)=\sum _{\delta \mid n}\mu \left({\frac {n}{\delta }}\right)\log(\delta ).}$       Möbius inversion

£#h5#£Sums of squares£#/h5#£
For all ${\displaystyle k\geq 4,\;\;\;r_{k}(n)>0.}$     (Lagrange's four-square theorem).

${\displaystyle r_{2}(n)=4\sum _{d\mid n}\left({\frac {-4}{d}}\right),}$
where the Kronecker symbol has the values

${\displaystyle \left({\frac {-4}{n}}\right)={\begin{cases}+1&{\text{if }}n\equiv 1{\pmod {4}}\\-1&{\text{if }}n\equiv 3{\pmod {4}}\\\;\;\;0&{\text{if }}n{\text{ is even}}.\\\end{cases}}}$
There is a formula for r3 in the section on class numbers below.

where ν = ν2(n).    
where ${\displaystyle \chi (n)=\left({\frac {-4}{n}}\right).}$
Define the function σk*(n) as

That is, if n is odd, σk*(n) is the sum of the kth powers of the divisors of n, that is, σk(n), and if n is even it is the sum of the kth powers of the even divisors of n minus the sum of the kth powers of the odd divisors of n.

${\displaystyle r_{8}(n)=16\sigma _{3}^{*}(n).}$    
Adopt the convention that Ramanujan's τ(x) = 0 if x is not an integer.

${\displaystyle r_{24}(n)={\frac {16}{691}}\sigma _{11}^{*}(n)+{\frac {128}{691}}\left\{(-1)^{n-1}259\tau (n)-512\tau \left({\frac {n}{2}}\right)\right\}}$    

£#h5#£Divisor sum convolutions£#/h5#£
Here "convolution" does not mean "Dirichlet convolution" but instead refers to the formula for the coefficients of the product of two power series:

${\displaystyle \left(\sum _{n=0}^{\infty }a_{n}x^{n}\right)\left(\sum _{n=0}^{\infty }b_{n}x^{n}\right)=\sum _{i=0}^{\infty }\sum _{j=0}^{\infty }a_{i}b_{j}x^{i+j}=\sum _{n=0}^{\infty }\left(\sum _{i=0}^{n}a_{i}b_{n-i}\right)x^{n}=\sum _{n=0}^{\infty }c_{n}x^{n}.}$
The sequence ${\displaystyle c_{n}=\sum _{i=0}^{n}a_{i}b_{n-i}}$ is called the convolution or the Cauchy product of the sequences an and bn.
These formulas may be proved analytically (see Eisenstein series) or by elementary methods.

${\displaystyle \sigma _{3}(n)={\frac {1}{5}}\left\{6n\sigma _{1}(n)-\sigma _{1}(n)+12\sum _{0<k<n}\sigma _{1}(k)\sigma _{1}(n-k)\right\}.}$    
${\displaystyle \sigma _{5}(n)={\frac {1}{21}}\left\{10(3n-1)\sigma _{3}(n)+\sigma _{1}(n)+240\sum _{0<k<n}\sigma _{1}(k)\sigma _{3}(n-k)\right\}.}$    
${\displaystyle {\begin{aligned}\sigma _{7}(n)&={\frac {1}{20}}\left\{21(2n-1)\sigma _{5}(n)-\sigma _{1}(n)+504\sum _{0<k<n}\sigma _{1}(k)\sigma _{5}(n-k)\right\}\\&=\sigma _{3}(n)+120\sum _{0<k<n}\sigma _{3}(k)\sigma _{3}(n-k).\end{aligned}}}$    
${\displaystyle {\begin{aligned}\sigma _{9}(n)&={\frac {1}{11}}\left\{10(3n-2)\sigma _{7}(n)+\sigma _{1}(n)+480\sum _{0<k<n}\sigma _{1}(k)\sigma _{7}(n-k)\right\}\\&={\frac {1}{11}}\left\{21\sigma _{5}(n)-10\sigma _{3}(n)+5040\sum _{0<k<n}\sigma _{3}(k)\sigma _{5}(n-k)\right\}.\end{aligned}}}$    
${\displaystyle \tau (n)={\frac {65}{756}}\sigma _{11}(n)+{\frac {691}{756}}\sigma _{5}(n)-{\frac {691}{3}}\sum _{0<k<n}\sigma _{5}(k)\sigma _{5}(n-k),}$     where τ(n) is Ramanujan's function.    
Since σk(n) (for natural number k) and τ(n) are integers, the above formulas can be used to prove congruences for the functions. See Ramanujan tau function for some examples.

Extend the domain of the partition function by setting p(0) = 1.

${\displaystyle p(n)={\frac {1}{n}}\sum _{1\leq k\leq n}\sigma (k)p(n-k).}$       This recurrence can be used to compute p(n).

£#h5#£Class number related£#/h5#£
Peter Gustav Lejeune Dirichlet discovered formulas that relate the class number h of quadratic number fields to the Jacobi symbol.

An integer D is called a fundamental discriminant if it is the discriminant of a quadratic number field. This is equivalent to D ≠ 1 and either a) D is squarefree and D ≡ 1 (mod 4) or b) D ≡ 0 (mod 4), D/4 is squarefree, and D/4 ≡ 2 or 3 (mod 4).

Extend the Jacobi symbol to accept even numbers in the "denominator" by defining the Kronecker symbol:

Then if D < −4 is a fundamental discriminant

There is also a formula relating r3 and h. Again, let D be a fundamental discriminant, D < −4. Then


£#h5#£Prime-count related£#/h5#£
Let ${\displaystyle H_{n}=1+{\frac {1}{2}}+{\frac {1}{3}}+\cdots +{\frac {1}{n}}}$   be the nth harmonic number. Then

${\displaystyle \sigma (n)\leq H_{n}+e^{H_{n}}\log H_{n}}$   is true for every natural number n if and only if the Riemann hypothesis is true.    
The Riemann hypothesis is also equivalent to the statement that, for all n > 5040,

(where γ is the Euler–Mascheroni constant). This is Robin's theorem.
${\displaystyle \sum _{p}\nu _{p}(n)=\Omega (n).}$
${\displaystyle \psi (x)=\sum _{n\leq x}\Lambda (n).}$    
${\displaystyle \Pi (x)=\sum _{n\leq x}{\frac {\Lambda (n)}{\log n}}.}$    
${\displaystyle e^{\theta (x)}=\prod _{p\leq x}p.}$    
${\displaystyle e^{\psi (x)}=\operatorname {lcm} [1,2,\dots ,\lfloor x\rfloor ].}$    

£#h5#£Menon's identity£#/h5#£
In 1965 P Kesava Menon proved

This has been generalized by a number of mathematicians. For example,

£#ul#££#li#£B. Sury £#/li#£ £#li#£N. Rao where a1, a2, ..., as are integers, gcd(a1, a2, ..., as, n) = 1.£#/li#£ £#li#£László Fejes Tóth where m1 and m2 are odd, m = lcm(m1, m2).£#/li#££#/ul#£
In fact, if f is any arithmetical function

where ${\displaystyle *}$ stands for Dirichlet convolution.
£#h5#£Miscellaneous£#/h5#£
Let m and n be distinct, odd, and positive. Then the Jacobi symbol satisfies the law of quadratic reciprocity:

Let D(n) be the arithmetic derivative. Then the logarithmic derivative

See Arithmetic derivative for details.
Let λ(n) be Liouville's function. Then

${\displaystyle |\lambda (n)|\mu (n)=\lambda (n)|\mu (n)|=\mu (n),}$     and
${\displaystyle \lambda (n)\mu (n)=|\mu (n)|=\mu ^{2}(n).}$    
Let λ(n) be Carmichael's function. Then

${\displaystyle \lambda (n)\mid \phi (n).}$     Further,
${\displaystyle \lambda (n)=\phi (n){\text{ if and only if }}n={\begin{cases}1,2,4;\\3,5,7,9,11,\ldots {\text{ (that is, }}p^{k}{\text{, where }}p{\text{ is an odd prime)}};\\6,10,14,18,\ldots {\text{ (that is, }}2p^{k}{\text{, where }}p{\text{ is an odd prime)}}.\end{cases}}}$
See Multiplicative group of integers modulo n and Primitive root modulo n.  

${\displaystyle 2^{\omega (n)}\leq d(n)\leq 2^{\Omega (n)}.}$    
${\displaystyle {\frac {6}{\pi ^{2}}}<{\frac {\phi (n)\sigma (n)}{n^{2}}}<1.}$    
${\displaystyle {\begin{aligned}c_{q}(n)&={\frac {\mu \left({\frac {q}{\gcd(q,n)}}\right)}{\phi \left({\frac {q}{\gcd(q,n)}}\right)}}\phi (q)\\&=\sum _{\delta \mid \gcd(q,n)}\mu \left({\frac {q}{\delta }}\right)\delta .\end{aligned}}}$         Note that   ${\displaystyle \phi (q)=\sum _{\delta \mid q}\mu \left({\frac {q}{\delta }}\right)\delta .}$    
${\displaystyle c_{q}(1)=\mu (q).}$
${\displaystyle c_{q}(q)=\phi (q).}$
${\displaystyle \sum _{\delta \mid n}d^{3}(\delta )=\left(\sum _{\delta \mid n}d(\delta )\right)^{2}.}$       Compare this with 13 + 23 + 33 + ... + n3 = (1 + 2 + 3 + ... + n)2
${\displaystyle d(uv)=\sum _{\delta \mid \gcd(u,v)}\mu (\delta )d\left({\frac {u}{\delta }}\right)d\left({\frac {v}{\delta }}\right).}$    
${\displaystyle \sigma _{k}(u)\sigma _{k}(v)=\sum _{\delta \mid \gcd(u,v)}\delta ^{k}\sigma _{k}\left({\frac {uv}{\delta ^{2}}}\right).}$    
${\displaystyle \tau (u)\tau (v)=\sum _{\delta \mid \gcd(u,v)}\delta ^{11}\tau \left({\frac {uv}{\delta ^{2}}}\right),}$     where τ(n) is Ramanujan's function.    

£#h5#£First 100 values of some arithmetic functions£#/h5#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Tom M. Apostol (1976), Introduction to Analytic Number Theory, Springer Undergraduate Texts in Mathematics, ISBN 0-387-90163-9£#/li#£ £#li#£Apostol, Tom M. (1989), Modular Functions and Dirichlet Series in Number Theory (2nd Edition), New York: Springer, ISBN 0-387-97127-0£#/li#£ £#li#£Bateman, Paul T.; Diamond, Harold G. (2004), Analytic number theory, an introduction, World Scientific, ISBN 978-981-238-938-1£#/li#£ £#li#£Cohen, Henri (1993), A Course in Computational Algebraic Number Theory, Berlin: Springer, ISBN 3-540-55640-0£#/li#£ £#li#£Edwards, Harold (1977). Fermat's Last Theorem. New York: Springer. ISBN 0-387-90230-9.£#/li#£ £#li#£Hardy, G. H. (1999), Ramanujan: Twelve Lectures on Subjects Suggested by his Life and work, Providence RI: AMS / Chelsea, hdl:10115/1436, ISBN 978-0-8218-2023-0£#/li#£ £#li#£Hardy, G. H.; Wright, E. M. (1979) [1938]. An Introduction to the Theory of Numbers (5th ed.). Oxford: Clarendon Press. ISBN 0-19-853171-0. MR 0568909. Zbl 0423.10001.£#/li#£ £#li#£Jameson, G. J. O. (2003), The Prime Number Theorem, Cambridge University Press, ISBN 0-521-89110-8£#/li#£ £#li#£Koblitz, Neal (1984), Introduction to Elliptic Curves and Modular Forms, New York: Springer, ISBN 0-387-97966-2£#/li#£ £#li#£Landau, Edmund (1966), Elementary Number Theory, New York: Chelsea£#/li#£ £#li#£William J. LeVeque (1996), Fundamentals of Number Theory, Courier Dover Publications, ISBN 0-486-68906-9£#/li#£ £#li#£Long, Calvin T. (1972), Elementary Introduction to Number Theory (2nd ed.), Lexington: D. C. Heath and Company, LCCN 77-171950£#/li#£ £#li#£Elliott Mendelson (1987), Introduction to Mathematical Logic, CRC Press, ISBN 0-412-80830-7£#/li#£ £#li#£Nagell, Trygve (1964), Introduction to number theory (2nd Edition), Chelsea, ISBN 978-0-8218-2833-5£#/li#£ £#li#£Niven, Ivan M.; Zuckerman, Herbert S. (1972), An introduction to the theory of numbers (3rd Edition), John Wiley & Sons, ISBN 0-471-64154-5£#/li#£ £#li#£Pettofrezzo, Anthony J.; Byrkit, Donald R. (1970), Elements of Number Theory, Englewood Cliffs: Prentice Hall, LCCN 77-81766£#/li#£ £#li#£Ramanujan, Srinivasa (2000), Collected Papers, Providence RI: AMS / Chelsea, ISBN 978-0-8218-2076-6£#/li#£ £#li#£Williams, Kenneth S. (2011), Number theory in the spirit of Liouville, London Mathematical Society Student Texts, vol. 76, Cambridge: Cambridge University Press, ISBN 978-0-521-17562-3, Zbl 1227.11002£#/li#££#/ul#£
£#h5#£Further reading£#/h5#£ £#ul#££#li#£Schwarz, Wolfgang; Spilker, Jürgen (1994), Arithmetical Functions. An introduction to elementary and analytic properties of arithmetic functions and to some of their almost-periodic properties, London Mathematical Society Lecture Note Series, vol. 184, Cambridge University Press, ISBN 0-521-42725-8, Zbl 0807.11001£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Arithmetic function", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#£ £#li#£Matthew Holden, Michael Orrison, Michael Varble Yet another Generalization of Euler's Totient Function£#/li#£ £#li#£Huard, Ou, Spearman, and Williams. Elementary Evaluation of Certain Convolution Sums Involving Divisor Functions£#/li#£ £#li#£Dineva, Rosica, The Euler Totient, the Möbius, and the Divisor Functions£#/li#£ £#li#£László Tóth, Menon's Identity and arithmetical sums representing functions of several variables£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Functions £#/li#££#/ul#£




£#h3#£Arithmetic Function£#/h3#£

In number theory, an arithmetic, arithmetical, or number-theoretic function is for most authors any function f(n) whose domain is the positive integers and whose range is a subset of the complex numbers. Hardy & Wright include in their definition the requirement that an arithmetical function "expresses some arithmetical property of n".

An example of an arithmetic function is the divisor function whose value at a positive integer n is equal to the number of divisors of n.

There is a larger class of number-theoretic functions that do not fit the above definition, for example, the prime-counting functions. This article provides links to functions of both classes.

Arithmetic functions are often extremely irregular (see table), but some of them have series expansions in terms of Ramanujan's sum.


£#h5#£Multiplicative and additive functions£#/h5#£
An arithmetic function a is

£#ul#££#li#£completely additive if a(mn) = a(m) + a(n) for all natural numbers m and n;£#/li#£ £#li#£completely multiplicative if a(mn) = a(m)a(n) for all natural numbers m and n;£#/li#££#/ul#£
Two whole numbers m and n are called coprime if their greatest common divisor is 1, that is, if there is no prime number that divides both of them.

Then an arithmetic function a is

£#ul#££#li#£additive if a(mn) = a(m) + a(n) for all coprime natural numbers m and n;£#/li#£ £#li#£multiplicative if a(mn) = a(m)a(n) for all coprime natural numbers m and n.£#/li#££#/ul#£
£#h5#£Notation£#/h5#£
${\textstyle \sum _{p}f(p)}$ and ${\textstyle \prod _{p}f(p)}$ mean that the sum or product is over all prime numbers:

Similarly, ${\textstyle \sum _{p^{k}}f(p^{k})}$ and ${\textstyle \prod _{p^{k}}f(p^{k})}$ mean that the sum or product is over all prime powers with strictly positive exponent (so k = 0 is not included):

${\textstyle \sum _{d\mid n}f(d)}$ and ${\textstyle \prod _{d\mid n}f(d)}$ mean that the sum or product is over all positive divisors of n, including 1 and n. For example, if n = 12,

The notations can be combined: ${\textstyle \sum _{p\mid n}f(p)}$ and ${\textstyle \prod _{p\mid n}f(p)}$ mean that the sum or product is over all prime divisors of n. For example, if n = 18,

and similarly ${\textstyle \sum _{p^{k}\mid n}f(p^{k})}$ and ${\textstyle \prod _{p^{k}\mid n}f(p^{k})}$ mean that the sum or product is over all prime powers dividing n. For example, if n = 24,
£#h5#£Ω(n), ω(n), νp(n) – prime power decomposition£#/h5#£
The fundamental theorem of arithmetic states that any positive integer n can be represented uniquely as a product of powers of primes: ${\displaystyle n=p_{1}^{a_{1}}\cdots p_{k}^{a_{k}}}$ where p1 < p2 < ... < pk are primes and the aj are positive integers. (1 is given by the empty product.)

It is often convenient to write this as an infinite product over all the primes, where all but a finite number have a zero exponent. Define the p-adic valuation νp(n) to be the exponent of the highest power of the prime p that divides n. That is, if p is one of the pi then νp(n) = ai, otherwise it is zero. Then

In terms of the above the prime omega functions ω and Ω are defined by

To avoid repetition, whenever possible formulas for the functions listed in this article are given in terms of n and the corresponding pi, ai, ω, and Ω.


£#h5#£Multiplicative functions£#/h5#£
£#h5#£σk(n), τ(n), d(n) – divisor sums£#/h5#£
σk(n) is the sum of the kth powers of the positive divisors of n, including 1 and n, where k is a complex number.

σ1(n), the sum of the (positive) divisors of n, is usually denoted by σ(n).

Since a positive number to the zero power is one, σ0(n) is therefore the number of (positive) divisors of n; it is usually denoted by d(n) or τ(n) (for the German Teiler = divisors).

Setting k = 0 in the second product gives


£#h5#£φ(n) – Euler totient function£#/h5#£
φ(n), the Euler totient function, is the number of positive integers not greater than n that are coprime to n.


£#h5#£Jk(n) – Jordan totient function£#/h5#£
Jk(n), the Jordan totient function, is the number of k-tuples of positive integers all less than or equal to n that form a coprime (k + 1)-tuple together with n. It is a generalization of Euler's totient, φ(n) = J1(n).


£#h5#£μ(n) – Möbius function£#/h5#£
μ(n), the Möbius function, is important because of the Möbius inversion formula. See Dirichlet convolution, below.

This implies that μ(1) = 1. (Because Ω(1) = ω(1) = 0.)


£#h5#£τ(n) – Ramanujan tau function£#/h5#£
τ(n), the Ramanujan tau function, is defined by its generating function identity:

Although it is hard to say exactly what "arithmetical property of n" it "expresses", (τ(n) is (2π)−12 times the nth Fourier coefficient in the q-expansion of the modular discriminant function) it is included among the arithmetical functions because it is multiplicative and it occurs in identities involving certain σk(n) and rk(n) functions (because these are also coefficients in the expansion of modular forms).


£#h5#£cq(n) – Ramanujan's sum£#/h5#£
cq(n), Ramanujan's sum, is the sum of the nth powers of the primitive qth roots of unity:

Even though it is defined as a sum of complex numbers (irrational for most values of q), it is an integer. For a fixed value of n it is multiplicative in q:

If q and r are coprime, then ${\displaystyle c_{q}(n)c_{r}(n)=c_{qr}(n).}$

£#h5#£ψ(n) - Dedekind psi function£#/h5#£
The Dedekind psi function, used in the theory of modular functions, is defined by the formula


£#h5#£Completely multiplicative functions£#/h5#£
£#h5#£λ(n) – Liouville function£#/h5#£
λ(n), the Liouville function, is defined by


£#h5#£χ(n) – characters£#/h5#£
All Dirichlet characters χ(n) are completely multiplicative. Two characters have special notations:

The principal character (mod n) is denoted by χ0(a) (or χ1(a)). It is defined as

The quadratic character (mod n) is denoted by the Jacobi symbol for odd n (it is not defined for even n):

In this formula ${\displaystyle ({\tfrac {a}{p}})}$ is the Legendre symbol, defined for all integers a and all odd primes p by

Following the normal convention for the empty product, ${\displaystyle \left({\frac {a}{1}}\right)=1.}$


£#h5#£Additive functions£#/h5#£
£#h5#£ω(n) – distinct prime divisors£#/h5#£
ω(n), defined above as the number of distinct primes dividing n, is additive (see Prime omega function).


£#h5#£Completely additive functions£#/h5#£
£#h5#£Ω(n) – prime divisors£#/h5#£
Ω(n), defined above as the number of prime factors of n counted with multiplicities, is completely additive (see Prime omega function).


£#h5#£νp(n) – p-adic valuation of an integer n£#/h5#£
For a fixed prime p, νp(n), defined above as the exponent of the largest power of p dividing n, is completely additive.


£#h5#£Logarithmic derivative£#/h5#£
${\displaystyle \operatorname {ld} (n)={\frac {D(n)}{n}}=\sum _{\stackrel {p\mid n}{p{\text{ prime}}}}{\frac {v_{p}(n)}{p}}}$ , where ${\displaystyle D(n)}$ is the arithmetic derivative.


£#h5#£Neither multiplicative nor additive£#/h5#£
£#h5#£π(x), Π(x), θ(x), ψ(x) – prime-counting functions£#/h5#£
These important functions (which are not arithmetic functions) are defined for non-negative real arguments, and are used in the various statements and proofs of the prime number theorem. They are summation functions (see the main section just below) of arithmetic functions which are neither multiplicative nor additive.

π(x), the prime-counting function, is the number of primes not exceeding x. It is the summation function of the characteristic function of the prime numbers.

A related function counts prime powers with weight 1 for primes, 1/2 for their squares, 1/3 for cubes, ... It is the summation function of the arithmetic function which takes the value 1/k on integers which are the k-th power of some prime number, and the value 0 on other integers.

θ(x) and ψ(x), the Chebyshev functions, are defined as sums of the natural logarithms of the primes not exceeding x.

The Chebyshev function ψ(x) is the summation function of the von Mangoldt function just below.


£#h5#£Λ(n) – von Mangoldt function£#/h5#£
Λ(n), the von Mangoldt function, is 0 unless the argument n is a prime power pk, in which case it is the natural log of the prime p:


£#h5#£p(n) – partition function£#/h5#£
p(n), the partition function, is the number of ways of representing n as a sum of positive integers, where two representations with the same summands in a different order are not counted as being different:


£#h5#£λ(n) – Carmichael function£#/h5#£
λ(n), the Carmichael function, is the smallest positive number such that ${\displaystyle a^{\lambda (n)}\equiv 1{\pmod {n}}}$   for all a coprime to n. Equivalently, it is the least common multiple of the orders of the elements of the multiplicative group of integers modulo n.

For powers of odd primes and for 2 and 4, λ(n) is equal to the Euler totient function of n; for powers of 2 greater than 4 it is equal to one half of the Euler totient function of n:

and for general n it is the least common multiple of λ of each of the prime power factors of n:
£#h5#£h(n) – Class number£#/h5#£
h(n), the class number function, is the order of the ideal class group of an algebraic extension of the rationals with discriminant n. The notation is ambiguous, as there are in general many extensions with the same discriminant. See quadratic field and cyclotomic field for classical examples.


£#h5#£rk(n) – Sum of k squares£#/h5#£
rk(n) is the number of ways n can be represented as the sum of k squares, where representations that differ only in the order of the summands or in the signs of the square roots are counted as different.


£#h5#£D(n) – Arithmetic derivative£#/h5#£
Using the Heaviside notation for the derivative, D(n) is a function such that

£#ul#££#li#£ ${\displaystyle D(n)=1}$ if n prime, and£#/li#£ £#li#£ ${\displaystyle D(mn)=mD(n)+D(m)n}$ (Product rule)£#/li#££#/ul#£
£#h5#£Summation functions£#/h5#£
Given an arithmetic function a(n), its summation function A(x) is defined by

A can be regarded as a function of a real variable. Given a positive integer m, A is constant along open intervals m < x < m + 1, and has a jump discontinuity at each integer for which a(m) ≠ 0.
Since such functions are often represented by series and integrals, to achieve pointwise convergence it is usual to define the value at the discontinuities as the average of the values to the left and right:

Individual values of arithmetic functions may fluctuate wildly – as in most of the above examples. Summation functions "smooth out" these fluctuations. In some cases it may be possible to find asymptotic behaviour for the summation function for large x.

A classical example of this phenomenon is given by the divisor summatory function, the summation function of d(n), the number of divisors of n:

An average order of an arithmetic function is some simpler or better-understood function which has the same summation function asymptotically, and hence takes the same values "on average". We say that g is an average order of f if

as x tends to infinity. The example above shows that d(n) has the average order log(n).


£#h5#£Dirichlet convolution£#/h5#£
Given an arithmetic function a(n), let Fa(s), for complex s, be the function defined by the corresponding Dirichlet series (where it converges):

Fa(s) is called a generating function of a(n). The simplest such series, corresponding to the constant function a(n) = 1 for all n, is ς(s) the Riemann zeta function.
The generating function of the Möbius function is the inverse of the zeta function:

Consider two arithmetic functions a and b and their respective generating functions Fa(s) and Fb(s). The product Fa(s)Fb(s) can be computed as follows:

It is a straightforward exercise to show that if c(n) is defined by

then
This function c is called the Dirichlet convolution of a and b, and is denoted by ${\displaystyle a*b}$ .

A particularly important case is convolution with the constant function a(n) = 1 for all n, corresponding to multiplying the generating function by the zeta function:

Multiplying by the inverse of the zeta function gives the Möbius inversion formula:

If f is multiplicative, then so is g. If f is completely multiplicative, then g is multiplicative, but may or may not be completely multiplicative.


£#h5#£Relations among the functions£#/h5#£
There are a great many formulas connecting arithmetical functions with each other and with the functions of analysis, especially powers, roots, and the exponential and log functions. The page divisor sum identities contains many more generalized and related examples of identities involving arithmetic functions.

Here are a few examples:


£#h5#£Dirichlet convolutions£#/h5#£
${\displaystyle \sum _{\delta \mid n}\mu (\delta )=\sum _{\delta \mid n}\lambda \left({\frac {n}{\delta }}\right)|\mu (\delta )|={\begin{cases}1&{\text{if }}n=1\\0&{\text{if }}n\neq 1\end{cases}}}$     where λ is the Liouville function.
${\displaystyle \sum _{\delta \mid n}\varphi (\delta )=n.}$      
${\displaystyle \varphi (n)=\sum _{\delta \mid n}\mu \left({\frac {n}{\delta }}\right)\delta =n\sum _{\delta \mid n}{\frac {\mu (\delta )}{\delta }}.}$       Möbius inversion
${\displaystyle \sum _{d\mid n}J_{k}(d)=n^{k}.}$      
${\displaystyle J_{k}(n)=\sum _{\delta \mid n}\mu \left({\frac {n}{\delta }}\right)\delta ^{k}=n^{k}\sum _{\delta \mid n}{\frac {\mu (\delta )}{\delta ^{k}}}.}$       Möbius inversion
${\displaystyle \sum _{\delta \mid n}\delta ^{s}J_{r}(\delta )J_{s}\left({\frac {n}{\delta }}\right)=J_{r+s}(n)}$      
${\displaystyle \sum _{\delta \mid n}\varphi (\delta )d\left({\frac {n}{\delta }}\right)=\sigma (n).}$      
${\displaystyle \sum _{\delta \mid n}|\mu (\delta )|=2^{\omega (n)}.}$      
${\displaystyle |\mu (n)|=\sum _{\delta \mid n}\mu \left({\frac {n}{\delta }}\right)2^{\omega (\delta )}.}$       Möbius inversion
${\displaystyle \sum _{\delta \mid n}2^{\omega (\delta )}=d(n^{2}).}$      
${\displaystyle 2^{\omega (n)}=\sum _{\delta \mid n}\mu \left({\frac {n}{\delta }}\right)d(\delta ^{2}).}$       Möbius inversion
${\displaystyle \sum _{\delta \mid n}d(\delta ^{2})=d^{2}(n).}$      
${\displaystyle d(n^{2})=\sum _{\delta \mid n}\mu \left({\frac {n}{\delta }}\right)d^{2}(\delta ).}$       Möbius inversion
${\displaystyle \sum _{\delta \mid n}d\left({\frac {n}{\delta }}\right)2^{\omega (\delta )}=d^{2}(n).}$      
${\displaystyle \sum _{\delta \mid n}\lambda (\delta )={\begin{cases}&1{\text{ if }}n{\text{ is a square }}\\&0{\text{ if }}n{\text{ is not square.}}\end{cases}}}$     where λ is the Liouville function.
${\displaystyle \sum _{\delta \mid n}\Lambda (\delta )=\log n.}$      
${\displaystyle \Lambda (n)=\sum _{\delta \mid n}\mu \left({\frac {n}{\delta }}\right)\log(\delta ).}$       Möbius inversion

£#h5#£Sums of squares£#/h5#£
For all ${\displaystyle k\geq 4,\;\;\;r_{k}(n)>0.}$     (Lagrange's four-square theorem).

${\displaystyle r_{2}(n)=4\sum _{d\mid n}\left({\frac {-4}{d}}\right),}$
where the Kronecker symbol has the values

${\displaystyle \left({\frac {-4}{n}}\right)={\begin{cases}+1&{\text{if }}n\equiv 1{\pmod {4}}\\-1&{\text{if }}n\equiv 3{\pmod {4}}\\\;\;\;0&{\text{if }}n{\text{ is even}}.\\\end{cases}}}$
There is a formula for r3 in the section on class numbers below.

where ν = ν2(n).    
where ${\displaystyle \chi (n)=\left({\frac {-4}{n}}\right).}$
Define the function σk*(n) as

That is, if n is odd, σk*(n) is the sum of the kth powers of the divisors of n, that is, σk(n), and if n is even it is the sum of the kth powers of the even divisors of n minus the sum of the kth powers of the odd divisors of n.

${\displaystyle r_{8}(n)=16\sigma _{3}^{*}(n).}$    
Adopt the convention that Ramanujan's τ(x) = 0 if x is not an integer.

${\displaystyle r_{24}(n)={\frac {16}{691}}\sigma _{11}^{*}(n)+{\frac {128}{691}}\left\{(-1)^{n-1}259\tau (n)-512\tau \left({\frac {n}{2}}\right)\right\}}$    

£#h5#£Divisor sum convolutions£#/h5#£
Here "convolution" does not mean "Dirichlet convolution" but instead refers to the formula for the coefficients of the product of two power series:

${\displaystyle \left(\sum _{n=0}^{\infty }a_{n}x^{n}\right)\left(\sum _{n=0}^{\infty }b_{n}x^{n}\right)=\sum _{i=0}^{\infty }\sum _{j=0}^{\infty }a_{i}b_{j}x^{i+j}=\sum _{n=0}^{\infty }\left(\sum _{i=0}^{n}a_{i}b_{n-i}\right)x^{n}=\sum _{n=0}^{\infty }c_{n}x^{n}.}$
The sequence ${\displaystyle c_{n}=\sum _{i=0}^{n}a_{i}b_{n-i}}$ is called the convolution or the Cauchy product of the sequences an and bn.
These formulas may be proved analytically (see Eisenstein series) or by elementary methods.

${\displaystyle \sigma _{3}(n)={\frac {1}{5}}\left\{6n\sigma _{1}(n)-\sigma _{1}(n)+12\sum _{0<k<n}\sigma _{1}(k)\sigma _{1}(n-k)\right\}.}$    
${\displaystyle \sigma _{5}(n)={\frac {1}{21}}\left\{10(3n-1)\sigma _{3}(n)+\sigma _{1}(n)+240\sum _{0<k<n}\sigma _{1}(k)\sigma _{3}(n-k)\right\}.}$    
${\displaystyle {\begin{aligned}\sigma _{7}(n)&={\frac {1}{20}}\left\{21(2n-1)\sigma _{5}(n)-\sigma _{1}(n)+504\sum _{0<k<n}\sigma _{1}(k)\sigma _{5}(n-k)\right\}\\&=\sigma _{3}(n)+120\sum _{0<k<n}\sigma _{3}(k)\sigma _{3}(n-k).\end{aligned}}}$    
${\displaystyle {\begin{aligned}\sigma _{9}(n)&={\frac {1}{11}}\left\{10(3n-2)\sigma _{7}(n)+\sigma _{1}(n)+480\sum _{0<k<n}\sigma _{1}(k)\sigma _{7}(n-k)\right\}\\&={\frac {1}{11}}\left\{21\sigma _{5}(n)-10\sigma _{3}(n)+5040\sum _{0<k<n}\sigma _{3}(k)\sigma _{5}(n-k)\right\}.\end{aligned}}}$    
${\displaystyle \tau (n)={\frac {65}{756}}\sigma _{11}(n)+{\frac {691}{756}}\sigma _{5}(n)-{\frac {691}{3}}\sum _{0<k<n}\sigma _{5}(k)\sigma _{5}(n-k),}$     where τ(n) is Ramanujan's function.    
Since σk(n) (for natural number k) and τ(n) are integers, the above formulas can be used to prove congruences for the functions. See Ramanujan tau function for some examples.

Extend the domain of the partition function by setting p(0) = 1.

${\displaystyle p(n)={\frac {1}{n}}\sum _{1\leq k\leq n}\sigma (k)p(n-k).}$       This recurrence can be used to compute p(n).

£#h5#£Class number related£#/h5#£
Peter Gustav Lejeune Dirichlet discovered formulas that relate the class number h of quadratic number fields to the Jacobi symbol.

An integer D is called a fundamental discriminant if it is the discriminant of a quadratic number field. This is equivalent to D ≠ 1 and either a) D is squarefree and D ≡ 1 (mod 4) or b) D ≡ 0 (mod 4), D/4 is squarefree, and D/4 ≡ 2 or 3 (mod 4).

Extend the Jacobi symbol to accept even numbers in the "denominator" by defining the Kronecker symbol:

Then if D < −4 is a fundamental discriminant

There is also a formula relating r3 and h. Again, let D be a fundamental discriminant, D < −4. Then


£#h5#£Prime-count related£#/h5#£
Let ${\displaystyle H_{n}=1+{\frac {1}{2}}+{\frac {1}{3}}+\cdots +{\frac {1}{n}}}$   be the nth harmonic number. Then

${\displaystyle \sigma (n)\leq H_{n}+e^{H_{n}}\log H_{n}}$   is true for every natural number n if and only if the Riemann hypothesis is true.    
The Riemann hypothesis is also equivalent to the statement that, for all n > 5040,

(where γ is the Euler–Mascheroni constant). This is Robin's theorem.
${\displaystyle \sum _{p}\nu _{p}(n)=\Omega (n).}$
${\displaystyle \psi (x)=\sum _{n\leq x}\Lambda (n).}$    
${\displaystyle \Pi (x)=\sum _{n\leq x}{\frac {\Lambda (n)}{\log n}}.}$    
${\displaystyle e^{\theta (x)}=\prod _{p\leq x}p.}$    
${\displaystyle e^{\psi (x)}=\operatorname {lcm} [1,2,\dots ,\lfloor x\rfloor ].}$    

£#h5#£Menon's identity£#/h5#£
In 1965 P Kesava Menon proved

This has been generalized by a number of mathematicians. For example,

£#ul#££#li#£B. Sury £#/li#£ £#li#£N. Rao where a1, a2, ..., as are integers, gcd(a1, a2, ..., as, n) = 1.£#/li#£ £#li#£László Fejes Tóth where m1 and m2 are odd, m = lcm(m1, m2).£#/li#££#/ul#£
In fact, if f is any arithmetical function

where ${\displaystyle *}$ stands for Dirichlet convolution.
£#h5#£Miscellaneous£#/h5#£
Let m and n be distinct, odd, and positive. Then the Jacobi symbol satisfies the law of quadratic reciprocity:

Let D(n) be the arithmetic derivative. Then the logarithmic derivative

See Arithmetic derivative for details.
Let λ(n) be Liouville's function. Then

${\displaystyle |\lambda (n)|\mu (n)=\lambda (n)|\mu (n)|=\mu (n),}$     and
${\displaystyle \lambda (n)\mu (n)=|\mu (n)|=\mu ^{2}(n).}$    
Let λ(n) be Carmichael's function. Then

${\displaystyle \lambda (n)\mid \phi (n).}$     Further,
${\displaystyle \lambda (n)=\phi (n){\text{ if and only if }}n={\begin{cases}1,2,4;\\3,5,7,9,11,\ldots {\text{ (that is, }}p^{k}{\text{, where }}p{\text{ is an odd prime)}};\\6,10,14,18,\ldots {\text{ (that is, }}2p^{k}{\text{, where }}p{\text{ is an odd prime)}}.\end{cases}}}$
See Multiplicative group of integers modulo n and Primitive root modulo n.  

${\displaystyle 2^{\omega (n)}\leq d(n)\leq 2^{\Omega (n)}.}$    
${\displaystyle {\frac {6}{\pi ^{2}}}<{\frac {\phi (n)\sigma (n)}{n^{2}}}<1.}$    
${\displaystyle {\begin{aligned}c_{q}(n)&={\frac {\mu \left({\frac {q}{\gcd(q,n)}}\right)}{\phi \left({\frac {q}{\gcd(q,n)}}\right)}}\phi (q)\\&=\sum _{\delta \mid \gcd(q,n)}\mu \left({\frac {q}{\delta }}\right)\delta .\end{aligned}}}$         Note that   ${\displaystyle \phi (q)=\sum _{\delta \mid q}\mu \left({\frac {q}{\delta }}\right)\delta .}$    
${\displaystyle c_{q}(1)=\mu (q).}$
${\displaystyle c_{q}(q)=\phi (q).}$
${\displaystyle \sum _{\delta \mid n}d^{3}(\delta )=\left(\sum _{\delta \mid n}d(\delta )\right)^{2}.}$       Compare this with 13 + 23 + 33 + ... + n3 = (1 + 2 + 3 + ... + n)2
${\displaystyle d(uv)=\sum _{\delta \mid \gcd(u,v)}\mu (\delta )d\left({\frac {u}{\delta }}\right)d\left({\frac {v}{\delta }}\right).}$    
${\displaystyle \sigma _{k}(u)\sigma _{k}(v)=\sum _{\delta \mid \gcd(u,v)}\delta ^{k}\sigma _{k}\left({\frac {uv}{\delta ^{2}}}\right).}$    
${\displaystyle \tau (u)\tau (v)=\sum _{\delta \mid \gcd(u,v)}\delta ^{11}\tau \left({\frac {uv}{\delta ^{2}}}\right),}$     where τ(n) is Ramanujan's function.    

£#h5#£First 100 values of some arithmetic functions£#/h5#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Tom M. Apostol (1976), Introduction to Analytic Number Theory, Springer Undergraduate Texts in Mathematics, ISBN 0-387-90163-9£#/li#£ £#li#£Apostol, Tom M. (1989), Modular Functions and Dirichlet Series in Number Theory (2nd Edition), New York: Springer, ISBN 0-387-97127-0£#/li#£ £#li#£Bateman, Paul T.; Diamond, Harold G. (2004), Analytic number theory, an introduction, World Scientific, ISBN 978-981-238-938-1£#/li#£ £#li#£Cohen, Henri (1993), A Course in Computational Algebraic Number Theory, Berlin: Springer, ISBN 3-540-55640-0£#/li#£ £#li#£Edwards, Harold (1977). Fermat's Last Theorem. New York: Springer. ISBN 0-387-90230-9.£#/li#£ £#li#£Hardy, G. H. (1999), Ramanujan: Twelve Lectures on Subjects Suggested by his Life and work, Providence RI: AMS / Chelsea, hdl:10115/1436, ISBN 978-0-8218-2023-0£#/li#£ £#li#£Hardy, G. H.; Wright, E. M. (1979) [1938]. An Introduction to the Theory of Numbers (5th ed.). Oxford: Clarendon Press. ISBN 0-19-853171-0. MR 0568909. Zbl 0423.10001.£#/li#£ £#li#£Jameson, G. J. O. (2003), The Prime Number Theorem, Cambridge University Press, ISBN 0-521-89110-8£#/li#£ £#li#£Koblitz, Neal (1984), Introduction to Elliptic Curves and Modular Forms, New York: Springer, ISBN 0-387-97966-2£#/li#£ £#li#£Landau, Edmund (1966), Elementary Number Theory, New York: Chelsea£#/li#£ £#li#£William J. LeVeque (1996), Fundamentals of Number Theory, Courier Dover Publications, ISBN 0-486-68906-9£#/li#£ £#li#£Long, Calvin T. (1972), Elementary Introduction to Number Theory (2nd ed.), Lexington: D. C. Heath and Company, LCCN 77-171950£#/li#£ £#li#£Elliott Mendelson (1987), Introduction to Mathematical Logic, CRC Press, ISBN 0-412-80830-7£#/li#£ £#li#£Nagell, Trygve (1964), Introduction to number theory (2nd Edition), Chelsea, ISBN 978-0-8218-2833-5£#/li#£ £#li#£Niven, Ivan M.; Zuckerman, Herbert S. (1972), An introduction to the theory of numbers (3rd Edition), John Wiley & Sons, ISBN 0-471-64154-5£#/li#£ £#li#£Pettofrezzo, Anthony J.; Byrkit, Donald R. (1970), Elements of Number Theory, Englewood Cliffs: Prentice Hall, LCCN 77-81766£#/li#£ £#li#£Ramanujan, Srinivasa (2000), Collected Papers, Providence RI: AMS / Chelsea, ISBN 978-0-8218-2076-6£#/li#£ £#li#£Williams, Kenneth S. (2011), Number theory in the spirit of Liouville, London Mathematical Society Student Texts, vol. 76, Cambridge: Cambridge University Press, ISBN 978-0-521-17562-3, Zbl 1227.11002£#/li#££#/ul#£
£#h5#£Further reading£#/h5#£ £#ul#££#li#£Schwarz, Wolfgang; Spilker, Jürgen (1994), Arithmetical Functions. An introduction to elementary and analytic properties of arithmetic functions and to some of their almost-periodic properties, London Mathematical Society Lecture Note Series, vol. 184, Cambridge University Press, ISBN 0-521-42725-8, Zbl 0807.11001£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Arithmetic function", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#£ £#li#£Matthew Holden, Michael Orrison, Michael Varble Yet another Generalization of Euler's Totient Function£#/li#£ £#li#£Huard, Ou, Spearman, and Williams. Elementary Evaluation of Certain Convolution Sums Involving Divisor Functions£#/li#£ £#li#£Dineva, Rosica, The Euler Totient, the Möbius, and the Divisor Functions£#/li#£ £#li#£László Tóth, Menon's Identity and arithmetical sums representing functions of several variables£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Atanassov, K. "An Arithmetic Function and Some of Its Applications." Bull. Number Th. Related Topics 9, 18-27, 1985.£#/li#££#li#£Jones, G. A. and Jones, J. M. "Arithmetic Functions." Ch. 8 in Elementary Number Theory. Berlin: Springer-Verlag, pp. 143-162, 1998.£#/li#££#li#£Trott, M. The Mathematica GuideBook for Programming. New York: Springer-Verlag, 2004. http://www.mathematicaguidebooks.org/.£#/li#££#li#£ Atanassov, K. "An Arithmetic Function and Some of Its Applications." Bull. Number Th. Related Topics 9, 18-27, 1985. £#/li#££#li#£ Jones, G. A. and Jones, J. M. "Arithmetic Functions." Ch. 8 in Elementary Number Theory. Berlin: Springer-Verlag, pp. 143-162, 1998. £#/li#££#li#£ Trott, M. The Mathematica GuideBook for Programming. New York: Springer-Verlag, 2004. http://www.mathematicaguidebooks.org/. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Functions £#/li#££#/ul#£




£#h3#£Arithmetic Mean£#/h3#£

In mathematics and statistics, the arithmetic mean ( air-ith-MET-ik) or arithmetic average, or just the mean or the average (when the context is clear), is the sum of a collection of numbers divided by the count of numbers in the collection. The collection is often a set of results of an experiment or an observational study, or frequently a set of results from a survey. The term "arithmetic mean" is preferred in some contexts in mathematics and statistics, because it helps distinguish it from other means, such as the geometric mean and the harmonic mean.

In addition to mathematics and statistics, the arithmetic mean is used frequently in many diverse fields such as economics, anthropology and history, and it is used in almost every academic field to some extent. For example, per capita income is the arithmetic average income of a nation's population.

While the arithmetic mean is often used to report central tendencies, it is not a robust statistic, meaning that it is greatly influenced by outliers (values that are very much larger or smaller than most of the values). For skewed distributions, such as the distribution of income for which a few people's incomes are substantially greater than most people's, the arithmetic mean may not coincide with one's notion of "middle", and robust statistics, such as the median, may provide better description of central tendency.


£#h5#£Definition£#/h5#£
Given a data set ${\displaystyle X=\{x_{1},\ldots ,x_{n}\}}$ , the arithmetic mean (or mean or average), denoted ${\displaystyle {\bar {x}}}$ (read ${\displaystyle x}$ bar), is the mean of the ${\displaystyle n}$ values ${\displaystyle x_{1},x_{2},\ldots ,x_{n}}$ .

The arithmetic mean is the most commonly used and readily understood measure of central tendency in a data set. In statistics, the term average refers to any of the measures of central tendency. The arithmetic mean of a set of observed data is defined as being equal to the sum of the numerical values of each and every observation, divided by the total number of observations. Symbolically, if we have a data set consisting of the values ${\displaystyle a_{1},a_{2},\ldots ,a_{n}}$ , then the arithmetic mean ${\displaystyle A}$ is defined by the formula:

${\displaystyle A={\frac {1}{n}}\sum _{i=1}^{n}a_{i}={\frac {a_{1}+a_{2}+\cdots +a_{n}}{n}}}$
(for an explanation of the summation operator, see summation.)

For example, consider the monthly salary of 10 employees of a firm: 2500, 2700, 2400, 2300, 2550, 2650, 2750, 2450, 2600, 2400. The arithmetic mean is

${\displaystyle {\frac {2500+2700+2400+2300+2550+2650+2750+2450+2600+2400}{10}}=2530.}$
If the data set is a statistical population (i.e., consists of every possible observation and not just a subset of them), then the mean of that population is called the population mean, and denoted by the Greek letter ${\displaystyle \mu }$ . If the data set is a statistical sample (a subset of the population), then we call the statistic resulting from this calculation a sample mean (which for a data set ${\displaystyle X}$ is denoted as ${\displaystyle {\overline {X}}}$ ).

The arithmetic mean can be similarly defined for vectors in multiple dimension, not only scalar values; this is often referred to as a centroid. More generally, because the arithmetic mean is a convex combination (coefficients sum to 1), it can be defined on a convex space, not only a vector space.


£#h5#£Motivating properties£#/h5#£
The arithmetic mean has several properties that make it useful, especially as a measure of central tendency. These include:

£#ul#££#li#£If numbers ${\displaystyle x_{1},\dotsc ,x_{n}}$ have mean ${\displaystyle {\bar {x}}}$ , then ${\displaystyle (x_{1}-{\bar {x}})+\dotsb +(x_{n}-{\bar {x}})=0}$ . Since ${\displaystyle x_{i}-{\bar {x}}}$ is the distance from a given number to the mean, one way to interpret this property is as saying that the numbers to the left of the mean are balanced by the numbers to the right of the mean. The mean is the only single number for which the residuals (deviations from the estimate) sum to zero.£#/li#£ £#li#£If it is required to use a single number as a "typical" value for a set of known numbers ${\displaystyle x_{1},\dotsc ,x_{n}}$ , then the arithmetic mean of the numbers does this best, in the sense of minimizing the sum of squared deviations from the typical value: the sum of ${\displaystyle (x_{i}-{\bar {x}})^{2}}$ . (It follows that the sample mean is also the best single predictor in the sense of having the lowest root mean squared error.) If the arithmetic mean of a population of numbers is desired, then the estimate of it that is unbiased is the arithmetic mean of a sample drawn from the population.£#/li#££#/ul#£
£#h5#£Additional properties£#/h5#£ £#ul#££#li#£ ${\displaystyle Avg(c*a_{1},c*a_{2}...c*a_{n})}$ = ${\displaystyle c*Avg(a_{1},a_{2}...a_{n})}$ £#/li#£ £#li#£The Arithmetic mean of any amount of equal-sized number groups together is the Arithmetic mean of the Arithmetic means of each group.£#/li#££#/ul#£
£#h5#£Contrast with median£#/h5#£
The arithmetic mean may be contrasted with the median. The median is defined such that no more than half the values are larger than, and no more than half are smaller than, the median. If elements in the data increase arithmetically, when placed in some order, then the median and arithmetic average are equal. For example, consider the data sample ${\displaystyle {1,2,3,4}}$ . The average is ${\displaystyle 2.5}$ , as is the median. However, when we consider a sample that cannot be arranged so as to increase arithmetically, such as ${\displaystyle {1,2,4,8,16}}$ , the median and arithmetic average can differ significantly. In this case, the arithmetic average is 6.2, while the median is 4. In general, the average value can vary significantly from most values in the sample, and can be larger or smaller than most of them.

There are applications of this phenomenon in many fields. For example, since the 1980s, the median income in the United States has increased more slowly than the arithmetic average of income.


£#h5#£Generalizations£#/h5#£
£#h5#£Weighted average£#/h5#£
A weighted average, or weighted mean, is an average in which some data points count more heavily than others, in that they are given more weight in the calculation. For example, the arithmetic mean of ${\displaystyle 3}$ and ${\displaystyle 5}$ is ${\displaystyle {\frac {(3+5)}{2}}=4}$ , or equivalently ${\displaystyle \left({\frac {1}{2}}\cdot 3\right)+\left({\frac {1}{2}}\cdot 5\right)=4}$ . In contrast, a weighted mean in which the first number receives, for example, twice as much weight as the second (perhaps because it is assumed to appear twice as often in the general population from which these numbers were sampled) would be calculated as ${\displaystyle \left({\frac {2}{3}}\cdot 3\right)+\left({\frac {1}{3}}\cdot 5\right)={\frac {11}{3}}}$ . Here the weights, which necessarily sum to the value one, are ${\displaystyle (2/3)}$ and ${\displaystyle (1/3)}$ , the former being twice the latter. The arithmetic mean (sometimes called the "unweighted average" or "equally weighted average") can be interpreted as a special case of a weighted average in which all the weights are equal to each other (equal to ${\displaystyle {\frac {1}{2}}}$ in the above example, and equal to ${\displaystyle {\frac {1}{n}}}$ in a situation with ${\displaystyle n}$ numbers being averaged).


£#h5#£Continuous probability distributions£#/h5#£
If a numerical property, and any sample of data from it, could take on any value from a continuous range, instead of, for example, just integers, then the probability of a number falling into some range of possible values can be described by integrating a continuous probability distribution across this range, even when the naive probability for a sample number taking one certain value from infinitely many is zero. The analog of a weighted average in this context, in which there are an infinite number of possibilities for the precise value of the variable in each range, is called the mean of the probability distribution. A most widely encountered probability distribution is called the normal distribution; it has the property that all measures of its central tendency, including not just the mean but also the aforementioned median and the mode (the three M's), are equal to each other. This equality does not hold for other probability distributions, as illustrated for the log-normal distribution here.


£#h5#£Angles£#/h5#£
Particular care must be taken when using cyclic data, such as phases or angles. Naively taking the arithmetic mean of 1° and 359° yields a result of 180°. This is incorrect for two reasons:

£#ul#££#li#£Firstly, angle measurements are only defined up to an additive constant of 360° (or 2π, if measuring in radians). Thus one could as easily call these 1° and −1°, or 361° and 719°, since each one of them gives a different average.£#/li#£ £#li#£Secondly, in this situation, 0° (equivalently, 360°) is geometrically a better average value: there is lower dispersion about it (the points are both 1° from it, and 179° from 180°, the putative average).£#/li#££#/ul#£
In general application, such an oversight will lead to the average value artificially moving towards the middle of the numerical range. A solution to this problem is to use the optimization formulation (viz., define the mean as the central point: the point about which one has the lowest dispersion), and redefine the difference as a modular distance (i.e., the distance on the circle: so the modular distance between 1° and 359° is 2°, not 358°).


£#h5#£Symbols and encoding£#/h5#£
The arithmetic mean is often denoted by a bar, (a.k.a vinculum or macron), for example as in ${\displaystyle {\bar {x}}}$ (read ${\displaystyle x}$ bar).

Some software (text processors, web browsers) may not display the x̄ symbol properly. For example, the x̄ symbol in HTML is actually a combination of two codes - the base letter x plus a code for the line above (&#772; or ¯).

In some texts, such as pdfs, the x̄ symbol may be replaced by a cent (¢) symbol (Unicode &#162), when copied to text processor such as Microsoft Word.


£#h5#£See also£#/h5#£ £#ul#££#li#£Fréchet mean£#/li#£ £#li#£Generalized mean£#/li#£ £#li#£Geometric mean£#/li#£ £#li#£Harmonic mean£#/li#£ £#li#£Inequality of arithmetic and geometric means£#/li#£ £#li#£Mode£#/li#£ £#li#£Sample mean and covariance£#/li#£ £#li#£Standard deviation£#/li#£ £#li#£Standard error of the mean£#/li#£ £#li#£Summary statistics£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£Further reading£#/h5#£ £#ul#££#li#£Huff, Darrell (1993). How to Lie with Statistics. W. W. Norton. ISBN 978-0-393-31072-6.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Calculations and comparisons between arithmetic mean and geometric mean of two numbers£#/li#£ £#li#£Calculate the arithmetic mean of a series of numbers on fxSolver£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Abramowitz, M. and Stegun, I. A. (Eds.). Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing. New York: Dover, p. 10, 1972.£#/li#££#li#£Alzer, H. "A Proof of the Arithmetic Mean-Geometric Mean Inequality." Amer. Math. Monthly 103, 585, 1996.£#/li#££#li#£Beckenbach, E. F. and Bellman, R. Inequalities. New York: Springer-Verlag, 1983.£#/li#££#li#£Beyer, W. H. CRC Standard Mathematical Tables, 28th ed. Boca Raton, FL: CRC Press, p. 471, 1987.£#/li#££#li#£Bullen, P. S.; Mitrinović, D. S.; and Vasić, P. M. Means and Their Inequalities. Dordrecht, Netherlands: Reidel, 1988.£#/li#££#li#£Havil, J. Gamma: Exploring Euler's Constant. Princeton, NJ: Princeton University Press, pp. 119-121, 2003.£#/li#££#li#£Hardy, G. H.; Littlewood, J. E.; and Pólya, G. Inequalities. Cambridge, England: Cambridge University Press, 1952.£#/li#££#li#£Hoehn, L. and Niven, I. "Averages on the Move." Math. Mag. 58, 151-156, 1985.£#/li#££#li#£Kenney, J. F. and Keeping, E. S. Mathematics of Statistics, Pt. 1, 3rd ed. Princeton, NJ: Van Nostrand, 1962.£#/li#££#li#£Mitrinović, D. S. Analytic Inequalities. New York: Springer-Verlag, 1970.£#/li#££#li#£Mitrinović, D. S.; Pečarić, J. E.; and Fink, A. M. Classical and New Inequalities in Analysis. Dordrecht, Netherlands: Kluwer, 1993.£#/li#££#li#£Zwillinger, D. (Ed.). CRC Standard Mathematical Tables and Formulae. Boca Raton, FL: CRC Press, p. 601, 1995.£#/li#££#li#£ Abramowitz, M. and Stegun, I. A. (Eds.). Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing. New York: Dover, p. 10, 1972. £#/li#££#li#£ Alzer, H. "A Proof of the Arithmetic Mean-Geometric Mean Inequality." Amer. Math. Monthly 103, 585, 1996. £#/li#££#li#£ Beckenbach, E. F. and Bellman, R. Inequalities. New York: Springer-Verlag, 1983. £#/li#££#li#£ Beyer, W. H. CRC Standard Mathematical Tables, 28th ed. Boca Raton, FL: CRC Press, p. 471, 1987. £#/li#££#li#£ Bullen, P. S.; Mitrinović, D. S.; and Vasić, P. M. Means and Their Inequalities. Dordrecht, Netherlands: Reidel, 1988. £#/li#££#li#£ Havil, J. Gamma: Exploring Euler's Constant. Princeton, NJ: Princeton University Press, pp. 119-121, 2003. £#/li#££#li#£ Hardy, G. H.; Littlewood, J. E.; and Pólya, G. Inequalities. Cambridge, England: Cambridge University Press, 1952. £#/li#££#li#£ Hoehn, L. and Niven, I. "Averages on the Move." Math. Mag. 58, 151-156, 1985. £#/li#££#li#£ Kenney, J. F. and Keeping, E. S. Mathematics of Statistics, Pt. 1, 3rd ed. Princeton, NJ: Van Nostrand, 1962. £#/li#££#li#£ Mitrinović, D. S. Analytic Inequalities. New York: Springer-Verlag, 1970. £#/li#££#li#£ Mitrinović, D. S.; Pečarić, J. E.; and Fink, A. M. Classical and New Inequalities in Analysis. Dordrecht, Netherlands: Kluwer, 1993. £#/li#££#li#£ Zwillinger, D. (Ed.). CRC Standard Mathematical Tables and Formulae. Boca Raton, FL: CRC Press, p. 601, 1995. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Means £#/li#££#/ul#£




£#h3#£Arithmetic Progression£#/h3#£

An arithmetic progression or arithmetic sequence is a sequence of numbers such that the difference between the consecutive terms is constant. For instance, the sequence 5, 7, 9, 11, 13, 15, . . . is an arithmetic progression with a common difference of 2.

If the initial term of an arithmetic progression is ${\displaystyle a_{1}}$ and the common difference of successive members is ${\displaystyle d}$ , then the ${\displaystyle n}$ -th term of the sequence ( ${\displaystyle a_{n}}$ ) is given by:

${\displaystyle \ a_{n}=a_{1}+(n-1)d}$ ,
and in general

${\displaystyle \ a_{n}=a_{m}+(n-m)d}$ .
A finite portion of an arithmetic progression is called a finite arithmetic progression and sometimes just called an arithmetic progression. The sum of a finite arithmetic progression is called an arithmetic series.


£#h5#£Sum£#/h5#£
The sum of the members of a finite arithmetic progression is called an arithmetic series. For example, consider the sum:

${\displaystyle 2+5+8+11+14}$
This sum can be found quickly by taking the number n of terms being added (here 5), multiplying by the sum of the first and last number in the progression (here 2 + 14 = 16), and dividing by 2:

${\displaystyle {\frac {n(a_{1}+a_{n})}{2}}}$
In the case above, this gives the equation:

${\displaystyle 2+5+8+11+14={\frac {5(2+14)}{2}}={\frac {5\times 16}{2}}=40.}$
This formula works for any real numbers ${\displaystyle a_{1}}$ and ${\displaystyle a_{n}}$ . For example:

${\displaystyle \left(-{\frac {3}{2}}\right)+\left(-{\frac {1}{2}}\right)+{\frac {1}{2}}={\frac {3\left(-{\frac {3}{2}}+{\frac {1}{2}}\right)}{2}}=-{\frac {3}{2}}.}$

£#h5#£Derivation£#/h5#£
To derive the above formula, begin by expressing the arithmetic series in two different ways:

${\displaystyle S_{n}=a_{1}+(a_{1}+d)+(a_{1}+2d)+\cdots +(a_{1}+(n-2)d)+(a_{1}+(n-1)d)}$
${\displaystyle S_{n}=(a_{n}-(n-1)d)+(a_{n}-(n-2)d)+\cdots +(a_{n}-2d)+(a_{n}-d)+a_{n}.}$
Adding both sides of the two equations, all terms involving d cancel:

${\displaystyle \ 2S_{n}=n(a_{1}+a_{n}).}$
Dividing both sides by 2 produces a common form of the equation:

${\displaystyle S_{n}={\frac {n}{2}}(a_{1}+a_{n}).}$
An alternate form results from re-inserting the substitution: ${\displaystyle a_{n}=a_{1}+(n-1)d}$ :

${\displaystyle S_{n}={\frac {n}{2}}[2a_{1}+(n-1)d].}$
Furthermore, the mean value of the series can be calculated via: ${\displaystyle S_{n}/n}$ :

${\displaystyle {\overline {a}}={\frac {a_{1}+a_{n}}{2}}.}$
The formula is very similar to the mean of a discrete uniform distribution.


£#h5#£Product£#/h5#£
The product of the members of a finite arithmetic progression with an initial element a1, common differences d, and n elements in total is determined in a closed expression

${\displaystyle a_{1}a_{2}a_{3}\cdots a_{n}=a_{1}(a_{1}+d)(a_{1}+2d)...(a_{1}+(n-1)d)=\prod _{k=0}^{n-1}(a_{1}+kd)=d^{n}{\frac {\Gamma \left({\frac {a_{1}}{d}}+n\right)}{\Gamma \left({\frac {a_{1}}{d}}\right)}}}$
where ${\displaystyle \Gamma }$ denotes the Gamma function. The formula is not valid when ${\displaystyle a_{1}/d}$ is negative or zero.

This is a generalization from the fact that the product of the progression ${\displaystyle 1\times 2\times \cdots \times n}$ is given by the factorial ${\displaystyle n!}$ and that the product

${\displaystyle m\times (m+1)\times (m+2)\times \cdots \times (n-2)\times (n-1)\times n}$
for positive integers ${\displaystyle m}$ and ${\displaystyle n}$ is given by

${\displaystyle {\frac {n!}{(m-1)!}}.}$

£#h5#£Derivation£#/h5#£
${\displaystyle {\begin{aligned}a_{1}a_{2}a_{3}\cdots a_{n}&=\prod _{k=0}^{n-1}(a_{1}+kd)\\&=\prod _{k=0}^{n-1}d\left({\frac {a_{1}}{d}}+k\right)=d\left({\frac {a_{1}}{d}}\right)d\left({\frac {a_{1}}{d}}+1\right)d\left({\frac {a_{1}}{d}}+2\right)\cdots d\left({\frac {a_{1}}{d}}+(n-1)\right)\\&=d^{n}\prod _{k=0}^{n-1}\left({\frac {a_{1}}{d}}+k\right)=d^{n}{\left({\frac {a_{1}}{d}}\right)}^{\overline {n}}\end{aligned}}}$
where ${\displaystyle x^{\overline {n}}}$ denotes the rising factorial.

By the recurrence formula ${\displaystyle \Gamma (z+1)=z\Gamma (z)}$ , valid for a complex number ${\displaystyle z>0}$ ,

${\displaystyle \Gamma (z+2)=(z+1)\Gamma (z+1)=(z+1)z\Gamma (z)}$ ,
${\displaystyle \Gamma (z+3)=(z+2)\Gamma (z+2)=(z+2)(z+1)z\Gamma (z)}$ ,
so that

${\displaystyle {\frac {\Gamma (z+m)}{\Gamma (z)}}=\prod _{k=0}^{m-1}(z+k)}$
for ${\displaystyle m}$ a positive integer and ${\displaystyle z}$ a positive complex number.

Thus, if ${\displaystyle a_{1}/d>0}$ ,

${\displaystyle \prod _{k=0}^{n-1}\left({\frac {a_{1}}{d}}+k\right)={\frac {\Gamma \left({\frac {a_{1}}{d}}+n\right)}{\Gamma \left({\frac {a_{1}}{d}}\right)}}}$ ,
and, finally,

${\displaystyle a_{1}a_{2}a_{3}\cdots a_{n}=d^{n}\prod _{k=0}^{n-1}\left({\frac {a_{1}}{d}}+k\right)=d^{n}{\frac {\Gamma \left({\frac {a_{1}}{d}}+n\right)}{\Gamma \left({\frac {a_{1}}{d}}\right)}}}$

£#h5#£Examples£#/h5#£
Example 1
Taking the example ${\displaystyle 3,8,13,18,23,28,\ldots }$ , the product of the terms of the arithmetic progression given by ${\displaystyle a_{n}=3+5(n-1)}$ up to the 50th term is

${\displaystyle P_{50}=5^{50}\cdot {\frac {\Gamma \left(3/5+50\right)}{\Gamma \left(3/5\right)}}\approx 3.78438\times 10^{98}.}$
Example 2
The product of the first 10 odd numbers ${\displaystyle (1,3,5,7,9,11,13,15,17,19)}$ is given by

${\displaystyle 1.3.5\cdots 19=\prod _{k=0}^{9}(1+2k)=2^{10}\cdot {\frac {\Gamma \left({\frac {1}{2}}+10\right)}{\Gamma \left({\frac {1}{2}}\right)}}}$ = 654,729,075

£#h5#£Standard deviation£#/h5#£
The standard deviation of any arithmetic progression can be calculated as

${\displaystyle \sigma =|d|{\sqrt {\frac {(n-1)(n+1)}{12}}}}$
where ${\displaystyle n}$ is the number of terms in the progression and ${\displaystyle d}$ is the common difference between terms. The formula is very similar to the standard deviation of a discrete uniform distribution.


£#h5#£Intersections£#/h5#£
The intersection of any two doubly infinite arithmetic progressions is either empty or another arithmetic progression, which can be found using the Chinese remainder theorem. If each pair of progressions in a family of doubly infinite arithmetic progressions have a non-empty intersection, then there exists a number common to all of them; that is, infinite arithmetic progressions form a Helly family. However, the intersection of infinitely many infinite arithmetic progressions might be a single number rather than itself being an infinite progression.


£#h5#£History£#/h5#£
According to an anecdote of uncertain reliability, young Carl Friedrich Gauss in primary school reinvented this method to compute the sum of the integers from 1 through 100, by multiplying n/2 pairs of numbers in the sum by the values of each pair n + 1. However, regardless of the truth of this story, Gauss was not the first to discover this formula, and some find it likely that its origin goes back to the Pythagoreans in the 5th century BC. Similar rules were known in antiquity to Archimedes, Hypsicles and Diophantus; in China to Zhang Qiujian; in India to Aryabhata, Brahmagupta and Bhaskara II; and in medieval Europe to Alcuin, Dicuil, Fibonacci, Sacrobosco and to anonymous commentators of Talmud known as Tosafists.


£#h5#£See also£#/h5#£ £#ul#££#li#£Geometric progression£#/li#£ £#li#£Harmonic progression£#/li#£ £#li#£Triangular number£#/li#£ £#li#£Arithmetico-geometric sequence£#/li#£ £#li#£Inequality of arithmetic and geometric means£#/li#£ £#li#£Primes in arithmetic progression£#/li#£ £#li#£Linear difference equation£#/li#£ £#li#£Generalized arithmetic progression, a set of integers constructed as an arithmetic progression is, but allowing several possible differences£#/li#£ £#li#£Heronian triangles with sides in arithmetic progression£#/li#£ £#li#£Problems involving arithmetic progressions£#/li#£ £#li#£Utonality£#/li#£ £#li#£Polynomials calculating sums of powers of arithmetic progressions£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Arithmetic series", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#£ £#li#£Weisstein, Eric W. "Arithmetic progression". MathWorld.£#/li#£ £#li#£Weisstein, Eric W. "Arithmetic series". MathWorld.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Series > General Series £#/li#££#/ul#£




£#h3#£Arithmetic Sequence£#/h3#£

An arithmetic progression or arithmetic sequence is a sequence of numbers such that the difference between the consecutive terms is constant. For instance, the sequence 5, 7, 9, 11, 13, 15, . . . is an arithmetic progression with a common difference of 2.

If the initial term of an arithmetic progression is ${\displaystyle a_{1}}$ and the common difference of successive members is ${\displaystyle d}$ , then the ${\displaystyle n}$ -th term of the sequence ( ${\displaystyle a_{n}}$ ) is given by:

${\displaystyle \ a_{n}=a_{1}+(n-1)d}$ ,
and in general

${\displaystyle \ a_{n}=a_{m}+(n-m)d}$ .
A finite portion of an arithmetic progression is called a finite arithmetic progression and sometimes just called an arithmetic progression. The sum of a finite arithmetic progression is called an arithmetic series.


£#h5#£Sum£#/h5#£
The sum of the members of a finite arithmetic progression is called an arithmetic series. For example, consider the sum:

${\displaystyle 2+5+8+11+14}$
This sum can be found quickly by taking the number n of terms being added (here 5), multiplying by the sum of the first and last number in the progression (here 2 + 14 = 16), and dividing by 2:

${\displaystyle {\frac {n(a_{1}+a_{n})}{2}}}$
In the case above, this gives the equation:

${\displaystyle 2+5+8+11+14={\frac {5(2+14)}{2}}={\frac {5\times 16}{2}}=40.}$
This formula works for any real numbers ${\displaystyle a_{1}}$ and ${\displaystyle a_{n}}$ . For example:

${\displaystyle \left(-{\frac {3}{2}}\right)+\left(-{\frac {1}{2}}\right)+{\frac {1}{2}}={\frac {3\left(-{\frac {3}{2}}+{\frac {1}{2}}\right)}{2}}=-{\frac {3}{2}}.}$

£#h5#£Derivation£#/h5#£
To derive the above formula, begin by expressing the arithmetic series in two different ways:

${\displaystyle S_{n}=a_{1}+(a_{1}+d)+(a_{1}+2d)+\cdots +(a_{1}+(n-2)d)+(a_{1}+(n-1)d)}$
${\displaystyle S_{n}=(a_{n}-(n-1)d)+(a_{n}-(n-2)d)+\cdots +(a_{n}-2d)+(a_{n}-d)+a_{n}.}$
Adding both sides of the two equations, all terms involving d cancel:

${\displaystyle \ 2S_{n}=n(a_{1}+a_{n}).}$
Dividing both sides by 2 produces a common form of the equation:

${\displaystyle S_{n}={\frac {n}{2}}(a_{1}+a_{n}).}$
An alternate form results from re-inserting the substitution: ${\displaystyle a_{n}=a_{1}+(n-1)d}$ :

${\displaystyle S_{n}={\frac {n}{2}}[2a_{1}+(n-1)d].}$
Furthermore, the mean value of the series can be calculated via: ${\displaystyle S_{n}/n}$ :

${\displaystyle {\overline {a}}={\frac {a_{1}+a_{n}}{2}}.}$
The formula is very similar to the mean of a discrete uniform distribution.


£#h5#£Product£#/h5#£
The product of the members of a finite arithmetic progression with an initial element a1, common differences d, and n elements in total is determined in a closed expression

${\displaystyle a_{1}a_{2}a_{3}\cdots a_{n}=a_{1}(a_{1}+d)(a_{1}+2d)...(a_{1}+(n-1)d)=\prod _{k=0}^{n-1}(a_{1}+kd)=d^{n}{\frac {\Gamma \left({\frac {a_{1}}{d}}+n\right)}{\Gamma \left({\frac {a_{1}}{d}}\right)}}}$
where ${\displaystyle \Gamma }$ denotes the Gamma function. The formula is not valid when ${\displaystyle a_{1}/d}$ is negative or zero.

This is a generalization from the fact that the product of the progression ${\displaystyle 1\times 2\times \cdots \times n}$ is given by the factorial ${\displaystyle n!}$ and that the product

${\displaystyle m\times (m+1)\times (m+2)\times \cdots \times (n-2)\times (n-1)\times n}$
for positive integers ${\displaystyle m}$ and ${\displaystyle n}$ is given by

${\displaystyle {\frac {n!}{(m-1)!}}.}$

£#h5#£Derivation£#/h5#£
${\displaystyle {\begin{aligned}a_{1}a_{2}a_{3}\cdots a_{n}&=\prod _{k=0}^{n-1}(a_{1}+kd)\\&=\prod _{k=0}^{n-1}d\left({\frac {a_{1}}{d}}+k\right)=d\left({\frac {a_{1}}{d}}\right)d\left({\frac {a_{1}}{d}}+1\right)d\left({\frac {a_{1}}{d}}+2\right)\cdots d\left({\frac {a_{1}}{d}}+(n-1)\right)\\&=d^{n}\prod _{k=0}^{n-1}\left({\frac {a_{1}}{d}}+k\right)=d^{n}{\left({\frac {a_{1}}{d}}\right)}^{\overline {n}}\end{aligned}}}$
where ${\displaystyle x^{\overline {n}}}$ denotes the rising factorial.

By the recurrence formula ${\displaystyle \Gamma (z+1)=z\Gamma (z)}$ , valid for a complex number ${\displaystyle z>0}$ ,

${\displaystyle \Gamma (z+2)=(z+1)\Gamma (z+1)=(z+1)z\Gamma (z)}$ ,
${\displaystyle \Gamma (z+3)=(z+2)\Gamma (z+2)=(z+2)(z+1)z\Gamma (z)}$ ,
so that

${\displaystyle {\frac {\Gamma (z+m)}{\Gamma (z)}}=\prod _{k=0}^{m-1}(z+k)}$
for ${\displaystyle m}$ a positive integer and ${\displaystyle z}$ a positive complex number.

Thus, if ${\displaystyle a_{1}/d>0}$ ,

${\displaystyle \prod _{k=0}^{n-1}\left({\frac {a_{1}}{d}}+k\right)={\frac {\Gamma \left({\frac {a_{1}}{d}}+n\right)}{\Gamma \left({\frac {a_{1}}{d}}\right)}}}$ ,
and, finally,

${\displaystyle a_{1}a_{2}a_{3}\cdots a_{n}=d^{n}\prod _{k=0}^{n-1}\left({\frac {a_{1}}{d}}+k\right)=d^{n}{\frac {\Gamma \left({\frac {a_{1}}{d}}+n\right)}{\Gamma \left({\frac {a_{1}}{d}}\right)}}}$

£#h5#£Examples£#/h5#£
Example 1
Taking the example ${\displaystyle 3,8,13,18,23,28,\ldots }$ , the product of the terms of the arithmetic progression given by ${\displaystyle a_{n}=3+5(n-1)}$ up to the 50th term is

${\displaystyle P_{50}=5^{50}\cdot {\frac {\Gamma \left(3/5+50\right)}{\Gamma \left(3/5\right)}}\approx 3.78438\times 10^{98}.}$
Example 2
The product of the first 10 odd numbers ${\displaystyle (1,3,5,7,9,11,13,15,17,19)}$ is given by

${\displaystyle 1.3.5\cdots 19=\prod _{k=0}^{9}(1+2k)=2^{10}\cdot {\frac {\Gamma \left({\frac {1}{2}}+10\right)}{\Gamma \left({\frac {1}{2}}\right)}}}$ = 654,729,075

£#h5#£Standard deviation£#/h5#£
The standard deviation of any arithmetic progression can be calculated as

${\displaystyle \sigma =|d|{\sqrt {\frac {(n-1)(n+1)}{12}}}}$
where ${\displaystyle n}$ is the number of terms in the progression and ${\displaystyle d}$ is the common difference between terms. The formula is very similar to the standard deviation of a discrete uniform distribution.


£#h5#£Intersections£#/h5#£
The intersection of any two doubly infinite arithmetic progressions is either empty or another arithmetic progression, which can be found using the Chinese remainder theorem. If each pair of progressions in a family of doubly infinite arithmetic progressions have a non-empty intersection, then there exists a number common to all of them; that is, infinite arithmetic progressions form a Helly family. However, the intersection of infinitely many infinite arithmetic progressions might be a single number rather than itself being an infinite progression.


£#h5#£History£#/h5#£
According to an anecdote of uncertain reliability, young Carl Friedrich Gauss in primary school reinvented this method to compute the sum of the integers from 1 through 100, by multiplying n/2 pairs of numbers in the sum by the values of each pair n + 1. However, regardless of the truth of this story, Gauss was not the first to discover this formula, and some find it likely that its origin goes back to the Pythagoreans in the 5th century BC. Similar rules were known in antiquity to Archimedes, Hypsicles and Diophantus; in China to Zhang Qiujian; in India to Aryabhata, Brahmagupta and Bhaskara II; and in medieval Europe to Alcuin, Dicuil, Fibonacci, Sacrobosco and to anonymous commentators of Talmud known as Tosafists.


£#h5#£See also£#/h5#£ £#ul#££#li#£Geometric progression£#/li#£ £#li#£Harmonic progression£#/li#£ £#li#£Triangular number£#/li#£ £#li#£Arithmetico-geometric sequence£#/li#£ £#li#£Inequality of arithmetic and geometric means£#/li#£ £#li#£Primes in arithmetic progression£#/li#£ £#li#£Linear difference equation£#/li#£ £#li#£Generalized arithmetic progression, a set of integers constructed as an arithmetic progression is, but allowing several possible differences£#/li#£ £#li#£Heronian triangles with sides in arithmetic progression£#/li#£ £#li#£Problems involving arithmetic progressions£#/li#£ £#li#£Utonality£#/li#£ £#li#£Polynomials calculating sums of powers of arithmetic progressions£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Arithmetic series", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#£ £#li#£Weisstein, Eric W. "Arithmetic progression". MathWorld.£#/li#£ £#li#£Weisstein, Eric W. "Arithmetic series". MathWorld.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Series > General Series £#/li#££#/ul#£




£#h3#£Arithmetic Series£#/h3#£

An arithmetic progression or arithmetic sequence is a sequence of numbers such that the difference between the consecutive terms is constant. For instance, the sequence 5, 7, 9, 11, 13, 15, . . . is an arithmetic progression with a common difference of 2.

If the initial term of an arithmetic progression is ${\displaystyle a_{1}}$ and the common difference of successive members is ${\displaystyle d}$ , then the ${\displaystyle n}$ -th term of the sequence ( ${\displaystyle a_{n}}$ ) is given by:

${\displaystyle \ a_{n}=a_{1}+(n-1)d}$ ,
and in general

${\displaystyle \ a_{n}=a_{m}+(n-m)d}$ .
A finite portion of an arithmetic progression is called a finite arithmetic progression and sometimes just called an arithmetic progression. The sum of a finite arithmetic progression is called an arithmetic series.


£#h5#£Sum£#/h5#£
The sum of the members of a finite arithmetic progression is called an arithmetic series. For example, consider the sum:

${\displaystyle 2+5+8+11+14}$
This sum can be found quickly by taking the number n of terms being added (here 5), multiplying by the sum of the first and last number in the progression (here 2 + 14 = 16), and dividing by 2:

${\displaystyle {\frac {n(a_{1}+a_{n})}{2}}}$
In the case above, this gives the equation:

${\displaystyle 2+5+8+11+14={\frac {5(2+14)}{2}}={\frac {5\times 16}{2}}=40.}$
This formula works for any real numbers ${\displaystyle a_{1}}$ and ${\displaystyle a_{n}}$ . For example:

${\displaystyle \left(-{\frac {3}{2}}\right)+\left(-{\frac {1}{2}}\right)+{\frac {1}{2}}={\frac {3\left(-{\frac {3}{2}}+{\frac {1}{2}}\right)}{2}}=-{\frac {3}{2}}.}$

£#h5#£Derivation£#/h5#£
To derive the above formula, begin by expressing the arithmetic series in two different ways:

${\displaystyle S_{n}=a_{1}+(a_{1}+d)+(a_{1}+2d)+\cdots +(a_{1}+(n-2)d)+(a_{1}+(n-1)d)}$
${\displaystyle S_{n}=(a_{n}-(n-1)d)+(a_{n}-(n-2)d)+\cdots +(a_{n}-2d)+(a_{n}-d)+a_{n}.}$
Adding both sides of the two equations, all terms involving d cancel:

${\displaystyle \ 2S_{n}=n(a_{1}+a_{n}).}$
Dividing both sides by 2 produces a common form of the equation:

${\displaystyle S_{n}={\frac {n}{2}}(a_{1}+a_{n}).}$
An alternate form results from re-inserting the substitution: ${\displaystyle a_{n}=a_{1}+(n-1)d}$ :

${\displaystyle S_{n}={\frac {n}{2}}[2a_{1}+(n-1)d].}$
Furthermore, the mean value of the series can be calculated via: ${\displaystyle S_{n}/n}$ :

${\displaystyle {\overline {a}}={\frac {a_{1}+a_{n}}{2}}.}$
The formula is very similar to the mean of a discrete uniform distribution.


£#h5#£Product£#/h5#£
The product of the members of a finite arithmetic progression with an initial element a1, common differences d, and n elements in total is determined in a closed expression

${\displaystyle a_{1}a_{2}a_{3}\cdots a_{n}=a_{1}(a_{1}+d)(a_{1}+2d)...(a_{1}+(n-1)d)=\prod _{k=0}^{n-1}(a_{1}+kd)=d^{n}{\frac {\Gamma \left({\frac {a_{1}}{d}}+n\right)}{\Gamma \left({\frac {a_{1}}{d}}\right)}}}$
where ${\displaystyle \Gamma }$ denotes the Gamma function. The formula is not valid when ${\displaystyle a_{1}/d}$ is negative or zero.

This is a generalization from the fact that the product of the progression ${\displaystyle 1\times 2\times \cdots \times n}$ is given by the factorial ${\displaystyle n!}$ and that the product

${\displaystyle m\times (m+1)\times (m+2)\times \cdots \times (n-2)\times (n-1)\times n}$
for positive integers ${\displaystyle m}$ and ${\displaystyle n}$ is given by

${\displaystyle {\frac {n!}{(m-1)!}}.}$

£#h5#£Derivation£#/h5#£
${\displaystyle {\begin{aligned}a_{1}a_{2}a_{3}\cdots a_{n}&=\prod _{k=0}^{n-1}(a_{1}+kd)\\&=\prod _{k=0}^{n-1}d\left({\frac {a_{1}}{d}}+k\right)=d\left({\frac {a_{1}}{d}}\right)d\left({\frac {a_{1}}{d}}+1\right)d\left({\frac {a_{1}}{d}}+2\right)\cdots d\left({\frac {a_{1}}{d}}+(n-1)\right)\\&=d^{n}\prod _{k=0}^{n-1}\left({\frac {a_{1}}{d}}+k\right)=d^{n}{\left({\frac {a_{1}}{d}}\right)}^{\overline {n}}\end{aligned}}}$
where ${\displaystyle x^{\overline {n}}}$ denotes the rising factorial.

By the recurrence formula ${\displaystyle \Gamma (z+1)=z\Gamma (z)}$ , valid for a complex number ${\displaystyle z>0}$ ,

${\displaystyle \Gamma (z+2)=(z+1)\Gamma (z+1)=(z+1)z\Gamma (z)}$ ,
${\displaystyle \Gamma (z+3)=(z+2)\Gamma (z+2)=(z+2)(z+1)z\Gamma (z)}$ ,
so that

${\displaystyle {\frac {\Gamma (z+m)}{\Gamma (z)}}=\prod _{k=0}^{m-1}(z+k)}$
for ${\displaystyle m}$ a positive integer and ${\displaystyle z}$ a positive complex number.

Thus, if ${\displaystyle a_{1}/d>0}$ ,

${\displaystyle \prod _{k=0}^{n-1}\left({\frac {a_{1}}{d}}+k\right)={\frac {\Gamma \left({\frac {a_{1}}{d}}+n\right)}{\Gamma \left({\frac {a_{1}}{d}}\right)}}}$ ,
and, finally,

${\displaystyle a_{1}a_{2}a_{3}\cdots a_{n}=d^{n}\prod _{k=0}^{n-1}\left({\frac {a_{1}}{d}}+k\right)=d^{n}{\frac {\Gamma \left({\frac {a_{1}}{d}}+n\right)}{\Gamma \left({\frac {a_{1}}{d}}\right)}}}$

£#h5#£Examples£#/h5#£
Example 1
Taking the example ${\displaystyle 3,8,13,18,23,28,\ldots }$ , the product of the terms of the arithmetic progression given by ${\displaystyle a_{n}=3+5(n-1)}$ up to the 50th term is

${\displaystyle P_{50}=5^{50}\cdot {\frac {\Gamma \left(3/5+50\right)}{\Gamma \left(3/5\right)}}\approx 3.78438\times 10^{98}.}$
Example 2
The product of the first 10 odd numbers ${\displaystyle (1,3,5,7,9,11,13,15,17,19)}$ is given by

${\displaystyle 1.3.5\cdots 19=\prod _{k=0}^{9}(1+2k)=2^{10}\cdot {\frac {\Gamma \left({\frac {1}{2}}+10\right)}{\Gamma \left({\frac {1}{2}}\right)}}}$ = 654,729,075

£#h5#£Standard deviation£#/h5#£
The standard deviation of any arithmetic progression can be calculated as

${\displaystyle \sigma =|d|{\sqrt {\frac {(n-1)(n+1)}{12}}}}$
where ${\displaystyle n}$ is the number of terms in the progression and ${\displaystyle d}$ is the common difference between terms. The formula is very similar to the standard deviation of a discrete uniform distribution.


£#h5#£Intersections£#/h5#£
The intersection of any two doubly infinite arithmetic progressions is either empty or another arithmetic progression, which can be found using the Chinese remainder theorem. If each pair of progressions in a family of doubly infinite arithmetic progressions have a non-empty intersection, then there exists a number common to all of them; that is, infinite arithmetic progressions form a Helly family. However, the intersection of infinitely many infinite arithmetic progressions might be a single number rather than itself being an infinite progression.


£#h5#£History£#/h5#£
According to an anecdote of uncertain reliability, young Carl Friedrich Gauss in primary school reinvented this method to compute the sum of the integers from 1 through 100, by multiplying n/2 pairs of numbers in the sum by the values of each pair n + 1. However, regardless of the truth of this story, Gauss was not the first to discover this formula, and some find it likely that its origin goes back to the Pythagoreans in the 5th century BC. Similar rules were known in antiquity to Archimedes, Hypsicles and Diophantus; in China to Zhang Qiujian; in India to Aryabhata, Brahmagupta and Bhaskara II; and in medieval Europe to Alcuin, Dicuil, Fibonacci, Sacrobosco and to anonymous commentators of Talmud known as Tosafists.


£#h5#£See also£#/h5#£ £#ul#££#li#£Geometric progression£#/li#£ £#li#£Harmonic progression£#/li#£ £#li#£Triangular number£#/li#£ £#li#£Arithmetico-geometric sequence£#/li#£ £#li#£Inequality of arithmetic and geometric means£#/li#£ £#li#£Primes in arithmetic progression£#/li#£ £#li#£Linear difference equation£#/li#£ £#li#£Generalized arithmetic progression, a set of integers constructed as an arithmetic progression is, but allowing several possible differences£#/li#£ £#li#£Heronian triangles with sides in arithmetic progression£#/li#£ £#li#£Problems involving arithmetic progressions£#/li#£ £#li#£Utonality£#/li#£ £#li#£Polynomials calculating sums of powers of arithmetic progressions£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Arithmetic series", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#£ £#li#£Weisstein, Eric W. "Arithmetic progression". MathWorld.£#/li#£ £#li#£Weisstein, Eric W. "Arithmetic series". MathWorld.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Abramowitz, M. and Stegun, I. A. (Eds.). Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing. New York: Dover, p. 10, 1972.£#/li#££#li#£Beyer, W. H. (Ed.). CRC Standard Mathematical Tables, 28th ed. Boca Raton, FL: CRC Press, p. 8, 1987.£#/li#££#li#£Burton, D. M. Elementary Number Theory, 4th ed. Boston, MA: Allyn and Bacon, 1989.£#/li#££#li#£Courant, R. and Robbins, H. "The Arithmetical Progression." §1.2.2 in What Is Mathematics?: An Elementary Approach to Ideas and Methods, 2nd ed. Oxford, England: Oxford University Press, pp. 12-13, 1996.£#/li#££#li#£Hoffman, P. The Man Who Loved Only Numbers: The Story of Paul Erdős and the Search for Mathematical Truth. New York: Hyperion, 1998.£#/li#££#li#£Pappas, T. The Joy of Mathematics. San Carlos, CA: Wide World Publ./Tetra, p. 164, 1989.£#/li#££#li#£ Abramowitz, M. and Stegun, I. A. (Eds.). Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing. New York: Dover, p. 10, 1972. £#/li#££#li#£ Beyer, W. H. (Ed.). CRC Standard Mathematical Tables, 28th ed. Boca Raton, FL: CRC Press, p. 8, 1987. £#/li#££#li#£ Burton, D. M. Elementary Number Theory, 4th ed. Boston, MA: Allyn and Bacon, 1989. £#/li#££#li#£ Courant, R. and Robbins, H. "The Arithmetical Progression." §1.2.2 in What Is Mathematics?: An Elementary Approach to Ideas and Methods, 2nd ed. Oxford, England: Oxford University Press, pp. 12-13, 1996. £#/li#££#li#£ Hoffman, P. The Man Who Loved Only Numbers: The Story of Paul Erdős and the Search for Mathematical Truth. New York: Hyperion, 1998. £#/li#££#li#£ Pappas, T. The Joy of Mathematics. San Carlos, CA: Wide World Publ./Tetra, p. 164, 1989. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Series > General Series £#/li#££#/ul#£




£#h3#£Arnold's Cat Map£#/h3#£

In mathematics, Arnold's cat map is a chaotic map from the torus into itself, named after Vladimir Arnold, who demonstrated its effects in the 1960s using an image of a cat, hence the name.

Thinking of the torus ${\displaystyle \mathbb {T} ^{2}}$ as the quotient space ${\displaystyle \mathbb {R} ^{2}/\mathbb {Z} ^{2}}$ , Arnold's cat map is the transformation ${\displaystyle \Gamma :\mathbb {T} ^{2}\to \mathbb {T} ^{2}}$ given by the formula

${\displaystyle \Gamma \,:\,(x,y)\to (2x+y,x+y){\bmod {1}}.}$
Equivalently, in matrix notation, this is

${\displaystyle \Gamma \left({\begin{bmatrix}x\\y\end{bmatrix}}\right)={\begin{bmatrix}2&1\\1&1\end{bmatrix}}{\begin{bmatrix}x\\y\end{bmatrix}}{\bmod {1}}={\begin{bmatrix}1&1\\0&1\end{bmatrix}}{\begin{bmatrix}1&0\\1&1\end{bmatrix}}{\begin{bmatrix}x\\y\end{bmatrix}}{\bmod {1}}.}$
That is, with a unit equal to the width of the square image, the image is sheared one unit up, then two units to the right, and all that lies outside that unit square is shifted back by the unit until it is within the square.


£#h5#£Properties£#/h5#£ £#ul#££#li#£Γ is invertible because the matrix has determinant 1 and therefore its inverse has integer entries,£#/li#£ £#li#£Γ is area preserving,£#/li#£ £#li#£Γ has a unique hyperbolic fixed point (the vertices of the square). The linear transformation which defines the map is hyperbolic: its eigenvalues are irrational numbers, one greater and the other smaller than 1 (in absolute value), so they are associated respectively to an expanding and a contracting eigenspace which are also the stable and unstable manifolds. The eigenspaces are orthogonal because the matrix is symmetric. Since the eigenvectors have rationally independent components both the eigenspaces densely cover the torus. Arnold's cat map is a particularly well-known example of a hyperbolic toral automorphism, which is an automorphism of a torus given by a square unimodular matrix having no eigenvalues of absolute value 1.£#/li#£ £#li#£The set of the points with a periodic orbit is dense on the torus. Actually a point is periodic if and only if its coordinates are rational.£#/li#£ £#li#£Γ is topologically transitive (i.e. there is a point whose orbit is dense).£#/li#£ £#li#£The number of points with period ${\displaystyle n}$ is exactly ${\displaystyle |\lambda _{1}^{n}+\lambda _{2}^{n}-2|}$ (where ${\displaystyle \lambda _{1}}$ and ${\displaystyle \lambda _{2}}$ are the eigenvalues of the matrix). For example, the first few terms of this series are 1, 5, 16, 45, 121, 320, 841, 2205 .... (The same equation holds for any unimodular hyperbolic toral automorphism if the eigenvalues are replaced.)£#/li#£ £#li#£Γ is ergodic and mixing,£#/li#£ £#li#£Γ is an Anosov diffeomorphism and in particular it is structurally stable.£#/li#££#/ul#£
£#h5#£The discrete cat map£#/h5#£
It is possible to define a discrete analogue of the cat map. One of this map's features is that image being apparently randomized by the transformation but returning to its original state after a number of steps. As can be seen in the adjacent picture, the original image of the cat is sheared and then wrapped around in the first iteration of the transformation. After some iterations, the resulting image appears rather random or disordered, yet after further iterations the image appears to have further order—ghost-like images of the cat, multiple smaller copies arranged in a repeating structure and even upside-down copies of the original image—and ultimately returns to the original image.

The discrete cat map describes the phase space flow corresponding to the discrete dynamics of a bead hopping from site qt (0 ≤ qt < N) to site qt+1 on a circular ring with circumference N, according to the second order equation:

${\displaystyle q_{t+1}-3q_{t}+q_{t-1}=0\mod N}$
Defining the momentum variable pt = qt − qt−1, the above second order dynamics can be re-written as a mapping of the square 0 ≤ q, p < N (the phase space of the discrete dynamical system) onto itself:

${\displaystyle q_{t+1}=2q_{t}+p_{t}\mod N}$
${\displaystyle p_{t+1}=q_{t}+p_{t}\mod N}$
This Arnold cat mapping shows mixing behavior typical for chaotic systems. However, since the transformation has a determinant equal to unity, it is area-preserving and therefore invertible the inverse transformation being:

${\displaystyle q_{t-1}=q_{t}-p_{t}\mod N}$
${\displaystyle p_{t-1}=-q_{t}+2p_{t}\mod N}$
For real variables q and p, it is common to set N = 1. In that case a mapping of the unit square with periodic boundary conditions onto itself results.

When N is set to an integer value, the position and momentum variables can be restricted to integers and the mapping becomes a mapping of a toroidial square grid of points onto itself. Such an integer cat map is commonly used to demonstrate mixing behavior with Poincaré recurrence utilising digital images. The number of iterations needed to restore the image can be shown never to exceed 3N.

For an image, the relationship between iterations could be expressed as follows:

${\displaystyle {\begin{array}{rrcl}n=0:\quad &T^{0}(x,y)&=&{\text{Input Image}}(x,y)\\n=1:\quad &T^{1}(x,y)&=&T^{0}\left({\bmod {(}}2x+y,N),{\bmod {(}}x+y,N)\right)\\&&\vdots \\n=k:\quad &T^{k}(x,y)&=&T^{k-1}\left({\bmod {(}}2x+y,N),{\bmod {(}}x+y,N)\right)\\&&\vdots \\n=m:\quad &{\text{Output Image}}(x,y)&=&T^{m}(x,y)\end{array}}}$

£#h5#£Models£#/h5#£
£#h5#£Python code for Arnold's Cat Map£#/h5#£
£#h5#£See also£#/h5#£ £#ul#££#li#£List of chaotic maps£#/li#£ £#li#£Recurrence plot£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Arnold's Cat Map". MathWorld.£#/li#£ £#li#£Effect of randomisation of initial conditions on recurrence time£#/li#£ £#li#£Arnold's Cat Map by Enrique Zeleny, The Wolfram Demonstrations Project.£#/li#£ £#li#£Arnold's Cat Map: An interactive graphical exploration £#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Dynamical Systems £#/li#££#li#£ History and Terminology > Disciplinary Terminology > Biological Terminology £#/li#££#/ul#£




£#h3#£Arsh£#/h3#£

The Mahabharata is one of the two major Sanskrit epics of ancient India; it was composed by the sage Vyasa. The most important characters of Mahabharata can be said to include: Krishna; the Pandavas — Yudhishthira, Bheema, Arjuna, Nakula and Sahadeva, along with their wife Draupadi; and the Kauravas (who were a hundred brothers), led by the eldest brother, Duryodhana. The most important other characters include Bhishma, Karna, Dronacharya, Shakuni, Dhritrashtra, Gandhari and Kunti. Some pivotal additional characters include Balarama, Subhadra, Vidura, Abhimanyu, Kripacharya, Pandu, Satyavati, Ashwatthama, and Amba. Deities who play a significant role in the epic include Vishnu, Brahma, Shiva, Ganga, Indra, Surya and Yama.

This list mentions notable characters and may also contain characters appearing in regional stories and folklores related to Mahabharata.


£#h5#£A£#/h5#£
£#h5#£Abhimanyu£#/h5#£
Abhimanyu is a legendary warrior from the ancient Hindu epic Mahabharata. He was born to the third Pandava prince Arjuna and Yadava princess Subhadra, who was also Krishna's sister. Abhimanyu was the son of third Pandava prince Arjuna and Yadava princess Subhadra. He was a disciple of his maternal uncles Krishna and Balrama. He was killed unfairly on the 13th day of Kurukshetra War. He is believed to be an incarnation of Varchas, son of Chandra.


£#h5#£Adhiratha£#/h5#£
Adhiratha was the foster father of Karna and the charioteer of Bheeshma. He was also the leader of all Sutas and royal charioteers. His wife was Radha. Shon was their biological son. As the Bhagavata Purana, Adhiratha descended from Yayati and therefore was related to Krishna. He was also the descendant of Romapada, the king of Anga and brother-in-law of Dashratha's descendant Shighra, king of Ayodhya.


£#h5#£Adrika£#/h5#£
Adrika was an apsara, who was cursed to become a fish and only to be liberated when she gives birth to a human. Adrika, as a fish, lived in the river Yamuna. Once she came in contact with the semen of Uparichara and impregnated herself. After 10 month, some fishermen caught her, cut open her womb and found two children — Matsyagandha and Matsya. After the incident, Adrika was liberated from her curse and returned to heaven.


£#h5#£Agastya£#/h5#£
Agastya is the famous sage are mentioned in Puranas and Itihasa epics. He introduced in Vana Parva of Mahabharata.


£#h5#£Agni£#/h5#£
Agni is the Hindu god of fire. In the Vana Parva, sage Markandeya told the story of Agni's marriage. In the Khandava-daha Parva, Agni in disguise approaches Krishna and Arjuna seeking sufficient food for gratification of his hunger and expressed his desire to consume the forest of Khandava protected by Indra for the sake of Takshaka, the chief of the Nagas. Aided by Krishna and Arjuna, Agni consumes the Khandava Forest. Later, as a boon, Arjuna got all his weapons from Indra and also the bow, Gandiva, from Varuna.


£#h5#£Alambusha£#/h5#£
Alambusha was a Rakshasa and younger brother of Baka and Kirmira. In the Kurukshetra War, he fought from the Kaurava side. During the war, he killed Iravan, son of Pandava prince Arjuna. Later, Alambusha was killed by Bhima's son, Ghatotkacha.


£#h5#£Alayudha£#/h5#£
He was a demon and friend of another demon named Alambhusha. He and Alambhusha were killed by Bhima's demon son Ghatotkacha during Night war.


£#h5#£Amba£#/h5#£
Amba was the eldest daughter of Kashya, the king of Kashi and the sister of Ambika and Ambalika. Amba was abducted by Kuru prince Bhishma and holds him responsible for her misfortune. Her sole goal in life becomes his destruction, to fulfill which she is reborn as Shikhandini/Shikhandi.


£#h5#£Ambalika£#/h5#£
Ambalika is the daughter of Kashya, the King of Kashi, and wife of Vichitravirya, the King of Hastinapur. She was also the mother of Pandu, step mother of Dhritarashtra and grand mother of Pandavas.


£#h5#£Ambika£#/h5#£
Ambika is the daughter of Kashya, the King of Kashi, and wife of Vichitravirya, the king of Hastinapura. She was also the mother of Dhritarashtra and stepmother of Pandu and grandmother to the Kauravas.


£#h5#£Ambika's maid£#/h5#£
The chief maid of Ambika—named Parishrami in later retelling—was sent by Ambika and Ambalika to Maharishi Vyasa for Niyoga. From their union, Vidura was born.


£#h5#£Amitaujas£#/h5#£
Amitaujas was the mighty prince of Panchala Kingdom. He was rebirth of Asura called Amitaujas. He was a maharathi on the side of Pandavas.


£#h5#£Anjanaparvana£#/h5#£
He was the son of Ghatotkacha and Mourvi. His grandparents were Bhima and Hidimbi. He was killed by Ashwatthama in the Kurukshetra War.


£#h5#£Aparajita£#/h5#£
He is the king who was prowess like Indra.


£#h5#£Arjuna£#/h5#£
Arjuna is one of the major characters in Mahabharata. Arjuna plays the listener's role in Bhagavad Gita. Arjuna was the son of Pandu and Kunti in the Kuru Kingdom. He was the spiritual son of Indra. He was the 3rd of the Pandava brothers and was married to Draupadi, Ulupi, Chitrāngadā and Subhadra at different times. His 4 children included Iravan, Babruvahana, Abhimanyu and Srutakarma. Krishna was his cousin and mentor.


£#h5#£Aruni£#/h5#£
In the Mahabharata, Aruni appeared in the Adi Parva. Aruni was a disciple of sage named Dhaumya. Once a flood took place in the fields of the ashram (school). A breach was formed in the embankment. Dhaumya sent Aruni to stop the water from entering the embankment. After a long time, Aruni had not returned. So, Dhaumya went out to find Aruni. The latter lying in the breach of the embankment to prevent the water from entering it. Because of his loyalty, Aruni is also known as Gurubhakta Aruni.


£#h5#£Asoka£#/h5#£
He was the charioteer of Bhima. When Bhima was fought with Srutayu, Asoka brought the chariot to Bhima.


£#h5#£Ashvins£#/h5#£
The Ashvins or Ashwini is a pair of twin gods. Their father is Surya and his mother is Saranyu. They are the gods of medicine and health. In the epic, Kunti felt bad for Madri as she didn't have any children due to a curse and shared her secret mantra with her. Madri, using the mantra, called the Ashwini and had one pair of twins, Nakula and Sahadeva.


£#h5#£Ashwatthama£#/h5#£
Ashwatthama was the son of guru Drona and the grandson of the sage Bharadwaja. Ashwatthama possessed the celestial weapon Narayanastra (which no one possessed in the Mahabharat era). He used Narayanastra and killed 1 akshouni of the Pandava army. He became a Chiranjivi (immortal) due to a curse on him by Lord Krishna after he shot the Brahmashirastra over Uttara's womb. Ashvatthama was appointed as the final commander-in-chief of the Kauravas in the Kurukshetra War. Overcome with grief and rage, he slaughters most of the Pandava camp in a single night offensive.


£#h5#£Ashtavakra£#/h5#£
£#h5#£Astika£#/h5#£
Astika was a rishi, and he was a son of Jaratkaru by the serpent goddess Manasa – a sister of the great serpent king Vasuki. He saved the life of a serpent Takshaka, the king of snakes, when the king Janamejaya organized a snake sacrifice known as Sarpa Satra, where he made great sacrifices of serpents, to avenge for the death of his father Parikshit.


£#h5#£Avantini£#/h5#£
She was the wife of Shalya and the mother of Madranjaya, Rukmanagada, and Rukmanaratha. She was the princess of Avanti.


£#h5#£Ayus£#/h5#£
Ayu or Ayus was an ancestor of Shantanu. He was a son Pururavas and his apsara wife, Urvashi. He married Prabha, an asura princess (daughter of Swarbhanu). He was succeeded by his son Nahusha.


£#h5#£Ayodhaumya£#/h5#£
Ayodhaumya/ Dhaumya was a sage of Avanti. He had three disciples namely Aruni of Panchal, Upamanyu and Veda. He even accompanied the Pandavas into the forest of Kurujangala during their exile. He sang songs of Sama Veda referring to Yama.


£#h5#£B£#/h5#£
£#h5#£Babruvahana£#/h5#£
Babruvahana was one of the sons of Arjuna, begotten through Chitrangada, the princess of Manipur. During the Ashvamedha yagna, he killed his father Arjuna without knowing his identity. But Arjuna's other wife Ulupi brought back his life with the help of Nagamani.


£#h5#£Babhru£#/h5#£
Babhru was a yadava and the friend of Krishna. In Mausala Parva of the epic, he get to penance and killed by the arrow from Veda Vyasa.


£#h5#£Bahlika£#/h5#£
Bahlika, also spelled as Vahlika, was the king of Bahlika kingdom in the Mahabharata, the elder brother of Shantanu, who was a king of Hastinapur and the uncle of Bhishma. He was the oldest warrior to fight in the Mahabharata war. He had a son, Somadatta, and grandson, Bhurishravas, who along with him fought on the side of the Kaurava army in the Kurukshetra War. He was slain by Bhima on the 14th day of the war when it continued after sunset. According to Yudhishthira, Bahlika's only wish was that there should be peace among the Bhāratas.


£#h5#£Bakasura£#/h5#£
Bakasura was a demon who was killed by Bhima near the city of Ekacakrā.


£#h5#£Balarama£#/h5#£
Balarama was the elder brother of Krishna and Subhadra. He is also known as Baladeva, Balabhadra, Haladhara and Halayudha. He was the son of Vasudeva and Rohini. He was the eighth incarnation of Vishnu, also sometimes considered to be an incarnation of Shesha. He is described as an extremely powerful warrior. He taught both Duryodhana of the Kauravas and Bhima of the Pandavas the art of fighting with a mace.


£#h5#£Bhadra and Madira£#/h5#£
Bhadra and Madira were two wives of Vasudeva, the others being Rohini Devi and Devaki. Bhadra's children were Upanidhi, Gada, and others. Madira's children were Nanda, Upananda, Kritaka, and others. They cremated themselves with Vasudeva.


£#h5#£Bhagadatta£#/h5#£
Bhagadatta was the son of Naraka, king of the Pragjyotisha Kingdom and second in a line of kings of Naraka dynasty. He was succeeded by his son Vajradatta. He sided with Kaurava in the Mahabharata war as he was an enemy of Lord Krishna. He was killed by Arjuna on the 12th day of battle. In the war, he defeated great Pandava warriors including Drishtadyumna, Drupada, Matsya king Virata, Bhima.


£#h5#£Bhanu£#/h5#£
He was the son of Shri Krishna and Satyabhama. He was the father of Bhanumati (not to be confused with Duryodhan's wife). According to the regional folklore, he married Yudhishthira and Draupadi's daughter, Suthanu and had a son named Vajra.


£#h5#£Bharadwaja£#/h5#£
Bharadwaja was a sage with divine powers. He was a son of God Brihaspati. Once he was visiting Haridwar, where he saw Ghritachi, an Apsara, bathing in river. He was filled with desire and discharged his seed. It fell into a pot and Drona was born. Bharadwaja trained his son and Drupada.


£#h5#£Bharata£#/h5#£
Bharata is an ancestor of the Pandavas and the Kauravas in the Sanskrit epic Mahabharata. Though the Bhāratas is a prominent community in the Rigveda, the story of Bharata is first told in the Adi Parva of the Mahabharata, wherein he is the son of Dushyanta and Shakuntala. According to the epic, Bharata was a Chakravartin.


£#h5#£Bhima£#/h5#£
Bhima is the 2nd born of the Pandavas. He was the son of Pandu and Kunti and spiritual son of Vayu. The Mahabharata relates many events which portray the might of Bhima. Physically, Bhima was the strongest person on Earth after Hanuman and Balarama. Bhima killed demons including Bakasura, Hidimbasura, Kirmira, Jatasura, etc. Bhima defeated and killed fearsome warrior Jarasandha. Bhima also slew Krodhavanshas, demon Maniman, and Kichaka. In Kurukshetra war, Bhima alone killed 100 Kaurava brothers. He was considered to have the physical strength of 10,000 elephants approximately. Bheema was an invincible wrestler and invincible mace fighter.


£#h5#£Bhima of Vidarbha£#/h5#£
Bhima was the king of Vidarbha and the father of Damayanti.


£#h5#£Bhishma£#/h5#£
Originally named as 'Devavrata', he was the eighth son of the Kuru King Shantanu and the river goddess Ganga. Bhishma was blessed with a boon from his father that he could choose the time of his death or he may remain immortal till he desires. He was related to both the Pandavas and the Kauravas through his half-brother, Vichitravirya (Son of Satyavati). He was one of the best archers and one of the greatest warriors of his time and he was trained by Lord Parashurama. On one of the occasions, he gave a tough fight to Parashurama- no Kshatriya achieved this feat.


£#h5#£Bhrigu£#/h5#£
Sauti said in the Pauloma Parva that Bhrigu was the son of Brahma. He was married to Puloma, who gave birth to Chyavana. When the demon Puloma was carrying off his wife Puloma, she gave birth to his son, Chyavana, by whose brightness the demon was burnt into ashes. When Bhrigu saw his wife crying, he asked the reason. Puloma stated that Agni had said to the demon Puloma that I was the girl with whom Puloma was betrothed. In anger, Bhrigu cursed Agni to engulf everything whether pure or impure.


£#h5#£Bhurishravas£#/h5#£
Bhurishravas was the son of Somadatta and the grandson of Bahlika, hence making him the cousin of Dhritarashtra, Pandu, and Vidura. Bhurishravas had 2 brothers – Bhuri and Shala. Bhurishravas, in the Kurukshetra War, is known to have a rivalry with Yadava general Satyaki. Bhurishravas is eventually killed by Satyaki in the War.


£#h5#£Bhuri£#/h5#£
Bhuri is the son of Somadatta and the brother of Bhurisravas. He attendented in Rajasuya Yajna of Yudhishithra. During Kurukshetra war, he was killed by Satyaki.


£#h5#£Brihadbala£#/h5#£
Brihadbala was the king of Kosala and desecedent of Rama. He was killed by Abhimanyu in Kurukshetra war.


£#h5#£Brihadratha£#/h5#£
Brihadratha was the father of Jarasandha and the king of Magadha.


£#h5#£Budha£#/h5#£
Budha is the illegitimate son of Chandra, the moon god, and Tara, wife of Brihaspati. He met Ilā and married her. From their union, a son was born, who was known as Pururava. Pururavas founded the great lunar dynasty.


£#h5#£C£#/h5#£
£#h5#£Chandra£#/h5#£
Chandra is the moon god. He is son of Anusuya and Atri. The Chandravanshi (lunar dynasty) is named after him as he started it. Chandra had an affair with Tara, Brihaspati's wife. From their union, Tara became pregnant with Chandra's son, Budha. Later, Chandra married Rohini and a son named Varchas was born.


£#h5#£Chandravarma Kamboja£#/h5#£
Chandravarma Kamboja is the first Kamboja king mentioned by name in the Mahābhārata. He was the rebirth of Asura Chandra. He was killed by Dhristadyumna in 12th day of Kurukshetra war.


£#h5#£Gandharva King Chitrasena£#/h5#£
Chitrasen appeared twice in the epic. Chitrasena was the King of the Gandharvas who prevented the Kauravas from putting up their camp near the pond where he himself had encamped.

Chitrasena was also introduced in the epic in the Vana Parva, as a teacher of music by Indra. Indra foresaw that Arjuna would have to spend one year at King Virata's palace as a eunuch, during which time he would need the knowledge of music and dance. He wanted Arjuna to be trained by the king of the Gandharvas, Chitrasena. Chitrasena began his classes soon and the two also became good friends. When Urvashi cursed Arjuna to remain a eunuch for life, it was Chitrasena along with Indra who mediated with her to reduce the tenure of her curse to a single year. Chitrasena was able to achieve this by narrating to her the story of the Pandavas and the bravery of Arjuna.


£#h5#£Chekitana£#/h5#£
Chekitana was the son of Kekaya king Dhrishtaketu and Queen Shrutakirti, a Yadava. Chekitana was described to be a valorous warrior, who fought with warriors like Susharma, Kripacharya and Dronacharya. He also rescued Nakula from the clutches of Duryodhana. On the 18th day, he was killed by Duryodhana.


£#h5#£Chitra and Chitrasena£#/h5#£
Chitra and Chitrasena were brothers and the 2 kings of the Abhisara Kingdom. Both of them sided with the Kauravas in the Kurukshetra War. Chitra was killed by Prativindhya on the 16th day, whereas Chitrasena was killed by Shrutakarma on the same day.


£#h5#£Chitrāngada£#/h5#£
Chitrāngada was a king in ancient India. In the Mahabharata, he is the elder son of Shantanu and Satyavati, ascending the throne of Hastinapura after his father's death. However, he is killed by a Gandharva named Chitrāngada soon after that.


£#h5#£Gandharva Chitrāngada£#/h5#£
Chitrangada was a Gandharva, who was jealous of Shantanu's son Chitrāngada, for sharing a name. One day, the Gandharva challenged the prince and killed him.


£#h5#£Chitrāngada of Kalinga£#/h5#£
Chitrāngada was the king of the Kalinga kingdom. In the Shanti Parva of the epic, Narada narrated that Chitrangada's daughter (Bhanumati) with Kaurava Duryodhana. After him, Srutayudha became the king of Kalinga as he had no son. Possibly, his wife was Chandramudra.


£#h5#£Chitrāngadā£#/h5#£
Chitrāngadā was the warrior princess of Manipura. She was the only heir of king Chitravahana and one of Arjuna's consorts. She had a son named Babhruvahana with him. Later, Babhruvahana unknowingly killed his father but was revived by Ulupi, Chitrāngadā's friend, and co-wife.


£#h5#£Chitravahana£#/h5#£
He was the king of Manipura and the father of Chitrangadaa. He was also the grandfather of Babruvahana. His wife was Queen Vasundhara.


£#h5#£Chitravarma£#/h5#£
Chitravarma was the mighty prince of Panchala. He and his brothers fought with Drona and killed by him in 14th day of Kurukshetra battle.


£#h5#£Charudeshna£#/h5#£
Charudeshna was the son of Krishna and Rukmini. He fought against Shalva's army to defending Dwaraka. He was killed by Andhakas and Vrishnis.


£#h5#£D£#/h5#£
£#h5#£Damayanti£#/h5#£
Damayanti is a character in a love story found in the Vana Parva book of the Mahabharata. She was a princess of the Vidarbha Kingdom, who married King Nala of the Nishadha Kingdom. Her story is set long before the Kurukshetra War.


£#h5#£Dantavakra£#/h5#£
Dantavakra was the king of Karusha according to the Mahabharata and the Puranas.


£#h5#£Dasharaja£#/h5#£
Dasharaja, also known as Nishadaraja and Kevataraja, was the chief fisherman of Hastinapura and the adoptive father of Satyavati. He was the one who demanded that Satyavati's heir be named the heir of Hastinapura, due to which Bhishma took a vow of celibacy and a vow not to rule Hastinapura.


£#h5#£Dadhicha£#/h5#£
Dadhica was the sage


£#h5#£Danda and Dandadhara£#/h5#£
Danda and Dandadhara was the 2 brother princes of Magadha Kingdom. In Kurukshetra war, this brothers fought the side of Kauravas and killed by Arjuna.


£#h5#£Darada£#/h5#£
Darada was the Kshatriya king of Bahlikas. At the time of his birth the earth was cleaved because of his weight.


£#h5#£Devaki£#/h5#£
Devaki was the daughter of Ugrasena, the sister of Kansa, wife of Vasudeva Anakadundubhi, the biological mother of Lord Krishna.


£#h5#£Devayani£#/h5#£
Devayani was the daughter of Shukra, the guru of the Asuras. She was married to Yayati and gave birth to two sons — Yadu and Turvasu, and a daughter —Madhavi. Before her marriage, she once fell in love with Brihaspati's son, Kacha. However, Kacha later refused to marry her. She had a friend named Sharmishtha who was secretly in relationship with her husband Yayati.


£#h5#£Devika£#/h5#£
Devika is a minor character in the Mahabharata. She was the daughter of Govasena, the king of the Sivi Kingdom, and the second wife of Yudhishthira they got married in a self choice ceremony. They had a son called Yaudheya.


£#h5#£Devasena£#/h5#£
Devasena was daughter of Indra and wife of Subrahmanya.


£#h5#£Dhrishtadyumna£#/h5#£
Dhrishtadyumna was the son of Drupada and the brother of Draupadi, Shikhandi, and Satyajit in the epic Mahabharata. He had 4 sons – Kshatradharman, Kshatravarman, Kshatranjaya, and Dhrishtaketu. He was the commander-in-chief of the Pandava army during the entire Kurukshetra War i.e. for 18 days. Dhrishtadyumna killed Drona, the royal guru, when he was meditating which was against the rules of engagement.


£#h5#£Dhritrashtra£#/h5#£
In the epic Mahabharata, Dhritarashtra is the King of Kuru Kingdom with its capital Hastinapur. He was born to Vichitravirya's first wife Ambika. Dhritarashtra was born blind and became father to 100 sons and one daughter Dushala by his wife Gandhari (Gāndhārī), and another son Yuyutsu by Sughada, his wife's maid. These children, including the eldest son Duryodhana, came to be known as the Kauravas.


£#h5#£Dhrishtaketu of Chedi£#/h5#£
Dhrishtaketu was the son of Chedi king Shishupala, who was a cousin of Krishna. Dhrishtaketu became the king of Chedi after his father's death and became an ally of the Pandavas. His sister Karenumati was married to Nakula. Dhrishtaketu and his brothers and sons participated in the Kurukshetra War, where they all were killed.


£#h5#£Dhrishtaketu of Kekeya£#/h5#£
Dhrishtaketu was the ruler of Kekeya, and his wife was Shrutakirti, a Yadava who was the daughter of Shurasena. Many of Dhrishtaketu's sons participated in the Kurukshetra War, participating on both sides. Vrihadkshatra and Chekitana were 2 of his notable sons. Dhrishtaketu's daughter Bhadra was married to Krishna, who bore him many sons.


£#h5#£Dirghaprajna£#/h5#£
Dirghaprajna was the Kshatriya king. He ruled over Ayodha, Arjuna defeated him in his victory march. The Pandavas had invited this King to take part before Kurukshetra war.


£#h5#£Draupadi£#/h5#£
Draupadi also referred to as Panchalī, is the most important female and one of the most important characters in Mahabharata. She was born from a yajna organized by Panchala King Drupada and is described to be the most beautiful woman of her time. She was the common wife of the Pandavas, who fought their cousins, the Kauravas in the great Kurukshetra War. She had five sons from each Pandava, who were collectively addressed as the Upapandavas.


£#h5#£Drona£#/h5#£
In the epic Mahabharata, Droṇa or Droṇāchārya was the royal preceptor to the Kauravas and Pandavas. He was a friend of Guru Sukracharya, the guru of Asuras, including Mahabali. He was the son of rishi Bharadwaja and a descendant of sage Angirasa. He was a master of advanced military arts, including the divine weapons or Astras. He was also the second commander- in- chief of kaurava army from 11th day to 15th day. He was beheaded by Dhrishtadyumna when he was meditating to release his soul on the battlefield.


£#h5#£Drupada£#/h5#£
Drupada was the son of King Prishata. He was the king of Southern Panchala. His capital was known as Kampilya. He was father of Shikhandi, Satyajit, Dhrishtadyumna and Draupadi. He was friend turned rival of Droṇa and rivalry developed when he humiliated Droṇa in front of his ministers. Later, with the help of Arjuna, Droṇa took half of Drupada's kingdom. This led Drupada to perform a yajna from which Draupadi and Dhrishtadyumna emerged. He was killed by Droṇa during the Kurukshetra war.


£#h5#£Drumasena£#/h5#£
Durmasena was the Kshatriya (warrior) king. He stood as guard of the wheel of Shalya and was killed by Yudhishthira in last day of Kurukshetra war.


£#h5#£Durmasena£#/h5#£
Durmasena was the son of Dushasana. He helped his father many times in the Kurukshetra war. He was also present inside the Chakra Vyuh on the thirteenth day of the war. He was deprived of his chariot by Abhimanyu and saved by Aswathamma by cutting Abhimanyu's arrow in mid air. After that, Durmasena killed brutally injured Abhimanyu in a mace duel. On 14th day, Durmasena was brutally killed by Draupadi's sons, the Upapandavas, in revenge for Abhimanyu.


£#h5#£Duryodhana£#/h5#£
Duryodhana also is known as Suyodhana, is a major antagonist in Mahabharata and was the eldest of the Kauravas, the hundred sons of a blind king Dhritarashtra and Queen Gandhari. Being the firstborn son of the blind king, he was the crown prince of Kuru Kingdom and its capital Hastinapura along with his cousin Yudhishtra who was older than him. Karna was Duryodhana's closest friend.


£#h5#£Duryodhana's wife (Bhanumati)£#/h5#£
Duryodhana's wife—named Bhanumati in later retelling—is a minor character is in Mahabharata, and mainly appears in the folk tales. She is unnamed in the epic, but it is described that she was the princess of Kalinga Kingdom and was the daughter of Chitrangada. She was abducted by Duryodhana with the help of his friend Karna. From Duryodhana, she is the mother of a son, Laxman Kumara, and daughter, Lakshmanaa. Bhanumati's mother-in-law Gandhari described her to Krishna in the posterior to the battle of Kurukshetra.


£#h5#£Dushala£#/h5#£
Dushala was the daughter of Dhritarashtra and Gandhari, the sister of the Kauravas and the wife of Jaydrath.


£#h5#£Dushasana£#/h5#£
Dushasana was a Kaurava prince, the second son of the blind king Dhritarashtra and Gandhari and the younger brother of Duryodhana in the Hindu epic Mahabharata.


£#h5#£Dushyanta£#/h5#£
Dushyanta was an ancestor of Shantanu and a king of Hastinapura. He was the husband of Shakuntala and the father of the Emperor Bharata.


£#h5#£Durvasa£#/h5#£
Durvasa was the minor character in the epic Mahabharata.


£#h5#£E£#/h5#£
£#h5#£Ekalavya£#/h5#£
Ekalavya (English: एकलव्य, ékalavya) is a character from the epic the Mahābhārata. He was a young prince of the Nishadha, a confederation of jungle tribes (Adivasi) in Ancient India.


£#h5#£G£#/h5#£
£#h5#£Gandhari£#/h5#£
Gandhari is a prominent character in the Indian epic the Mahabharata. She was a princess of Gandhara (modern-day Khyber-Pakhtunkhwa) and the wife of Dhritrashtra, the blind king of Hastinapura, and the mother of a hundred sons, the Kauravas.


£#h5#£Gandhari's maid£#/h5#£
The chief maid of Gandhari—named Sughada in later retelling—was the mother of Yuyutsu. When Gandhari was pregnant for more than nine months, Dhritrashtra, in fear that there would be no heir, impregnated the maid. Later Gandhari gave birth to the 101 Kauravas and Sughada gave birth to Yuyutsu.


£#h5#£Ganesha£#/h5#£
Ganesha is the god of beginnings. He is the son of Shiva and Parvati. The epic poem Mahabharata says that the sage Vyasa asked him to serve as his scribe to transcribe the poem as he dictated it to him. Ganesha agreed but only on the condition that Vyasa recites the poem uninterrupted, that is, without pausing. The sage agreed but found that to get any rest he needed to recite very complex passages so Ganesha would have to ask for clarifications.


£#h5#£Ganga£#/h5#£
In the Mahabharata, Ganga was the first wife of Shantanu, and the mother of heroic warrior-patriarch, Bhishma. She had 8 children, who were Vasus reborn as mortals due to a curse. Ganga drowned her seven sons as the Vasus requested her to do so. However, Shantanu stopped her from drowning their eighth son, who was Bhishma, and asked her questions. Ganga's condition was broken and she left Shantanu. However, she promised him to return his son. When Bhishma is mortally wounded in the Kurukshetra War, Ganga came out of the water in human form and wept uncontrollably over his body.


£#h5#£Gada£#/h5#£
Gada was the son of Vasudeva and Rohini and brother of Balarama. Gada also was present on the occasion when the Pandavas first stepped into the beautiful palace built for them by Maya at Indraprastha. He particapted Rajasuya and Aswamedha yajnas of Yudhishthira. Gada was attacked and killed by Andhakas.


£#h5#£Ghatotkacha£#/h5#£
Ghatotkacha was the son of the Pandava Bhima and Hidimbi. His name comes from the fact that his head was hairless (utkaca) and shaped like a ghatam. He died in Kurukshetra War in the hands of Karna.


£#h5#£Ghritachi£#/h5#£
Ghritachi is one of the prominent Apsara. In the Mahabharata, she appeared in Adi Parva. According to the story, she was bathing in a river. Bharadwaja was passing by, then he saw her. He was filled with desire and discharged his seed. It fell into a pot and Drona was born.


£#h5#£H£#/h5#£
£#h5#£Hanuman£#/h5#£
Unlike Ramayana, lord Hanuman doesn't have a large role in Mahabharata. He appears during the exile of Pandavas. In the story, Bhima, Hanuman's celestial brother, performed a penance to gain more strength. Hanuman wanted to test Bhima and appeared as a normal monkey in front of him. The monkey asked Bhima to lift his tail if he believed in his strength. But, Bhima wasn't able to lift the tail. Later, he realised who the monkey was and apologized. Hanuman taught battle-skills to Bheema for some time.


£#h5#£Hayagriva£#/h5#£
Hayagriva was an horse headed avatar of Lord Vishnu. He incarnated to kill the demons named Madhu and Kaitabha and brings the Vedas to Brahma.


£#h5#£Hamsa£#/h5#£
Hamsa was the swan-incarnation of Lord Vishnu. In Santi Parva of the epic, Hamsa and Sadhyas discussed about patience.


£#h5#£Hidimba£#/h5#£
Hidimba was a Rakshasa and the brother of Hidimbi. He was killed by Bhima, who later married his sister.


£#h5#£Hidimbi£#/h5#£
Hiḍimbī or Hiḍimbā was a Rakshasi in the Mahābhārata. Hidimbi, along with her brother, Hidimba, tried to eat the Pandavas, when they entered their forest. But when she met Bhima, she fell in love with him and told them the plan. After Bhima killed Hidimba, Hidimbi married Bhima and gave birth to Ghatotkacha.


£#h5#£I£#/h5#£
£#h5#£Ila£#/h5#£
Ila or Ilā was a character from Mahabharata who could change his/her gender. As a woman, she married Budha, son of Chandra, and had a son named Pururavas. Pururavas's descendants founded the lunar dynasty.


£#h5#£Indra£#/h5#£
In the epic, Indra appears numerous times. He is son of Kashyapa and Aditi. He is the spiritual father of Arjuna. He was the reason for the separation of Urvashi and Pururavas. During his temporary absence, Nahusha took his place as the king. He is called by Kunti after Dharmaraj and Vayu. Later in the epic, he is shown protecting Takshaka's forest from Arjuna. The Pandavas named their capital, Indraprastha, after him. During the exile of Pandavas, Arjuna came to meet him. During the Kurukshetra war, he took the indestructible armor and earrings from Karna and gave him a powerful weapon. These were some of his appearances in the epic.


£#h5#£Iravan£#/h5#£
Iravan also is known as Aravan and Iravat is a minor character in Mahabharata. He was a son of Pandava prince Arjuna (one of the main heroes of the Mahabharata) and the Naga princess Ulupi, Iravan is the central deity of the cult of Kuttantavar which is also the name commonly given to him in that cult—and plays a major role in the cult of Draupadi. Iravan played a huge role in the Kurukshetra War. On the 7th day, he massacred the Kaurava army and killed many brothers of Shakuni. However, on the 8th day, in a battle of many illusions and magical powers, Iravan is beheaded by the demon Alambusha.


£#h5#£Indradyumna£#/h5#£
Indradyumna was the king who mentioned in 198th chapter of Vana Parva of the epic Mahabharata.


£#h5#£J£#/h5#£
£#h5#£Jambavati£#/h5#£
Jambavati is second of the Ashtabharya, the eight principal queen-consorts of Krishna. She was the only daughter of the bear-king Jambavan. Krishna married her, when he defeated Jambavan to retrieve the stolen Syamantaka jewel.


£#h5#£Janamejaya£#/h5#£
Janamejaya was a Kuru king and a descendant of Arjuna. He was the son of Parikshit and the grandson of Abhimanyu and Uttarā. He performed a snake sacrifice called Sarpa Satra to avenge his father's death, who was killed by Takshaka, Arjuna's naga enemy. Astika, son of Manasa, stopped the sacrifice.


£#h5#£Janapadi£#/h5#£
Janapadi is an Apsara, who once roamed in the forests. One day, upon seeing her, Shardavan, son of Gautama Maharishi discharged his seed. From his seed, Kripa and Kripi were born.


£#h5#£Jarasandha£#/h5#£
According to the Hindu epic Mahabharata, Jarasandha was a powerful king of Magadha. He was a descendant of a king Brihadratha, the creator of the Barhadratha dynasty of Magadha. He was killed by 2nd Pandava Bhima.


£#h5#£Jaratkaru£#/h5#£
Jaratkaru was a sage who wandered all over the earth and remained unmarried. He encountered his ancestors who hung upside down, leading to hell for he did not have any son. This led to his marriage with the snake goddess, Manasa. They gave birth to Astika who saved the snakes from being burnt during the snake sacrifice.


£#h5#£Jayadratha£#/h5#£
Jayadratha was King of Sindhu Kingdom. He was the son of King Vridhakshtra. He was married to Kauravas' only sister and only daughter of Dhritarashtra and Gandhari, Dushala. He kidnapped Draupadi on Duryodhana's order but was stopped by Arjuna and Bhima. His hairs were cut off as a punishment. He was the biggest reason for Abhimanyu's death. Abhimanyu's father Arjuna swore to kill Jayadratha and he fulfill his oath.


£#h5#£Jayatsena£#/h5#£
He was the king of Magadha Kingdom and the son of Jarasandha. Before the Kurukshetra war, the Pandavas sent a letter of invitation to him and he come to help Pandavas with an akshauhini army.


£#h5#£K£#/h5#£
£#h5#£Kadru£#/h5#£
Kadru was the daughter of Daksha and wife of Kasyapa. She was the mother of thousand nāgas. She even cursed her children for not obeying her to be burnt in the snake sacrifice.


£#h5#£Kaalvakra£#/h5#£
He was the most loyal companion, commander-in-chief and main bodyguard of Kansa. He was always appreciated by Kansa. He was also cruel like Kansa. When Krishna was killing Kansa, Balarama killed him by beating him and cutting his head with hands.


£#h5#£Kalayavana£#/h5#£
Kalyavana was a ruler. He was an ally of Magadha ruler, Jarasandha, and enemy of Krishna and Mathura. Krishna, using his wit, killed Kalyavana.


£#h5#£Kacha£#/h5#£
Kacha's story is mentioned in Mahabharata's Adi Parva. He was the son of Brihaspati. He was sent by Devas to Sukracharya's ashram to learn about Mrita Sanjeevani mantra. Sukra's daughter Devyani fell in love with him. However, Kacha later refused to marry her.


£#h5#£Kalki£#/h5#£
Kalki is the final incarnation of supreme god Vishnu. He will incarnate at the end of Kaliyuga and protect Dharma by destroying the sinners and Mlecchas.


£#h5#£Kamsa£#/h5#£
Kamsa or Kansa was the tyrant ruler of the Vrishni kingdom with its capital at Mathura. He is the brother of Devaki, the mother of the god Krishna who later slew Kamsa.


£#h5#£Kanika£#/h5#£
Kanika was a sage of Hastinapur. He acted as a counselor to Dhritarashtra. When Yudhisthira was announced the crown prince, Dhritarashtra became sad for his sons were deceived. And at this time Kanika was summoned to counsel the king, who advised Dhritarashtra not to resort to fight but remove his foes secretly. Unethical methods may also be adopted for killing a foe, was his advice. Then he narrated a story of a jackal, who deceived his companions (tiger, mongoose, wolf, and mouse) by tricking them. Influenced by his counsels Dhritarashtra exiled the Pandavas to Varanavata and constructed the house of lac.


£#h5#£Karenumati£#/h5#£
Karenumati was the daughter of Chedi king Shishupala, and sister of his successor Dhrishtaketu. She was the wife of Pandava Nakula and begot him a son, Niramitra. Niramitra succeeded his father Nakula to the throne of the Northern Madra Kingdom.


£#h5#£Karna£#/h5#£
Karna is one of the main protagonists of the epic Mahabharata. In the epic, Karna was the spiritual son of Surya (the Sun deity) and son of princess Kunti (later the Pandu's queen). He was raised by foster Suta parents named Radha and Adhiratha. Adhiratha was the charioteer and poet profession working for king Dhritarashtra. Karna grows up to be an accomplished warrior, a gifted speaker and becomes a loyal friend of Duryodhana. He is appointed the king of Anga (Bengal) by Duryodhana. Karna joined the Duryodhana's side in the Kurukshetra War. He defeated many warriors including Bhima, Yudhishthira, Nakula, Sahadeva, Bhagadatta, Jarasandha, Ghatotkacha.


£#h5#£Karna's adoptive brothers£#/h5#£
Adhiratha and Radha, the adoptive parents of Karna, had some biological children. Karna's adoptive brothers were killed during the Kurukshetra War. In later retelling, one of them is named Shon, who was killed by Abhimanyu on the 13th day of Kurukshetra War.


£#h5#£Karna's wives£#/h5#£
In the original Mahabharata, there are some mentions of Karna's wife. Her name is not revealed, though it is described that she belonged to Suta (charioteer) community. The names and stories appear in later texts and interpolation.


£#h5#£Kauravas£#/h5#£
Kauravas were the 102 sons of Dhritarashtra. Out of which, 101 were his legitimate children from his wife Gandhari. He had one illegitimate son named Yuyutsu, who was conceived through a maid during Gandhari's two year long pregnancy. Out of these children, Dushala is the only girl. The names of the 102 Kauravas are


£#h5#£Kauravya£#/h5#£
He was the father of Ulupi and grandfather of Iravan. His wife was Vishvahini.


£#h5#£Ketuman£#/h5#£
Ketuman is the son of Ekalavya and who become the king of Nishadas. In Kurukshetra war, he fought the side of Kauravas and killed by 3rd Pandava, Arjuna.


£#h5#£Kichaka£#/h5#£
Kichaka was the general of the Mastya kingdom. He was the brother of Sudeshna, queen of Matsya. He was very powerful and feared by Virata and the citizens of the kingdom. He was killed by Bhima when he tried to force himself on Draupadi.


£#h5#£Kirmira£#/h5#£
Kirmira was a demon and younger brother of demons Alambhusha and Baka. When Pandavas and Draupadi went Kamyaka Forest, Kirmira encountered them and challenged Bhima for a fight as Bhima killed his brother Baka. After a tough fight, Bhima beheaded Kirmira.


£#h5#£Kratha£#/h5#£
Kratha was the Kshatriya king, who was the incarnation of Rahu. In Kurukshetra war this King attacked Abhimanyu.


£#h5#£Kratu£#/h5#£
Kratu is the mind-born son of Brahma and a maharishi. He come to save Rakshashas from the Rakshasha satra, a yajna was performed by sage Parashara.


£#h5#£Kripa£#/h5#£
Kripacharya was the son of Śaradvān and Jānapadī, born in a particularly extraordinary manner. He was the grandson of Maharishi Gautama. He was a descendant of sage Angiras. He along with his sister Kripi were adopted by King Shantanu. Later on Kripa became an acharya, teacher of the royal children, giving him the name Kripacharya. His twin sister Kripi married Drona. Kripa was among the Maharathis who fought on the Kauravas's side against the Pandavas in the Kurukshetra war in the Hindu epic of the Mahabharata.


£#h5#£Kripi£#/h5#£
Kripi was the sister of Kripacharya. She and her brother were adopted by the Rajguru of King Shantanu. Her actual parents were Saradvan and Janapadi. She married Dronacharya, who was poor at that time. When they wanted a powerful son, they prayed to Shiva, and a son named Ashwathama was born.


£#h5#£Krishna£#/h5#£
Lord Krishna is a Hindu deity. He is also a major character in epic Mahabharata. He was an eighth avatar of lord Vishnu/Narayana. He was born to Devaki and her husband, Vasudeva of the Yadava clan in Mathura. During the Kurukshetra War, he became strategist of Padavas and charioteer of Arjuna. At the start of the Dharma Yudhha (Kurukshetra war) between Pandavas and Kauravas, Arjuna is filled with moral dilemma and despair about the violence and death the war will cause in the battle against his own kin. He wonders if he should renounce and seeks Krishna's counsel, whose answers and discourse constitute the Bhagavad Gita. Krishna counsels Arjuna to "fulfill his Kshatriya (warrior) duty to uphold the Dharma" through "selfless action".


£#h5#£Kritavarma£#/h5#£
Kritavarma was one of the Yadava warriors and chieftain, and a contemporary of Krishna. During Kuruksetra war, Kritavarma fought for Kauravas along with Krishna's Narayani sena and was one of survivors of the war.


£#h5#£Kshema£#/h5#£
Kshema was the Kshatriya king; In Kurukshetra war, he fought with Drona and killed by him. In Sambhava Parva, he was the incarnation of a Krodhavasa.


£#h5#£Kshemadhurti£#/h5#£
He was the king of Kulatha Kingdom. He fought the side of Kauravas and killed by Bhima in 15th day of Kurukshetra battle.


£#h5#£Kunti-Bhoja£#/h5#£
In Hindu mythology, Kunti-Bhoja (or Kuntibhoja) was the adoptive father of Kunti and cousin of Shurasena. He was the ruler of the Kunti Kingdom. Kunti was a daughter of King Shurasena but was later given to Kuntibhoja since he was devoid of children. Kuntibhoja raised her as his own daughter and loved her. She was very beautiful and intelligent and later married Pandu. When Kunti was a young girl, the sage Durvasa visited Kuntibhoja one day and sought his hospitality. The king entrusted the sage to Kunti's care and tasked Kunti with the responsibility of serving the sage and meeting all his needs during his stay with them. Eventually, the sage was gratified. Before departing, he rewarded Kunti by teaching her Atharvaveda mantras which enabled her to invoke any god of her choice to beget children by them. His son Purujit succeeded him who was killed by Duryodhana on the 8th day.


£#h5#£Kunti£#/h5#£
Kunti or Pritha was the daughter of Shurasena, and the foster daughter of his cousin Kuntibhoja. She was married to King Pandu of Hastinapur and was the mother of Karna and the Pandavas Yudhishthira, Bhima, Arjuna. She was the paternal aunt of Krishna, Balarama, and Subhadra. She was the step mother of Nakula and Sahadeva. She was very beautiful and intelligent


£#h5#£Kuru£#/h5#£
In the literature, Kuru is an ancestor of Pandu and his descendants, the Pandavas, and also of Dhritarashtra and his descendants, the Kauravas. This latter name derived as a patronym from "Kuru", is only used for the descendants of Dhritarashtra.

King Kuru had two wives named Shubhangi and Vahini. He had a son named Viduratha with Shubhangi, and five sons with Vahini, named Ashvavat, Abhishyat, Citraratha, Muni, and Janamejaya. Due to his merits and great ascetic practices the region "Kurujangal" was named after him. It has also been known as Kurukshetra since ancient Vedic times.


£#h5#£L£#/h5#£
£#h5#£Lakshmana Kumara£#/h5#£
In the Hindu epic Mahabharata, Laxman Kumara or simply Laxman (Lakshman(a)) is the son of Duryodhana, and grandson of Dhritarashtra. He had a twin sister called Lakshmanaa who was kidnapped by Samba (Krishna's son). Not much is revealed about Laxman in the Mahabharata.


£#h5#£Lakshmanaa£#/h5#£
In the Bhagavata Purana, Lakshmanaa (also spelled Laxmanaa or Lakshmanā), also known as Lakshana, is the daughter of Duryodhana. Little is revealed about Laxmanaa in the text other than her marriage to Krishna's son Samba.


£#h5#£Lomasa£#/h5#£
Lomasa was a sage. He tells many stories including the stories of Agastya, Bhagiratha etc.


£#h5#£M£#/h5#£
£#h5#£Madanjaya£#/h5#£
He was Prime Minister of Kuru Kingdom before Vidura. When Bhishma gave his post to Vidura, he tried to kill Vidura but he fought and was beheaded by Bhishma.


£#h5#£Madranjaya£#/h5#£
He was eldest son of Shalya and Avantini who was killed on 2nd day of war by Virata.


£#h5#£Madrasena£#/h5#£
He was younger brother of Shalya and elder brother Madri and Dyutiman. He was uncle of Nakula and Sahadeva. He was unmarried and was killed by Yudhishthira along with Shalya on the last day of war.


£#h5#£Madri£#/h5#£
In the Mahabharata epic, Madri, was sister of Shalya, princess of the Madra Kingdom, second wife of Pandu and the mother of two sons: Nakula and Sahadeva. One day, Pandu and Madri made love, this led Pandu to die due to his curse and Madri to commit suicide.


£#h5#£Malini£#/h5#£
She was maid of Draupadi married to a Kshatriyan soldier Pralanksena. Her son Nakusha was Bodyguard of Drupada. Her husband and son were killed by Drona before Drupada's death on the 15th day of war.


£#h5#£Manasa£#/h5#£
In the Mahabharata, Naga Goddess Manasa is the wife of Jaratkaru. They had a son, Astika, who saved the serpents including Takshaka from Sarpa Satra organised by king Janamejaya to avenge his father's death.


£#h5#£Maniman£#/h5#£
Manimat is the king who was the rebirth of Vrtra, the son of Danayu. Once, Bhima while on his victory march defeated this king. He fought the side of Pandavas and killed by Bhurishravas in the Kurukshetra war.


£#h5#£Marisha£#/h5#£
Shurasena was married to a Nāga (or serpent) woman named Marisha. She bore all of his children and was the cause for Vasuki’s boon to Bhima. after whom the Surasena Kingdom or mahajanpada and the Yadava sect of Surasenas were named. She was the mother of Kunti and Vasudeva as well.


£#h5#£Markandeya£#/h5#£
Markandeya was blessed by Lord Shiva to remain young till the end of Kali Yuga. In the Mahabharat, Markandeya visits the Pandavas during their exile and tells them the story of Nala and Damayanti, Savitri and Satyavan, etc.


£#h5#£Meghavarna£#/h5#£
He was the son of Ghatotkach and Maurvi. He was the grandson of Bhima and Hidimbi. He was the brother of Anjanaparvana. He did not fight the War, and hence, was the only alive son of Ghatotkacha.


£#h5#£Menaka£#/h5#£
Menaka was a beautiful apsara. She was sent by Indra to fill Vishwamitra with lust and destroy his penance. Upon seeing her, Vishwamitra was filled with desire and from their union, Shakuntala, mother of great king Bharat, was born. Menaka left Shakuntala and Vishwamitra again started to meditate. Shakuntala was left with sage Kanva.


£#h5#£Muchukunda£#/h5#£
Muchukunda, son of King Mandhata, and brother of equally illustrious Ambarisha, was born in the Ikshvaku dynasty. He later became a sage and his divine powers killed Kalyavana.


£#h5#£N£#/h5#£
£#h5#£Nala£#/h5#£
Nala is the main character of a love story in the Vana Parva of Mahabharata. He was king of Nishada. He fell in love with Damayanti and married her. But they struggled a lot after their marriage. His story is set long before the Kurukshetra war.


£#h5#£Nahusha£#/h5#£
Nahusha was a king from lunar dynasty and an ancestor of Shantanu. He was the son Ayu and Prabha. He was equal to Indra in every way and was made the ruler of Swarga in Indra's absence. He married Ashokasundari/Viraja, the daughter of Devi Parvati and Lord Shiva or mind-born daughter of Pitrs, and had a son named Yayati. He was removed from his position as the king because of his arrogance and cursed to a snake. His curse was over when he met Yudhishthira in a forest.


£#h5#£Nakula£#/h5#£
Nakula was fourth of the five Pandava brothers. Nakula and Sahadeva were twins born to Madri, who had invoked the Ashwini Kumaras. Nakula and his brother Sahadeva, are both called as Ashvineya(आश्विनेय), as they were born from Ashvinas. Nakula was said to be a skilled master in sword-fighting. On the 18th day of Kurukshetra War, Nakula had killed three sons of Karna.


£#h5#£Nanda£#/h5#£
Nanda was the head of the Gopas tribe of Yadava cowherds referred as Holy Gwals. He was a friend of Vasudeva, spouse of Yashoda and the foster father of Krishna.


£#h5#£Narakasura£#/h5#£
Narakasura was the son of Bhumi, the earth goddess. He gained a boon that only his mother could kill him. He captured and married women forcefully. Lord Krishna and Satyabhama ( the human incarnation of Bhumi) killed him.


£#h5#£Nagnajita£#/h5#£
Nagnajita was the another king of Gandhar and the brother of Subala. He was the incarnation of Asura Ishupa. Karna smashes Nagnajita's kingdom and Krishna defeated him and his sons.


£#h5#£Nila£#/h5#£
Nila was the king of Mahishmati. He come to help with an akshauhini army and he was among the Maharathis who fought on the Kauravas's side in the Kurukshetra war.


£#h5#£Niramitra£#/h5#£
In the Hindu epic Mahabharata, Niramitra (Sanskrit: निरमित्र, lit. he who has no enemies) was the son of Nakula and his wife Karenumati.


£#h5#£Nishatha£#/h5#£
Nishatha was the son of Balarama and his wife Revati.


£#h5#£P£#/h5#£
£#h5#£Padmavati£#/h5#£
Padmavati was the name of wife of Ugrasena. She was the mother of tyrant Kamsa.


£#h5#£Parashara£#/h5#£
Parashara was a sage. He was the grandson of Vasishtha, the son of Śakti Maharṣi, and the father of Vyasa. Before Satyavati married Shantanu, she had an affair with Parashara. During that time, she was known as Matsyagandha. Later they had a child named Vyasa. However they parted away but before leaving, Parashara restored Matsyagandha's virginity and gave her an enchanting scent.


£#h5#£Parashuram£#/h5#£
Parashuram is the sixth avatar of Vishnu in Hinduism and he is one of the chiranjeevis who will appear at the end of the Kali Yuga. He was born to destroy evil Kshatriya, who had begun to abuse their power. Parashurama is also the Guru of Bhishma, Dronacharya, and Karna.


£#h5#£Parikshit£#/h5#£
Parikshit was a king from kuru lineage. He was the son of Abhimanyu (Arjuna's son) and Uttarā. After the Pandavas and Draupadi retired for heaven, he was crowned as the new king. Later, Kali (demon) manipulated Parikshit and he placed a dead snake on a meditating rishi. The Rishi's son saw it and cursed him to die by a snakebite. After he was bitten and killed by Takshaka, his son Janamejaya performed Sarpa Satra. This is where he hears the story of his great-grandfathers.


£#h5#£Pandu£#/h5#£
Pandu was the king of Hastinapur, the son of Ambalika and Vichitravirya. He is popularly known as the father of the Pandavas, who were called so after him. Pandu was responsible and a great warrior, who expanded his kingdom during his rule. He had two wives named Kunti and Madri. He died early due to a curse of a sage.


£#h5#£Pandya£#/h5#£
He was the king of Pandya Kingdom. He gave gifts at Rajasuya yagna of Yudhishthira. During the Kurukshetra war, he come to help Pandavas with the akshushini army. He was a Maharathi on the side of Pandavas.


£#h5#£Prabha£#/h5#£
Prabha, sometimes Indumati, was the daughter of Asura Svarbhanu, who later became Rahu and Ketu. She married Ayu, son of Pururavas of lunar dynasty, and had a son named Nahusha.


£#h5#£Pradyumna£#/h5#£
Pradyumna was the eldest son of Sri Krishna and Rukmini. He is the reincarnation of Kamadeva, who was burnt by lord Shiva for shooting arrow of love at him. After his birth, he was kidnapped by Sambara and thrown into water. However, he survived and was raised by Mayawati (reincarnation of devi Rati). Later, he defeated Sambara and returned to Dwarka. He married Mayawati, Prabhavati and Vidarbha princess Rukmavati, and had a son Aniruddha.


£#h5#£Pratipa£#/h5#£
Pratipa was a king in the Mahabharata, who was the father of Shantanu and grandfather of Bhishma.


£#h5#£Prativindhya£#/h5#£
Prativindhya was the son of Yudhisthir and Draupadi. He was the eldest brother among Upapandavas.


£#h5#£Pritivindhya£#/h5#£
Pritivindhya was the Kshatriya king; He was the rebirth of Asura Ekacakra. He was defeated by Arjuna in his Digvijaya (victory march).


£#h5#£Prishati£#/h5#£
Prishati (lit. daughter-in-law of Prishata) was the wife of King Drupada and the mother of Shikhandini and Satyajit. After Drupada performed a yajna (fire-sacrifice) to obtain a powerful son, she was asked by the sages to consume the sacrificial offering to conceive a child. However, Prishati had perfumed saffron in her mouth and requested the sages to wait till she had a bath and washed her mouth. The sages criticised her untimely request and poured the offering into the flames of the yajna, from which Dhrishtadhyumna and Draupadi emerged. Overwhelmed by their arrival, Prishati requested the sages to declared her as the mother of Dhrishtadhyumna and Draupadi.


£#h5#£Purochana£#/h5#£
Purochana was the builder of the Lakshagraha. However, he, along with his wife and her sons, perished in the fire. He was the royal chief architect in Hastinapura. He was a friend of Shakuni and Duryodhana. Purochana built the Lakshagraha palace and burnt it. He was killed by Bhima in the Lakshagraha palace. Purochana had a wife and many sons. In his last life, Purochana had been Prahasta, Ravana's uncle and commander-in-chief of his army. Shakuni and Duryodhana made another plan to kill the Pandavas. Shakuni told Purochana to build a really beautiful palace in Varnavrata out of only materials that can catch and spread fire easily. Purochana quickly did as Shakuni had said. Purochana called the palace Lakshagraha. It was made out of materials such as wax and twigs. After some time, Shakuni convinced the Pandavas and Kunti to visit Lakshagraha. Purochana and his wife welcomed the Pandavas and Kunti grandly. After 10 days, during the night, Purochana set fire on the palace. The Pandavas woke up and realized that this had been another one of Duryodhana and Shakuni's evil schemes. Bhima got really mad. While Purochana and his sons and wife were trying to escape, Bhima killed all of them, including Purochana.The Pandavas barely managed to escape the fire.


£#h5#£Pururavas£#/h5#£
Pururavas was the first king from the lunar dynasty (Shantanu's dynasty). He was the son Budha, son of Chandra, and Ilā. He married Urvashi but she left him. . He was succeeded by his son, Ayu.


£#h5#£Purujit£#/h5#£
He was the son of Kunti Bhoja and the adpotive brother of Kunti. In Kurukshetra war, he fought against Durmukha and he was killed in the war.


£#h5#£Paurava£#/h5#£
Paurava was the Kshatriya king. Once Arjuna defeated this king in his victory march. The Pandavas invited him for the Kurukshetra battle. But he did not accept it but joined sides with the Kauravas against the Pandavas. Paurava was considered a prominent commander in the Kaurava army. In the Kurukshetra war he at first fought against Dhristaketu and then was wounded when fought against Abhimanyu. Arjuna killed him in 11th day of battle.


£#h5#£R£#/h5#£
£#h5#£Radha£#/h5#£
Radha was the foster mother of Karna, one of the central characters in the Hindu epic Mahabharata. She was the wife of Adhiratha, the charioteer of Bhishma. Radha also bore a son named Shon. The young Kunti used a mantra to beget a son from the Sun god Surya. Afraid of the taint of being an unwed mother, she placed the baby in a basket and set him afloat a river. The child later known as Karna was found and adopted by Radha and Adiratha, who raised Karna as their own. Karna is known by the matronymic Radheya. Karna, once he knows from Krishna and Kunti about his birth secret, having done so much harm to his brothers Pandavas, was in no position to abandon Duryodhana.


£#h5#£Rama£#/h5#£
Rama was the seventh-incarnation of Lord Vishnu. In Vana Parva of the epic, Markandeya tells Rama's story to Yudhishthira.


£#h5#£Revati£#/h5#£
In Mahabharata, Revati was daughter of King Kakudmi and consort of Balarama, the elder brother of Krishna.


£#h5#£Rohini (wife of Vasudeva)£#/h5#£
She was the wife of Vasudeva and mother of Balrama and Subhadra. She looked after Balaram in his childhood. After Vasudeva and Devaki were released, she started living with them. After the passing of Vasudeva in the Yadu massacre, Rohini cremates herself on Vasudeva's pyre along with his other wives Devaki, Bhadra and Madira.


£#h5#£Rukmi£#/h5#£
Rukmi was the ruler of Vidarbha. He was the son of king Bhishmaka and elder brother of Rukmini.


£#h5#£Rukmini£#/h5#£
Rukmini was the first and chief queen consort of Krishna. She was an avatar of goddess Lakshmi. She was the daughter of king Bhishmaka, sister of Rukmi and the princess of Vidarbha.


£#h5#£Ruru£#/h5#£
Ruru was a rishi(sage) of the epic Mahabharata. He was the son of Pramati and Ghritachi, the celestial danseuse and a descendant of Bhrigu. Ruru married Pramadvara, foster-daughter of sage Sthulakesha. He was the father of Sunaka.


£#h5#£Rochamana£#/h5#£
Rochamana was Kshatriya (warrior) king of Aswamedha kingdom. He was killed by Karna in the Kurukshetra war.


£#h5#£S£#/h5#£
£#h5#£Sahadeva£#/h5#£
Sahadeva was the youngest of the five Pandava brothers. Nakula and Sahadev were twins born to Madri who had invoked the Ashwini Kumaras. Sahadeva had two wives Draupadi and Vijaya. Draupadi was the common wife of Pandavas while Vijaya was the beloved wife of Sahadeva. Similar to his twin brother Nakula, Sahadeva was also accomplished in swordsmanship. On the 18th of war, Sahadeva had killed Shakuni who was mainly responsible for the Kurukshetra War.


£#h5#£Sahadeva of Magadha£#/h5#£
Sahadeva was the son of powerful king Jarasandha. When Bhima slayed his father, Krishna declared him to be the new ruler of Magadha. Sahadeva is a frequent ally of the Pandavas, and attended the Rajsuya of Yudhishthira. During the Kurukshetra War, he fought from the side of Pandavas and was slayed by Shakuni.


£#h5#£Sakradeva£#/h5#£
He was son of King Srutayudha and Queen Sakrayani of Kalinga. He was Yuvaraja (Crown Prince) of Kalinga. He was killed by Bhima on the 2nd day of war along with many soldiers and two generals Satya and Satyadeva.


£#h5#£Samba£#/h5#£
Samba was the mischievous son of Krishna and his second wife, Jambavati. He was born as a boon of Lord Shiva. Samba was the husband of Lakshmanaa, Duryodhana's daughter. Later in the epic, his mischief becomes the reason for the destruction of Krishna's Yaduvansha, to whom Gandhari cursed.


£#h5#£Shamika£#/h5#£
Shamika was a sage in the epic Mahabharata. One day, while hunting Parikshit had wounded a deer but lost it in the woods. Searching for it, fatigued he asked meditating Shamika about the deer. The sage did not answer as he was observing the vow of silence. This angered the king, who placed a dead snake on Shamika's shoulder. Sringin, son of Shamika enraged by this act cursed Parikshit to be killed by Takshaka(snake) within seven days.


£#h5#£Samudrasena£#/h5#£
Samudrasena is the Kshatriya king; He was the rebirth of Kalakeyas. Bhima defeated him and his son Candrasena in his victory march. He fought the side of Pandavas and he was killed in the Kurukshetra war.


£#h5#£Samvarana£#/h5#£
Samvarana was a king from lunar dynasty and an ancestor of Shantanu. He married Tapati, daughter of Surya, and had a child named Kuru.


£#h5#£Sanjaya£#/h5#£
Sanjaya was Dhritarashtra's advisor and also his charioteer. Sanjaya was a disciple of sage Krishna Dwaipayana Veda Vyasa and was immensely devoted to his master, King Dhritarashtra. Sanjaya – who has the gift of seeing events at a distance (divya-drishti) right in front of him, granted by the sage Vyasa – narrates to Dhritarashtra the action in the climactic battle of Kurukshetra, which includes the Bhagavad Gita.


£#h5#£Sarama£#/h5#£
Sarama, according to Mahabharata, is a celestial female dog. Janamejaya and his brothers beat one of her sons without any reason when the dog arrives at an occasion of sacrifice. This angers Sarama, and she curses the princes and Janamejaya that evil may happen to them.


£#h5#£Satrajit£#/h5#£
In the Hindu scriptures like the Mahabharata and Bhagvata Puran, Satrajit was a Yadava king famous for his role in the story of Syamantaka gem. He was the father of Satyabhama, who was Bhumidevi's incarnation and Sri Krishna's third wife.


£#h5#£Satyabhama£#/h5#£
Satyabhama is the third consort of the God Krishna, the eighth avatar of the god Vishnu. Satyabhama is believed to be an avatar of Bhumī Devī, the Goddess of Earth who is Prakriti form of Mahalakshmi. She aided Krishna in defeating the demon Narakasura. Later she visited the Pandavas during their exile and had a chat with Draupadi.


£#h5#£Satyajit£#/h5#£
He was second born child of King Drupada and Queen Prishati. He was younger brother of Shikhandini/Shikhandi and elder brother of Dhrishtadyumna and Draupadi. He succeeded the throne of Panchala.


£#h5#£Satyaki£#/h5#£
Yuyudhana, better known as Satyaki, was a powerful warrior belonging to the Vrishni clan of the Yadavas, to which Krishna also belonged. Satyaki was also student of Arjuna due to which he fought on Pandavas side.


£#h5#£Satyavati£#/h5#£
Satyavati is the matriarch of the Mahabharata. She was a fisherwoman before her marriage with Shantanu. She along with her father, Dashraj, proposed the conditions which led Bhishma to take his vow. With Shantanu, she is the mother of Chitrangada and Vichitravirya. She is also the mother of the Vyasa, author of the epic, whom she called for Niyoga when Vichitravirya died without any heir.


£#h5#£Savitri and Satyavan£#/h5#£
In the Mahabharata, Savitri and Satyavan are characters appearing in the Vana Parva of the epic. Savitri is a princess born by the boon of Savitra. She is wise and beautiful. She fell in love with Satyavan, a prince who is destined to die at very young age. Savitri, knowing that she would become a widow at a young age, married Satyavan. The later part of the story is about how Savitri's love and wit saves her husband from Yama, god of death.


£#h5#£Senabindu£#/h5#£
Senabindu was the king who was the incarnation of Asura Tuhunda. Arjuna, during his regional conquest of the north, with the King of Uluka had attacked this King who lost his kingdom. The Pandvas had invited this King to take part in the Kurukshetra battle. He was killed in Kurukshetra war.


£#h5#£Shakuni£#/h5#£
Shakuni was the prince of Gandhara Kingdom in present-day Gandhara, later to become the King after his uncle Nagnajita's death. He is the main antagonist in the Hindu epic Mahabharata. He was the brother of Gandhari and hence Duryodhana's maternal uncle. Shakuni was killed by Sahadeva on the 18th day of the Kurukshetra War.


£#h5#£Shakuni's wife (Arshi)£#/h5#£
Shakuni's wife was the queen of Gandhara and the mother of Uluka. In later retellings she is named Arshi and is also known as Arsh and Charulata. Her brothers Keturaja and Ketusena were killed by Drupada on the 11th day of the war. She was very close to her sister-in-law, Gandhari.


£#h5#£Shakuntala£#/h5#£
Shakuntala was wife of Dushyanta and the mother of Emperor Bharata. Her story is told in the Mahabharata and dramatized by many writers, the most famous adaption being Kalidasa's play Abhijñānaśākuntala (The Sign of Shakuntala).


£#h5#£Shalva£#/h5#£
Shalva was the king of Shalva kingdom. He and Amba, the princess of Kashi, fell in love and Amba decided to choose him during her Swayamvara. However, Bhishma won the princesses for his brother Vichitravirya. When, Amba told Bhishma about her love, he sent her with honour to Shalva. But, Shalva rejected her and told her that he cannot marry her as she was won by Bhishma. He become Krishna's enemy and he was killed by Krishna.


£#h5#£Shalya£#/h5#£
In the epic Mahabharata, King Shalya was the brother of Madri (mother of Nakula and Sahadeva), as well as the ruler of the Madra kingdom. Shalya, a powerful spear fighter and a formidable charioteer, was tricked by Duryodhana to fight the war on the side of the Kauravas. On the last day of the Kurukshetra War, Yudhishthira killed him during a spear fight.


£#h5#£Shala£#/h5#£
Shala was the son of Somadatta and the brother of Bhurishravas. He fought with Abhimanyu. He was killed by Shrutkarma in the Kurukshetra war.


£#h5#£Shankha£#/h5#£
Shankha was 3rd son of King Virata. He was killed by Bhishma on very first day of war.


£#h5#£Shantanu£#/h5#£
Shantanu was a Kuru king of Hastinapura in the epic Mahabharata. He was the youngest son of King Pratipa of Hastinapura and had been born in the latter's old age. He was husband of Ganga and Satyavati. He was father of Devavrat (Bhishma), Chitrāngad and Vichitravirya.


£#h5#£Sharmishtha£#/h5#£
Sharmishtha was an Asura princess and a spouse of Yayati, an ancestor of Shantanu.


£#h5#£Sarana£#/h5#£
Sarana was the son of Vasudeva and Rohini and the brother of Balarama and Krishna. He participted in Rajasuya and Aswamedha yajnas of Yudhishthira.


£#h5#£Shatanika£#/h5#£
Shatanika was the son of Nakula and Draupadi. He was the third brother among Upapandavas.


£#h5#£Shaunaka£#/h5#£
Shaunaka headed the sages during their conclave at his twelve-year sacrifice, where Ugrashravas Sauti recited the Mahabharata.


£#h5#£Shikhandi£#/h5#£
Shikhandi was born as a baby girl, named "Shikhandini," to Drupada, the king of Panchala, and his wife. Later she changed her sex and took the name Shikhandi. He fought in the Kurukshetra war for the Pandavas along with his father Drupada and brother Dhristadyumna. He was Kashi's Amba in previous birth.


£#h5#£Shishupala£#/h5#£
Shishupala was the son of Damaghosha. He was slain by his cousin Krishna, at the great coronation ceremony of Yudhishthira in punishment for the opprobrious abuse made against his august personage. He was also called Chaidya, being a member of Chedi kingdom.


£#h5#£Shrutkarma£#/h5#£
Shrutkarma was the son of Arjuna and Draupadi. He was the youngest brother among Upapandavas.


£#h5#£Shrutsena£#/h5#£
Shrutsena was the son of Sahadeva and Draupadi. He was the fourth brother among Upapandavas.


£#h5#£Shukracharya£#/h5#£
Shukracharya is the son of sage Bhrigu and his wife Kavyamata. After the Devas killed his mother (who was later revived), Shukra developed a deep hatred towards the Devas and became the guru of Asuras. He had a daughter named Devayani, who was married to Lunar king Yayati. But Yayati developed an affair with Devayani's maid, Sharmishtha. This led Shukra to curse Yayati to lose his youth.


£#h5#£Shveta£#/h5#£
He was second son of Virata. He was killed on the first day of war by Shalya.


£#h5#£Shvetaki£#/h5#£
Shvetaki was a king who performed numerous Yajnas.


£#h5#£Sreniman£#/h5#£
Sreniman was a Kshatriya king. He was the incarnation of Kalakeyas. He over ruled the Kumaradesa, Nakula defeated him during his victory march. In Kurukshetra war, he was killed by Drona.


£#h5#£Sons of Karna£#/h5#£
Karna's sons were Vrishasena, Vrishaketu, Banasena, Chitrasena, Satyasena, Sushena, Shatrunjaya, Dvipata and Prasena. All except for Vrishaketu were killed in the war.


£#h5#£Sons of Shalya£#/h5#£
Shalya and Avantini's three sons were Madranjaya, Rukmanagada and Rukmaratha. Madranjaya was the eldest than other two with a gap of 10 years. Rukmanagada and Rukmanaratha were twins. Madranjaya was killed on 2nd day of war by Virata and other two were killed by Abhimanyu inside the Chakravyuha on 13th day.


£#h5#£Sons of Shishupala£#/h5#£
The four sons of Chedi King Shishupala were Dhrishtaketu, Mahipala, Suketu, Sarabha. They had a sister named Karenumati who was younger than Dhrishtaketu but elder than other three. Dhrishtaketu succeeded the throne of Chedi after Shishupala's death. Dhrishtaketu was killed by Dronacharya on 6th day of war and other three were killed by Shakuni's son Vrikaasur.


£#h5#£Somaka£#/h5#£
Somaka was the king of Panchala kingdom and the grandfather of Drupada.


£#h5#£Subala£#/h5#£
Subala was father of Shakuni and Gandhari. He was King of Gandhara and later King-Father under Shakuni's rule. He and Nagnajita (not Nagnajiti's father) was the disciples of Prahlaada, the great monarch of Bahlikas.


£#h5#£Subhadra£#/h5#£
In the epic, she is the sister of Krishna and Balarama, wife of Arjuna and mother of Abhimanyu and grandmother of Parikshit. She is the daughter of Vasudeva and Rohini. When Arjuna visited Dwarka, he fell in love with Subhadra and ran away with her. Hindus believe Subhadra to be a goddess named Yogmaya.


£#h5#£Sudakshina£#/h5#£
Sudakshina (Sanskrit: सुदक्षिण) was a king of the Kambojas, and fought on the side of the Kauravas in the Kurukshetra War.


£#h5#£Sudeshna£#/h5#£
Sudeshna was the wife of King Virata, at whose court the Pandavas spent a year in concealment during their exile. She was the mother of Uttara, Uttarā, Shveta and Shankha. She had a younger brother named Kichaka and a brother-in-law named Sahtanika.


£#h5#£Sunaka£#/h5#£
Sunaka was the son of Sage Ruru and Pramadvara. this royal sagwas a member of Yudhishthira's assembly. He get a sword from king Harivansa and he giving it to king Ushinara.


£#h5#£Surya£#/h5#£
Surya is the god of sun and day. He is son of Aditi and Kashyapa. He is consort of Saranyu. In the epic, he was the first god called by Kunti using the mantra given by sage Durvasa to obtain a child. She did it out of curiosity and gave birth to Karna, who was born with indestructible armour and earrings. During that time she wasn't married and had to abandon the child. Later in the epic, Surya gave Akshaypatra to Yudhishthira.


£#h5#£Suratha£#/h5#£
Suratha is the son of Jayadratha and Dushala. In Aswamedha Parva of the epic, he became sorrow by his father's death and died.


£#h5#£Susharma£#/h5#£
Susharma was the king of Trigartas.


£#h5#£Sutsoma£#/h5#£
Sutsoma was the son of Bhima and Draupadi. He was the second brother among Upapandavas.


£#h5#£Suvahu£#/h5#£
Suvahu was the king of Kashi. He was defeated by Bhima in his Digvijaya (victory march).


£#h5#£Svaha£#/h5#£
Svaha is the daughter of Prajapati Daksha and the wife of Agni. In the Vana Parva, sage Markandeya narrated her story to the Pandavas. As per the story, Agni visited the ashram of the seven Saptarshi and saw their wives. He was attracted towards them but none responded to him. Svaha was present there and was attracted to Agni, but he wasn't. Later Agni went to forest to calm down his mind. Svaha, taking the form of the wives of sages (except for Arundhati), slept with Agni one by one. Later Agni realised Svaha's love and married her.


£#h5#£T£#/h5#£
£#h5#£Takshaka£#/h5#£
Takshaka was the king of nagas. He lived in a city named Takshasila, which was the new territory of Takshaka after his race was banished by Pandavas led by Arjuna from the Khandava Forest and Kurukshetra, where they built their new kingdom. Because of this, he made a fierce rivalry with Arjuna. During the Kurukshetra war, he sat on an arrow of Karna which was shot at Arjuna. However Krishna saved Arjuna. After his failure, Takshaka vowed to end Arjuna's lineage. After the Pandavas and Draupadi left for heaven, Takshaka killed Parikshit.


£#h5#£Tapati£#/h5#£
Tapati is a river goddess. She is daughter of Surya and Chhaya. She married Samvarana and had a child named, Kuru. Kuru was an ancestor of Pandavas and Kauravas.


£#h5#£Tilottama£#/h5#£
In the Hindu epic Mahabharata, Tilottama is described to have been created by the divine architect Vishwakarma, at Brahma's request, by taking the best quality of everything as the ingredients. She was responsible for bringing about the mutual destruction of the Asuras, Sunda and Upasunda. Even gods like Indra are described to be enamoured by Tilottama. Her story was told by sage Narada to the Pandavas as he wanted to tell them how a woman can lead to rivalry between brothers.


£#h5#£Tara£#/h5#£
Tara is the goddess of felicity. She is spouse of Brihaspati, a guru of gods. Brihaspati often ignored Tara and she started to have an affair with Chandra, the moon god. From their union, Budha was born, whose son, Pururavas, founded the lunar dynasty.


£#h5#£U£#/h5#£
£#h5#£Usha£#/h5#£
Uṣā or Usha was daughter of Banasura, powerful king of Sonitpur and a devotee of Lord Shiva. Later Usha was married to Aniruddha, grandson of Lord Krishna.


£#h5#£Ugrasena£#/h5#£
Ugrasena (Sanskrit: उग्रसेन) is a Yadava king in Mahabharata epic. He was the king of Mathura, a kingdom that was established by the powerful Vrishni tribes from Yaduvanshi clan. Lord Krishna was the grandson of Ugrasena. He established his grandfather as the ruler of Mathura again after defeating his uncle, King Kamsa who was a wicked ruler. Before this, King Ugrasena was overthrown from power by his own son Kansa and was sentenced to prison along with his daughter Devaki and son in law Vasudeva to prison. Devki and Vasudev were parents of Lord Krishna.


£#h5#£Ugrashravas Sauti£#/h5#£
Ugrashravas Sauti was the son of Lomaharsana. He was the Lomaharshana. He was a disciple of Vyasa. He was the narrator of Mahabharata and several Puranas before the gathering of the sages in Naimisha Forest.


£#h5#£Uluka£#/h5#£
Uluka was eldest son of Shakuni and Arshi. He was sent as messenger to Pandavas by Duryodhana. He was killed by Sahadeva on 18th day of war before his father's death.


£#h5#£Ulupi£#/h5#£
Ulupi was daughter of Kauravya, the king of Nāgas, she was among the four wives of Arjuna. She had a son named Iravan.


£#h5#£Urvashi£#/h5#£
Urvashi was a celestial maiden in Indra's court and was considered the most beautiful of all the Apsaras. She was the consort of Pururavas, an ancestor of Pandavas and Kauravas. Later she left him and returned to heaven. When Arjuna came to heaven to meet Indra, she fell in love with him. But Arjuna refused her as he thought her as his mother.


£#h5#£Uttamaujas£#/h5#£
In the epic Mahabharata, Uttamaujas was a powerful Panchala warrior. He is described to be a protector of Arjuna. With his brother Yudhamanyu, they fought a battle against Duryodhana. He was killed during Ashwatthama's night raid. Sometimes, he is described to be Drupada's son and identified with Satyajit.


£#h5#£Uttanka£#/h5#£
In the Mahabharata, Uttanka is described as the disciple of the sage Ayodhaumya. In both legends, he is a learned sage who goes through many hurdles in procuring the earrings demanded by his guru's wife as the fee for the teacher (gurudakshina).


£#h5#£Uttara£#/h5#£
Uttara Kumar was the prince of Matsya Kingdom and the son of King Virata, at whose court the Pandavas spent one year in concealment during their exile. His sister Uttarā was given in marriage to Abhimanyu, son of Arjuna.


£#h5#£Uttarā£#/h5#£
Uttarā or Anglicized as Uttaraa (उत्तरा) was daughter of King Virata, at whose court the Pandavas spent a year in concealment during their exile. She was sister of Prince Uttara. She was wife of Abhimanyu and mother of Parikshit.


£#h5#£V£#/h5#£
£#h5#£Vajra£#/h5#£
He was the son of Aniruddha and his first wife, Rochana. Vajra was crowned as the King of Indraprastha on the request of Krishna by the Pandavas after the Yadava fratricide just before the Pandavas' exile.


£#h5#£Vaishampanaya£#/h5#£
Vaishampayana was the first narrator of the Mahabharata. He recited the chapters of the Mahabharata to King Janamejaya during his snake sacrifice. Vaishampayana was an disciple of Veda Vyasa.


£#h5#£Valandhara£#/h5#£
Valandhara was the princess of Kashi Kingdom, and wife of Pandava Bhima. They both had a son Sarvaga, who became the King of Kashi after the Kurukshetra War. Sarvaga's granddaughter Vapusthama married Janamejaya, the great-grandson of Arjuna, and bore him 2 sons – Shatanika and Sahashranika.


£#h5#£Vapusthama£#/h5#£
Vapusthama was the princess of Kashi, the daughter of King Suvarnavarman and great-granddaughter of Bhima, the 2nd Pandava. Vapusthama was married to Arjuna's great-grandson Janamejaya, and bore him 2 sons – Shatanika and Sankukarna.


£#h5#£Varaha£#/h5#£
Varaha was the boar-incarnation of Lord Vishnu. Once, the earth falled into Pralaya (great-flood), he will incarnated to re-estalibted earth from a great-flood.


£#h5#£Vasudeva£#/h5#£
Vasudeva the father of the Hindu deities Krishna, Balarama and Subhadra. He was king of the Vrishnis and a Yadava prince. He was the son of the Yadava king Shurasena. His sister Kunti was married to Pandu.


£#h5#£Vasundhara£#/h5#£
She was the queen of Manipura and the mother of Chitrangada. She was also the grandmother of Babruvahana. Her husband was King Chitravahana.


£#h5#£Vayu£#/h5#£
Vayu deva is the god of wind. He is son of Aditi and Kashyapa. In the epic, he is the spiritual father of Hanuman and the Pandava, Bhima. He was the second god called by Kunti after her marriage using a mantra as her husband couldn't conceive due to a curse.


£#h5#£Veerabhadra£#/h5#£
£#h5#£Vichitravirya£#/h5#£
Vichitravirya (Sanskrit: विचित्रवीर्य, vicitravīrya) was a king in Indian Religious Texts. In the Mahabharata he was the younger son of queen Satyavati and king Shantanu and grandfather of the Pandavas and Kauravas.


£#h5#£Vidura£#/h5#£
In the epic Mahabharata, Vidura is described as the prime minister of the Kuru Kingdom and also the uncle of the Pandavas and Kauravas. He was born from Niyoga- between sage Vyasa and Parishrami, a handmaiden to the queens- Ambika and Ambalika.


£#h5#£Vidura's wife (Sulabha)£#/h5#£
The wife of Vidura, the half-brother of King Dhritarashtra and the Prime Minister of Hastinapur. She was also a chaste woman of supreme order. She too had a high degree of devotion and abdication. When Lord Krishna visited Hastinapur as an emissary of Pandavas, he had not accepted Duryodhana's request to stay in his palace but instead he chose to stay at Vidura's home and accepted a simple meal there. She is named Sulabha in later versions of the Mahabharata. Sulabha was a great devotee of Lord Krishna. One day he came to her home for a surprise meal. She was enchanted by his glowing face. In absence of her husband, she offered him peels of banana instead of the fruit. And he ate them respecting her bhakti note.


£#h5#£Vijaya£#/h5#£
In the Hindu epic Mahabharat, Vijaya was the daughter of king Dyutiman of Madra and wife of Sahadeva. They got married in a self choice ceremony. Vijaya was Nakula's maternal uncle's daughter. They had a son Suhotra. After the Kurukshetra War, Vijaya and Suhotra lived in Madra, when Sahadeva was appointed as the king of Madra Kingdom.


£#h5#£Vikarna£#/h5#£
Vikarna was third Kaurava, son of Dhritarashtra and Gandhari and a brother to the crown prince Duryodhana. Vikarna is universally referred to as the third-most reputable of Kauravas. Usually, he is also indicated as the third-oldest son, but in other sources, the "third-strongest" reputation remained and it is implied that Vikarna is just one of Gandhari's 99 children (after Duryodhana and Dussasana). Vikarna was the only Kaurava who questioned the humiliation of Draupadi, the wife of his cousin Pandavas after they lost her in a game of dice to Duryodhana.


£#h5#£Vinata£#/h5#£
Vinata, was the mother of Aruna and Garuda (the birds). She was also the daughter of Daksha and wife of sage Kasyapa.


£#h5#£Vinda and Anuvinda£#/h5#£
Vinda and Anuvinda were brothers, and the two kings of Avanti. They were the sons of Jayasena and Rajadhidevi. They also had a sister, Mitravinda, who married Lord Krishna. They were good friends of Duryodhana, and fought for his cause in the Kurukshetra War.


£#h5#£Viraja£#/h5#£
In the Harivamsa, (an appendix of Mahabharata), the spouse of Nahusha is mentioned to be Viraja, the daughter of Pitrs. Later, she was replaced by Ashokasundari, the daughter of goddess Parvati and lord Shiva.


£#h5#£Virata£#/h5#£
In the epic, Virata was the king of Matsya Kingdom with its Virata Kingdom, in whose court the Pandavas spent a year in concealment during their exile. Virata was married to Queen Sudeshna and was the father of Prince Uttara and Princess Uttarā, who married Abhimanyu, the son of Arjuna.


£#h5#£Vishoka£#/h5#£
Vishoka was the charioteer of Pandava Bhima during the Kurukshetra War.


£#h5#£Vrihanta£#/h5#£
Vrihanta was king of the Ulukas. His name appears at several places in the Mahabharata.


£#h5#£Vridhakshtra£#/h5#£
He was former king of Sindhu Kingdom. He was father of Jayadratha and Vijayadratha. He later became a rishi. When Arjuna beheaded his son Jayadratha, his head came on his lap while he was dping Tapasya and when he stood up and Jayadratha's head blasted, killing Vridhakshtra.


£#h5#£Vrishaketu£#/h5#£
Vrishaketu is a figure in the Sanskrit epic Mahabharata. He was the son of King of Anga Karna and his chief consort Maharani Supriya also youngest and surviving son of Karna and Arjuna teaches him many more skills of great warrior. Later, he becomes King of Anga.


£#h5#£Vrishasena£#/h5#£
Vrishasena was the son of Karna and Vrishali. With his father, he entered battle field on the 11th day of Kurukshetra war and fought for Kauravas. On the 17th of the war he was killed by Arjuna.


£#h5#£Vyasa£#/h5#£
Vyasa was the author of epic Mahabharata. According to the Mahabharata, the sage Vyasa was the son of Satyavati and Parashara. He was also the surrogate father of Dhritarashtra, Pandu and Vidura. They were born through Niyoga. Later, he helped in birth of 101 children of Dhritarashtra and Gandhari. He also helped the Pandavas many times.


£#h5#£Y£#/h5#£
£#h5#£Yama or Dharma£#/h5#£
In the epic, the death god Yama— often identified with the god Dharma—is the spiritual father of Yudhishthira. He was the first god called by Kunti after her marriage using a mantra as her husband couldn't conceive. Yama also ppeared in the tale of Savitri and Satyavan. In the story, he tried to take Satyavan's soul, but Savitri tricked him. Dharma, later in the epic, appears testing Yudhishthira by taking form of a Yaksha. When the Pandavas and Draupadi went for heaven, he accompanied them by taking form of a dog and was only surviving left along with Yudhishthira. At the end, he showed his true form to Yudhishthira.


£#h5#£Yamuna£#/h5#£
Devi Yamuna or Yami is the river goddess of life. She is daughter of Surya and Saranyu as well as the twin of Yamraj. In the epic, she appears as Kalindi. She is one of lord Krishna's eight wives.


£#h5#£Yajnavalkya£#/h5#£
Yajnavalkya is a sage and the priest of king Janaka. He was mentioned in various chapters of Shanti Parva in the Mahabharata.


£#h5#£Yashoda£#/h5#£
She is the wife of Nanda (head of Gokul) and foster mother of Lord Krishna and Balarama. She is popularly and best known as Mother of Krishna. Krishna and Balarama spent their childhood with Yashoda and Nanda and the couple took care of them. There is a popular story about replacement of Krishna with Yashoda's daughter by Vasudeva after an agreement with Nanda.


£#h5#£Yaudheya£#/h5#£
Yaudheya was the son of Yudhishthira and Devika, and the grandson of Govasena, who was the king of Sivi Kingdom. Yaudheya succeeded his grandfather after his death in the Kurukshetra War.

According to the Matsya Purana, Yaudheya is also the name of the son of Prativindhya, however he does not succeed Yudhishthira to the throne of Hastinapur as he inherits his maternal kingdom.


£#h5#£Yayati£#/h5#£
Yayati was an ancestor of Shantanu and the son of king Nahusha and Ashokasundari, the daughter of goddess Parvati. He had two wives, Devayani and Sharmishtha.


£#h5#£Yogmaya£#/h5#£
Yogmaya or Vindhyavasini is an incarnation of goddess Adi Parashakti. She was the daughter of Yashoda and Nanda, the foster parents of Krishna. Krishna and Yogmaya were born on the same day. They were exchanged by their parents to save Krishna from Kamsa. Many believe that Subhadra was her reincarnation.


£#h5#£Yudhisthira£#/h5#£
Yudhishtira was the first among the five Pandavas and was blessed by death god Yama to Pandu and Kunti. He became the king of Indraprastha and later of Hastinapura (Kuru). He was the leader of the successful Pandava side in the Kurukshetra War. At the end of the epic, he ascended to heaven. He was also blessed with the spiritual vision of second sight by a celestial Rishi as a boon.


£#h5#£Yuyutsu£#/h5#£
Yuyutsu was the illegitimate son of Dhritarashtra with Dasi aka Sughada/Sauvali, his wife Gandhari's maid. He was the paternal half – sibling to Gandhari's children: Duryodhana and the rest of the 100 Kaurava brothers and their sister Dushala. Eventually, he was the only son of Dhritarashtra who survived the Kurukshetra war.


£#h5#£See also£#/h5#£ £#ul#££#li#£Ramayana£#/li#£ £#li#£Mahabharata£#/li#£ £#li#£Bhagavad Gita£#/li#£ £#li#£Kurukshetra War£#/li#£ £#li#£History of India£#/li#£ £#li#£List of historic Indian texts£#/li#£ £#li#£Historicity of the Mahabharata£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£
£#h5#£Sources£#/h5#£
£#h5#£External links£#/h5#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Trigonometric Functions £#/li#££#/ul#£




£#h3#£Arth£#/h3#£

Arth is a village, a town, and a municipality in Schwyz District in the canton of Schwyz in Switzerland.

The municipality consists of the villages Arth, Oberarth, and Goldau. The four settlements Rigi Kulm, Rigi First, Rigi Klösterli, and Rigi Staffel on the mountain Rigi to the west of Arth are also part of the municipality.

The official language of Arth is (the Swiss variety of Standard) German, but the main spoken language is the local variant of the Alemannic Swiss German dialect.


£#h5#£History£#/h5#£
Arth is first mentioned in 1036 as Arta. In 1353 it was mentioned as ze Arth.


£#h5#£Geography£#/h5#£
Arth has an area, as of 2006, of 42.1 km2 (16.3 sq mi). Of this area, 40.8% is used for agricultural purposes, while 46.3% is forested. Of the rest of the land, 8.5% is settled (buildings or roads) and the remainder (4.3%) is non-productive (rivers, glaciers or mountains).

The municipality is situated on the southern shore of Lake Zug, and along the Gotthard route between Rigi and Rossberg. It consists of the villages of Arth Oberarth and Goldau as well as the hamlets of Klösterli and Kulm an der Rigi.


£#h5#£Demographics£#/h5#£
Arth has a population (as of 31 December 2020) of 12,184. As of 2007, 23.6% of the population was made up of foreign nationals. Over the last 10 years the population has grown at a rate of 6.3%. Most of the population (as of 2000) speaks German (86.9%), with Albanian being second most common ( 3.9%) and Serbo-Croatian being third ( 3.2%).

As of 2000 the gender distribution of the population was 50.3% male and 49.7% female. The age distribution, as of 2008, in Arth is; 2,555 people or 26.6% of the population is between 0 and 19. 2,870 people or 29.9% are 20 to 39, and 2,832 people or 29.5% are 40 to 64. The senior population distribution is 734 people or 7.7% are 65 to 74. There are 467 people or 4.9% who are 70 to 79 and 135 people or 1.41% of the population who are over 80. There is one person in Arth who is over 100 years old.

As of 2000 there are 3,806 households, of which 1,156 households (or about 30.4%) contain only a single individual. 275 or about 7.2% are large households, with at least five members.

In the 2007 election the most popular party was the SVP which received 38.8% of the vote. The next three most popular parties were the CVP (20.8%), the FDP (20.4%) and the SPS (16.3%).

In Arth about 63.6% of the population (between age 25-64) have completed either non-mandatory upper secondary education or additional higher education (either university or a Fachhochschule).

Arth has an unemployment rate of 1.55%. As of 2005, there were 329 people employed in the primary economic sector and about 129 businesses involved in this sector. 810 people are employed in the secondary sector and there are 88 businesses in this sector. 1868 people are employed in the tertiary sector, with 302 businesses in this sector.

From the 2000 census, 6,927 or 72.2% are Roman Catholic, while 939 or 9.8% belonged to the Swiss Reformed Church. Of the rest of the population, there are less than 5 individuals who belong to the Christian Catholic faith, there are 273 individuals (or about 2.85% of the population) who belong to the Orthodox Church, and there are 7 individuals (or about 0.07% of the population) who belong to another Christian church. There are less than 5 individuals who are Jewish, and 717 (or about 7.47% of the population) who are Islamic. There are 84 individuals (or about 0.88% of the population) who belong to another church (not listed on the census), 337 (or about 3.51% of the population) belong to no church, are agnostic or atheist, and 302 individuals (or about 3.15% of the population) did not answer the question.

The historical population is given in the following table:


£#h5#£Transport£#/h5#£
The railway station at Goldau, named Arth-Goldau, is an important junction of the Swiss Federal Railways. The Voralpen Express train connects here to Luzern and St. Gallen. Meanwhile, trains to Bellinzona, Lugano and Italy, as well as trains to Zug and Zürich join here with trains heading toward Basel.


£#h5#£Notable people£#/h5#£ £#ul#££#li#£Karl Jakob Weber (1712 in Arth – 1764) a Swiss architect and engineer who was in charge of the first organized excavations at Herculaneum, Pompeii and Stabiae£#/li#£ £#li#£Robbie Hunter (born 1977) a retired South African professional road racing cyclist who competed professionally between 1999 and 2013; now lives in Arth£#/li#££#/ul#£
£#h5#£Gallery£#/h5#£
£#/ul#£
£#h5#£References£#/h5#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Arth in German, French and Italian in the online Historical Dictionary of Switzerland.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Trigonometric Functions £#/li#££#/ul#£




£#h3#£Artin's Conjecture£#/h3#£

In number theory, Artin's conjecture on primitive roots states that a given integer a that is neither a perfect square nor −1 is a primitive root modulo infinitely many primes p. The conjecture also ascribes an asymptotic density to these primes. This conjectural density equals Artin's constant or a rational multiple thereof.

The conjecture was made by Emil Artin to Helmut Hasse on September 27, 1927, according to the latter's diary. The conjecture is still unresolved as of 2022. In fact, there is no single value of a for which Artin's conjecture is proved.


£#h5#£Formulation£#/h5#£
Let a be an integer that is not a perfect square and not −1. Write a = a0b2 with a0 square-free. Denote by S(a) the set of prime numbers p such that a is a primitive root modulo p. Then the conjecture states

£#li#£S(a) has a positive asymptotic density inside the set of primes. In particular, S(a) is infinite.£#/li#£ £#li#£Under the conditions that a is not a perfect power and that a0 is not congruent to 1 modulo 4 (sequence A085397 in the OEIS), this density is independent of a and equals Artin's constant, which can be expressed as an infinite product
${\displaystyle C_{\mathrm {Artin} }=\prod _{p\ \mathrm {prime} }\left(1-{\frac {1}{p(p-1)}}\right)=0.3739558136\ldots }$ (sequence A005596 in the OEIS).
£#/li#£
Similar conjectural product formulas exist for the density when a does not satisfy the above conditions. In these cases, the conjectural density is always a rational multiple of CArtin.


£#h5#£Example£#/h5#£
For example, take a = 2. The conjecture claims that the set of primes p for which 2 is a primitive root has the above density CArtin. The set of such primes is (sequence A001122 in the OEIS)

S(2) = {3, 5, 11, 13, 19, 29, 37, 53, 59, 61, 67, 83, 101, 107, 131, 139, 149, 163, 173, 179, 181, 197, 211, 227, 269, 293, 317, 347, 349, 373, 379, 389, 419, 421, 443, 461, 467, 491, ...}.
It has 38 elements smaller than 500 and there are 95 primes smaller than 500. The ratio (which conjecturally tends to CArtin) is 38/95 = 2/5 = 0.4.


£#h5#£Partial results£#/h5#£
In 1967, Christopher Hooley published a conditional proof for the conjecture, assuming certain cases of the generalized Riemann hypothesis.

Without the generalized Riemann hypothesis, there is no single value of a for which Artin's conjecture is proved. D. R. Heath-Brown proved (Corollary 1) that at least one of 2, 3, or 5 is a primitive root modulo infinitely many primes p. He also proved (Corollary 2) that there are at most two primes for which Artin's conjecture fails.


£#h5#£See also£#/h5#£ £#ul#££#li#£Stephens' constant, a number that plays the same role in a generalization of Artin's conjecture as Artin's constant plays here£#/li#£ £#li#£Brown–Zassenhaus conjecture£#/li#£ £#li#£Full reptend prime£#/li#£ £#li#£Cyclic number (group theory)£#/li#££#/ul#£
£#h5#£References£#/h5#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Artin, E. "Über eine neue Art von L-Reihen." Abh. Math. Sem. Univ. Hamburg 3, 89-108, 1923/1924.£#/li#££#li#£Matthews, K. R. "A Generalization of Artin's Conjecture for Primitive Roots." Acta Arith. 29, 113-146, 1976.£#/li#££#li#£Moree, P. "A Note on Artin's Conjecture." Simon Stevin 67, 255-257, 1993.£#/li#££#li#£Ram Murty, M. "Artin's Conjecture for Primitive Roots." Math. Intell. 10, 59-67, 1988.£#/li#££#li#£Shanks, D. Solved and Unsolved Problems in Number Theory, 4th ed. New York: Chelsea, pp. 31, 80-83, and 147, 1993.£#/li#££#li#£ Artin, E. "Über eine neue Art von -Reihen." Abh. Math. Sem. Univ. Hamburg 3, 89-108, 1923/1924. £#/li#££#li#£ Matthews, K. R. "A Generalization of Artin's Conjecture for Primitive Roots." Acta Arith. 29, 113-146, 1976. £#/li#££#li#£ Moree, P. "A Note on Artin's Conjecture." Simon Stevin 67, 255-257, 1993. £#/li#££#li#£ Ram Murty, M. "Artin's Conjecture for Primitive Roots." Math. Intell. 10, 59-67, 1988. £#/li#££#li#£ Shanks, D. Solved and Unsolved Problems in Number Theory, 4th ed. New York: Chelsea, pp. 31, 80-83, and 147, 1993. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Number Theory > Congruences £#/li#££#li#£ Calculus and Analysis > Special Functions > Riemann Zeta Function £#/li#££#li#£ Foundations of Mathematics > Mathematical Problems > Unsolved Problems £#/li#££#/ul#£




£#h3#£Ascending Factorial£#/h3#£

In mathematics, the falling factorial (sometimes called the descending factorial, falling sequential product, or lower factorial) is defined as the polynomial

${\displaystyle {\begin{aligned}(x)_{n}=x^{\underline {n}}&=\overbrace {x(x-1)(x-2)\cdots (x-n+1)} ^{n{\text{ factors}}}\\&=\prod _{k=1}^{n}(x-k+1)=\prod _{k=0}^{n-1}(x-k)\,.\end{aligned}}}$
The rising factorial (sometimes called the Pochhammer function, Pochhammer polynomial, ascending factorial, rising sequential product, or upper factorial) is defined as

${\displaystyle {\begin{aligned}x^{(n)}=x^{\overline {n}}&=\overbrace {x(x+1)(x+2)\cdots (x+n-1)} ^{n{\text{ factors}}}\\&=\prod _{k=1}^{n}(x+k-1)=\prod _{k=0}^{n-1}(x+k)\,.\end{aligned}}}$
The value of each is taken to be 1 (an empty product) when n = 0. These symbols are collectively called factorial powers.

The Pochhammer symbol, introduced by Leo August Pochhammer, is the notation (x)n, where n is a non-negative integer. It may represent either the rising or the falling factorial, with different articles and authors using different conventions. Pochhammer himself actually used (x)n with yet another meaning, namely to denote the binomial coefficient ${\displaystyle {\tbinom {x}{n}}}$ .

In this article, the symbol (x)n is used to represent the falling factorial, and the symbol x(n) is used for the rising factorial. These conventions are used in combinatorics, although Knuth's underline and overline notations ${\displaystyle x^{\underline {n}}}$ and ${\displaystyle x^{\overline {n}}}$ are increasingly popular. In the theory of special functions (in particular the hypergeometric function) and in the standard reference work Abramowitz and Stegun, the Pochhammer symbol (x)n is used to represent the rising factorial.

When x is a positive integer, (x)n gives the number of n-permutations of an x-element set, or equivalently the number of injective functions from a set of size n to a set of size x.


£#h5#£Examples and combinatorial interpretation£#/h5#£
The first few rising factorials are as follows:

${\displaystyle {\begin{array}{rll}x^{(0)}&&=1\\x^{(1)}&&=x\\x^{(2)}&=x(x+1)&=x^{2}+x\\x^{(3)}&=x(x+1)(x+2)&=x^{3}+3x^{2}+2x\\x^{(4)}&=x(x+1)(x+2)(x+3)&=x^{4}+6x^{3}+11x^{2}+6x\end{array}}}$
The first few falling factorials are as follows:

${\displaystyle {\begin{array}{rll}(x)_{0}&&=1\\(x)_{1}&&=x\\(x)_{2}&=x(x-1)&=x^{2}-x\\(x)_{3}&=x(x-1)(x-2)&=x^{3}-3x^{2}+2x\\(x)_{4}&=x(x-1)(x-2)(x-3)&=x^{4}-6x^{3}+11x^{2}-6x\end{array}}}$
The coefficients that appear in the expansions are Stirling numbers of the first kind.

When the variable x is a positive integer, the number (x)n is equal to the number of n-permutations from an x-set, that is, the number of ways of choosing an ordered list of length n consisting of distinct elements drawn from a collection of size x. For example, (8)3 = 8 × 7 × 6 = 336 is the number of different podiums—assignments of gold, silver, and bronze medals—possible in an eight-person race. Also, (x)n is "the number of ways to arrange n flags on x flagpoles", where all flags must be used and each flagpole can have at most one flag. In this context, other notations like xPn, xPn or P(x, n) are also sometimes used.


£#h5#£Properties£#/h5#£
The rising and falling factorials are simply related to one another:

${\displaystyle {\begin{array}{rll}m^{(n)}&={(m+n-1)}_{n}&=(-1)^{n}(-m)_{n}\\{(m)}_{n}&={(m-n+1)}^{(n)}&=(-1)^{n}(-m)^{(n)}\end{array}}}$
The rising and falling factorials are directly related to the ordinary factorial:

${\displaystyle {\begin{aligned}n!&=1^{(n)}=(n)_{n}\\[6pt](m)_{n}&={\frac {m!}{(m-n)!}}\\[6pt]m^{(n)}&={\frac {(m+n-1)!}{(m-1)!}}\end{aligned}}}$
The rising and falling factorials can be used to express a binomial coefficient:

${\displaystyle {\begin{aligned}{\frac {x^{(n)}}{n!}}&={\binom {x+n-1}{n}}\\[6pt]{\frac {(x)_{n}}{n!}}&={\binom {x}{n}}\end{aligned}}}$
Thus many identities on binomial coefficients carry over to the falling and rising factorials.

The rising and falling factorials are well defined in any unital ring, and therefore x can be taken to be, for example, a complex number, including negative integers, or a polynomial with complex coefficients, or any complex-valued function.

The rising factorial can be extended to real values of n using the gamma function provided x and x + n are real numbers that are not negative integers:

${\displaystyle x^{(n)}={\frac {\Gamma (x+n)}{\Gamma (x)}}\,,}$
and so can the falling factorial:

${\displaystyle (x)_{n}={\frac {\Gamma (x+1)}{\Gamma (x-n+1)}}\,.}$
Falling factorials appear in multiple differentiation of simple power functions:

${\displaystyle {\frac {d^{n}}{dx^{n}}}\,x^{a}=(a)_{n}x^{a-n}\,.}$
The rising factorial is also integral to the definition of the hypergeometric function: The hypergeometric function is defined for |z| < 1 by the power series

${\displaystyle {}_{2}F_{1}(a,b;c;z)=\sum _{n=0}^{\infty }{\frac {a^{(n)}b^{(n)}}{c^{(n)}}}{\frac {z^{n}}{n!}}}$
provided that c ≠ 0, −1, −2,.... Note, however, that the hypergeometric function literature typically uses the notation (a)n for rising factorials.


£#h5#£Relation to umbral calculus£#/h5#£
The falling factorial occurs in a formula which represents polynomials using the forward difference operator Δ and which is formally similar to Taylor's theorem:

${\displaystyle f(x)=\sum _{n=0}^{\infty }{\frac {\,\Delta ^{n}\!f(0)\,}{n!}}\,(x)_{n}.}$
In this formula and in many other places, the falling factorial (x)n in the calculus of finite differences plays the role of xn in differential calculus. Note for instance the similarity of Δ (x)n = n(x)n−1 to d/dx xn = nxn−1.

A similar result holds for the rising factorial.

The study of analogies of this type is known as umbral calculus. A general theory covering such relations, including the falling and rising factorial functions, is given by the theory of polynomial sequences of binomial type and Sheffer sequences. Rising and falling factorials are Sheffer sequences of binomial type, as shown by the relations:

${\displaystyle {\begin{aligned}(a+b)^{(n)}&=\sum _{j=0}^{n}{\binom {n}{j}}(a)^{(n-j)}(b)^{(j)}\\[6pt](a+b)_{n}&=\sum _{j=0}^{n}{\binom {n}{j}}(a)_{n-j}(b)_{j}\end{aligned}}}$
where the coefficients are the same as the ones in the expansion of a power of a binomial (Chu–Vandermonde identity).

Similarly, the generating function of Pochhammer polynomials then amounts to the umbral exponential,

${\displaystyle \sum _{n=0}^{\infty }(x)_{n}{\frac {t^{n}}{n!}}=\left(1+t\right)^{x}\,,}$
since

${\displaystyle \operatorname {\Delta } _{x}\left(1+t\right)^{x}=t\,\left(1+t\right)^{x}\,.}$

£#h5#£Connection coefficients and identities£#/h5#£
The falling and rising factorials are related to one another through the Lah numbers:

${\displaystyle {\begin{aligned}(x)_{n}&=\sum _{k=1}^{n}{\binom {n-1}{k-1}}{\frac {n!}{k!}}x^{(k)}\\&=(-1)^{n}(-x)^{(n)}=(x-n+1)^{(n)}\\[6pt]x^{(n)}&=\sum _{k=0}^{n}{\binom {n}{k}}(n-1)_{n-k}(x)_{k}\\&=(-1)^{n}(-x)_{n}=(x+n-1)_{n}\,.\end{aligned}}}$
The following formulas relate integral powers of a variable x through sums using the Stirling numbers of the second kind, notated by curly brackets {n
k}:

${\displaystyle {\begin{aligned}x^{n}&=\sum _{k=0}^{n}{\begin{Bmatrix}n\\k\end{Bmatrix}}(x)_{k}\\&=\sum _{k=0}^{n}{\begin{Bmatrix}n\\k\end{Bmatrix}}(-1)^{n-k}x^{(k)}\,.\end{aligned}}}$
Since the falling factorials are a basis for the polynomial ring, one can express the product of two of them as a linear combination of falling factorials:

${\displaystyle (x)_{m}(x)_{n}=\sum _{k=0}^{m}{\binom {m}{k}}{\binom {n}{k}}k!\cdot (x)_{m+n-k}\,.}$
The coefficients ${\displaystyle {\tbinom {m}{k}}{\tbinom {n}{k}}k!}$ are called connection coefficients, and have a combinatorial interpretation as the number of ways to identify (or "glue together") k elements each from a set of size m and a set of size n.

There is also a connection formula for the ratio of two rising factorials given by

${\displaystyle {\frac {x^{(n)}}{x^{(i)}}}=(x+i)^{(n-i)}\,,\quad {\text{for }}n\geq i\,.}$
Additionally, we can expand generalized exponent laws and negative rising and falling powers through the following identities:

${\displaystyle {\begin{aligned}(x)_{m+n}&=(x)_{m}(x-m)_{n}=(x)_{n}(x-n)_{m}\\[6pt]x^{(m+n)}&=x^{(m)}(x+m)^{(n)}=x^{(n)}(x+n)^{(m)}\\[6pt]x^{(-n)}&={\frac {\Gamma (x-n)}{\Gamma (x)}}={\frac {(x-n-1)!}{(x-1)!}}={\frac {1}{(x-n)^{(n)}}}={\frac {1}{(x-1)_{n}}}={\frac {1}{(x-1)(x-2)\cdots (x-n)}}={\frac {1}{n!{\binom {x-1}{n}}}}=(-n)!{\binom {x-n-1}{-n}}\\[6pt](x)_{-n}&={\frac {\Gamma (x+1)}{\Gamma (x+n-1)}}={\frac {x!}{(x+n)!}}={\frac {1}{(x+n)_{n}}}={\frac {1}{(x+1)^{(n)}}}={\frac {1}{(x+1)(x+2)\cdots (x+n)}}={\frac {1}{(-n)!{\binom {x+n}{-n}}}}=(-n)!{\binom {x}{-n}}\,.\end{aligned}}}$
Finally, duplication and multiplication formulas for the falling and rising factorials provide the next relations:

${\displaystyle {\begin{aligned}x_{k+mn}&=x^{(k)}m^{mn}\prod _{j=0}^{m-1}\left({\frac {x-k-j}{m}}\right)_{n}\,,&{\text{for }}m&\in \mathbb {N} \\[6pt]x^{(k+mn)}&=x^{(k)}m^{mn}\prod _{j=0}^{m-1}\left({\frac {x+k+j}{m}}\right)^{(n)},&{\text{for }}m&\in \mathbb {N} \\[6pt](ax+b)^{(n)}&=x^{n}\prod _{j=0}^{n-1}\left(a+{\frac {b+j}{x}}\right)\,,&{\text{for }}x&\in \mathbb {Z} ^{+}\\[6pt](2x)^{(2n)}&=2^{2n}x^{(n)}\left(x+{\frac {1}{2}}\right)^{(n)}\,.\end{aligned}}}$

£#h5#£Alternative notations£#/h5#£
An alternative notation for the rising factorial

${\displaystyle x^{\overline {m}}\equiv (x)_{+m}\equiv (x)_{m}=\overbrace {x(x+1)\ldots (x+m-1)} ^{m{\text{ factors}}}\quad {\text{for integer }}m\geq 0\,,}$
and for the falling factorial

${\displaystyle x^{\underline {m}}\equiv (x)_{-m}=\overbrace {x(x-1)\ldots (x-m+1)} ^{m{\text{ factors}}}\quad {\text{for integer }}m\geq 0\,;}$
goes back to A. Capelli (1893) and L. Toscano (1939), respectively. Graham, Knuth, and Patashnik propose to pronounce these expressions as "x to the m rising" and "x to the m falling", respectively.

Other notations for the falling factorial include P(x,n), xPn, Px,n, or xPn. (See permutation and combination.)

An alternative notation for the rising factorial x(n) is the less common (x)+
n . When (x)+
n is used to denote the rising factorial, the notation (x)−
n is typically used for the ordinary falling factorial, to avoid confusion.


£#h5#£Generalizations£#/h5#£
The Pochhammer symbol has a generalized version called the generalized Pochhammer symbol, used in multivariate analysis. There is also a q-analogue, the q-Pochhammer symbol.

A generalization of the falling factorial in which a function is evaluated on a descending arithmetic sequence of integers and the values are multiplied is:

${\displaystyle {\bigl [}f(x){\bigr ]}^{k/-h}=f(x)\cdot f(x-h)\cdot f(x-2h)\cdots f{\bigl (}x-(k-1)h{\bigr )},}$
where −h is the decrement and k is the number of factors. The corresponding generalization of the rising factorial is

${\displaystyle {\bigl [}f(x){\bigr ]}^{k/h}=f(x)\cdot f(x+h)\cdot f(x+2h)\cdots f{\bigl (}x+(k-1)h{\bigr )}.}$
This notation unifies the rising and falling factorials, which are [x]k/1 and [x]k/−1 respectively.

For any fixed arithmetic function ${\displaystyle f:\mathbb {N} \rightarrow \mathbb {C} }$ and symbolic parameters x, t, related generalized factorial products of the form

${\displaystyle (x)_{n,f,t}:=\prod _{k=0}^{n-1}\left(x+{\frac {f(k)}{t^{k}}}\right)}$
may be studied from the point of view of the classes of generalized Stirling numbers of the first kind defined by the following coefficients of the powers of x in the expansions of (x)n,f,t and then by the next corresponding triangular recurrence relation:

${\displaystyle {\begin{aligned}\left[{\begin{matrix}n\\k\end{matrix}}\right]_{f,t}&=\left[x^{k-1}\right](x)_{n,f,t}\\&=f(n-1)t^{1-n}\left[{\begin{matrix}n-1\\k\end{matrix}}\right]_{f,t}+\left[{\begin{matrix}n-1\\k-1\end{matrix}}\right]_{f,t}+\delta _{n,0}\delta _{k,0}.\end{aligned}}}$
These coefficients satisfy a number of analogous properties to those for the Stirling numbers of the first kind as well as recurrence relations and functional equations related to the f-harmonic numbers,

${\displaystyle F_{n}^{(r)}(t):=\sum _{k\leq n}{\frac {t^{k}}{f(k)^{r}}}\,.}$
A symmetric generalization can be defined as

${\displaystyle x^{\underline {\overline {m}}}\equiv {\frac {x^{\overline {m}}x^{\underline {m}}}{x}}=x^{{\overline {m}}+{\underline {m}}-1}}$

£#h5#£See also£#/h5#£ £#ul#££#li#£Pochhammer k-symbol£#/li#£ £#li#£Vandermonde identity£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Pochhammer Symbol". MathWorld.£#/li#£ £#li#£Elementary Proofs£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Factorials £#/li#££#/ul#£




£#h3#£Asin£#/h3#£

Asin Thottumkal (born 26 October 1985), known mononymously as Asin, is a former Indian actress who appeared in Tamil, Hindi, Telugu and Malayalam films. She is a trained Bharatanatyam dancer. She has received three Filmfare Awards from ten nominations. She began her acting career in the South Indian film industry, but later shifted her focus to Bollywood. She speaks eight languages, and dubs her own films. She is the only Malayali actress, other than Padmini, to have dubbed in her own voice for all her films, irrespective of language. Asin has been referred to as the "Queen of Kollywood" by online portals in 2007.

Making her acting debut with Sathyan Anthikkad's Malayalam film Narendran Makan Jayakanthan Vaka (2001), Asin had her first commercial success with the Telugu film Amma Nanna O Tamila Ammayi in 2003, and won a Filmfare Best Telugu Actress Award for the film. M. Kumaran Son of Mahalakshmi (2004) was her debut in Tamil and a huge success. She received her Filmfare Best Tamil Actress Award for her most noted critically acclaimed performance in her third Tamil film, Ghajini (2005). She then played the lead female roles in many successful films, the most notable being the action films Sivakasi (2005), Varalaru (2006), Pokkiri (2007), Vel (2008) and Dasavathaaram (2008), hence establishing herself as the leading actress of Tamil cinema and was also honoured by the Tamil Nadu government with the Kalaimamani award for her excellence in the field of art and literature. In 2013, Asin was conferred with the Pride of South Indian Cinema award at SIIMA for her contribution to Tamil cinema.

In late 2008, Asin made her debut in the Bollywood film Ghajini (2008), opposite Aamir Khan, which was the first Bollywood film to have collected more than ₹1 billion in the domestic box office, subsequently collecting ₹1.9 billion (US$25 million) worldwide. Asin won the Filmfare Best Female Debut Award and many accolades for Ghajini. 2011 marked the most successful phase of Asin's Bollywood career, as she starred in Anees Bazmee's romantic comedy Ready, in which she co-starred alongside Salman Khan. The film was a major hit at the box office, collecting ₹1.84 billion (US$24 million) worldwide. In 2012, Asin first starred in Sajid Khan's multistarrer Housefull 2, which collected more than ₹1 billion. She then featured in Bol Bachchan and Khiladi 786, which were also commercially successful with both grossing over ₹1 billion.


£#h5#£Early life£#/h5#£
Asin was born on 26 October 1985 at Kochi in the state Kerala in a Malayali Syro-Malabar Catholic family. Her father Joseph Thottumkal is an Ex-CBI officer and later managed several businesses. Her mother Seline Thottumkal, who moved from Kochi to Chennai and then to Mumbai to live with her daughter, is a surgeon. According to custom, Asin was to be named Mary, after her paternal grandmother. Asin's father however, named her Asin as the name had a beautiful meaning. Asin has quoted that her name means "pure and without blemish". She states that the 'A' in her name is from Sanskrit meaning "without", and "sin" from English.

She attended Naval Public School from LKG through X standard. She then attended St. Teresa's School in Kochi for her Kerala Higher Secondary Examination Board (Plus Two) Education. After that, she attended St. Teresa's College in Kochi, a college affiliated with MG University, where she graduated with a Bachelor of Arts degree in English Literature.


£#h5#£Career£#/h5#£
£#h5#£Early work (2001–2004)£#/h5#£
Asin's first assignment was an advertisement for BPL Mobile. She debuted in the Malayalam film Narendran Makan Jayakanthan Vaka in 2001, at the age of 15. After taking a year out to pursue her education, Asin returned with her breakthrough film as an actress, Amma Nanna O Tamila Ammayi opposite Ravi Teja, portraying a Tamil girl in her first Telugu language film, which subsequently fetched her the Telugu Filmfare Award for Best Actress. In the same year she won the Santosham Best Actress Award for her performance alongside Nagarjuna in her second Telugu film Shivamani. Her following two Telugu films Lakshmi Narasimha and Gharshana, both portrayed Asin as the love interest for police officers and both were successful ventures.

Asin's first Tamil language film was M. Kumaran Son of Mahalakshmi, in which she co-starred with Jayam Ravi. Asin reprised her role from her film Amma Nanna O Tamila Ammayi for the remake, which saw her portraying a Malayali girl instead of the Tamil girl in the original. The movie subsequently went on to super-hit in Tamil cinema during 2004, introducing Asin to Tamil film industry. After a brief return to Telugu films, to do Chakram, she appeared in Ullam Ketkumae. The film, initially launched in 2002, was supposed to be her debut as a lead heroine, is a college love story, directed by Jeeva, which was long-delayed but eventually became a successful venture at the box office, creating wider opportunities for her and the rest of the cast.


£#h5#£Tamil film breakthrough and stardom (2005–2007)£#/h5#£
After the release of Ullam Ketkumae, Asin was cited as a leading heroine of the Tamil film industry after starring in blockbuster films such as Ghajini, Sivakasi, Varalaru and Pokkiri respectively. The film which provided the breakthrough for Asin was Ghajini. The film, co-starring Surya, directed by AR Murugadoss, earned her the Tamil Filmfare Award for Best Actress. Her role was that of a vivacious young model named Kalpana. Sify.com praised her portrayal as "magical", describing her character as a "lovable chatterbox", played with "sheer ability in the romantic interludes, the poignant and heartfelt scenes when she rescues minor girls from villains and her gory end are touching". On the following Diwali in 2005, Asin had two releases, Sivakasi and Majaa. Despite the latter becoming an average grosser, the former went on to be a successful venture at the box office.

The following year, her long-delayed venture Varalaru opposite Ajith Kumar also succeeded at the box office, and The Hindu said she "looks lustrous and sails through her role smoothly". Asin appeared in Pawan Kalyan's Annavaram, another successful venture, which was a remake of Thirupaachi. In January 2007, Asin appeared opposite Ajith Kumar and Vijay in two different films, Aalwar and Pokkiri, with the latter becoming a success, whilst Aalwar became a failure. Though Asin's role in Aalwar got mixed to positive reviews, her performance in Pokkiri was lauded by critics. Her final project of the year was opposite Surya, in Hari's Vel, which had a Diwali 2007 release, subsequently becoming her third successful film which released during the Diwali season in three years. Asin, who portrayed a TV anchor, was appreciated for her role in the film.


£#h5#£Hindi film debut (2008–2010)£#/h5#£
Asin appeared in her first dual role in K. S. Ravikumar's Dasavathaaram opposite Kamal Haasan, who portrayed ten roles. The film had been under production since September 2006. Despite being overshadowed by Haasan's ten roles, Asin's portrayals in the film were praised as her "best-to-date", with one of her roles being that of a Vaishnavite in 12th century Tamil Nadu; whilst the other character was a Brahmin girl from Chidambaram, for which she won her first ITFA Best Actress Award. Dasavathaaram subsequently went on to become one of the blockbusters.

Since establishing herself as the leading actress in South India, Asin opted to move into Bollywood. Her first film Ghajini, opposite Aamir Khan, was the remake of her breakthrough film of the same name. Upon release, the film garnered positive reactions from critics and masses alike, with Asin being singled out for her "fabulous" portrayal. Noted critic Taran Adarsh described her Hindi debut as "fabulous" and that "to share the screen space with an actor of the stature of Aamir Khan and yet remain in your memory even after the show has ended is no cakewalk. She looks fresh and photogenic and acts her part brilliantly, giving Asin's performance a positive outlook."

Ghajini became the highest grossing Bollywood film of 2008 and the sixth highest-grossing Bollywood film of all time according to net collections. Asin later appeared in Vipul Shah's London Dreams, alongside Salman Khan and Ajay Devgn, in which she played a south Indian girl, Priya, who is emotional but strong willed and an ambitious dancer at the same time. The film received mixed reviews from critics.

Her first film of 2011 was Kaavalan, which released on 15 January 2011. The film was a critical and commercial success, with Behindwoods praising her performance in a review and her being awarded a SIIMA award for Best Actress.


£#h5#£Final work (2011–2015)£#/h5#£
Her next movie with Salman Khan, Ready, was released on 3 June 2011 to mixed reviews. Critics praised the chemistry between Salman and Asin. Upon release, Ready became the second highest weekend grossing Bollywood film, after Dabangg and went on to become the fifth film in Bollywood history to cross the coveted ₹1 billion (US$13 million) mark in the domestic market.

In 2012, she first appeared alongside Akshay Kumar in Sajid Khan's multistarrer comedy Housefull 2. Though it received a mixed critical reception, it was a major commercial success. Asin generally received favourable reviews for her performance, with Adarsh stating that "[a]mongst the female leads, Asin has the meatiest role and gives a decent account of herself."

On 6 July 2012, Rohit Shetty's Bol Bachchan, alongside Ajay Devgn and Abhishek Bachchan, released with mixed reviews with the critics praising the performances of Asin, Ajay Devgn, and Abhishek Bachchan.

Khiladi 786, in which she reunited with her Housefull 2 costar, Akshay Kumar, released on 7 December 2012 to mixed reviews, but was strong at the box office and eventually became a commercially successful venture.

Asin did not appear in any movie in 2013 or 2014. Her only movie release of 2015 was All Is Well, directed by Umesh Shukla and produced by Bhushan Kumar. In one interview, director Umesh Shukla stated that Asin would be seen in an "important dramatic role".


£#h5#£Other work£#/h5#£
£#h5#£Humanitarian work£#/h5#£
During the shoot of her film Ready in Sri Lanka, she was part of camps that were meant to help the Sri Lankan Tamil people who were affected by the Sri Lankan Civil War.

In an interview during the DNA I Can Women's Half Marathon in 2012, Asin, speaking about the importance of health and the necessity of education said:

This cause is probably one of the biggest women-centric issues in our country. I support the education of six children each year, five of which are girls. I have closely worked with these children and I know that if given the opportunity, women can educate themselves much better and know what is wrong or right for them. Education is a basic right and women should not be deprived of that.


£#h5#£Brand ambassador£#/h5#£
Asin has been the brand ambassador for several companies and products, and has appeared in commercials for Avon, Mirinda, Colgate, Fairever, Tanishq, Big Bazaar, Parachute, Spinz talc, Lux, Amrutanjan Healthcare, and Clinic All Clear. Asin has been endorsing Mirinda soft drinks since 2004.


£#h5#£Personal life£#/h5#£
Asin is a Catholic Christian belonging to the Syro-Malabar rite, and currently resides in Mumbai. She also owns an apartment in Marine Drive, Kochi and a farmhouse in Vagamon, Kerala. The actress is a polyglot. She can speak seven languages; being well-versed in Malayalam (her mother tongue), Tamil, Telugu, Sanskrit, English, Hindi and French. Asin also speaks a smattering of Italian and learnt basic Marathi for her movie Khiladi 786, in which she played a typical Marathi girl. In June 2013, it was reported in the Indian media that Asin was learning German during her free time. She began learning Spanish after visiting Spain in early 2014.

Asin married Micromax co-founder Rahul Sharma in January 2016 in a Christian wedding that was followed by a Hindu ceremony, and quit acting after marriage. Their first child, a daughter Arin, was born on 24 October 2017.


£#h5#£Filmography£#/h5#£
£#h5#£Awards and nominations£#/h5#£
£#h5#£See also£#/h5#£ £#ul#££#li#£List of Indian film actresses£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Asin at IMDb£#/li#£ £#li#£Asin on Instagram£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Trigonometric Functions £#/li#££#/ul#£




£#h3#£Associated Laguerre Polynomial£#/h3#£

In mathematics, the Laguerre polynomials, named after Edmond Laguerre (1834–1886), are solutions of Laguerre's equation:

which is a second-order linear differential equation. This equation has nonsingular solutions only if n is a non-negative integer.
Sometimes the name Laguerre polynomials is used for solutions of

where n is still a non-negative integer. Then they are also named generalized Laguerre polynomials, as will be done here (alternatively associated Laguerre polynomials or, rarely, Sonine polynomials, after their inventor Nikolay Yakovlevich Sonin).
More generally, a Laguerre function is a solution when n is not necessarily a non-negative integer.

The Laguerre polynomials are also used for Gaussian quadrature to numerically compute integrals of the form

These polynomials, usually denoted L0, L1, …, are a polynomial sequence which may be defined by the Rodrigues formula,

reducing to the closed form of a following section.
They are orthogonal polynomials with respect to an inner product

The sequence of Laguerre polynomials n! Ln is a Sheffer sequence,

The rook polynomials in combinatorics are more or less the same as Laguerre polynomials, up to elementary changes of variables. Further see the Tricomi–Carlitz polynomials.

The Laguerre polynomials arise in quantum mechanics, in the radial part of the solution of the Schrödinger equation for a one-electron atom. They also describe the static Wigner functions of oscillator systems in quantum mechanics in phase space. They further enter in the quantum mechanics of the Morse potential and of the 3D isotropic harmonic oscillator.

Physicists sometimes use a definition for the Laguerre polynomials that is larger by a factor of n! than the definition used here. (Likewise, some physicists may use somewhat different definitions of the so-called associated Laguerre polynomials.)


£#h5#£The first few polynomials£#/h5#£
These are the first few Laguerre polynomials:


£#h5#£Recursive definition, closed form, and generating function£#/h5#£
One can also define the Laguerre polynomials recursively, defining the first two polynomials as

and then using the following recurrence relation for any k ≥ 1: Furthermore,
In solution of some boundary value problems, the characteristic values can be useful:

The closed form is

The generating function for them likewise follows,

Polynomials of negative index can be expressed using the ones with positive index:


£#h5#£Generalized Laguerre polynomials£#/h5#£
For arbitrary real α the polynomial solutions of the differential equation

are called generalized Laguerre polynomials, or associated Laguerre polynomials.
One can also define the generalized Laguerre polynomials recursively, defining the first two polynomials as

and then using the following recurrence relation for any k ≥ 1:

The simple Laguerre polynomials are the special case α = 0 of the generalized Laguerre polynomials:

The Rodrigues formula for them is

The generating function for them is


£#h5#£Explicit examples and properties of the generalized Laguerre polynomials£#/h5#£ £#ul#££#li#£Laguerre functions are defined by confluent hypergeometric functions and Kummer's transformation as where ${\textstyle {n+\alpha \choose n}}$ is a generalized binomial coefficient. When n is an integer the function reduces to a polynomial of degree n. It has the alternative expression in terms of Kummer's function of the second kind.£#/li#£ £#li#£The closed form for these generalized Laguerre polynomials of degree n is derived by applying Leibniz's theorem for differentiation of a product to Rodrigues' formula.£#/li#£ £#li#£The first few generalized Laguerre polynomials are: £#/li#£ £#li#£The coefficient of the leading term is (−1)n/n!;£#/li#£ £#li#£The constant term, which is the value at 0, is £#/li#£ £#li#£If α is non-negative, then Ln(α) has n real, strictly positive roots (notice that ${\displaystyle \left((-1)^{n-i}L_{n-i}^{(\alpha )}\right)_{i=0}^{n}}$ is a Sturm chain), which are all in the interval ${\displaystyle \left(0,n+\alpha +(n-1){\sqrt {n+\alpha }}\,\right].}$ £#/li#£ £#li#£The polynomials' asymptotic behaviour for large n, but fixed α and x > 0, is given by and summarizing by where ${\displaystyle J_{\alpha }}$ is the Bessel function.£#/li#££#/ul#£
£#h5#£As a contour integral£#/h5#£
Given the generating function specified above, the polynomials may be expressed in terms of a contour integral

where the contour circles the origin once in a counterclockwise direction without enclosing the essential singularity at 1
£#h5#£Recurrence relations£#/h5#£
The addition formula for Laguerre polynomials:

Laguerre's polynomials satisfy the recurrence relations

in particular and or moreover
They can be used to derive the four 3-point-rules

combined they give this additional, useful recurrence relations

Since ${\displaystyle L_{n}^{(\alpha )}(x)}$ is a monic polynomial of degree ${\displaystyle n}$ in ${\displaystyle \alpha }$ , there is the partial fraction decomposition

The second equality follows by the following identity, valid for integer i and n and immediate from the expression of ${\displaystyle L_{n}^{(\alpha )}(x)}$ in terms of Charlier polynomials: For the third equality apply the fourth and fifth identities of this section.
£#h5#£Derivatives of generalized Laguerre polynomials£#/h5#£
Differentiating the power series representation of a generalized Laguerre polynomial k times leads to

This points to a special case (α = 0) of the formula above: for integer α = k the generalized polynomial may be written

the shift by k sometimes causing confusion with the usual parenthesis notation for a derivative.
Moreover, the following equation holds:

which generalizes with Cauchy's formula to
The derivative with respect to the second variable α has the form,

This is evident from the contour integral representation below.
The generalized Laguerre polynomials obey the differential equation

which may be compared with the equation obeyed by the kth derivative of the ordinary Laguerre polynomial,
where ${\displaystyle L_{n}^{[k]}(x)\equiv {\frac {d^{k}L_{n}(x)}{dx^{k}}}}$ for this equation only.
In Sturm–Liouville form the differential equation is

which shows that L(α)
n is an eigenvector for the eigenvalue n.


£#h5#£Orthogonality£#/h5#£
The generalized Laguerre polynomials are orthogonal over [0, ∞) with respect to the measure with weighting function xα e−x:

which follows from

If ${\displaystyle \Gamma (x,\alpha +1,1)}$ denotes the Gamma distribution then the orthogonality relation can be written as

The associated, symmetric kernel polynomial has the representations (Christoffel–Darboux formula)

recursively

Moreover,

Turán's inequalities can be derived here, which is

The following integral is needed in the quantum mechanical treatment of the hydrogen atom,


£#h5#£Series expansions£#/h5#£
Let a function have the (formal) series expansion

Then

The series converges in the associated Hilbert space L2[0, ∞) if and only if


£#h5#£Further examples of expansions£#/h5#£
Monomials are represented as

while binomials have the parametrization
This leads directly to

for the exponential function. The incomplete gamma function has the representation
£#h5#£In quantum mechanics£#/h5#£
In quantum mechanics the Schrödinger equation for the hydrogen-like atom is exactly solvable by separation of variables in spherical coordinates. The radial part of the wave function is a (generalized) Laguerre polynomial.

Vibronic transitions in the Franck-Condon approximation can also be described using Laguerre polynomials.


£#h5#£Multiplication theorems£#/h5#£
Erdélyi gives the following two multiplication theorems


£#h5#£Relation to Hermite polynomials£#/h5#£
The generalized Laguerre polynomials are related to the Hermite polynomials:

where the Hn(x) are the Hermite polynomials based on the weighting function exp(−x2), the so-called "physicist's version."
Because of this, the generalized Laguerre polynomials arise in the treatment of the quantum harmonic oscillator.


£#h5#£Relation to hypergeometric functions£#/h5#£
The Laguerre polynomials may be defined in terms of hypergeometric functions, specifically the confluent hypergeometric functions, as

where ${\displaystyle (a)_{n}}$ is the Pochhammer symbol (which in this case represents the rising factorial).
£#h5#£Hardy–Hille formula£#/h5#£
The generalized Laguerre polynomials satisfy the Hardy–Hille formula

where the series on the left converges for ${\displaystyle \alpha >-1}$ and ${\displaystyle |t|<1}$ . Using the identity (see generalized hypergeometric function), this can also be written as This formula is a generalization of the Mehler kernel for Hermite polynomials, which can be recovered from it by using the relations between Laguerre and Hermite polynomials given above.
£#h5#£Physicist Scaling Convention£#/h5#£
The generalized Laguerre polynomials are used to describe the quantum wavefunction for hydrogen atom orbitals. In the introductory literature on this topic, a different scaling is used for the generalized Laguerre polynomials than the scaling presented in this article. In the convention taken here, the generalized Laguerre polynomials can be expressed as

where ${\displaystyle \,_{1}F_{1}(a;b;x)}$ is the confluent hypergeometric function. In the physicist literature, such as , the generalized Laguerre polynomials are instead defined as

The physicist version is related to the standard version by

There is yet another convention in use, though less frequently, in the physics literature. Under this convention the Laguerre polynomials are given by


£#h5#£See also£#/h5#£ £#ul#££#li#£Angelescu polynomials£#/li#£ £#li#£Bessel polynomials£#/li#£ £#li#£Denisyuk polynomials£#/li#£ £#li#£Transverse mode, an important application of Laguerre polynomials to describe the field intensity within a waveguide or laser beam profile.£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Abramowitz, Milton; Stegun, Irene Ann, eds. (1983) [June 1964]. "Chapter 22". Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables. Applied Mathematics Series. Vol. 55 (Ninth reprint with additional corrections of tenth original printing with corrections (December 1972); first ed.). Washington D.C.; New York: United States Department of Commerce, National Bureau of Standards; Dover Publications. p. 773. ISBN 978-0-486-61272-0. LCCN 64-60036. MR 0167642. LCCN 65-12253.£#/li#£ £#li#£G. Szegő, Orthogonal polynomials, 4th edition, Amer. Math. Soc. Colloq. Publ., vol. 23, Amer. Math. Soc., Providence, RI, 1975.£#/li#£ £#li#£Koornwinder, Tom H.; Wong, Roderick S. C.; Koekoek, Roelof; Swarttouw, René F. (2010), "Orthogonal Polynomials", in Olver, Frank W. J.; Lozier, Daniel M.; Boisvert, Ronald F.; Clark, Charles W. (eds.), NIST Handbook of Mathematical Functions, Cambridge University Press, ISBN 978-0-521-19225-5, MR 2723248£#/li#£ £#li#£B. Spain, M.G. Smith, Functions of mathematical physics, Van Nostrand Reinhold Company, London, 1970. Chapter 10 deals with Laguerre polynomials.£#/li#£ £#li#£"Laguerre polynomials", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#£ £#li#£Eric W. Weisstein, "Laguerre Polynomial", From MathWorld—A Wolfram Web Resource.£#/li#£ £#li#£George Arfken and Hans Weber (2000). Mathematical Methods for Physicists. Academic Press. ISBN 978-0-12-059825-0.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Timothy Jones. "The Legendre and Laguerre Polynomials and the elementary quantum mechanical model of the Hydrogen Atom".£#/li#£ £#li#£Weisstein, Eric W. "Laguerre polynomial". MathWorld.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Abramowitz, M. and Stegun, I. A. (Eds.). "Orthogonal Polynomials." Ch. 22 in Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing. New York: Dover, pp. 771-802, 1972.£#/li#££#li#£Andrews, G. E.; Askey, R.; and Roy, R. "Laguerre Polynomials." §6.2 in Special Functions. Cambridge, England: Cambridge University Press, pp. 282-293, 1999.£#/li#££#li#£Arfken, G. "Laguerre Functions." §13.2 in Mathematical Methods for Physicists, 3rd ed. Orlando, FL: Academic Press, pp. 721-731, 1985.£#/li#££#li#£Chebyshev, P. L. "Sur le développement des fonctions à une seule variable." Bull. Ph.-Math., Acad. Imp. Sc. St. Pétersbourg 1, 193-200, 1859.£#/li#££#li#£Chebyshev, P. L. Oeuvres, Vol. 1. New York: Chelsea, pp. 499-508, 1987.£#/li#££#li#£Iyanaga, S. and Kawada, Y. (Eds.). "Laguerre Functions." Appendix A, Table 20.VI in Encyclopedic Dictionary of Mathematics. Cambridge, MA: MIT Press, p. 1481, 1980.£#/li#££#li#£Koekoek, R. and Swarttouw, R. F. "Laguerre." §1.11 in The Askey-Scheme of Hypergeometric Orthogonal Polynomials and its q-Analogue. Delft, Netherlands: Technische Universiteit Delft, Faculty of Technical Mathematics and Informatics Report 98-17, pp. 47-49, 1998.£#/li#££#li#£Laguerre, E. de. "Sur l'intégrale int_x^(+infty)x^(-1)e^(-x)dx." Bull. Soc. math. France 7, 72-81, 1879. Reprinted in Oeuvres, Vol. 1. New York: Chelsea, pp. 428-437, 1971.£#/li#££#li#£Petkovšek, M.; Wilf, H. S.; and Zeilberger, D. A=B. Wellesley, MA: A K Peters, pp. 61-62, 1996. http://www.cis.upenn.edu/~wilf/AeqB.html.£#/li#££#li#£Roman, S. "The Laguerre Polynomials." §3.1 i The Umbral Calculus. New York: Academic Press, pp. 108-113, 1984.£#/li#££#li#£Rota, G.-C.; Kahaner, D.; Odlyzko, A. "Laguerre Polynomials." §11 in "On the Foundations of Combinatorial Theory. VIII: Finite Operator Calculus." J. Math. Anal. Appl. 42, 684-760, 1973.£#/li#££#li#£Sansone, G. "Expansions in Laguerre and Hermite Series." Ch. 4 in Orthogonal Functions, rev. English ed. New York: Dover, pp. 295-385, 1991.£#/li#££#li#£Sloane, N. J. A. Sequences A000142/M1675 and A021009 in "The On-Line Encyclopedia of Integer Sequences."£#/li#££#li#£Sonine, N. J. "Sur les fonctions cylindriques et le développement des fonctions continues en séries." Math. Ann. 16, 1-80, 1880.£#/li#££#li#£Spanier, J. and Oldham, K. B. "The Laguerre Polynomials L_n(x)." Ch. 23 in An Atlas of Functions. Washington, DC: Hemisphere, pp. 209-216, 1987.£#/li#££#li#£Szegö, G. Orthogonal Polynomials, 4th ed. Providence, RI: Amer. Math. Soc., 1975.£#/li#££#li#£Whittaker, E. T. and Watson, G. N. Ch. 16, Ex. 8 in A Course in Modern Analysis, 4th ed. Cambridge, England: Cambridge University Press, p. 352, 1990.£#/li#££#li#£ Abramowitz, M. and Stegun, I. A. (Eds.). "Orthogonal Polynomials." Ch. 22 in Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing. New York: Dover, pp. 771-802, 1972. £#/li#££#li#£ Andrews, G. E.; Askey, R.; and Roy, R. "Laguerre Polynomials." §6.2 in Special Functions. Cambridge, England: Cambridge University Press, pp. 282-293, 1999. £#/li#££#li#£ Arfken, G. "Laguerre Functions." §13.2 in Mathematical Methods for Physicists, 3rd ed. Orlando, FL: Academic Press, pp. 721-731, 1985. £#/li#££#li#£ Chebyshev, P. L. "Sur le développement des fonctions à une seule variable." Bull. Ph.-Math., Acad. Imp. Sc. St. Pétersbourg 1, 193-200, 1859. £#/li#££#li#£ Chebyshev, P. L. Oeuvres, Vol. 1. New York: Chelsea, pp. 499-508, 1987. £#/li#££#li#£ Iyanaga, S. and Kawada, Y. (Eds.). "Laguerre Functions." Appendix A, Table 20.VI in Encyclopedic Dictionary of Mathematics. Cambridge, MA: MIT Press, p. 1481, 1980. £#/li#££#li#£ Koekoek, R. and Swarttouw, R. F. "Laguerre." §1.11 in The Askey-Scheme of Hypergeometric Orthogonal Polynomials and its -Analogue. Delft, Netherlands: Technische Universiteit Delft, Faculty of Technical Mathematics and Informatics Report 98-17, pp. 47-49, 1998. £#/li#££#li#£ Laguerre, E. de. "Sur l'intégrale ." Bull. Soc. math. France 7, 72-81, 1879. Reprinted in Oeuvres, Vol. 1. New York: Chelsea, pp. 428-437, 1971. £#/li#££#li#£ Petkovšek, M.; Wilf, H. S.; and Zeilberger, D. A=B. Wellesley, MA: A K Peters, pp. 61-62, 1996. http://www.cis.upenn.edu/~wilf/AeqB.html. £#/li#££#li#£ Roman, S. "The Laguerre Polynomials." §3.1 i The Umbral Calculus. New York: Academic Press, pp. 108-113, 1984. £#/li#££#li#£ Rota, G.-C.; Kahaner, D.; Odlyzko, A. "Laguerre Polynomials." §11 in "On the Foundations of Combinatorial Theory. VIII: Finite Operator Calculus." J. Math. Anal. Appl. 42, 684-760, 1973. £#/li#££#li#£ Sansone, G. "Expansions in Laguerre and Hermite Series." Ch. 4 in Orthogonal Functions, rev. English ed. New York: Dover, pp. 295-385, 1991. £#/li#££#li#£ Sloane, N. J. A. Sequences A000142/M1675 and A021009 in "The On-Line Encyclopedia of Integer Sequences." £#/li#££#li#£ Sonine, N. J. "Sur les fonctions cylindriques et le développement des fonctions continues en séries." Math. Ann. 16, 1-80, 1880. £#/li#££#li#£ Spanier, J. and Oldham, K. B. "The Laguerre Polynomials ." Ch. 23 in An Atlas of Functions. Washington, DC: Hemisphere, pp. 209-216, 1987. £#/li#££#li#£ Szegö, G. Orthogonal Polynomials, 4th ed. Providence, RI: Amer. Math. Soc., 1975. £#/li#££#li#£ Whittaker, E. T. and Watson, G. N. Ch. 16, Ex. 8 in A Course in Modern Analysis, 4th ed. Cambridge, England: Cambridge University Press, p. 352, 1990. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Orthogonal Polynomials £#/li#££#/ul#£




£#h3#£Associated Legendre Differential Equation£#/h3#£

£#h5#£ References £#/h5#£ £#ul#££#li#£Abramowitz, M. and Stegun, I. A. (Eds.). Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing. New York: Dover, p. 332, 1972.£#/li#££#li#£Moon, P. and Spencer, D. E. Field Theory for Engineers. New York: Van Nostrand, 1961.£#/li#££#li#£Zwillinger, D. Handbook of Differential Equations, 3rd ed. Boston, MA: Academic Press, 1997.£#/li#££#li#£ Abramowitz, M. and Stegun, I. A. (Eds.). Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing. New York: Dover, p. 332, 1972. £#/li#££#li#£ Moon, P. and Spencer, D. E. Field Theory for Engineers. New York: Van Nostrand, 1961. £#/li#££#li#£ Zwillinger, D. Handbook of Differential Equations, 3rd ed. Boston, MA: Academic Press, 1997. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Differential Equations > Ordinary Differential Equations £#/li#££#/ul#£




£#h3#£Associated Legendre Polynomial£#/h3#£

In mathematics, the associated Legendre polynomials are the canonical solutions of the general Legendre equation

or equivalently

where the indices ℓ and m (which are integers) are referred to as the degree and order of the associated Legendre polynomial respectively. This equation has nonzero solutions that are nonsingular on [−1, 1] only if ℓ and m are integers with 0 ≤ m ≤ ℓ, or with trivially equivalent negative values. When in addition m is even, the function is a polynomial. When m is zero and ℓ integer, these functions are identical to the Legendre polynomials. In general, when ℓ and m are integers, the regular solutions are sometimes called "associated Legendre polynomials", even though they are not polynomials when m is odd. The fully general class of functions with arbitrary real or complex values of ℓ and m are Legendre functions. In that case the parameters are usually labelled with Greek letters.

The Legendre ordinary differential equation is frequently encountered in physics and other technical fields. In particular, it occurs when solving Laplace's equation (and related partial differential equations) in spherical coordinates. Associated Legendre polynomials play a vital role in the definition of spherical harmonics.


£#h5#£Definition for non-negative integer parameters ℓ and m£#/h5#£
These functions are denoted ${\displaystyle P_{\ell }^{m}(x)}$ , where the superscript indicates the order and not a power of P. Their most straightforward definition is in terms of derivatives of ordinary Legendre polynomials (m ≥ 0)

The (−1)m factor in this formula is known as the Condon–Shortley phase. Some authors omit it. That the functions described by this equation satisfy the general Legendre differential equation with the indicated values of the parameters ℓ and m follows by differentiating m times the Legendre equation for Pℓ:

Moreover, since by Rodrigues' formula,

the Pm
ℓ can be expressed in the form
This equation allows extension of the range of m to: −ℓ ≤ m ≤ ℓ. The definitions of Pℓ±m, resulting from this expression by substitution of ±m, are proportional. Indeed, equate the coefficients of equal powers on the left and right hand side of

then it follows that the proportionality constant is so that
£#h5#£Alternative notations£#/h5#£
The following alternative notations are also used in literature:


£#h5#£Closed Form£#/h5#£
The Associated Legendre Polynomial can also be written as:

with simple monomials and the generalized form of the binomial coefficient.
£#h5#£Orthogonality£#/h5#£
The associated Legendre polynomials are not mutually orthogonal in general. For example, ${\displaystyle P_{1}^{1}}$ is not orthogonal to ${\displaystyle P_{2}^{2}}$ . However, some subsets are orthogonal. Assuming 0 ≤ m ≤ ℓ, they satisfy the orthogonality condition for fixed m:

Where δk,ℓ is the Kronecker delta.

Also, they satisfy the orthogonality condition for fixed ℓ:


£#h5#£Negative m and/or negative ℓ£#/h5#£
The differential equation is clearly invariant under a change in sign of m.

The functions for negative m were shown above to be proportional to those of positive m:

(This followed from the Rodrigues' formula definition. This definition also makes the various recurrence formulas work for positive or negative m.)

The differential equation is also invariant under a change from ℓ to −ℓ − 1, and the functions for negative ℓ are defined by


£#h5#£Parity£#/h5#£
From their definition, one can verify that the Associated Legendre functions are either even or odd according to


£#h5#£The first few associated Legendre functions£#/h5#£
The first few associated Legendre functions, including those for negative values of m, are:


£#h5#£Recurrence formula£#/h5#£
These functions have a number of recurrence properties:

Helpful identities (initial values for the first recursion):

with !! the double factorial.


£#h5#£Gaunt's formula£#/h5#£
The integral over the product of three associated Legendre polynomials (with orders matching as shown below) is a necessary ingredient when developing products of Legendre polynomials into a series linear in the Legendre polynomials. For instance, this turns out to be necessary when doing atomic calculations of the Hartree–Fock variety where matrix elements of the Coulomb operator are needed. For this we have Gaunt's formula

This formula is to be used under the following assumptions:
£#li#£the degrees are non-negative integers ${\displaystyle l,m,n\geq 0}$ £#/li#£ £#li#£all three orders are non-negative integers ${\displaystyle u,v,w\geq 0}$ £#/li#£ £#li#£ ${\displaystyle u}$ is the largest of the three orders£#/li#£ £#li#£the orders sum up ${\displaystyle u=v+w}$ £#/li#£ £#li#£the degrees obey ${\displaystyle m\geq n}$ £#/li#£
Other quantities appearing in the formula are defined as

The integral is zero unless

£#li#£the sum of degrees is even so that ${\displaystyle s}$ is an integer£#/li#£ £#li#£the triangular condition is satisfied ${\displaystyle m+n\geq l\geq m-n}$ £#/li#£
Dong and Lemus (2002) generalized the derivation of this formula to integrals over a product of an arbitrary number of associated Legendre polynomials.


£#h5#£Generalization via hypergeometric functions£#/h5#£
These functions may actually be defined for general complex parameters and argument:

where ${\displaystyle \Gamma }$ is the gamma function and ${\displaystyle _{2}F_{1}}$ is the hypergeometric function

They are called the Legendre functions when defined in this more general way. They satisfy the same differential equation as before:

Since this is a second order differential equation, it has a second solution, ${\displaystyle Q_{\lambda }^{\mu }(z)}$ , defined as:

${\displaystyle P_{\lambda }^{\mu }(z)}$ and ${\displaystyle Q_{\lambda }^{\mu }(z)}$ both obey the various recurrence formulas given previously.


£#h5#£Reparameterization in terms of angles£#/h5#£
These functions are most useful when the argument is reparameterized in terms of angles, letting ${\displaystyle x=\cos \theta }$ :

Using the relation ${\displaystyle (1-x^{2})^{1/2}=\sin \theta }$ , the list given above yields the first few polynomials, parameterized this way, as:

The orthogonality relations given above become in this formulation: for fixed m, ${\displaystyle P_{\ell }^{m}(\cos \theta )}$ are orthogonal, parameterized by θ over ${\displaystyle [0,\pi ]}$ , with weight ${\displaystyle \sin \theta }$ :

Also, for fixed ℓ:

In terms of θ, ${\displaystyle P_{\ell }^{m}(\cos \theta )}$ are solutions of

More precisely, given an integer m ${\displaystyle \geq }$ 0, the above equation has nonsingular solutions only when ${\displaystyle \lambda =\ell (\ell +1)\,}$ for ℓ an integer ≥ m, and those solutions are proportional to ${\displaystyle P_{\ell }^{m}(\cos \theta )}$ .


£#h5#£Applications in physics: spherical harmonics£#/h5#£
In many occasions in physics, associated Legendre polynomials in terms of angles occur where spherical symmetry is involved. The colatitude angle in spherical coordinates is the angle ${\displaystyle \theta }$ used above. The longitude angle, ${\displaystyle \phi }$ , appears in a multiplying factor. Together, they make a set of functions called spherical harmonics. These functions express the symmetry of the two-sphere under the action of the Lie group SO(3).

What makes these functions useful is that they are central to the solution of the equation ${\displaystyle \nabla ^{2}\psi +\lambda \psi =0}$ on the surface of a sphere. In spherical coordinates θ (colatitude) and φ (longitude), the Laplacian is

When the partial differential equation

is solved by the method of separation of variables, one gets a φ-dependent part ${\displaystyle \sin(m\phi )}$ or ${\displaystyle \cos(m\phi )}$ for integer m≥0, and an equation for the θ-dependent part

for which the solutions are ${\displaystyle P_{\ell }^{m}(\cos \theta )}$ with ${\displaystyle \ell {\geq }m}$ and ${\displaystyle \lambda =\ell (\ell +1)}$ .

Therefore, the equation

has nonsingular separated solutions only when ${\displaystyle \lambda =\ell (\ell +1)}$ , and those solutions are proportional to

and

For each choice of ℓ, there are 2ℓ + 1 functions for the various values of m and choices of sine and cosine. They are all orthogonal in both ℓ and m when integrated over the surface of the sphere.

The solutions are usually written in terms of complex exponentials:

The functions ${\displaystyle Y_{\ell ,m}(\theta ,\phi )}$ are the spherical harmonics, and the quantity in the square root is a normalizing factor. Recalling the relation between the associated Legendre functions of positive and negative m, it is easily shown that the spherical harmonics satisfy the identity
The spherical harmonic functions form a complete orthonormal set of functions in the sense of Fourier series. Workers in the fields of geodesy, geomagnetism and spectral analysis use a different phase and normalization factor than given here (see spherical harmonics).

When a 3-dimensional spherically symmetric partial differential equation is solved by the method of separation of variables in spherical coordinates, the part that remains after removal of the radial part is typically of the form

and hence the solutions are spherical harmonics.


£#h5#£Generalizations£#/h5#£
The Legendre polynomials are closely related to hypergeometric series. In the form of spherical harmonics, they express the symmetry of the two-sphere under the action of the Lie group SO(3). There are many other Lie groups besides SO(3), and an analogous generalization of the Legendre polynomials exist to express the symmetries of semi-simple Lie groups and Riemannian symmetric spaces. Crudely speaking, one may define a Laplacian on symmetric spaces; the eigenfunctions of the Laplacian can be thought of as generalizations of the spherical harmonics to other settings.


£#h5#£See also£#/h5#£ £#ul#££#li#£Angular momentum£#/li#£ £#li#£Gaussian quadrature£#/li#£ £#li#£Legendre polynomials£#/li#£ £#li#£Spherical harmonics£#/li#£ £#li#£Whipple's transformation of Legendre functions£#/li#£ £#li#£Laguerre polynomials£#/li#£ £#li#£Hermite polynomials£#/li#££#/ul#£
£#h5#£Notes and references£#/h5#£ £#ul#££#li#£Arfken, G.B.; Weber, H.J. (2001), Mathematical methods for physicists, Academic Press, ISBN 978-0-12-059825-0; Section 12.5. (Uses a different sign convention.)£#/li#£ £#li#£Belousov, S. L. (1962), Tables of normalized associated Legendre polynomials, Mathematical tables, vol. 18, Pergamon Press.£#/li#£ £#li#£Condon, E. U.; Shortley, G. H. (1970), The Theory of Atomic Spectra, Cambridge, England: Cambridge University Press, OCLC 5388084; Chapter 3.£#/li#£ £#li#£Courant, Richard; Hilbert, David (1953), Methods of Mathematical Physics, Volume 1, New York: Interscience Publischer, Inc.£#/li#£ £#li#£Dunster, T. M. (2010), "Legendre and Related Functions", in Olver, Frank W. J.; Lozier, Daniel M.; Boisvert, Ronald F.; Clark, Charles W. (eds.), NIST Handbook of Mathematical Functions, Cambridge University Press, ISBN 978-0-521-19225-5, MR 2723248£#/li#£ £#li#£Edmonds, A.R. (1957), Angular Momentum in Quantum Mechanics, Princeton University Press, ISBN 978-0-691-07912-7; Chapter 2.£#/li#£ £#li#£Hildebrand, F. B. (1976), Advanced Calculus for Applications, Prentice Hall, ISBN 978-0-13-011189-0.£#/li#£ £#li#£Koornwinder, Tom H.; Wong, Roderick S. C.; Koekoek, Roelof; Swarttouw, René F. (2010), "Orthogonal Polynomials", in Olver, Frank W. J.; Lozier, Daniel M.; Boisvert, Ronald F.; Clark, Charles W. (eds.), NIST Handbook of Mathematical Functions, Cambridge University Press, ISBN 978-0-521-19225-5, MR 2723248£#/li#£ £#li#£Schach, S. R. (1973) New Identities for Legendre Associated Functions of Integral Order and Degree , Society for Industrial and Applied Mathematics Journal on Mathematical Analysis, 1976, Vol. 7, No. 1 : pp. 59–69£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Associated Legendre polynomials in MathWorld£#/li#£ £#li#£Legendre polynomials in MathWorld£#/li#£ £#li#£Legendre and Related Functions in DLMF£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Abramowitz, M. and Stegun, I. A. (Eds.). "Legendre Functions" and "Orthogonal Polynomials." Ch. 22 in Chs. 8 and 22 in Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing. New York: Dover, pp. 331-339 and 771-802, 1972.£#/li#££#li#£Arfken, G. "Legendre Functions." Ch. 12 in Mathematical Methods for Physicists, 3rd ed. Orlando, FL: Academic Press, pp. 637-711, 1985.£#/li#££#li#£Bailey, W. N. "On the Product of Two Legendre Polynomials." Proc. Cambridge Philos. Soc. 29, 173-177, 1933.£#/li#££#li#£Bailey, W. N. Generalised Hypergeometric Series. Cambridge, England: Cambridge University Press, 1935.£#/li#££#li#£Byerly, W. E. "Zonal Harmonics." Ch. 5 in An Elementary Treatise on Fourier's Series, and Spherical, Cylindrical, and Ellipsoidal Harmonics, with Applications to Problems in Mathematical Physics. New York: Dover, pp. 144-194, 1959.£#/li#££#li#£Gradshteyn, I. S. and Ryzhik, I. M. Tables of Integrals, Series, and Products, 6th ed. San Diego, CA: Academic Press, 2000.£#/li#££#li#£Hildebrand, F. B. Introduction to Numerical Analysis. New York: McGraw-Hill, 1956.£#/li#££#li#£Iyanaga, S. and Kawada, Y. (Eds.). "Legendre Function" and "Associated Legendre Function." Appendix A, Tables 18.II and 18.III in Encyclopedic Dictionary of Mathematics. Cambridge, MA: MIT Press, pp. 1462-1468, 1980.£#/li#££#li#£Koekoek, R. and Swarttouw, R. F. "Legendre / Spherical." §1.8.3 in The Askey-Scheme of Hypergeometric Orthogonal Polynomials and its q-Analogue. Delft, Netherlands: Technische Universiteit Delft, Faculty of Technical Mathematics and Informatics Report 98-17, p. 44, 1998.£#/li#££#li#£Koepf, W. Hypergeometric Summation: An Algorithmic Approach to Summation and Special Function Identities. Braunschweig, Germany: Vieweg, 1998.£#/li#££#li#£Lagrange, R. Polynomes et fonctions de Legendre. Paris: Gauthier-Villars, 1939.£#/li#££#li#£Legendre, A. M. "Sur l'attraction des Sphéroides." Mém. Math. et Phys. présentés à l'Ac. r. des. sc. par divers savants 10, 1785.£#/li#££#li#£Morse, P. M. and Feshbach, H. Methods of Theoretical Physics, Part I. New York: McGraw-Hill, pp. 593-597, 1953.£#/li#££#li#£Press, W. H.; Flannery, B. P.; Teukolsky, S. A.; and Vetterling, W. T. Numerical Recipes in FORTRAN: The Art of Scientific Computing, 2nd ed. Cambridge, England: Cambridge University Press, p. 252, 1992.£#/li#££#li#£Sansone, G. "Expansions in Series of Legendre Polynomials and Spherical Harmonics." Ch. 3 in Orthogonal Functions, rev. English ed. New York: Dover, pp. 169-294, 1991.£#/li#££#li#£Sloane, N. J. A. Sequences A001790/M2508, A002596/M3768, A008316, A008317, A046161, A060818, A078297, and A078298 in "The On-Line Encyclopedia of Integer Sequences."£#/li#££#li#£Snow, C. Hypergeometric and Legendre Functions with Applications to Integral Equations of Potential Theory. Washington, DC: U. S. Government Printing Office, 1952.£#/li#££#li#£Spanier, J. and Oldham, K. B. "The Legendre Polynomials P_n(x)" and "The Legendre Functions P_nu(x) and Q_nu(x)." Chs. 21 and 59 in An Atlas of Functions. Washington, DC: Hemisphere, pp. 183-192 and 581-597, 1987.£#/li#££#li#£Strutt, J. W. "On the Values of the Integral int_0^1Q_nQ_n^'dmu, Q_n, Q_n^' being LaPlace's Coefficients of the orders n, n^', with an Application to the Theory of Radiation." Philos. Trans. Roy. Soc. London 160, 579-590, 1870.£#/li#££#li#£Szegö, G. Orthogonal Polynomials, 4th ed. Providence, RI: Amer. Math. Soc., 1975.£#/li#££#li#£Whittaker, E. T. and Watson, G. N. A Course in Modern Analysis, 4th ed. Cambridge, England: Cambridge University Press, 1990.£#/li#££#li#£ Abramowitz, M. and Stegun, I. A. (Eds.). "Legendre Functions" and "Orthogonal Polynomials." Ch. 22 in Chs. 8 and 22 in Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing. New York: Dover, pp. 331-339 and 771-802, 1972. £#/li#££#li#£ Arfken, G. "Legendre Functions." Ch. 12 in Mathematical Methods for Physicists, 3rd ed. Orlando, FL: Academic Press, pp. 637-711, 1985. £#/li#££#li#£ Bailey, W. N. "On the Product of Two Legendre Polynomials." Proc. Cambridge Philos. Soc. 29, 173-177, 1933. £#/li#££#li#£ Bailey, W. N. Generalised Hypergeometric Series. Cambridge, England: Cambridge University Press, 1935. £#/li#££#li#£ Byerly, W. E. "Zonal Harmonics." Ch. 5 in An Elementary Treatise on Fourier's Series, and Spherical, Cylindrical, and Ellipsoidal Harmonics, with Applications to Problems in Mathematical Physics. New York: Dover, pp. 144-194, 1959. £#/li#££#li#£ Gradshteyn, I. S. and Ryzhik, I. M. Tables of Integrals, Series, and Products, 6th ed. San Diego, CA: Academic Press, 2000. £#/li#££#li#£ Hildebrand, F. B. Introduction to Numerical Analysis. New York: McGraw-Hill, 1956. £#/li#££#li#£ Iyanaga, S. and Kawada, Y. (Eds.). "Legendre Function" and "Associated Legendre Function." Appendix A, Tables 18.II and 18.III in Encyclopedic Dictionary of Mathematics. Cambridge, MA: MIT Press, pp. 1462-1468, 1980. £#/li#££#li#£ Koekoek, R. and Swarttouw, R. F. "Legendre / Spherical." §1.8.3 in The Askey-Scheme of Hypergeometric Orthogonal Polynomials and its -Analogue. Delft, Netherlands: Technische Universiteit Delft, Faculty of Technical Mathematics and Informatics Report 98-17, p. 44, 1998. £#/li#££#li#£ Koepf, W. Hypergeometric Summation: An Algorithmic Approach to Summation and Special Function Identities. Braunschweig, Germany: Vieweg, 1998. £#/li#££#li#£ Lagrange, R. Polynomes et fonctions de Legendre. Paris: Gauthier-Villars, 1939. £#/li#££#li#£ Legendre, A. M. "Sur l'attraction des Sphéroides." Mém. Math. et Phys. présentés à l'Ac. r. des. sc. par divers savants 10, 1785. £#/li#££#li#£ Morse, P. M. and Feshbach, H. Methods of Theoretical Physics, Part I. New York: McGraw-Hill, pp. 593-597, 1953. £#/li#££#li#£ Press, W. H.; Flannery, B. P.; Teukolsky, S. A.; and Vetterling, W. T. Numerical Recipes in FORTRAN: The Art of Scientific Computing, 2nd ed. Cambridge, England: Cambridge University Press, p. 252, 1992. £#/li#££#li#£ Sansone, G. "Expansions in Series of Legendre Polynomials and Spherical Harmonics." Ch. 3 in Orthogonal Functions, rev. English ed. New York: Dover, pp. 169-294, 1991. £#/li#££#li#£ Sloane, N. J. A. Sequences A001790/M2508, A002596/M3768, A008316, A008317, A046161, A060818, A078297, and A078298 in "The On-Line Encyclopedia of Integer Sequences." £#/li#££#li#£ Snow, C. Hypergeometric and Legendre Functions with Applications to Integral Equations of Potential Theory. Washington, DC: U. S. Government Printing Office, 1952. £#/li#££#li#£ Spanier, J. and Oldham, K. B. "The Legendre Polynomials " and "The Legendre Functions and ." Chs. 21 and 59 in An Atlas of Functions. Washington, DC: Hemisphere, pp. 183-192 and 581-597, 1987. £#/li#££#li#£ Strutt, J. W. "On the Values of the Integral , , being LaPlace's Coefficients of the orders , , with an Application to the Theory of Radiation." Philos. Trans. Roy. Soc. London 160, 579-590, 1870. £#/li#££#li#£ Szegö, G. Orthogonal Polynomials, 4th ed. Providence, RI: Amer. Math. Soc., 1975. £#/li#££#li#£ Whittaker, E. T. and Watson, G. N. A Course in Modern Analysis, 4th ed. Cambridge, England: Cambridge University Press, 1990. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Orthogonal Polynomials £#/li#££#/ul#£




£#h3#£Associated Polynomial£#/h3#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Orthogonal Polynomials £#/li#££#/ul#£




£#h3#£Asymptotic£#/h3#£

In analytic geometry, an asymptote () of a curve is a line such that the distance between the curve and the line approaches zero as one or both of the x or y coordinates tends to infinity. In projective geometry and related contexts, an asymptote of a curve is a line which is tangent to the curve at a point at infinity.

The word asymptote is derived from the Greek ἀσύμπτωτος (asumptōtos) which means "not falling together", from ἀ priv. + σύν "together" + πτωτ-ός "fallen". The term was introduced by Apollonius of Perga in his work on conic sections, but in contrast to its modern meaning, he used it to mean any line that does not intersect the given curve.

There are three kinds of asymptotes: horizontal, vertical and oblique. For curves given by the graph of a function y = ƒ(x), horizontal asymptotes are horizontal lines that the graph of the function approaches as x tends to +∞ or −∞. Vertical asymptotes are vertical lines near which the function grows without bound. An oblique asymptote has a slope that is non-zero but finite, such that the graph of the function approaches it as x tends to +∞ or −∞.

More generally, one curve is a curvilinear asymptote of another (as opposed to a linear asymptote) if the distance between the two curves tends to zero as they tend to infinity, although the term asymptote by itself is usually reserved for linear asymptotes.

Asymptotes convey information about the behavior of curves in the large, and determining the asymptotes of a function is an important step in sketching its graph. The study of asymptotes of functions, construed in a broad sense, forms a part of the subject of asymptotic analysis.


£#h5#£Introduction£#/h5#£
The idea that a curve may come arbitrarily close to a line without actually becoming the same may seem to counter everyday experience. The representations of a line and a curve as marks on a piece of paper or as pixels on a computer screen have a positive width. So if they were to be extended far enough they would seem to merge, at least as far as the eye could discern. But these are physical representations of the corresponding mathematical entities; the line and the curve are idealized concepts whose width is 0 (see Line). Therefore, the understanding of the idea of an asymptote requires an effort of reason rather than experience.

Consider the graph of the function ${\displaystyle f(x)={\frac {1}{x}}}$ shown in this section. The coordinates of the points on the curve are of the form ${\displaystyle \left(x,{\frac {1}{x}}\right)}$ where x is a number other than 0. For example, the graph contains the points (1, 1), (2, 0.5), (5, 0.2), (10, 0.1), ... As the values of ${\displaystyle x}$ become larger and larger, say 100, 1,000, 10,000 ..., putting them far to the right of the illustration, the corresponding values of ${\displaystyle y}$ , .01, .001, .0001, ..., become infinitesimal relative to the scale shown. But no matter how large ${\displaystyle x}$ becomes, its reciprocal ${\displaystyle {\frac {1}{x}}}$ is never 0, so the curve never actually touches the x-axis. Similarly, as the values of ${\displaystyle x}$ become smaller and smaller, say .01, .001, .0001, ..., making them infinitesimal relative to the scale shown, the corresponding values of ${\displaystyle y}$ , 100, 1,000, 10,000 ..., become larger and larger. So the curve extends farther and farther upward as it comes closer and closer to the y-axis. Thus, both the x and y-axis are asymptotes of the curve. These ideas are part of the basis of concept of a limit in mathematics, and this connection is explained more fully below.


£#h5#£Asymptotes of functions£#/h5#£
The asymptotes most commonly encountered in the study of calculus are of curves of the form y = ƒ(x). These can be computed using limits and classified into horizontal, vertical and oblique asymptotes depending on their orientation. Horizontal asymptotes are horizontal lines that the graph of the function approaches as x tends to +∞ or −∞. As the name indicates they are parallel to the x-axis. Vertical asymptotes are vertical lines (perpendicular to the x-axis) near which the function grows without bound. Oblique asymptotes are diagonal lines such that the difference between the curve and the line approaches 0 as x tends to +∞ or −∞.


£#h5#£Vertical asymptotes£#/h5#£
The line x = a is a vertical asymptote of the graph of the function y = ƒ(x) if at least one of the following statements is true:

£#li#£ ${\displaystyle \lim _{x\to a^{-}}f(x)=\pm \infty ,}$ £#/li#£ £#li#£ ${\displaystyle \lim _{x\to a^{+}}f(x)=\pm \infty ,}$ £#/li#£
where ${\displaystyle \lim _{x\to a^{-}}}$ is the limit as x approaches the value a from the left (from lesser values), and ${\displaystyle \lim _{x\to a^{+}}}$ is the limit as x approaches a from the right.

For example, if ƒ(x) = x/(x–1), the numerator approaches 1 and the denominator approaches 0 as x approaches 1. So

${\displaystyle \lim _{x\to 1^{+}}{\frac {x}{x-1}}=+\infty }$
${\displaystyle \lim _{x\to 1^{-}}{\frac {x}{x-1}}=-\infty }$
and the curve has a vertical asymptote x = 1.

The function ƒ(x) may or may not be defined at a, and its precise value at the point x = a does not affect the asymptote. For example, for the function

${\displaystyle f(x)={\begin{cases}{\frac {1}{x}}&{\text{if }}x>0,\\5&{\text{if }}x\leq 0.\end{cases}}}$
has a limit of +∞ as x → 0+, ƒ(x) has the vertical asymptote x = 0, even though ƒ(0) = 5. The graph of this function does intersect the vertical asymptote once, at (0, 5). It is impossible for the graph of a function to intersect a vertical asymptote (or a vertical line in general) in more than one point. Moreover, if a function is continuous at each point where it is defined, it is impossible that its graph does intersect any vertical asymptote.

A common example of a vertical asymptote is the case of a rational function at a point x such that the denominator is zero and the numerator is non-zero.

If a function has a vertical asymptote, then it isn't necessarily true that the derivative of the function has a vertical asymptote at the same place. An example is

${\displaystyle f(x)={\tfrac {1}{x}}+\sin({\tfrac {1}{x}})\quad }$ at ${\displaystyle \quad x=0}$ .
This function has a vertical asymptote at ${\displaystyle x=0,}$ because

${\displaystyle \lim _{x\to 0^{+}}f(x)=\lim _{x\to 0^{+}}\left({\tfrac {1}{x}}+\sin \left({\tfrac {1}{x}}\right)\right)=+\infty ,}$
and

${\displaystyle \lim _{x\to 0^{-}}f(x)=\lim _{x\to 0^{-}}\left({\tfrac {1}{x}}+\sin \left({\tfrac {1}{x}}\right)\right)=-\infty }$ .
The derivative of ${\displaystyle f}$ is the function

${\displaystyle f'(x)={\frac {-(\cos({\tfrac {1}{x}})+1)}{x^{2}}}}$ .
For the sequence of points

${\displaystyle x_{n}={\frac {(-1)^{n}}{(2n+1)\pi }},\quad }$ for ${\displaystyle \quad n=0,1,2,\ldots }$
that approaches ${\displaystyle x=0}$ both from the left and from the right, the values ${\displaystyle f'(x_{n})}$ are constantly ${\displaystyle 0}$ . Therefore, both one-sided limits of ${\displaystyle f'}$ at ${\displaystyle 0}$ can be neither ${\displaystyle +\infty }$ nor ${\displaystyle -\infty }$ . Hence ${\displaystyle f'(x)}$ doesn't have a vertical asymptote at ${\displaystyle x=0}$ .


£#h5#£Horizontal asymptotes£#/h5#£
Horizontal asymptotes are horizontal lines that the graph of the function approaches as x → ±∞. The horizontal line y = c is a horizontal asymptote of the function y = ƒ(x) if

${\displaystyle \lim _{x\rightarrow -\infty }f(x)=c}$ or ${\displaystyle \lim _{x\rightarrow +\infty }f(x)=c}$ .
In the first case, ƒ(x) has y = c as asymptote when x tends to −∞, and in the second ƒ(x) has y = c as an asymptote as x tends to +∞.

For example, the arctangent function satisfies

${\displaystyle \lim _{x\rightarrow -\infty }\arctan(x)=-{\frac {\pi }{2}}}$ and ${\displaystyle \lim _{x\rightarrow +\infty }\arctan(x)={\frac {\pi }{2}}.}$
So the line y = –π/2 is a horizontal asymptote for the arctangent when x tends to –∞, and y = π/2 is a horizontal asymptote for the arctangent when x tends to +∞.

Functions may lack horizontal asymptotes on either or both sides, or may have one horizontal asymptote that is the same in both directions. For example, the function ƒ(x) = 1/(x2+1) has a horizontal asymptote at y = 0 when x tends both to −∞ and +∞ because, respectively,

${\displaystyle \lim _{x\to -\infty }{\frac {1}{x^{2}+1}}=\lim _{x\to +\infty }{\frac {1}{x^{2}+1}}=0.}$
Other common functions that have one or two horizontal asymptotes include x ↦ 1/x (that has an hyperbola as it graph), the Gaussian function ${\displaystyle x\mapsto \exp(-x^{2}),}$ the error function, and the logistic function.


£#h5#£Oblique asymptotes£#/h5#£
When a linear asymptote is not parallel to the x- or y-axis, it is called an oblique asymptote or slant asymptote. A function ƒ(x) is asymptotic to the straight line y = mx + n (m ≠ 0) if

${\displaystyle \lim _{x\to +\infty }\left[f(x)-(mx+n)\right]=0\,{\mbox{ or }}\lim _{x\to -\infty }\left[f(x)-(mx+n)\right]=0.}$
In the first case the line y = mx + n is an oblique asymptote of ƒ(x) when x tends to +∞, and in the second case the line y = mx + n is an oblique asymptote of ƒ(x) when x tends to −∞.

An example is ƒ(x) = x + 1/x, which has the oblique asymptote y = x (that is m = 1, n = 0) as seen in the limits

${\displaystyle \lim _{x\to \pm \infty }\left[f(x)-x\right]}$
${\displaystyle =\lim _{x\to \pm \infty }\left[\left(x+{\frac {1}{x}}\right)-x\right]}$
${\displaystyle =\lim _{x\to \pm \infty }{\frac {1}{x}}=0.}$

£#h5#£Elementary methods for identifying asymptotes£#/h5#£
The asymptotes of many elementary functions can be found without the explicit use of limits (although the derivations of such methods typically use limits).


£#h5#£General computation of oblique asymptotes for functions£#/h5#£
The oblique asymptote, for the function f(x), will be given by the equation y = mx + n. The value for m is computed first and is given by

${\displaystyle m\;{\stackrel {\text{def}}{=}}\,\lim _{x\rightarrow a}f(x)/x}$
where a is either ${\displaystyle -\infty }$ or ${\displaystyle +\infty }$ depending on the case being studied. It is good practice to treat the two cases separately. If this limit doesn't exist then there is no oblique asymptote in that direction.

Having m then the value for n can be computed by

${\displaystyle n\;{\stackrel {\text{def}}{=}}\,\lim _{x\rightarrow a}(f(x)-mx)}$
where a should be the same value used before. If this limit fails to exist then there is no oblique asymptote in that direction, even should the limit defining m exist. Otherwise y = mx + n is the oblique asymptote of ƒ(x) as x tends to a.

For example, the function ƒ(x) = (2x2 + 3x + 1)/x has

${\displaystyle m=\lim _{x\rightarrow +\infty }f(x)/x=\lim _{x\rightarrow +\infty }{\frac {2x^{2}+3x+1}{x^{2}}}=2}$ and then
${\displaystyle n=\lim _{x\rightarrow +\infty }(f(x)-mx)=\lim _{x\rightarrow +\infty }\left({\frac {2x^{2}+3x+1}{x}}-2x\right)=3}$
so that y = 2x + 3 is the asymptote of ƒ(x) when x tends to +∞.

The function ƒ(x) = ln x has

${\displaystyle m=\lim _{x\rightarrow +\infty }f(x)/x=\lim _{x\rightarrow +\infty }{\frac {\ln x}{x}}=0}$ and then
${\displaystyle n=\lim _{x\rightarrow +\infty }(f(x)-mx)=\lim _{x\rightarrow +\infty }\ln x}$ , which does not exist.
So y = ln x does not have an asymptote when x tends to +∞.


£#h5#£Asymptotes for rational functions£#/h5#£
A rational function has at most one horizontal asymptote or oblique (slant) asymptote, and possibly many vertical asymptotes.

The degree of the numerator and degree of the denominator determine whether or not there are any horizontal or oblique asymptotes. The cases are tabulated below, where deg(numerator) is the degree of the numerator, and deg(denominator) is the degree of the denominator.

The vertical asymptotes occur only when the denominator is zero (If both the numerator and denominator are zero, the multiplicities of the zero are compared). For example, the following function has vertical asymptotes at x = 0, and x = 1, but not at x = 2.

${\displaystyle f(x)={\frac {x^{2}-5x+6}{x^{3}-3x^{2}+2x}}={\frac {(x-2)(x-3)}{x(x-1)(x-2)}}}$

£#h5#£Oblique asymptotes of rational functions£#/h5#£
When the numerator of a rational function has degree exactly one greater than the denominator, the function has an oblique (slant) asymptote. The asymptote is the polynomial term after dividing the numerator and denominator. This phenomenon occurs because when dividing the fraction, there will be a linear term, and a remainder. For example, consider the function

${\displaystyle f(x)={\frac {x^{2}+x+1}{x+1}}=x+{\frac {1}{x+1}}}$
shown to the right. As the value of x increases, f approaches the asymptote y = x. This is because the other term, 1/(x+1), approaches 0.

If the degree of the numerator is more than 1 larger than the degree of the denominator, and the denominator does not divide the numerator, there will be a nonzero remainder that goes to zero as x increases, but the quotient will not be linear, and the function does not have an oblique asymptote.


£#h5#£Transformations of known functions£#/h5#£
If a known function has an asymptote (such as y=0 for f(x)=ex), then the translations of it also have an asymptote.

£#ul#££#li#£If x=a is a vertical asymptote of f(x), then x=a+h is a vertical asymptote of f(x-h)£#/li#£ £#li#£If y=c is a horizontal asymptote of f(x), then y=c+k is a horizontal asymptote of f(x)+k£#/li#££#/ul#£
If a known function has an asymptote, then the scaling of the function also have an asymptote.

£#ul#££#li#£If y=ax+b is an asymptote of f(x), then y=cax+cb is an asymptote of cf(x)£#/li#££#/ul#£
For example, f(x)=ex-1+2 has horizontal asymptote y=0+2=2, and no vertical or oblique asymptotes.


£#h5#£General definition£#/h5#£
Let A : (a,b) → R2 be a parametric plane curve, in coordinates A(t) = (x(t),y(t)). Suppose that the curve tends to infinity, that is:

${\displaystyle \lim _{t\rightarrow b}(x^{2}(t)+y^{2}(t))=\infty .}$
A line ℓ is an asymptote of A if the distance from the point A(t) to ℓ tends to zero as t → b. From the definition, only open curves that have some infinite branch can have an asymptote. No closed curve can have an asymptote.

For example, the upper right branch of the curve y = 1/x can be defined parametrically as x = t, y = 1/t (where t > 0). First, x → ∞ as t → ∞ and the distance from the curve to the x-axis is 1/t which approaches 0 as t → ∞. Therefore, the x-axis is an asymptote of the curve. Also, y → ∞ as t → 0 from the right, and the distance between the curve and the y-axis is t which approaches 0 as t → 0. So the y-axis is also an asymptote. A similar argument shows that the lower left branch of the curve also has the same two lines as asymptotes.

Although the definition here uses a parameterization of the curve, the notion of asymptote does not depend on the parameterization. In fact, if the equation of the line is ${\displaystyle ax+by+c=0}$ then the distance from the point A(t) = (x(t),y(t)) to the line is given by

${\displaystyle {\frac {|ax(t)+by(t)+c|}{\sqrt {a^{2}+b^{2}}}}}$
if γ(t) is a change of parameterization then the distance becomes

${\displaystyle {\frac {|ax(\gamma (t))+by(\gamma (t))+c|}{\sqrt {a^{2}+b^{2}}}}}$
which tends to zero simultaneously as the previous expression.

An important case is when the curve is the graph of a real function (a function of one real variable and returning real values). The graph of the function y = ƒ(x) is the set of points of the plane with coordinates (x,ƒ(x)). For this, a parameterization is

${\displaystyle t\mapsto (t,f(t)).}$
This parameterization is to be considered over the open intervals (a,b), where a can be −∞ and b can be +∞.

An asymptote can be either vertical or non-vertical (oblique or horizontal). In the first case its equation is x = c, for some real number c. The non-vertical case has equation y = mx + n, where m and ${\displaystyle n}$ are real numbers. All three types of asymptotes can be present at the same time in specific examples. Unlike asymptotes for curves that are graphs of functions, a general curve may have more than two non-vertical asymptotes, and may cross its vertical asymptotes more than once.


£#h5#£Curvilinear asymptotes£#/h5#£
Let A : (a,b) → R2 be a parametric plane curve, in coordinates A(t) = (x(t),y(t)), and B be another (unparameterized) curve. Suppose, as before, that the curve A tends to infinity. The curve B is a curvilinear asymptote of A if the shortest distance from the point A(t) to a point on B tends to zero as t → b. Sometimes B is simply referred to as an asymptote of A, when there is no risk of confusion with linear asymptotes.

For example, the function

${\displaystyle y={\frac {x^{3}+2x^{2}+3x+4}{x}}}$
has a curvilinear asymptote y = x2 + 2x + 3, which is known as a parabolic asymptote because it is a parabola rather than a straight line.


£#h5#£Asymptotes and curve sketching£#/h5#£
Asymptotes are used in procedures of curve sketching. An asymptote serves as a guide line to show the behavior of the curve towards infinity. In order to get better approximations of the curve, curvilinear asymptotes have also been used although the term asymptotic curve seems to be preferred.


£#h5#£Algebraic curves£#/h5#£
The asymptotes of an algebraic curve in the affine plane are the lines that are tangent to the projectivized curve through a point at infinity. For example, one may identify the asymptotes to the unit hyperbola in this manner. Asymptotes are often considered only for real curves, although they also make sense when defined in this way for curves over an arbitrary field.

A plane curve of degree n intersects its asymptote at most at n−2 other points, by Bézout's theorem, as the intersection at infinity is of multiplicity at least two. For a conic, there are a pair of lines that do not intersect the conic at any complex point: these are the two asymptotes of the conic.

A plane algebraic curve is defined by an equation of the form P(x,y) = 0 where P is a polynomial of degree n

${\displaystyle P(x,y)=P_{n}(x,y)+P_{n-1}(x,y)+\cdots +P_{1}(x,y)+P_{0}}$
where Pk is homogeneous of degree k. Vanishing of the linear factors of the highest degree term Pn defines the asymptotes of the curve: setting Q = Pn, if Pn(x, y) = (ax − by) Qn−1(x, y), then the line

${\displaystyle Q'_{x}(b,a)x+Q'_{y}(b,a)y+P_{n-1}(b,a)=0}$
is an asymptote if ${\displaystyle Q'_{x}(b,a)}$ and ${\displaystyle Q'_{y}(b,a)}$ are not both zero. If ${\displaystyle Q'_{x}(b,a)=Q'_{y}(b,a)=0}$ and ${\displaystyle P_{n-1}(b,a)\neq 0}$ , there is no asymptote, but the curve has a branch that looks like a branch of parabola. Such a branch is called a parabolic branch, even when it does not have any parabola that is a curvilinear asymptote. If ${\displaystyle Q'_{x}(b,a)=Q'_{y}(b,a)=P_{n-1}(b,a)=0,}$ the curve has a singular point at infinity which may have several asymptotes or parabolic branches.

Over the complex numbers, Pn splits into linear factors, each of which defines an asymptote (or several for multiple factors). Over the reals, Pn splits in factors that are linear or quadratic factors. Only the linear factors correspond to infinite (real) branches of the curve, but if a linear factor has multiplicity greater than one, the curve may have several asymptotes or parabolic branches. It may also occur that such a multiple linear factor corresponds to two complex conjugate branches, and does not corresponds to any infinite branch of the real curve. For example, the curve x4 + y2 - 1 = 0 has no real points outside the square ${\displaystyle |x|\leq 1,|y|\leq 1}$ , but its highest order term gives the linear factor x with multiplicity 4, leading to the unique asymptote x=0.


£#h5#£Asymptotic cone£#/h5#£
The hyperbola

${\displaystyle {\frac {x^{2}}{a^{2}}}-{\frac {y^{2}}{b^{2}}}=1}$
has the two asymptotes

${\displaystyle y=\pm {\frac {b}{a}}x.}$
The equation for the union of these two lines is

${\displaystyle {\frac {x^{2}}{a^{2}}}-{\frac {y^{2}}{b^{2}}}=0.}$
Similarly, the hyperboloid

${\displaystyle {\frac {x^{2}}{a^{2}}}-{\frac {y^{2}}{b^{2}}}-{\frac {z^{2}}{c^{2}}}=1}$
is said to have the asymptotic cone

${\displaystyle {\frac {x^{2}}{a^{2}}}-{\frac {y^{2}}{b^{2}}}-{\frac {z^{2}}{c^{2}}}=0.}$
The distance between the hyperboloid and cone approaches 0 as the distance from the origin approaches infinity.

More generally, consider a surface that has an implicit equation ${\displaystyle P_{d}(x,y,z)+P_{d-2}(x,y,z)+\cdots P_{0}=0,}$ where the ${\displaystyle P_{i}}$ are homogeneous polynomials of degree ${\displaystyle i}$ and ${\displaystyle P_{d-1}=0}$ . Then the equation ${\displaystyle P_{d}(x,y,z)=0}$ defines a cone which is centered at the origin. It is called an asymptotic cone, because the distance to the cone of a point of the surface tends to zero when the point on the surface tends to infinity.


£#h5#£See also£#/h5#£ £#ul#££#li#£Big O notation£#/li#££#/ul#£
£#h5#£References£#/h5#£
General references
£#ul#££#li#£Kuptsov, L.P. (2001) [1994], "Asymptote", Encyclopedia of Mathematics, EMS Press£#/li#££#/ul#£
Specific references

£#h5#£External links£#/h5#£ £#ul#££#li#£Asymptote at PlanetMath.£#/li#£ £#li#£Hyperboloid and Asymptotic Cone, string surface model, 1872 from the Science Museum£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Hardy, G. H. and Wright, E. M. An Introduction to the Theory of Numbers, 5th ed. Oxford, England: Clarendon Press, pp. 7-8, 1979.£#/li#££#li#£ Hardy, G. H. and Wright, E. M. An Introduction to the Theory of Numbers, 5th ed. Oxford, England: Clarendon Press, pp. 7-8, 1979. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Calculus > Limits £#/li#££#li#£ History and Terminology > Terminology £#/li#££#/ul#£




£#h3#£Asymptotic Curve£#/h3#£

In the differential geometry of surfaces, an asymptotic curve is a curve always tangent to an asymptotic direction of the surface (where they exist). It is sometimes called an asymptotic line, although it need not be a line.


£#h5#£Definitions£#/h5#£
An asymptotic direction is one in which the normal curvature is zero. Which is to say: for a point on an asymptotic curve, take the plane which bears both the curve's tangent and the surface's normal at that point. The curve of intersection of the plane and the surface will have zero curvature at that point. Asymptotic directions can only occur when the Gaussian curvature is negative (or zero). There will be two asymptotic directions through every point with negative Gaussian curvature, bisected by the principal directions. If the surface is minimal, the asymptotic directions are orthogonal to one another.


£#h5#£Related notions£#/h5#£
The direction of the asymptotic direction are the same as the asymptotes of the hyperbola of the Dupin indicatrix.

A related notion is a curvature line, which is a curve always tangent to a principal direction.


£#h5#£References£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Asymptotic Curve". MathWorld.£#/li#£ £#li#£Lines of Curvature, Geodesic Torsion, Asymptotic Lines£#/li#£ £#li#£"Asymptotic line of a surface" at Encyclopédie des Formes Mathématiques Remarquables (in French)£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Gray, A. "Asymptotic Curves," "Examples of Asymptotic Curves," and "Using Mathematica to Find Asymptotic Curves." §18.1, 18.2, and 18.3 in Modern Differential Geometry of Curves and Surfaces with Mathematica, 2nd ed. Boca Raton, FL: CRC Press, pp. 417-429, 1997.£#/li#££#li#£ Gray, A. "Asymptotic Curves," "Examples of Asymptotic Curves," and "Using Mathematica to Find Asymptotic Curves." §18.1, 18.2, and 18.3 in Modern Differential Geometry of Curves and Surfaces with Mathematica, 2nd ed. Boca Raton, FL: CRC Press, pp. 417-429, 1997. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Differential Geometry > General Differential Geometry £#/li#££#/ul#£




£#h3#£Asymptotic Direction£#/h3#£

In the differential geometry of surfaces, an asymptotic curve is a curve always tangent to an asymptotic direction of the surface (where they exist). It is sometimes called an asymptotic line, although it need not be a line.


£#h5#£Definitions£#/h5#£
An asymptotic direction is one in which the normal curvature is zero. Which is to say: for a point on an asymptotic curve, take the plane which bears both the curve's tangent and the surface's normal at that point. The curve of intersection of the plane and the surface will have zero curvature at that point. Asymptotic directions can only occur when the Gaussian curvature is negative (or zero). There will be two asymptotic directions through every point with negative Gaussian curvature, bisected by the principal directions. If the surface is minimal, the asymptotic directions are orthogonal to one another.


£#h5#£Related notions£#/h5#£
The direction of the asymptotic direction are the same as the asymptotes of the hyperbola of the Dupin indicatrix.

A related notion is a curvature line, which is a curve always tangent to a principal direction.


£#h5#£References£#/h5#£ £#ul#££#li#£Weisstein, Eric W. "Asymptotic Curve". MathWorld.£#/li#£ £#li#£Lines of Curvature, Geodesic Torsion, Asymptotic Lines£#/li#£ £#li#£"Asymptotic line of a surface" at Encyclopédie des Formes Mathématiques Remarquables (in French)£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Gray, A. Modern Differential Geometry of Curves and Surfaces with Mathematica, 2nd ed. Boca Raton, FL: CRC Press, pp. 364 and 418, 1997.£#/li#££#li#£ Gray, A. Modern Differential Geometry of Curves and Surfaces with Mathematica, 2nd ed. Boca Raton, FL: CRC Press, pp. 364 and 418, 1997. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Differential Geometry > General Differential Geometry £#/li#££#/ul#£




£#h3#£Asymptotic Expansion£#/h3#£

In mathematics, an asymptotic expansion, asymptotic series or Poincaré expansion (after Henri Poincaré) is a formal series of functions which has the property that truncating the series after a finite number of terms provides an approximation to a given function as the argument of the function tends towards a particular, often infinite, point. Investigations by Dingle (1973) revealed that the divergent part of an asymptotic expansion is latently meaningful, i.e. contains information about the exact value of the expanded function.

The most common type of asymptotic expansion is a power series in either positive or negative powers. Methods of generating such expansions include the Euler–Maclaurin summation formula and integral transforms such as the Laplace and Mellin transforms. Repeated integration by parts will often lead to an asymptotic expansion.

Since a convergent Taylor series fits the definition of asymptotic expansion as well, the phrase "asymptotic series" usually implies a non-convergent series. Despite non-convergence, the asymptotic expansion is useful when truncated to a finite number of terms. The approximation may provide benefits by being more mathematically tractable than the function being expanded, or by an increase in the speed of computation of the expanded function. Typically, the best approximation is given when the series is truncated at the smallest term. This way of optimally truncating an asymptotic expansion is known as superasymptotics. The error is then typically of the form ~ exp(−c/ε) where ε is the expansion parameter. The error is thus beyond all orders in the expansion parameter. It is possible to improve on the superasymptotic error, e.g. by employing resummation methods such as Borel resummation to the divergent tail. Such methods are often referred to as hyperasymptotic approximations.

See asymptotic analysis and big O notation for the notation used in this article.


£#h5#£Formal definition£#/h5#£
First we define an asymptotic scale, and then give the formal definition of an asymptotic expansion.

If ${\displaystyle \ \varphi _{n}\ }$ is a sequence of continuous functions on some domain, and if ${\displaystyle \ L\ }$ is a limit point of the domain, then the sequence constitutes an asymptotic scale if for every n,

${\displaystyle \varphi _{n+1}(x)=o(\varphi _{n}(x))\ (x\to L)\ .}$
( ${\displaystyle \ L\ }$ may be taken to be infinity.) In other words, a sequence of functions is an asymptotic scale if each function in the sequence grows strictly slower (in the limit ${\displaystyle \ x\to L\ }$ ) than the preceding function.

If ${\displaystyle \ f\ }$ is a continuous function on the domain of the asymptotic scale, then f has an asymptotic expansion of order ${\displaystyle \ N\ }$ with respect to the scale as a formal series

${\displaystyle \sum _{n=0}^{N}a_{n}\varphi _{n}(x)}$
if

${\displaystyle f(x)-\sum _{n=0}^{N-1}a_{n}\varphi _{n}(x)=O(\varphi _{N}(x))\ (x\to L)}$
or

${\displaystyle f(x)-\sum _{n=0}^{N-1}a_{n}\varphi _{n}(x)=o(\varphi _{N-1}(x))\ (x\to L)\ .}$
If one or the other holds for all ${\displaystyle \ N\ }$ , then we write

${\displaystyle f(x)\sim \sum _{n=0}^{\infty }a_{n}\varphi _{n}(x)\ (x\to L)\ .}$
In contrast to a convergent series for ${\displaystyle \ f\ }$ , wherein the series converges for any fixed ${\displaystyle \ x\ }$ in the limit ${\displaystyle N\to \infty }$ , one can think of the asymptotic series as converging for fixed ${\displaystyle \ N\ }$ in the limit ${\displaystyle \ x\to L\ }$ (with ${\displaystyle \ L\ }$ possibly infinite).


£#h5#£Examples£#/h5#£ £#ul#££#li#£Gamma function (Stirling's approximation)£#/li#£ £#li#£Exponential integral£#/li#£ £#li#£Logarithmic integral£#/li#£ £#li#£Riemann zeta functionwhere ${\displaystyle B_{2m}}$ are Bernoulli numbers and ${\displaystyle s^{\overline {2m-1}}}$ is a rising factorial. This expansion is valid for all complex s and is often used to compute the zeta function by using a large enough value of N, for instance ${\displaystyle N>|s|}$ .£#/li#£ £#li#£Error function where (2n − 1)!! is the double factorial.£#/li#££#/ul#£
£#h5#£Worked example£#/h5#£
Asymptotic expansions often occur when an ordinary series is used in a formal expression that forces the taking of values outside of its domain of convergence. Thus, for example, one may start with the ordinary series

${\displaystyle {\frac {1}{1-w}}=\sum _{n=0}^{\infty }w^{n}.}$
The expression on the left is valid on the entire complex plane ${\displaystyle w\neq 1}$ , while the right hand side converges only for ${\displaystyle |w|<1}$ . Multiplying by ${\displaystyle e^{-w/t}}$ and integrating both sides yields

${\displaystyle \int _{0}^{\infty }{\frac {e^{-{\frac {w}{t}}}}{1-w}}\,dw=\sum _{n=0}^{\infty }t^{n+1}\int _{0}^{\infty }e^{-u}u^{n}\,du,}$
after the substitution ${\displaystyle u=w/t}$ on the right hand side. The integral on the left hand side, understood as a Cauchy principal value, can be expressed in terms of the exponential integral. The integral on the right hand side may be recognized as the gamma function. Evaluating both, one obtains the asymptotic expansion

${\displaystyle e^{-{\frac {1}{t}}}\operatorname {Ei} \left({\frac {1}{t}}\right)=\sum _{n=0}^{\infty }n!t^{n+1}.}$
Here, the right hand side is clearly not convergent for any non-zero value of t. However, by truncating the series on the right to a finite number of terms, one may obtain a fairly good approximation to the value of ${\displaystyle \operatorname {Ei} \left({\tfrac {1}{t}}\right)}$ for sufficiently small t. Substituting ${\displaystyle x=-{\tfrac {1}{t}}}$ and noting that ${\displaystyle \operatorname {Ei} (x)=-E_{1}(-x)}$ results in the asymptotic expansion given earlier in this article.


£#h5#£Properties£#/h5#£
£#h5#£Uniqueness for a given asymptotic scale£#/h5#£
For a given asymptotic scale ${\displaystyle \{\varphi _{n}(x)\}}$ the asymptotic expansion of function ${\displaystyle f(x)}$ is unique. That is the coefficients ${\displaystyle \{a_{n}\}}$ are uniquely determined in the following way:

where ${\displaystyle L}$ is the limit point of this asymptotic expansion (may be ${\displaystyle \pm \infty }$ ).
£#h5#£Non-uniqueness for a given function£#/h5#£
A given function ${\displaystyle f(x)}$ may have many asymptotic expansions (each with a different asymptotic scale).


£#h5#£Subdominance£#/h5#£
An asymptotic expansion may be asymptotic expansion to more than one function.


£#h5#£See also£#/h5#£
£#h5#£Related fields£#/h5#£ £#ul#££#li#£Asymptotic analysis£#/li#£ £#li#£Singular perturbation£#/li#££#/ul#£
£#h5#£Asymptotic methods£#/h5#£ £#ul#££#li#£Watson's lemma£#/li#£ £#li#£Mellin transform£#/li#£ £#li#£Laplace's method£#/li#£ £#li#£Stationary phase approximation£#/li#£ £#li#£Method of steepest descent£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Ablowitz, M. J., & Fokas, A. S. (2003). Complex variables: introduction and applications. Cambridge University Press.£#/li#£ £#li#£Bender, C. M., & Orszag, S. A. (2013). Advanced mathematical methods for scientists and engineers I: Asymptotic methods and perturbation theory. Springer Science & Business Media.£#/li#£ £#li#£Bleistein, N., Handelsman, R. (1975), Asymptotic Expansions of Integrals, Dover Publications.£#/li#£ £#li#£Carrier, G. F., Krook, M., & Pearson, C. E. (2005). Functions of a complex variable: Theory and technique. Society for Industrial and Applied Mathematics.£#/li#£ £#li#£Copson, E. T. (1965), Asymptotic Expansions, Cambridge University Press.£#/li#£ £#li#£Dingle, R. B. (1973), Asymptotic Expansions: Their Derivation and Interpretation, Academic Press.£#/li#£ £#li#£Erdélyi, A. (1955), Asymptotic Expansions, Dover Publications.£#/li#£ £#li#£Fruchard, A., Schäfke, R. (2013), Composite Asymptotic Expansions, Springer.£#/li#£ £#li#£Hardy, G. H. (1949), Divergent Series, Oxford University Press.£#/li#£ £#li#£Olver, F. (1997). Asymptotics and Special functions. AK Peters/CRC Press.£#/li#£ £#li#£Paris, R. B., Kaminsky, D. (2001), Asymptotics and Mellin-Barnes Integrals, Cambridge University Press.£#/li#£ £#li#£Whittaker, E. T., Watson, G. N. (1963), A Course of Modern Analysis, fourth edition, Cambridge University Press.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Asymptotic expansion", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#£ £#li#£Wolfram Mathworld: Asymptotic Series£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Series > Asymptotic Series £#/li#££#/ul#£




£#h3#£Asymptotic Notation£#/h3#£

Big O notation is a mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity. Big O is a member of a family of notations invented by Paul Bachmann, Edmund Landau, and others, collectively called Bachmann–Landau notation or asymptotic notation. The letter O was chosen by Bachmann to stand for Ordnung, meaning the order of approximation.

In computer science, big O notation is used to classify algorithms according to how their run time or space requirements grow as the input size grows. In analytic number theory, big O notation is often used to express a bound on the difference between an arithmetical function and a better understood approximation; a famous example of such a difference is the remainder term in the prime number theorem. Big O notation is also used in many other fields to provide similar estimates.

Big O notation characterizes functions according to their growth rates: different functions with the same growth rate may be represented using the same O notation. The letter O is used because the growth rate of a function is also referred to as the order of the function. A description of a function in terms of big O notation usually only provides an upper bound on the growth rate of the function.

Associated with big O notation are several related notations, using the symbols o, Ω, ω, and Θ, to describe other kinds of bounds on asymptotic growth rates.


£#h5#£Formal definition£#/h5#£
Let f, the function to be estimated, be a real or complex valued function and let g, the comparison function, be a real valued function. Let both functions be defined on some unbounded subset of the positive real numbers, and ${\displaystyle g(x)}$ be strictly positive for all large enough values of x. One writes

if the absolute value of ${\displaystyle f(x)}$ is at most a positive constant multiple of ${\displaystyle g(x)}$ for all sufficiently large values of x. That is, ${\displaystyle f(x)=O{\bigl (}g(x){\bigr )}}$ if there exists a positive real number M and a real number x0 such that
In many contexts, the assumption that we are interested in the growth rate as the variable x goes to infinity is left unstated, and one writes more simply that

The notation can also be used to describe the behavior of f near some real number a (often, a = 0): we say if there exist positive numbers ${\displaystyle \delta }$ and M such that for all x with ${\displaystyle 0<|x-a|<\delta }$ , As g(x) is chosen to be non-zero for values of x sufficiently close to a, both of these definitions can be unified using the limit superior: if
In computer science, a slightly more restrictive definition is common: ${\displaystyle f}$ and ${\displaystyle g}$ are both required to be functions from the positive integers to the nonnegative real numbers; ${\displaystyle f(x)=O{\bigl (}g(x){\bigr )}}$ if there exist positive integer numbers M and n0 such that ${\displaystyle f(n)\leq Mg(n)}$ for all ${\displaystyle n\geq n_{0}}$ . Where necessary, finite ranges are (tacitly) excluded from ${\displaystyle f}$ 's and ${\displaystyle g}$ 's domain by choosing n0 sufficiently large. (For example, ${\displaystyle \log(n)}$ is undefined at ${\displaystyle n=0}$ .)


£#h5#£Example£#/h5#£
In typical usage the O notation is asymptotical, that is, it refers to very large x. In this setting, the contribution of the terms that grow "most quickly" will eventually make the other ones irrelevant. As a result, the following simplification rules can be applied:

£#ul#££#li#£If f(x) is a sum of several terms, if there is one with largest growth rate, it can be kept, and all others omitted.£#/li#£ £#li#£If f(x) is a product of several factors, any constants (terms in the product that do not depend on x) can be omitted.£#/li#££#/ul#£
For example, let f(x) = 6x4 − 2x3 + 5, and suppose we wish to simplify this function, using O notation, to describe its growth rate as x approaches infinity. This function is the sum of three terms: 6x4, −2x3, and 5. Of these three terms, the one with the highest growth rate is the one with the largest exponent as a function of x, namely 6x4. Now one may apply the second rule: 6x4 is a product of 6 and x4 in which the first factor does not depend on x. Omitting this factor results in the simplified form x4. Thus, we say that f(x) is a "big O" of x4. Mathematically, we can write f(x) = O(x4). One may confirm this calculation using the formal definition: let f(x) = 6x4 − 2x3 + 5 and g(x) = x4. Applying the formal definition from above, the statement that f(x) = O(x4) is equivalent to its expansion,

for some suitable choice of x0 and M and for all x > x0. To prove this, let x0 = 1 and M = 13. Then, for all x > x0: so
£#h5#£Usage£#/h5#£
Big O notation has two main areas of application:

£#ul#££#li#£In mathematics, it is commonly used to describe how closely a finite series approximates a given function, especially in the case of a truncated Taylor series or asymptotic expansion£#/li#£ £#li#£In computer science, it is useful in the analysis of algorithms£#/li#££#/ul#£
In both applications, the function g(x) appearing within the O(·) is typically chosen to be as simple as possible, omitting constant factors and lower order terms.

There are two formally close, but noticeably different, usages of this notation:

£#ul#££#li#£infinite asymptotics£#/li#£ £#li#£infinitesimal asymptotics.£#/li#££#/ul#£
This distinction is only in application and not in principle, however—the formal definition for the "big O" is the same for both cases, only with different limits for the function argument.


£#h5#£Infinite asymptotics£#/h5#£
Big O notation is useful when analyzing algorithms for efficiency. For example, the time (or the number of steps) it takes to complete a problem of size n might be found to be T(n) = 4n2 − 2n + 2. As n grows large, the n2 term will come to dominate, so that all other terms can be neglected—for instance when n = 500, the term 4n2 is 1000 times as large as the 2n term. Ignoring the latter would have negligible effect on the expression's value for most purposes. Further, the coefficients become irrelevant if we compare to any other order of expression, such as an expression containing a term n3 or n4. Even if T(n) = 1,000,000n2, if U(n) = n3, the latter will always exceed the former once n grows larger than 1,000,000 (T(1,000,000) = 1,000,0003 = U(1,000,000)). Additionally, the number of steps depends on the details of the machine model on which the algorithm runs, but different types of machines typically vary by only a constant factor in the number of steps needed to execute an algorithm. So the big O notation captures what remains: we write either

${\displaystyle T(n)=O(n^{2})}$
or

${\displaystyle T(n)\in O(n^{2})}$
and say that the algorithm has order of n2 time complexity. The sign "=" is not meant to express "is equal to" in its normal mathematical sense, but rather a more colloquial "is", so the second expression is sometimes considered more accurate (see the "Equals sign" discussion below) while the first is considered by some as an abuse of notation.


£#h5#£Infinitesimal asymptotics£#/h5#£
Big O can also be used to describe the error term in an approximation to a mathematical function. The most significant terms are written explicitly, and then the least-significant terms are summarized in a single big O term. Consider, for example, the exponential series and two expressions of it that are valid when x is small:

${\displaystyle {\begin{aligned}e^{x}&=1+x+{\frac {x^{2}}{2!}}+{\frac {x^{3}}{3!}}+{\frac {x^{4}}{4!}}+\dotsb &{\text{for all }}x\\[4pt]&=1+x+{\frac {x^{2}}{2}}+O(x^{3})&{\text{as }}x\to 0\\[4pt]&=1+x+O(x^{2})&{\text{as }}x\to 0\end{aligned}}}$
The second expression (the one with O(x3)) means the absolute-value of the error ex − (1 + x + x2/2) is at most some constant times |x3| when x is close enough to 0.


£#h5#£Properties£#/h5#£
If the function f can be written as a finite sum of other functions, then the fastest growing one determines the order of f(n). For example,

${\displaystyle f(n)=9\log n+5(\log n)^{4}+3n^{2}+2n^{3}=O(n^{3})\qquad {\text{as }}n\to \infty .}$
In particular, if a function may be bounded by a polynomial in n, then as n tends to infinity, one may disregard lower-order terms of the polynomial. The sets O(nc) and O(cn) are very different. If c is greater than one, then the latter grows much faster. A function that grows faster than nc for any c is called superpolynomial. One that grows more slowly than any exponential function of the form cn is called subexponential. An algorithm can require time that is both superpolynomial and subexponential; examples of this include the fastest known algorithms for integer factorization and the function nlog n.

We may ignore any powers of n inside of the logarithms. The set O(log n) is exactly the same as O(log(nc)). The logarithms differ only by a constant factor (since log(nc) = c log n) and thus the big O notation ignores that. Similarly, logs with different constant bases are equivalent. On the other hand, exponentials with different bases are not of the same order. For example, 2n and 3n are not of the same order.

Changing units may or may not affect the order of the resulting algorithm. Changing units is equivalent to multiplying the appropriate variable by a constant wherever it appears. For example, if an algorithm runs in the order of n2, replacing n by cn means the algorithm runs in the order of c2n2, and the big O notation ignores the constant c2. This can be written as c2n2 = O(n2). If, however, an algorithm runs in the order of 2n, replacing n with cn gives 2cn = (2c)n. This is not equivalent to 2n in general. Changing variables may also affect the order of the resulting algorithm. For example, if an algorithm's run time is O(n) when measured in terms of the number n of digits of an input number x, then its run time is O(log x) when measured as a function of the input number x itself, because n = O(log x).


£#h5#£Product£#/h5#£
${\displaystyle f_{1}=O(g_{1}){\text{ and }}f_{2}=O(g_{2})\Rightarrow f_{1}f_{2}=O(g_{1}g_{2})}$
${\displaystyle f\cdot O(g)=O(fg)}$

£#h5#£Sum£#/h5#£
If ${\displaystyle f_{1}=O(g_{1})}$ and ${\displaystyle f_{2}=O(g_{2})}$ then ${\displaystyle f_{1}+f_{2}=O(\max(g_{1},g_{2}))}$ . It follows that if ${\displaystyle f_{1}=O(g)}$ and ${\displaystyle f_{2}=O(g)}$ then ${\displaystyle f_{1}+f_{2}\in O(g)}$ . In other words, this second statement says that ${\displaystyle O(g)}$ is a convex cone.


£#h5#£Multiplication by a constant£#/h5#£
Let k be a nonzero constant. Then ${\displaystyle O(|k|\cdot g)=O(g)}$ . In other words, if ${\displaystyle f=O(g)}$ , then ${\displaystyle k\cdot f=O(g).}$


£#h5#£Multiple variables£#/h5#£
Big O (and little o, Ω, etc.) can also be used with multiple variables. To define big O formally for multiple variables, suppose ${\displaystyle f}$ and ${\displaystyle g}$ are two functions defined on some subset of ${\displaystyle \mathbb {R} ^{n}}$ . We say

${\displaystyle f(\mathbf {x} ){\text{ is }}O(g(\mathbf {x} ))\quad {\text{ as }}\mathbf {x} \to \infty }$
if and only if there exist constants ${\displaystyle M}$ and ${\displaystyle C>0}$ such that ${\displaystyle |f(\mathbf {x} )|\leq C|g(\mathbf {x} )|}$ for all ${\displaystyle \mathbf {x} }$ with ${\displaystyle x_{i}\geq M}$ for some ${\displaystyle i.}$ Equivalently, the condition that ${\displaystyle x_{i}\geq M}$ for some ${\displaystyle i}$ can be written ${\displaystyle \|\mathbf {x} \|_{\infty }\geq M}$ , where ${\displaystyle \|\mathbf {x} \|_{\infty }}$ denotes the Chebyshev norm. For example, the statement

${\displaystyle f(n,m)=n^{2}+m^{3}+O(n+m)\quad {\text{ as }}n,m\to \infty }$
asserts that there exist constants C and M such that

${\displaystyle |f(n,m)-(n^{2}+m^{3})|\leq C|n+m|}$
whenever either ${\displaystyle m\geq M}$ or ${\displaystyle n\geq M}$ holds. This definition allows all of the coordinates of ${\displaystyle \mathbf {x} }$ to increase to infinity. In particular, the statement

${\displaystyle f(n,m)=O(n^{m})\quad {\text{ as }}n,m\to \infty }$
(i.e., ${\displaystyle \exists C\,\exists M\,\forall n\,\forall m\,\cdots }$ ) is quite different from

${\displaystyle \forall m\colon ~f(n,m)=O(n^{m})\quad {\text{ as }}n\to \infty }$
(i.e., ${\displaystyle \forall m\,\exists C\,\exists M\,\forall n\,\cdots }$ ).

Under this definition, the subset on which a function is defined is significant when generalizing statements from the univariate setting to the multivariate setting. For example, if ${\displaystyle f(n,m)=1}$ and ${\displaystyle g(n,m)=n}$ , then ${\displaystyle f(n,m)=O(g(n,m))}$ if we restrict ${\displaystyle f}$ and ${\displaystyle g}$ to ${\displaystyle [1,\infty )^{2}}$ , but not if they are defined on ${\displaystyle [0,\infty )^{2}}$ .

This is not the only generalization of big O to multivariate functions, and in practice, there is some inconsistency in the choice of definition.


£#h5#£Matters of notation£#/h5#£
£#h5#£Equals sign£#/h5#£
The statement "f(x) is O(g(x))" as defined above is usually written as f(x) = O(g(x)). Some consider this to be an abuse of notation, since the use of the equals sign could be misleading as it suggests a symmetry that this statement does not have. As de Bruijn says, O(x) = O(x2) is true but O(x2) = O(x) is not. Knuth describes such statements as "one-way equalities", since if the sides could be reversed, "we could deduce ridiculous things like n = n2 from the identities n = O(n2) and n2 = O(n2)." In another letter, Knuth also pointed out that "the equality sign is not symmetric with respect to such notations", as, in this notation, "mathematicians customarily use the = sign as they use the word "is" in English: Aristotle is a man, but a man isn't necessarily Aristotle".

For these reasons, it would be more precise to use set notation and write f(x) ∈ O(g(x)) (read as: "f(x) is an element of O(g(x))", or "f(x) is in the set O(g(x))"), thinking of O(g(x)) as the class of all functions h(x) such that |h(x)| ≤ C|g(x)| for some constant C. However, the use of the equals sign is customary.


£#h5#£Other arithmetic operators£#/h5#£
Big O notation can also be used in conjunction with other arithmetic operators in more complicated equations. For example, h(x) + O(f(x)) denotes the collection of functions having the growth of h(x) plus a part whose growth is limited to that of f(x). Thus,

${\displaystyle g(x)=h(x)+O(f(x))}$
expresses the same as

${\displaystyle g(x)-h(x)=O(f(x)).}$

£#h5#£Example £#/h5#£
Suppose an algorithm is being developed to operate on a set of n elements. Its developers are interested in finding a function T(n) that will express how long the algorithm will take to run (in some arbitrary measurement of time) in terms of the number of elements in the input set. The algorithm works by first calling a subroutine to sort the elements in the set and then perform its own operations. The sort has a known time complexity of O(n2), and after the subroutine runs the algorithm must take an additional 55n3 + 2n + 10 steps before it terminates. Thus the overall time complexity of the algorithm can be expressed as T(n) = 55n3 + O(n2). Here the terms 2n + 10 are subsumed within the faster-growing O(n2). Again, this usage disregards some of the formal meaning of the "=" symbol, but it does allow one to use the big O notation as a kind of convenient placeholder.


£#h5#£Multiple uses£#/h5#£
In more complicated usage, O(·) can appear in different places in an equation, even several times on each side. For example, the following are true for ${\displaystyle n\to \infty }$ :

The meaning of such statements is as follows: for any functions which satisfy each O(·) on the left side, there are some functions satisfying each O(·) on the right side, such that substituting all these functions into the equation makes the two sides equal. For example, the third equation above means: "For any function f(n) = O(1), there is some function g(n) = O(en) such that nf(n) = g(n)." In terms of the "set notation" above, the meaning is that the class of functions represented by the left side is a subset of the class of functions represented by the right side. In this use the "=" is a formal symbol that unlike the usual use of "=" is not a symmetric relation. Thus for example nO(1) = O(en) does not imply the false statement O(en) = nO(1).
£#h5#£Typesetting£#/h5#£
Big O is typeset as an italicized uppercase "O", as in the following example: ${\displaystyle O(n^{2})}$ . In TeX, it is produced by simply typing O inside math mode. Unlike Greek-named Bachmann–Landau notations, it needs no special symbol. Yet, some authors use the calligraphic variant ${\displaystyle {\mathcal {O}}}$ instead.


£#h5#£Orders of common functions£#/h5#£
Here is a list of classes of functions that are commonly encountered when analyzing the running time of an algorithm. In each case, c is a positive constant and n increases without bound. The slower-growing functions are generally listed first.

The statement ${\displaystyle f(n)=O(n!)}$ is sometimes weakened to ${\displaystyle f(n)=O\left(n^{n}\right)}$ to derive simpler formulas for asymptotic complexity. For any ${\displaystyle k>0}$ and ${\displaystyle c>0}$ , ${\displaystyle O(n^{c}(\log n)^{k})}$ is a subset of ${\displaystyle O(n^{c+\varepsilon })}$ for any ${\displaystyle \varepsilon >0}$ , so may be considered as a polynomial with some bigger order.


£#h5#£Related asymptotic notations£#/h5#£
Big O is widely used in computer science. Together with some other related notations it forms the family of Bachmann–Landau notations.


£#h5#£Little-o notation£#/h5#£
Intuitively, the assertion "f(x) is o(g(x))" (read "f(x) is little-o of g(x)") means that g(x) grows much faster than f(x). Let as before f be a real or complex valued function and g a real valued function, both defined on some unbounded subset of the positive real numbers, such that g(x) is strictly positive for all large enough values of x. One writes

${\displaystyle f(x)=o(g(x))\quad {\text{ as }}x\to \infty }$
if for every positive constant ε there exists a constant ${\displaystyle x_{0}}$ such that

${\displaystyle |f(x)|\leq \varepsilon g(x)\quad {\text{ for all }}x\geq x_{0}.}$
For example, one has

${\displaystyle 2x=o(x^{2})}$ and ${\displaystyle 1/x=o(1),}$     both as ${\displaystyle x\to \infty .}$
The difference between the definition of the big-O notation and the definition of little-o is that while the former has to be true for at least one constant M, the latter must hold for every positive constant ε, however small. In this way, little-o notation makes a stronger statement than the corresponding big-O notation: every function that is little-o of g is also big-O of g, but not every function that is big-O of g is also little-o of g. For example, ${\displaystyle 2x^{2}=O(x^{2})}$ but ${\displaystyle 2x^{2}\neq o(x^{2})}$ .

As g(x) is nonzero, or at least becomes nonzero beyond a certain point, the relation ${\displaystyle f(x)=o(g(x))}$ is equivalent to

${\displaystyle \lim _{x\to \infty }{\frac {f(x)}{g(x)}}=0}$ (and this is in fact how Landau originally defined the little-o notation).
Little-o respects a number of arithmetic operations. For example,

if c is a nonzero constant and ${\displaystyle f=o(g)}$ then ${\displaystyle c\cdot f=o(g)}$ , and
if ${\displaystyle f=o(F)}$ and ${\displaystyle g=o(G)}$ then ${\displaystyle f\cdot g=o(F\cdot G).}$
It also satisfies a transitivity relation:

if ${\displaystyle f=o(g)}$ and ${\displaystyle g=o(h)}$ then ${\displaystyle f=o(h).}$

£#h5#£Big Omega notation£#/h5#£
Another asymptotic notation is ${\displaystyle \Omega }$ , read "big omega". There are two widespread and incompatible definitions of the statement

${\displaystyle f(x)=\Omega (g(x))}$ as ${\displaystyle x\to a}$ ,
where a is some real number, ∞, or −∞, where f and g are real functions defined in a neighbourhood of a, and where g is positive in this neighbourhood.

The Hardy–Littlewood definition is used mainly in analytic number theory, and the Knuth definition mainly in computational complexity theory; the definitions are not equivalent.


£#h5#£The Hardy–Littlewood definition£#/h5#£
In 1914 Godfrey Harold Hardy and John Edensor Littlewood introduced the new symbol ${\displaystyle \Omega }$ , which is defined as follows:

${\displaystyle f(x)=\Omega (g(x))}$ as ${\displaystyle x\to \infty }$ if ${\displaystyle \limsup _{x\to \infty }\left|{\frac {f(x)}{g(x)}}\right|>0.}$
Thus ${\displaystyle f(x)=\Omega (g(x))}$ is the negation of ${\displaystyle f(x)=o(g(x))}$ .

In 1916 the same authors introduced the two new symbols ${\displaystyle \Omega _{R}}$ and ${\displaystyle \Omega _{L}}$ , defined as:

${\displaystyle f(x)=\Omega _{R}(g(x))}$ as ${\displaystyle x\to \infty }$ if ${\displaystyle \limsup _{x\to \infty }{\frac {f(x)}{g(x)}}>0}$ ;
${\displaystyle f(x)=\Omega _{L}(g(x))}$ as ${\displaystyle x\to \infty }$ if ${\displaystyle \liminf _{x\to \infty }{\frac {f(x)}{g(x)}}<0.}$
These symbols were used by Edmund Landau, with the same meanings, in 1924. After Landau, the notations were never used again exactly thus; ${\displaystyle \Omega _{R}}$ became ${\displaystyle \Omega _{+}}$ and ${\displaystyle \Omega _{L}}$ became ${\displaystyle \Omega _{-}}$ .

These three symbols ${\displaystyle \Omega ,\Omega _{+},\Omega _{-}}$ , as well as ${\displaystyle f(x)=\Omega _{\pm }(g(x))}$ (meaning that ${\displaystyle f(x)=\Omega _{+}(g(x))}$ and ${\displaystyle f(x)=\Omega _{-}(g(x))}$ are both satisfied), are now currently used in analytic number theory.

Simple examples
We have

${\displaystyle \sin x=\Omega (1)}$ as ${\displaystyle x\to \infty ,}$
and more precisely

${\displaystyle \sin x=\Omega _{\pm }(1)}$ as ${\displaystyle x\to \infty .}$
We have

${\displaystyle \sin x+1=\Omega (1)}$ as ${\displaystyle x\to \infty ,}$
and more precisely

${\displaystyle \sin x+1=\Omega _{+}(1)}$ as ${\displaystyle x\to \infty ;}$
however

${\displaystyle \sin x+1\not =\Omega _{-}(1)}$ as ${\displaystyle x\to \infty .}$

£#h5#£The Knuth definition£#/h5#£
In 1976 Donald Knuth published a paper to justify his use of the ${\displaystyle \Omega }$ -symbol to describe a stronger property. Knuth wrote: "For all the applications I have seen so far in computer science, a stronger requirement ... is much more appropriate". He defined

${\displaystyle f(x)=\Omega (g(x))\Leftrightarrow g(x)=O(f(x))}$
with the comment: "Although I have changed Hardy and Littlewood's definition of ${\displaystyle \Omega }$ , I feel justified in doing so because their definition is by no means in wide use, and because there are other ways to say what they want to say in the comparatively rare cases when their definition applies."


£#h5#£Family of Bachmann–Landau notations£#/h5#£
The limit definitions assume ${\displaystyle g(n)>0}$ for sufficiently large ${\displaystyle n}$ . The table is (partly) sorted from smallest to largest, in the sense that ${\displaystyle o,O,\Theta ,\sim ,}$ (Knuth's version of) ${\displaystyle \Omega ,\omega }$ on functions correspond to ${\displaystyle <,\leq ,\approx ,=,}$ ${\displaystyle \geq ,>}$ on the real line (the Hardy-Littlewood version of ${\displaystyle \Omega }$ , however, doesn't correspond to any such description).

Computer science uses the big ${\displaystyle O}$ , big Theta ${\displaystyle \Theta }$ , little ${\displaystyle o}$ , little omega ${\displaystyle \omega }$ and Knuth's big Omega ${\displaystyle \Omega }$ notations. Analytic number theory often uses the big ${\displaystyle O}$ , small ${\displaystyle o}$ , Hardy–Littlewood's big Omega ${\displaystyle \Omega }$ (with or without the +, − or ± subscripts) and ${\displaystyle \sim }$ notations. The small omega ${\displaystyle \omega }$ notation is not used as often in analysis.


£#h5#£Use in computer science£#/h5#£
Informally, especially in computer science, the big O notation often can be used somewhat differently to describe an asymptotic tight bound where using big Theta Θ notation might be more factually appropriate in a given context. For example, when considering a function T(n) = 73n3 + 22n2 + 58, all of the following are generally acceptable, but tighter bounds (such as numbers 2 and 3 below) are usually strongly preferred over looser bounds (such as number 1 below).

£#li#£T(n) = O(n100)£#/li#£ £#li#£T(n) = O(n3)£#/li#£ £#li#£T(n) = Θ(n3)£#/li#£
The equivalent English statements are respectively:

£#li#£T(n) grows asymptotically no faster than n100£#/li#£ £#li#£T(n) grows asymptotically no faster than n3£#/li#£ £#li#£T(n) grows asymptotically as fast as n3.£#/li#£
So while all three statements are true, progressively more information is contained in each. In some fields, however, the big O notation (number 2 in the lists above) would be used more commonly than the big Theta notation (items numbered 3 in the lists above). For example, if T(n) represents the running time of a newly developed algorithm for input size n, the inventors and users of the algorithm might be more inclined to put an upper asymptotic bound on how long it will take to run without making an explicit statement about the lower asymptotic bound.


£#h5#£Other notation£#/h5#£
In their book Introduction to Algorithms, Cormen, Leiserson, Rivest and Stein consider the set of functions f which satisfy

${\displaystyle f(n)=O(g(n))\quad (n\to \infty )~.}$
In a correct notation this set can, for instance, be called O(g), where

${\displaystyle O(g)=\{f:{\text{there exist positive constants}}~c~{\text{and}}~n_{0}~{\text{such that}}~0\leq f(n)\leq cg(n){\text{ for all }}n\geq n_{0}\}.}$
The authors state that the use of equality operator (=) to denote set membership rather than the set membership operator (∈) is an abuse of notation, but that doing so has advantages. Inside an equation or inequality, the use of asymptotic notation stands for an anonymous function in the set O(g), which eliminates lower-order terms, and helps to reduce inessential clutter in equations, for example:

${\displaystyle 2n^{2}+3n+1=2n^{2}+O(n).}$

£#h5#£Extensions to the Bachmann–Landau notations£#/h5#£
Another notation sometimes used in computer science is Õ (read soft-O): f(n) = Õ(g(n)) is shorthand for f(n) = O(g(n) logk n) for some k. Some authors write O* for the same purpose. Essentially, it is big O notation, ignoring logarithmic factors because the growth-rate effects of some other super-logarithmic function indicate a growth-rate explosion for large-sized input parameters that is more important to predicting bad run-time performance than the finer-point effects contributed by the logarithmic-growth factor(s). This notation is often used to obviate the "nitpicking" within growth-rates that are stated as too tightly bounded for the matters at hand (since logk n is always o(nε) for any constant k and any ε > 0).

Also the L notation, defined as

${\displaystyle L_{n}[\alpha ,c]=e^{(c+o(1))(\ln n)^{\alpha }(\ln \ln n)^{1-\alpha }}}$
is convenient for functions that are between polynomial and exponential in terms of ${\displaystyle \ln n}$ .


£#h5#£Generalizations and related usages£#/h5#£
The generalization to functions taking values in any normed vector space is straightforward (replacing absolute values by norms), where f and g need not take their values in the same space. A generalization to functions g taking values in any topological group is also possible. The "limiting process" x → xo can also be generalized by introducing an arbitrary filter base, i.e. to directed nets f and g. The o notation can be used to define derivatives and differentiability in quite general spaces, and also (asymptotical) equivalence of functions,

${\displaystyle f\sim g\iff (f-g)\in o(g)}$
which is an equivalence relation and a more restrictive notion than the relationship "f is Θ(g)" from above. (It reduces to lim f / g = 1 if f and g are positive real valued functions.) For example, 2x is Θ(x), but 2x − x is not o(x).


£#h5#£History (Bachmann–Landau, Hardy, and Vinogradov notations)£#/h5#£
The symbol O was first introduced by number theorist Paul Bachmann in 1894, in the second volume of his book Analytische Zahlentheorie ("analytic number theory"). The number theorist Edmund Landau adopted it, and was thus inspired to introduce in 1909 the notation o; hence both are now called Landau symbols. These notations were used in applied mathematics during the 1950s for asymptotic analysis. The symbol ${\displaystyle \Omega }$ (in the sense "is not an o of") was introduced in 1914 by Hardy and Littlewood. Hardy and Littlewood also introduced in 1916 the symbols ${\displaystyle \Omega _{R}}$ ("right") and ${\displaystyle \Omega _{L}}$ ("left"), precursors of the modern symbols ${\displaystyle \Omega _{+}}$ ("is not smaller than a small o of") and ${\displaystyle \Omega _{-}}$ ("is not larger than a small o of"). Thus the Omega symbols (with their original meanings) are sometimes also referred to as "Landau symbols". This notation ${\displaystyle \Omega }$ became commonly used in number theory at least since the 1950s. In the 1970s the big O was popularized in computer science by Donald Knuth, who introduced the related Theta notation, and proposed a different definition for the Omega notation.

Landau never used the big Theta and small omega symbols.

Hardy's symbols were (in terms of the modern O notation)

${\displaystyle f\preccurlyeq g\iff f\in O(g)}$   and   ${\displaystyle f\prec g\iff f\in o(g);}$
(Hardy however never defined or used the notation ${\displaystyle \prec \!\!\prec }$ , nor ${\displaystyle \ll }$ , as it has been sometimes reported). Hardy introduced the symbols ${\displaystyle \preccurlyeq }$ and ${\displaystyle \prec }$ (as well as some other symbols) in his 1910 tract "Orders of Infinity", and made use of them only in three papers (1910–1913). In his nearly 400 remaining papers and books he consistently used the Landau symbols O and o.

Hardy's notation is not used anymore. On the other hand, in the 1930s, the Russian number theorist Ivan Matveyevich Vinogradov introduced his notation ${\displaystyle \ll }$ , which has been increasingly used in number theory instead of the ${\displaystyle O}$ notation. We have

${\displaystyle f\ll g\iff f\in O(g),}$
and frequently both notations are used in the same paper.

The big-O originally stands for "order of" ("Ordnung", Bachmann 1894), and is thus a Latin letter. Neither Bachmann nor Landau ever call it "Omicron". The symbol was much later on (1976) viewed by Knuth as a capital omicron, probably in reference to his definition of the symbol Omega. The digit zero should not be used.


£#h5#£See also£#/h5#£ £#ul#££#li#£Asymptotic expansion: Approximation of functions generalizing Taylor's formula£#/li#£ £#li#£Asymptotically optimal algorithm: A phrase frequently used to describe an algorithm that has an upper bound asymptotically within a constant of a lower bound for the problem£#/li#£ £#li#£Big O in probability notation: Op, op£#/li#£ £#li#£Limit superior and limit inferior: An explanation of some of the limit notation used in this article£#/li#£ £#li#£Master theorem (analysis of algorithms): For analyzing divide-and-conquer recursive algorithms using Big O notation£#/li#£ £#li#£Nachbin's theorem: A precise method of bounding complex analytic functions so that the domain of convergence of integral transforms can be stated£#/li#£ £#li#£Orders of approximation£#/li#£ £#li#£Computational complexity of mathematical operations£#/li#££#/ul#£
£#h5#£References and notes£#/h5#£
£#h5#£Further reading£#/h5#£ £#ul#££#li#£Hardy, G. H. (1910). Orders of Infinity: The 'Infinitärcalcül' of Paul du Bois-Reymond. Cambridge University Press.£#/li#£ £#li#£Knuth, Donald (1997). "1.2.11: Asymptotic Representations". Fundamental Algorithms. The Art of Computer Programming. Vol. 1 (3rd ed.). Addison-Wesley. ISBN 978-0-201-89683-1.£#/li#£ £#li#£Cormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L.; Stein, Clifford (2001). "3.1: Asymptotic notation". Introduction to Algorithms (2nd ed.). MIT Press and McGraw-Hill. ISBN 978-0-262-03293-3.£#/li#£ £#li#£Sipser, Michael (1997). Introduction to the Theory of Computation. PWS Publishing. pp. 226–228. ISBN 978-0-534-94728-6.£#/li#£ £#li#£Avigad, Jeremy; Donnelly, Kevin (2004). Formalizing O notation in Isabelle/HOL (PDF). International Joint Conference on Automated Reasoning. doi:10.1007/978-3-540-25984-8_27.£#/li#£ £#li#£Black, Paul E. (11 March 2005). Black, Paul E. (ed.). "big-O notation". Dictionary of Algorithms and Data Structures. U.S. National Institute of Standards and Technology. Retrieved December 16, 2006.£#/li#£ £#li#£Black, Paul E. (17 December 2004). Black, Paul E. (ed.). "little-o notation". Dictionary of Algorithms and Data Structures. U.S. National Institute of Standards and Technology. Retrieved December 16, 2006.£#/li#£ £#li#£Black, Paul E. (17 December 2004). Black, Paul E. (ed.). "Ω". Dictionary of Algorithms and Data Structures. U.S. National Institute of Standards and Technology. Retrieved December 16, 2006.£#/li#£ £#li#£Black, Paul E. (17 December 2004). Black, Paul E. (ed.). "ω". Dictionary of Algorithms and Data Structures. U.S. National Institute of Standards and Technology. Retrieved December 16, 2006.£#/li#£ £#li#£Black, Paul E. (17 December 2004). Black, Paul E. (ed.). "Θ". Dictionary of Algorithms and Data Structures. U.S. National Institute of Standards and Technology. Retrieved December 16, 2006.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Growth of sequences — OEIS (Online Encyclopedia of Integer Sequences) Wiki£#/li#£ £#li#£Introduction to Asymptotic Notations£#/li#£ £#li#£Landau Symbols£#/li#£ £#li#£Big-O Notation – What is it good for£#/li#£ £#li#£Big O Notation explained in plain english£#/li#£ £#li#£An example of Big O in accuracy of central divided difference scheme for first derivative Archived 2018-10-07 at the Wayback Machine£#/li#£ £#li#£A Gentle Introduction to Algorithm Complexity Analysis£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Hardy, G. H. and Wright, E. M. "Some Notations." §1.6 in An Introduction to the Theory of Numbers, 5th ed. Oxford, England: Clarendon Press, pp. 7-8, 1979.£#/li#££#li#£Jeffreys, H. and Jeffreys, B. S. "Increasing and Decreasing Functions." §1.065 in Methods of Mathematical Physics, 3rd ed. Cambridge, England: Cambridge University Press, p. 22, 1988.£#/li#££#li#£ Hardy, G. H. and Wright, E. M. "Some Notations." §1.6 in An Introduction to the Theory of Numbers, 5th ed. Oxford, England: Clarendon Press, pp. 7-8, 1979. £#/li#££#li#£ Jeffreys, H. and Jeffreys, B. S. "Increasing and Decreasing Functions." §1.065 in Methods of Mathematical Physics, 3rd ed. Cambridge, England: Cambridge University Press, p. 22, 1988. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ History and Terminology > Notation £#/li#££#li#£ Calculus and Analysis > Series > Asymptotic Series £#/li#££#/ul#£




£#h3#£Asymptotic Series£#/h3#£

In mathematics, an asymptotic expansion, asymptotic series or Poincaré expansion (after Henri Poincaré) is a formal series of functions which has the property that truncating the series after a finite number of terms provides an approximation to a given function as the argument of the function tends towards a particular, often infinite, point. Investigations by Dingle (1973) revealed that the divergent part of an asymptotic expansion is latently meaningful, i.e. contains information about the exact value of the expanded function.

The most common type of asymptotic expansion is a power series in either positive or negative powers. Methods of generating such expansions include the Euler–Maclaurin summation formula and integral transforms such as the Laplace and Mellin transforms. Repeated integration by parts will often lead to an asymptotic expansion.

Since a convergent Taylor series fits the definition of asymptotic expansion as well, the phrase "asymptotic series" usually implies a non-convergent series. Despite non-convergence, the asymptotic expansion is useful when truncated to a finite number of terms. The approximation may provide benefits by being more mathematically tractable than the function being expanded, or by an increase in the speed of computation of the expanded function. Typically, the best approximation is given when the series is truncated at the smallest term. This way of optimally truncating an asymptotic expansion is known as superasymptotics. The error is then typically of the form ~ exp(−c/ε) where ε is the expansion parameter. The error is thus beyond all orders in the expansion parameter. It is possible to improve on the superasymptotic error, e.g. by employing resummation methods such as Borel resummation to the divergent tail. Such methods are often referred to as hyperasymptotic approximations.

See asymptotic analysis and big O notation for the notation used in this article.


£#h5#£Formal definition£#/h5#£
First we define an asymptotic scale, and then give the formal definition of an asymptotic expansion.

If ${\displaystyle \ \varphi _{n}\ }$ is a sequence of continuous functions on some domain, and if ${\displaystyle \ L\ }$ is a limit point of the domain, then the sequence constitutes an asymptotic scale if for every n,

${\displaystyle \varphi _{n+1}(x)=o(\varphi _{n}(x))\ (x\to L)\ .}$
( ${\displaystyle \ L\ }$ may be taken to be infinity.) In other words, a sequence of functions is an asymptotic scale if each function in the sequence grows strictly slower (in the limit ${\displaystyle \ x\to L\ }$ ) than the preceding function.

If ${\displaystyle \ f\ }$ is a continuous function on the domain of the asymptotic scale, then f has an asymptotic expansion of order ${\displaystyle \ N\ }$ with respect to the scale as a formal series

${\displaystyle \sum _{n=0}^{N}a_{n}\varphi _{n}(x)}$
if

${\displaystyle f(x)-\sum _{n=0}^{N-1}a_{n}\varphi _{n}(x)=O(\varphi _{N}(x))\ (x\to L)}$
or

${\displaystyle f(x)-\sum _{n=0}^{N-1}a_{n}\varphi _{n}(x)=o(\varphi _{N-1}(x))\ (x\to L)\ .}$
If one or the other holds for all ${\displaystyle \ N\ }$ , then we write

${\displaystyle f(x)\sim \sum _{n=0}^{\infty }a_{n}\varphi _{n}(x)\ (x\to L)\ .}$
In contrast to a convergent series for ${\displaystyle \ f\ }$ , wherein the series converges for any fixed ${\displaystyle \ x\ }$ in the limit ${\displaystyle N\to \infty }$ , one can think of the asymptotic series as converging for fixed ${\displaystyle \ N\ }$ in the limit ${\displaystyle \ x\to L\ }$ (with ${\displaystyle \ L\ }$ possibly infinite).


£#h5#£Examples£#/h5#£ £#ul#££#li#£Gamma function (Stirling's approximation)£#/li#£ £#li#£Exponential integral£#/li#£ £#li#£Logarithmic integral£#/li#£ £#li#£Riemann zeta functionwhere ${\displaystyle B_{2m}}$ are Bernoulli numbers and ${\displaystyle s^{\overline {2m-1}}}$ is a rising factorial. This expansion is valid for all complex s and is often used to compute the zeta function by using a large enough value of N, for instance ${\displaystyle N>|s|}$ .£#/li#£ £#li#£Error function where (2n − 1)!! is the double factorial.£#/li#££#/ul#£
£#h5#£Worked example£#/h5#£
Asymptotic expansions often occur when an ordinary series is used in a formal expression that forces the taking of values outside of its domain of convergence. Thus, for example, one may start with the ordinary series

${\displaystyle {\frac {1}{1-w}}=\sum _{n=0}^{\infty }w^{n}.}$
The expression on the left is valid on the entire complex plane ${\displaystyle w\neq 1}$ , while the right hand side converges only for ${\displaystyle |w|<1}$ . Multiplying by ${\displaystyle e^{-w/t}}$ and integrating both sides yields

${\displaystyle \int _{0}^{\infty }{\frac {e^{-{\frac {w}{t}}}}{1-w}}\,dw=\sum _{n=0}^{\infty }t^{n+1}\int _{0}^{\infty }e^{-u}u^{n}\,du,}$
after the substitution ${\displaystyle u=w/t}$ on the right hand side. The integral on the left hand side, understood as a Cauchy principal value, can be expressed in terms of the exponential integral. The integral on the right hand side may be recognized as the gamma function. Evaluating both, one obtains the asymptotic expansion

${\displaystyle e^{-{\frac {1}{t}}}\operatorname {Ei} \left({\frac {1}{t}}\right)=\sum _{n=0}^{\infty }n!t^{n+1}.}$
Here, the right hand side is clearly not convergent for any non-zero value of t. However, by truncating the series on the right to a finite number of terms, one may obtain a fairly good approximation to the value of ${\displaystyle \operatorname {Ei} \left({\tfrac {1}{t}}\right)}$ for sufficiently small t. Substituting ${\displaystyle x=-{\tfrac {1}{t}}}$ and noting that ${\displaystyle \operatorname {Ei} (x)=-E_{1}(-x)}$ results in the asymptotic expansion given earlier in this article.


£#h5#£Properties£#/h5#£
£#h5#£Uniqueness for a given asymptotic scale£#/h5#£
For a given asymptotic scale ${\displaystyle \{\varphi _{n}(x)\}}$ the asymptotic expansion of function ${\displaystyle f(x)}$ is unique. That is the coefficients ${\displaystyle \{a_{n}\}}$ are uniquely determined in the following way:

where ${\displaystyle L}$ is the limit point of this asymptotic expansion (may be ${\displaystyle \pm \infty }$ ).
£#h5#£Non-uniqueness for a given function£#/h5#£
A given function ${\displaystyle f(x)}$ may have many asymptotic expansions (each with a different asymptotic scale).


£#h5#£Subdominance£#/h5#£
An asymptotic expansion may be asymptotic expansion to more than one function.


£#h5#£See also£#/h5#£
£#h5#£Related fields£#/h5#£ £#ul#££#li#£Asymptotic analysis£#/li#£ £#li#£Singular perturbation£#/li#££#/ul#£
£#h5#£Asymptotic methods£#/h5#£ £#ul#££#li#£Watson's lemma£#/li#£ £#li#£Mellin transform£#/li#£ £#li#£Laplace's method£#/li#£ £#li#£Stationary phase approximation£#/li#£ £#li#£Method of steepest descent£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Ablowitz, M. J., & Fokas, A. S. (2003). Complex variables: introduction and applications. Cambridge University Press.£#/li#£ £#li#£Bender, C. M., & Orszag, S. A. (2013). Advanced mathematical methods for scientists and engineers I: Asymptotic methods and perturbation theory. Springer Science & Business Media.£#/li#£ £#li#£Bleistein, N., Handelsman, R. (1975), Asymptotic Expansions of Integrals, Dover Publications.£#/li#£ £#li#£Carrier, G. F., Krook, M., & Pearson, C. E. (2005). Functions of a complex variable: Theory and technique. Society for Industrial and Applied Mathematics.£#/li#£ £#li#£Copson, E. T. (1965), Asymptotic Expansions, Cambridge University Press.£#/li#£ £#li#£Dingle, R. B. (1973), Asymptotic Expansions: Their Derivation and Interpretation, Academic Press.£#/li#£ £#li#£Erdélyi, A. (1955), Asymptotic Expansions, Dover Publications.£#/li#£ £#li#£Fruchard, A., Schäfke, R. (2013), Composite Asymptotic Expansions, Springer.£#/li#£ £#li#£Hardy, G. H. (1949), Divergent Series, Oxford University Press.£#/li#£ £#li#£Olver, F. (1997). Asymptotics and Special functions. AK Peters/CRC Press.£#/li#£ £#li#£Paris, R. B., Kaminsky, D. (2001), Asymptotics and Mellin-Barnes Integrals, Cambridge University Press.£#/li#£ £#li#£Whittaker, E. T., Watson, G. N. (1963), A Course of Modern Analysis, fourth edition, Cambridge University Press.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£"Asymptotic expansion", Encyclopedia of Mathematics, EMS Press, 2001 [1994]£#/li#£ £#li#£Wolfram Mathworld: Asymptotic Series£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Abramowitz, M. and Stegun, I. A. (Eds.). Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing. New York: Dover, p. 15, 1972.£#/li#££#li#£Arfken, G. "Asymptotic of Semiconvergent Series." §5.10 in Mathematical Methods for Physicists, 3rd ed. Orlando, FL: Academic Press, pp. 339-346, 1985.£#/li#££#li#£Bleistein, N. and Handelsman, R. A. Asymptotic Expansions of Integrals. New York: Dover, 1986.£#/li#££#li#£Boyd, J. P. "The Devil's Invention: Asymptotic, Superasymptotic and Hyperasymptotic Series." Acta Appl. Math. 56, 1-98, 1999.£#/li#££#li#£Copson, E. T. Asymptotic Expansions. Cambridge, England: Cambridge University Press, 1965.£#/li#££#li#£de Bruijn, N. G. Asymptotic Methods in Analysis. New York: Dover, pp. 3-10, 1981.£#/li#££#li#£Dingle, R. B. Asymptotic Expansions: Their Derivation and Interpretation. London: Academic Press, 1973.£#/li#££#li#£Erdélyi, A. Asymptotic Expansions. New York: Dover, 1987.£#/li#££#li#£Gradshteyn, I. S. and Ryzhik, I. M. "Asymptotic Series." §0.33 in Tables of Integrals, Series, and Products, 6th ed. San Diego, CA: Academic Press, p. 20, 2000.£#/li#££#li#£Morse, P. M. and Feshbach, H. "Asymptotic Series; Method of Steepest Descent." §4.6 in Methods of Theoretical Physics, Part I. New York: McGraw-Hill, pp. 434-443, 1953.£#/li#££#li#£Olver, F. W. J. Asymptotics and Special Functions. New York: Academic Press, 1974.£#/li#££#li#£Wasow, W. R. Asymptotic Expansions for Ordinary Differential Equations. New York: Dover, 1987.£#/li#££#li#£Weisstein, E. W. "Books about Asymptotic Series." http://www.ericweisstein.com/encyclopedias/books/AsymptoticSeries.html.£#/li#££#li#£ Abramowitz, M. and Stegun, I. A. (Eds.). Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing. New York: Dover, p. 15, 1972. £#/li#££#li#£ Arfken, G. "Asymptotic of Semiconvergent Series." §5.10 in Mathematical Methods for Physicists, 3rd ed. Orlando, FL: Academic Press, pp. 339-346, 1985. £#/li#££#li#£ Bleistein, N. and Handelsman, R. A. Asymptotic Expansions of Integrals. New York: Dover, 1986. £#/li#££#li#£ Boyd, J. P. "The Devil's Invention: Asymptotic, Superasymptotic and Hyperasymptotic Series." Acta Appl. Math. 56, 1-98, 1999. £#/li#££#li#£ Copson, E. T. Asymptotic Expansions. Cambridge, England: Cambridge University Press, 1965. £#/li#££#li#£ de Bruijn, N. G. Asymptotic Methods in Analysis. New York: Dover, pp. 3-10, 1981. £#/li#££#li#£ Dingle, R. B. Asymptotic Expansions: Their Derivation and Interpretation. London: Academic Press, 1973. £#/li#££#li#£ Erdélyi, A. Asymptotic Expansions. New York: Dover, 1987. £#/li#££#li#£ Gradshteyn, I. S. and Ryzhik, I. M. "Asymptotic Series." §0.33 in Tables of Integrals, Series, and Products, 6th ed. San Diego, CA: Academic Press, p. 20, 2000. £#/li#££#li#£ Morse, P. M. and Feshbach, H. "Asymptotic Series; Method of Steepest Descent." §4.6 in Methods of Theoretical Physics, Part I. New York: McGraw-Hill, pp. 434-443, 1953. £#/li#££#li#£ Olver, F. W. J. Asymptotics and Special Functions. New York: Academic Press, 1974. £#/li#££#li#£ Wasow, W. R. Asymptotic Expansions for Ordinary Differential Equations. New York: Dover, 1987. £#/li#££#li#£ Weisstein, E. W. "Books about Asymptotic Series." http://www.ericweisstein.com/encyclopedias/books/AsymptoticSeries.html. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Series > Asymptotic Series £#/li#££#/ul#£




£#h3#£Atan£#/h3#£

Atan may refer to:


£#h5#£Places£#/h5#£ £#ul#££#li#£Atan, Armenia£#/li#£ £#li#£Atan, Iran£#/li#££#/ul#£
£#h5#£People£#/h5#£ £#ul#££#li#£Atan Shansonga (born 1955), Zambian diplomat£#/li#£ £#li#£Çağdaş Atan, Turkish footballer£#/li#£ £#li#£Cem Atan, Turkish footballer£#/li#££#/ul#£
£#h5#£Other£#/h5#£ £#ul#££#li#£Attan, a Pashtun and Afghan traditional dance£#/li#£ £#li#£arctangent, a trigonometric function £#ul#££#li#£atan2, the two-argument function implementing the arctangent in many computer languages£#/li#££#/ul#££#/li#££#/ul#£



£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Trigonometric Functions £#/li#££#/ul#£




£#h3#£Atiyah-Singer Index Theorem£#/h3#£

In differential geometry, the Atiyah–Singer index theorem, proved by Michael Atiyah and Isadore Singer (1963), states that for an elliptic differential operator on a compact manifold, the analytical index (related to the dimension of the space of solutions) is equal to the topological index (defined in terms of some topological data). It includes many other theorems, such as the Chern–Gauss–Bonnet theorem and Riemann–Roch theorem, as special cases, and has applications to theoretical physics.


£#h5#£History£#/h5#£
The index problem for elliptic differential operators was posed by Israel Gel'fand. He noticed the homotopy invariance of the index, and asked for a formula for it by means of topological invariants. Some of the motivating examples included the Riemann–Roch theorem and its generalization the Hirzebruch–Riemann–Roch theorem, and the Hirzebruch signature theorem. Friedrich Hirzebruch and Armand Borel had proved the integrality of the Â genus of a spin manifold, and Atiyah suggested that this integrality could be explained if it were the index of the Dirac operator (which was rediscovered by Atiyah and Singer in 1961).

The Atiyah–Singer theorem was announced in 1963. The proof sketched in this announcement was never published by them, though it appears in the Palais's book. It appears also in the "Séminaire Cartan-Schwartz 1963/64" that was held in Paris simultaneously with the seminar led by Richard Palais at Princeton University. The last talk in Paris was by Atiyah on manifolds with boundary. Their first published proof replaced the cobordism theory of the first proof with K-theory, and they used this to give proofs of various generalizations in another sequence of papers.

£#ul#££#li#£1965: Sergey P. Novikov published his results on the topological invariance of the rational Pontryagin classes on smooth manifolds.£#/li#£ £#li#£Robion Kirby and Laurent C. Siebenmann's results, combined with René Thom's paper proved the existence of rational Pontryagin classes on topological manifolds. The rational Pontryagin classes are essential ingredients of the index theorem on smooth and topological manifolds.£#/li#£ £#li#£1969: Michael Atiyah defines abstract elliptic operators on arbitrary metric spaces. Abstract elliptic operators became protagonists in Kasparov's theory and Connes's noncommutative differential geometry.£#/li#£ £#li#£1971: Isadore Singer proposes a comprehensive program for future extensions of index theory.£#/li#£ £#li#£1972: Gennadi G. Kasparov publishes his work on the realization of K-homology by abstract elliptic operators.£#/li#£ £#li#£1973: Atiyah, Raoul Bott, and Vijay Patodi gave a new proof of the index theorem using the heat equation, described in a paper by Melrose.£#/li#£ £#li#£1977: Dennis Sullivan establishes his theorem on the existence and uniqueness of Lipschitz and quasiconformal structures on topological manifolds of dimension different from 4.£#/li#£ £#li#£1983: Ezra Getzler motivated by ideas of Edward Witten and Luis Alvarez-Gaume, gave a short proof of the local index theorem for operators that are locally Dirac operators; this covers many of the useful cases.£#/li#£ £#li#£1983: Nicolae Teleman proves that the analytical indices of signature operators with values in vector bundles are topological invariants.£#/li#£ £#li#£1984: Teleman establishes the index theorem on topological manifolds.£#/li#£ £#li#£1986: Alain Connes publishes his fundamental paper on noncommutative geometry.£#/li#£ £#li#£1989: Simon K. Donaldson and Sullivan study Yang–Mills theory on quasiconformal manifolds of dimension 4. They introduce the signature operator S defined on differential forms of degree two.£#/li#£ £#li#£1990: Connes and Henri Moscovici prove the local index formula in the context of non-commutative geometry.£#/li#£ £#li#£1994: Connes, Sullivan, and Teleman prove the index theorem for signature operators on quasiconformal manifolds.£#/li#££#/ul#£
£#h5#£Notation£#/h5#£ £#ul#££#li#£X is a compact smooth manifold (without boundary).£#/li#£ £#li#£E and F are smooth vector bundles over X.£#/li#£ £#li#£D is an elliptic differential operator from E to F. So in local coordinates it acts as a differential operator, taking smooth sections of E to smooth sections of F.£#/li#££#/ul#£
£#h5#£Symbol of a differential operator£#/h5#£
If D is a differential operator on a Euclidean space of order n in k variables ${\displaystyle x_{1},\dots ,x_{k}}$ , then its symbol is the function of 2k variables ${\displaystyle x_{1},\dots ,x_{k},y_{1},\dots ,y_{k}}$ , given by dropping all terms of order less than n and replacing ${\displaystyle \partial /\partial x_{i}}$ by ${\displaystyle y_{i}}$ . So the symbol is homogeneous in the variables y, of degree n. The symbol is well defined even though ${\displaystyle \partial /\partial x_{i}}$ does not commute with ${\displaystyle x_{i}}$ because we keep only the highest order terms and differential operators commute "up to lower-order terms". The operator is called elliptic if the symbol is nonzero whenever at least one y is nonzero.

Example: The Laplace operator in k variables has symbol ${\displaystyle y_{1}^{2}+\cdots +y_{k}^{2}}$ , and so is elliptic as this is nonzero whenever any of the ${\displaystyle y_{i}}$ 's are nonzero. The wave operator has symbol ${\displaystyle -y_{1}^{2}+\cdots +y_{k}^{2}}$ , which is not elliptic if ${\displaystyle k\geq 2}$ , as the symbol vanishes for some non-zero values of the ys.

The symbol of a differential operator of order n on a smooth manifold X is defined in much the same way using local coordinate charts, and is a function on the cotangent bundle of X, homogeneous of degree n on each cotangent space. (In general, differential operators transform in a rather complicated way under coordinate transforms (see jet bundle); however, the highest order terms transform like tensors so we get well defined homogeneous functions on the cotangent spaces that are independent of the choice of local charts.) More generally, the symbol of a differential operator between two vector bundles E and F is a section of the pullback of the bundle Hom(E, F) to the cotangent space of X. The differential operator is called elliptic if the element of Hom(Ex, Fx) is invertible for all non-zero cotangent vectors at any point x of X.

A key property of elliptic operators is that they are almost invertible; this is closely related to the fact that their symbols are almost invertible. More precisely, an elliptic operator D on a compact manifold has a (non-unique) parametrix (or pseudoinverse) D′ such that DD′ -1 and D′D -1 are both compact operators. An important consequence is that the kernel of D is finite-dimensional, because all eigenspaces of compact operators, other than the kernel, are finite-dimensional. (The pseudoinverse of an elliptic differential operator is almost never a differential operator. However, it is an elliptic pseudodifferential operator.)


£#h5#£Analytical index£#/h5#£
As the elliptic differential operator D has a pseudoinverse, it is a Fredholm operator. Any Fredholm operator has an index, defined as the difference between the (finite) dimension of the kernel of D (solutions of Df = 0), and the (finite) dimension of the cokernel of D (the constraints on the right-hand-side of an inhomogeneous equation like Df = g, or equivalently the kernel of the adjoint operator). In other words,

Index(D) = dim Ker(D) − dim Coker(D) = dim Ker(D) − dim Ker(D*).
This is sometimes called the analytical index of D.

Example: Suppose that the manifold is the circle (thought of as R/Z), and D is the operator d/dx − λ for some complex constant λ. (This is the simplest example of an elliptic operator.) Then the kernel is the space of multiples of exp(λx) if λ is an integral multiple of 2πi and is 0 otherwise, and the kernel of the adjoint is a similar space with λ replaced by its complex conjugate. So D has index 0. This example shows that the kernel and cokernel of elliptic operators can jump discontinuously as the elliptic operator varies, so there is no nice formula for their dimensions in terms of continuous topological data. However the jumps in the dimensions of the kernel and cokernel are the same, so the index, given by the difference of their dimensions, does indeed vary continuously, and can be given in terms of topological data by the index theorem.


£#h5#£Topological index£#/h5#£
The topological index of an elliptic differential operator ${\displaystyle D}$ between smooth vector bundles ${\displaystyle E}$ and ${\displaystyle F}$ on an ${\displaystyle n}$ -dimensional compact manifold ${\displaystyle X}$ is given by

${\displaystyle (-1)^{n}\operatorname {ch} (D)\operatorname {Td} (X)[X]=(-1)^{n}\int _{X}\operatorname {ch} (D)\operatorname {Td} (X)}$
in other words the value of the top dimensional component of the mixed cohomology class ${\displaystyle \operatorname {ch} (D)\operatorname {Td} (X)}$ on the fundamental homology class of the manifold ${\displaystyle X}$ up to a difference of sign. Here,

£#ul#££#li#£ ${\displaystyle \operatorname {Td} (X)}$ is the Todd class of the complexified tangent bundle of ${\displaystyle X}$ .£#/li#£ £#li#£ ${\displaystyle \operatorname {ch} (D)}$ is equal to ${\displaystyle \varphi ^{-1}(\operatorname {ch} (d(p^{*}E,p^{*}F,\sigma (D))))}$ , where £#ul#££#li#£ ${\displaystyle \varphi :H^{k}(X;\mathbb {Q} )\to H^{n+k}(B(X)/S(X);\mathbb {Q} )}$ is the Thom isomorphism for the sphere bundle ${\displaystyle p:B(X)/S(X)\to X}$ £#/li#£ £#li#£ ${\displaystyle \operatorname {ch} :K(X)\otimes \mathbb {Q} \to H^{*}(X;\mathbb {Q} )}$ is the Chern character£#/li#£ £#li#£ ${\displaystyle d(p^{*}E,p^{*}F,\sigma (D))}$ is the "difference element" in ${\displaystyle K(B(X)/S(X))}$ associated to two vector bundles ${\displaystyle p^{*}E}$ and ${\displaystyle p^{*}F}$ on ${\displaystyle B(X)}$ and an isomorphism ${\displaystyle \sigma (D)}$ between them on the subspace ${\displaystyle S(X)}$ .£#/li#£ £#li#£ ${\displaystyle \sigma (D)}$ is the symbol of ${\displaystyle D}$ £#/li#££#/ul#££#/li#££#/ul#£
In some situations, it is possible to simplify the above formula for computational purposes. In particular, if ${\displaystyle X}$ is a ${\displaystyle 2m}$ -dimensional orientable (compact) manifold with non-zero Euler class ${\displaystyle e(TX)}$ , then applying the Thom isomorphism and dividing by the Euler class, the topological index may be expressed as

${\displaystyle (-1)^{m}\int _{X}{\frac {\operatorname {ch} (E)-\operatorname {ch} (F)}{e(TX)}}\operatorname {Td} (X)}$
where division makes sense by pulling ${\displaystyle e(TX)^{-1}}$ back from the cohomology ring of the classifying space ${\displaystyle BSO}$ .

One can also define the topological index using only K-theory (and this alternative definition is compatible in a certain sense with the Chern-character construction above). If X is a compact submanifold of a manifold Y then there is a pushforward (or "shriek") map from K(TX) to K(TY). The topological index of an element of K(TX) is defined to be the image of this operation with Y some Euclidean space, for which K(TY) can be naturally identified with the integers Z (as a consequence of Bott-periodicity). This map is independent of the embedding of X in Euclidean space. Now a differential operator as above naturally defines an element of K(TX), and the image in Z under this map "is" the topological index.

As usual, D is an elliptic differential operator between vector bundles E and F over a compact manifold X.

The index problem is the following: compute the (analytical) index of D using only the symbol s and topological data derived from the manifold and the vector bundle. The Atiyah–Singer index theorem solves this problem, and states:

The analytical index of D is equal to its topological index.
In spite of its formidable definition, the topological index is usually straightforward to evaluate explicitly. So this makes it possible to evaluate the analytical index. (The cokernel and kernel of an elliptic operator are in general extremely hard to evaluate individually; the index theorem shows that we can usually at least evaluate their difference.) Many important invariants of a manifold (such as the signature) can be given as the index of suitable differential operators, so the index theorem allows us to evaluate these invariants in terms of topological data.

Although the analytical index is usually hard to evaluate directly, it is at least obviously an integer. The topological index is by definition a rational number, but it is usually not at all obvious from the definition that it is also integral. So the Atiyah–Singer index theorem implies some deep integrality properties, as it implies that the topological index is integral.

The index of an elliptic differential operator obviously vanishes if the operator is self adjoint. It also vanishes if the manifold X has odd dimension, though there are pseudodifferential elliptic operators whose index does not vanish in odd dimensions.


£#h5#£Relation to Grothendieck–Riemann–Roch£#/h5#£
The Grothendieck–Riemann–Roch theorem was one of the main motivations behind the index theorem because the index theorem is the counterpart of this theorem in the setting of real manifolds. Now, if there's a map ${\displaystyle f:X\to Y}$ of compact stably almost complex manifolds, then there is a commutative diagram

if ${\displaystyle Y=*}$ is a point, then we recover the statement above. Here ${\displaystyle K(X)}$ is the Grothendieck group of complex vector bundles. This commutative diagram is formally very similar to the GRR theorem because the cohomology groups on the right are replaced by the Chow ring of a smooth variety, and the Grothendieck group on the left is given by the Grothendieck group of algebraic vector bundles.


£#h5#£Extensions of the Atiyah–Singer index theorem£#/h5#£
£#h5#£Teleman index theorem£#/h5#£
Due to (Teleman 1983), (Teleman 1984):

For any abstract elliptic operator (Atiyah 1970) on a closed, oriented, topological manifold, the analytical index equals the topological index.
The proof of this result goes through specific considerations, including the extension of Hodge theory on combinatorial and Lipschitz manifolds (Teleman 1980), (Teleman 1983), the extension of Atiyah–Singer's signature operator to Lipschitz manifolds (Teleman 1983), Kasparov's K-homology (Kasparov 1972) and topological cobordism (Kirby & Siebenmann 1977).

This result shows that the index theorem is not merely a differentiability statement, but rather a topological statement.


£#h5#£Connes–Donaldson–Sullivan–Teleman index theorem£#/h5#£
Due to (Donaldson & Sullivan 1989), (Connes, Sullivan & Teleman 1994):

For any quasiconformal manifold there exists a local construction of the Hirzebruch–Thom characteristic classes.
This theory is based on a signature operator S, defined on middle degree differential forms on even-dimensional quasiconformal manifolds (compare (Donaldson & Sullivan 1989)).

Using topological cobordism and K-homology one may provide a full statement of an index theorem on quasiconformal manifolds (see page 678 of (Connes, Sullivan & Teleman 1994)). The work (Connes, Sullivan & Teleman 1994) "provides local constructions for characteristic classes based on higher dimensional relatives of the measurable Riemann mapping in dimension two and the Yang–Mills theory in dimension four."

These results constitute significant advances along the lines of Singer's program Prospects in Mathematics (Singer 1971). At the same time, they provide, also, an effective construction of the rational Pontrjagin classes on topological manifolds. The paper (Teleman 1985) provides a link between Thom's original construction of the rational Pontrjagin classes (Thom 1956) and index theory.

It is important to mention that the index formula is a topological statement. The obstruction theories due to Milnor, Kervaire, Kirby, Siebenmann, Sullivan, Donaldson show that only a minority of topological manifolds possess differentiable structures and these are not necessarily unique. Sullivan's result on Lipschitz and quasiconformal structures (Sullivan 1979) shows that any topological manifold in dimension different from 4 possesses such a structure which is unique (up to isotopy close to identity).

The quasiconformal structures (Connes, Sullivan & Teleman 1994) and more generally the Lp-structures, p > n(n+1)/2, introduced by M. Hilsum (Hilsum 1999), are the weakest analytical structures on topological manifolds of dimension n for which the index theorem is known to hold.


£#h5#£Other extensions£#/h5#£ £#ul#££#li#£The Atiyah–Singer theorem applies to elliptic pseudodifferential operators in much the same way as for elliptic differential operators. In fact, for technical reasons most of the early proofs worked with pseudodifferential rather than differential operators: their extra flexibility made some steps of the proofs easier.£#/li#£ £#li#£Instead of working with an elliptic operator between two vector bundles, it is sometimes more convenient to work with an elliptic complex of vector bundles. The difference is that the symbols now form an exact sequence (off the zero section). In the case when there are just two non-zero bundles in the complex this implies that the symbol is an isomorphism off the zero section, so an elliptic complex with 2 terms is essentially the same as an elliptic operator between two vector bundles. Conversely the index theorem for an elliptic complex can easily be reduced to the case of an elliptic operator: the two vector bundles are given by the sums of the even or odd terms of the complex, and the elliptic operator is the sum of the operators of the elliptic complex and their adjoints, restricted to the sum of the even bundles.£#/li#£ £#li#£If the manifold is allowed to have boundary, then some restrictions must be put on the domain of the elliptic operator in order to ensure a finite index. These conditions can be local (like demanding that the sections in the domain vanish at the boundary) or more complicated global conditions (like requiring that the sections in the domain solve some differential equation). The local case was worked out by Atiyah and Bott, but they showed that many interesting operators (e.g., the signature operator) do not admit local boundary conditions. To handle these operators, Atiyah, Patodi and Singer introduced global boundary conditions equivalent to attaching a cylinder to the manifold along the boundary and then restricting the domain to those sections that are square integrable along the cylinder. This point of view is adopted in the proof of Melrose (1993) of the Atiyah–Patodi–Singer index theorem.£#/li#£ £#li#£Instead of just one elliptic operator, one can consider a family of elliptic operators parameterized by some space Y. In this case the index is an element of the K-theory of Y, rather than an integer. If the operators in the family are real, then the index lies in the real K-theory of Y. This gives a little extra information, as the map from the real K-theory of Y to the complex K-theory is not always injective.£#/li#£ £#li#£If there is a group action of a group G on the compact manifold X, commuting with the elliptic operator, then one replaces ordinary K-theory with equivariant K-theory. Moreover, one gets generalizations of the Lefschetz fixed-point theorem, with terms coming from fixed-point submanifolds of the group G. See also: equivariant index theorem.£#/li#£ £#li#£Atiyah (1976) showed how to extend the index theorem to some non-compact manifolds, acted on by a discrete group with compact quotient. The kernel of the elliptic operator is in general infinite dimensional in this case, but it is possible to get a finite index using the dimension of a module over a von Neumann algebra; this index is in general real rather than integer valued. This version is called the L2 index theorem, and was used by Atiyah & Schmid (1977) to rederive properties of the discrete series representations of semisimple Lie groups.£#/li#£ £#li#£The Callias index theorem is an index theorem for a Dirac operator on a noncompact odd-dimensional space. The Atiyah–Singer index is only defined on compact spaces, and vanishes when their dimension is odd. In 1978 Constantine Callias, at the suggestion of his Ph.D. advisor Roman Jackiw, used the axial anomaly to derive this index theorem on spaces equipped with a Hermitian matrix called the Higgs field. The index of the Dirac operator is a topological invariant which measures the winding of the Higgs field on a sphere at infinity. If U is the unit matrix in the direction of the Higgs field, then the index is proportional to the integral of U(dU)n−1 over the (n−1)-sphere at infinity. If n is even, it is always zero. £#ul#££#li#£The topological interpretation of this invariant and its relation to the Hörmander index proposed by Boris Fedosov, as generalized by Lars Hörmander, was published by Raoul Bott and Robert Thomas Seeley.£#/li#££#/ul#££#/li#££#/ul#£
£#h5#£Examples£#/h5#£
£#h5#£Chern-Gauss-Bonnet theorem£#/h5#£
Suppose that ${\displaystyle M}$ is a compact oriented manifold of dimension ${\displaystyle n=2r}$ . If we take ${\displaystyle \Lambda ^{even}}$ to be the sum of the even exterior powers of the cotangent bundle, and ${\displaystyle \Lambda ^{odd}}$ to be the sum of the odd powers, define ${\displaystyle D=d+d^{*}}$ , considered as a map from ${\displaystyle \Lambda ^{even}}$ to ${\displaystyle \Lambda ^{odd}}$ . Then the analytical index of ${\displaystyle D}$ is the Euler characteristic ${\displaystyle \chi (M)}$ of the Hodge cohomology of ${\displaystyle M}$ , and the topological index is the integral of the Euler class over the manifold. The index formula for this operator yields the Chern–Gauss–Bonnet theorem.

The concrete computation goes as follows: according to one variation of the splitting principle, if ${\displaystyle E}$ is a real vector bundle of dimension ${\displaystyle n=2r}$ , in order to prove assertions involving characteristic classes, we may suppose that there are complex line bundles ${\displaystyle l_{1},...l_{r}}$ such that ${\displaystyle E\otimes \mathbb {C} =l_{1}\oplus {\overline {l_{1}}}\oplus ...l_{r}\oplus {\overline {l_{r}}}}$ . Therefore, we can consider the Chern roots ${\displaystyle x_{i}(E\otimes \mathbb {C} )=c_{1}(l_{i})}$ , ${\displaystyle x_{r+i}(E\otimes \mathbb {C} )=c_{1}({\overline {l_{i}}})=-x_{i}(E\otimes \mathbb {C} )}$ , ${\displaystyle i=1,...,r}$ .

Using Chern roots as above and the standard properties of the Euler class, we have that ${\displaystyle e(TM)=\prod _{i}^{r}x_{i}(TM\otimes \mathbb {C} )}$ . As for the Chern and Todd classes,

Applying the index theorem,

${\displaystyle \chi (M)=(-1)^{r}\int _{M}{\frac {\prod _{i}^{n}(1-e^{-x_{i}})}{\prod _{i}^{r}x_{i}}}\prod _{i}^{n}{\frac {x_{i}}{1-e^{-x_{i}}}}(TM\otimes \mathbb {C} )=(-1)^{r}\int _{M}(-1)^{r}\prod _{i}^{r}x_{i}(TM\otimes \mathbb {C} )=\int _{M}e(TM)}$
which is the "topological" version of the Chern-Gauss-Bonnet theorem (the geometric one being obtained by applying the Chern-Weil homomorphism).


£#h5#£Hirzebruch–Riemann–Roch theorem£#/h5#£
Take X to be a complex manifold with a holomorphic vector bundle V. We let the vector bundles E and F be the sums of the bundles of differential forms with coefficients in V of type (0,i) with i even or odd, and we let the differential operator D be the sum

${\displaystyle {\overline {\partial }}+{\overline {\partial }}^{*}}$
restricted to E. Then the analytical index of D is the holomorphic Euler characteristic of V:

${\displaystyle {\textrm {index}}(D)=\sum _{p}(-1)^{p}{\textrm {dim}}\,H^{p}(X,V)}$
The topological index of D is given by

${\displaystyle {\textrm {index}}(D)={\textrm {ch}}(V)\,{\textrm {Td}}(X)[X]}$ ,
the product of the Chern character of V and the Todd class of X evaluated on the fundamental class of X. By equating the topological and analytical indices we get the Hirzebruch–Riemann–Roch theorem. In fact we get a generalization of it to all complex manifolds: Hirzebruch's proof only worked for projective complex manifolds X.

This derivation of the Hirzebruch–Riemann–Roch theorem is more natural if we use the index theorem for elliptic complexes rather than elliptic operators. We can take the complex to be

${\displaystyle 0\rightarrow V\rightarrow V\otimes \Lambda ^{0,1}T^{*}(X)\rightarrow V\otimes \Lambda ^{0,2}T^{*}(X)\rightarrow \,...}$
with the differential given by ${\displaystyle {\overline {\partial }}}$ . Then the i'th cohomology group is just the coherent cohomology group Hi(X, V), so the analytical index of this complex is the holomorphic Euler characteristic Σ (−1)i dim(Hi(X, V)). As before, the topological index is ch(V)Td(X)[X].


£#h5#£Hirzebruch signature theorem£#/h5#£
The Hirzebruch signature theorem states that the signature of a compact oriented manifold X of dimension 4k is given by the L genus of the manifold. This follows from the Atiyah–Singer index theorem applied to the following signature operator.

The bundles E and F are given by the +1 and −1 eigenspaces of the operator on the bundle of differential forms of X, that acts on k-forms as

${\displaystyle i^{k(k-1)}}$
times the Hodge * operator. The operator D is the Hodge Laplacian

${\displaystyle D\equiv \Delta \mathrel {:=} \left(\mathbf {d} +\mathbf {d^{*}} \right)^{2}}$
restricted to E, where d is the Cartan exterior derivative and d* is its adjoint.

The analytic index of D is the signature of the manifold X, and its topological index is the L genus of X, so these are equal.


£#h5#£Â genus and Rochlin's theorem£#/h5#£
The Â genus is a rational number defined for any manifold, but is in general not an integer. Borel and Hirzebruch showed that it is integral for spin manifolds, and an even integer if in addition the dimension is 4 mod 8. This can be deduced from the index theorem, which implies that the Â genus for spin manifolds is the index of a Dirac operator. The extra factor of 2 in dimensions 4 mod 8 comes from the fact that in this case the kernel and cokernel of the Dirac operator have a quaternionic structure, so as complex vector spaces they have even dimensions, so the index is even.

In dimension 4 this result implies Rochlin's theorem that the signature of a 4-dimensional spin manifold is divisible by 16: this follows because in dimension 4 the Â genus is minus one eighth of the signature.


£#h5#£Proof techniques£#/h5#£
£#h5#£Pseudodifferential operators£#/h5#£
Pseudodifferential operators can be explained easily in the case of constant coefficient operators on Euclidean space. In this case, constant coefficient differential operators are just the Fourier transforms of multiplication by polynomials, and constant coefficient pseudodifferential operators are just the Fourier transforms of multiplication by more general functions.

Many proofs of the index theorem use pseudodifferential operators rather than differential operators. The reason for this is that for many purposes there are not enough differential operators. For example, a pseudoinverse of an elliptic differential operator of positive order is not a differential operator, but is a pseudodifferential operator. Also, there is a direct correspondence between data representing elements of K(B(X), S(X)) (clutching functions) and symbols of elliptic pseudodifferential operators.

Pseudodifferential operators have an order, which can be any real number or even −∞, and have symbols (which are no longer polynomials on the cotangent space), and elliptic differential operators are those whose symbols are invertible for sufficiently large cotangent vectors. Most version of the index theorem can be extended from elliptic differential operators to elliptic pseudodifferential operators.


£#h5#£Cobordism£#/h5#£
The initial proof was based on that of the Hirzebruch–Riemann–Roch theorem (1954), and involved cobordism theory and pseudodifferential operators.

The idea of this first proof is roughly as follows. Consider the ring generated by pairs (X, V) where V is a smooth vector bundle on the compact smooth oriented manifold X, with relations that the sum and product of the ring on these generators are given by disjoint union and product of manifolds (with the obvious operations on the vector bundles), and any boundary of a manifold with vector bundle is 0. This is similar to the cobordism ring of oriented manifolds, except that the manifolds also have a vector bundle. The topological and analytical indices are both reinterpreted as functions from this ring to the integers. Then one checks that these two functions are in fact both ring homomorphisms. In order to prove they are the same, it is then only necessary to check they are the same on a set of generators of this ring. Thom's cobordism theory gives a set of generators; for example, complex vector spaces with the trivial bundle together with certain bundles over even dimensional spheres. So the index theorem can be proved by checking it on these particularly simple cases.


£#h5#£K-theory£#/h5#£
Atiyah and Singer's first published proof used K-theory rather than cobordism. If i is any inclusion of compact manifolds from X to Y, they defined a 'pushforward' operation i! on elliptic operators of X to elliptic operators of Y that preserves the index. By taking Y to be some sphere that X embeds in, this reduces the index theorem to the case of spheres. If Y is a sphere and X is some point embedded in Y, then any elliptic operator on Y is the image under i! of some elliptic operator on the point. This reduces the index theorem to the case of a point, where it is trivial.


£#h5#£Heat equation£#/h5#£
Atiyah, Bott, and Patodi (1973) gave a new proof of the index theorem using the heat equation, see e.g. Berline, Getzler & Vergne (1992). The proof is also published in (Melrose 1993) and (Gilkey 1994).

If D is a differential operator with adjoint D*, then D*D and DD* are self adjoint operators whose non-zero eigenvalues have the same multiplicities. However their zero eigenspaces may have different multiplicities, as these multiplicities are the dimensions of the kernels of D and D*. Therefore, the index of D is given by

${\displaystyle \operatorname {Index} (D)=\dim \operatorname {Ker} (D^{*})=\operatorname {Tr} \left(e^{-tD^{*}D}\right)-\operatorname {Tr} \left(e^{-tDD^{*}}\right)}$
for any positive t. The right hand side is given by the trace of the difference of the kernels of two heat operators. These have an asymptotic expansion for small positive t, which can be used to evaluate the limit as t tends to 0, giving a proof of the Atiyah–Singer index theorem. The asymptotic expansions for small t appear very complicated, but invariant theory shows that there are huge cancellations between the terms, which makes it possible to find the leading terms explicitly. These cancellations were later explained using supersymmetry.


£#h5#£Citations£#/h5#£
£#h5#£References£#/h5#£
The papers by Atiyah are reprinted in volumes 3 and 4 of his collected works, (Atiyah 1988a, 1988b)


£#h5#£External links£#/h5#£
£#h5#£Links on the theory£#/h5#£ £#ul#££#li#£Mazzeo, Rafe. "The Atiyah–Singer Index Theorem: What it is and why you should care" (PDF). Archived from the original (PDF) on October 10, 2002. Pdf presentation.£#/li#£ £#li#£Voitsekhovskii, M.I.; Shubin, M.A. (2001) [1994], "Index formulas", Encyclopedia of Mathematics, EMS Press£#/li#£ £#li#£Wassermann, Antony. "Lecture notes on the Atiyah–Singer Index Theorem". Archived from the original on March 29, 2017.£#/li#££#/ul#£
£#h5#£Links of interviews£#/h5#£ £#ul#££#li#£Raussen, Martin; Skau, Christian (2005), "Interview with Michael Atiyah and Isadore Singer" (PDF), Notices of AMS, pp. 223–231£#/li#£ £#li#£R. R. Seeley and other (1999) Recollections from the early days of index theory and pseudo-differential operators - A partial transcript of informal post–dinner conversation during a symposium held in Roskilde, Denmark, in September 1998.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Atiyah, M. F. and Singer, I. M. "The Index of Elliptic Operators on Compact Manifolds." Bull. Amer. Math. Soc. 69, 322-433, 1963.£#/li#££#li#£Atiyah, M. F. and Singer, I. M. "The Index of Elliptic Operators I, II, III." Ann. Math. 87, 484-604, 1968.£#/li#££#li#£Petkovšek, M.; Wilf, H. S.; and Zeilberger, D. A=B. Wellesley, MA: A K Peters, p. 4, 1996. http://www.cis.upenn.edu/~wilf/AeqB.html.£#/li#££#li#£Rognes, J. "On the Atiyah-Singer Index Theorem." http://www.abelprisen.no/nedlastning/2004/popular_english_2004.pdf.£#/li#££#li#£ Atiyah, M. F. and Singer, I. M. "The Index of Elliptic Operators on Compact Manifolds." Bull. Amer. Math. Soc. 69, 322-433, 1963. £#/li#££#li#£ Atiyah, M. F. and Singer, I. M. "The Index of Elliptic Operators I, II, III." Ann. Math. 87, 484-604, 1968. £#/li#££#li#£ Petkovšek, M.; Wilf, H. S.; and Zeilberger, D. A=B. Wellesley, MA: A K Peters, p. 4, 1996. http://www.cis.upenn.edu/~wilf/AeqB.html. £#/li#££#li#£ Rognes, J. "On the Atiyah-Singer Index Theorem." http://www.abelprisen.no/nedlastning/2004/popular_english_2004.pdf. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Topology > Manifolds £#/li#££#/ul#£




£#h3#£Atlas£#/h3#£

An atlas is a collection of maps; it is typically a bundle of maps of Earth or of a region of Earth.

Atlases have traditionally been bound into book form, but today many atlases are in multimedia formats. In addition to presenting geographic features and political boundaries, many atlases often feature geopolitical, social, religious and economic statistics. They also have information about the map and places in it.


£#h5#£Etymology£#/h5#£
The use of the word "atlas" in a geographical context dates from 1595 when the German-Flemish geographer Gerardus Mercator published Atlas Sive Cosmographicae Meditationes de Fabrica Mundi et Fabricati Figura ("Atlas or cosmographical meditations upon the creation of the universe and the universe as created"). This title provides Mercator's definition of the word as a description of the creation and form of the whole universe, not simply as a collection of maps. The volume that was published posthumously one year after his death is a wide-ranging text but, as the editions evolved, it became simply a collection of maps and it is in that sense that the word was used from the middle of the 17th century. The neologism coined by Mercator was a mark of his respect for the Titan, Atlas, the "King of Mauretania", whom he considered to be the first great geographer.


£#h5#£History of atlases£#/h5#£
The first work that contained systematically arranged maps of uniform size representing the first modern atlas was prepared by Italian cartographer Pietro Coppo in the early 16th century; however it was not published at that time, so it is conventionally not considered the first atlas. Rather, that title is awarded to the collection of maps Theatrum Orbis Terrarum by the Brabantian cartographer Abraham Ortelius printed in 1570.

However atlases published nowadays are quite different from those published in the 16th–19th centuries. Unlike today, most atlases were not bound and ready for the customer to buy, but their possible components were shelved separately. The client could select the contents to their liking, and have the maps coloured/gilded or not. The atlas was then bound. Thus early printed atlases with the same title page can be different in contents.


£#h5#£Types of atlases£#/h5#£
A travel atlas is made for easy use during travel, and often has spiral bindings so it may be folded flat (for example Geographers' A-Z Map Company famous A–Z atlases). It has maps at a large zoom so the maps can be reviewed easily. A travel atlas may also be referred to as a road map.

A desk atlas is made similar to a reference book. It may be in hardback or paperback form.

There are atlases of the other planets (and their satellites) in the Solar System.

Atlases of anatomy exist, mapping out organs of the human body or other organisms.


£#h5#£Selected atlases£#/h5#£
Some cartographically or commercially important atlases are:

17th century and earlier:

£#ul#££#li#£Atlas Sive Cosmographicae (Mercator, Duisburg, in present-day Germany, 1595)£#/li#£ £#li#£Atlas Novus (Blaeu, Netherlands, 1635–1658)£#/li#£ £#li#£Atlas Maior (Blaeu, Netherlands, 1662–1667)£#/li#£ £#li#£Cartes générales de toutes les parties du monde (France, 1658–1676)£#/li#£ £#li#£Dell'Arcano del Mare (England/Italy, 1645–1661)£#/li#£ £#li#£Piri Reis map (Ottoman Empire, 1570–1612)£#/li#£ £#li#£Theatrum Orbis Terrarum (Ortelius, Netherlands, 1570–1612)£#/li#£ £#li#£Klencke Atlas (1660; one of the world's largest books)£#/li#£ £#li#£The Brittania (John Ogilby, 1670–1676)£#/li#££#/ul#£
18th century:

£#ul#££#li#£Atlas Nouveau (Amsterdam, 1742)£#/li#£ £#li#£Britannia Depicta (London, 1720)£#/li#£ £#li#£Cary's New and Correct English Atlas (London, 1787)£#/li#££#/ul#£
19th century:

£#ul#££#li#£Andrees Allgemeiner Handatlas (Germany, 1881–1939; in the UK as Times Atlas of the World, 1895)£#/li#£ £#li#£Rand McNally Atlas (United States, 1881–present)£#/li#£ £#li#£Stielers Handatlas (Germany, 1817–1944)£#/li#£ £#li#£Times Atlas of the World (United Kingdom, 1895–present)£#/li#££#/ul#£
20th century:

£#ul#££#li#£Atlante Internazionale del Touring Club Italiano (Italy, 1927–1978)£#/li#£ £#li#£Atlas Linguisticus (Austria, 1934)£#/li#£ £#li#£Atlas Mira (Soviet Union/Russia, 1937–present)£#/li#£ £#li#£Geographers' A–Z Street Atlas (United Kingdom, 1938–present)£#/li#£ £#li#£Gran Atlas Aguilar (Spain, 1969/1970)£#/li#£ £#li#£The Historical Atlas of China (China)£#/li#£ £#li#£National Geographic Atlas of the World (United States, 1963–present)£#/li#£ £#li#£Pergamon World Atlas (1962/1968)£#/li#££#/ul#£
21st century:

£#ul#££#li#£North American Environmental Atlas£#/li#££#/ul#£
£#h5#£See also£#/h5#£
£#h5#£References£#/h5#£
£#h5#£External links£#/h5#£
Sources
£#ul#££#li#£On the origin of the term "Atlas"£#/li#££#/ul#£
Online atlases
£#ul#££#li#£World Atlas£#/li#£ £#li#£ÖROK-Atlas Online: Atlas on spatial development in Austria£#/li#£ £#li#£Geography Network£#/li#£ £#li#£MapChart EarthAtlas, free online atlas with interactive maps about topics like demography, economy, health and environment.£#/li#£ £#li#£National Geographic MapMachine£#/li#££#/ul#£
History of atlases
£#ul#££#li#£Atlases, at the US Library of Congress site - a discussion of many significant atlases, with some illustrations. Part of Geography and Maps, an Illustrated Guide.£#/li#££#/ul#£
Historical atlases online
£#ul#££#li#£Centennia Historical Atlas required reading at the US Naval Academy for over a decade.£#/li#£ £#li#£Historical map web sites list, Perry–Castañeda Library, University of Texas£#/li#£ £#li#£Ryhiner Collection Composite atlas with maps, plans and views from the 16th-18th centuries, covering the globe, with about 16,000 images in total.£#/li#£ £#li#£Manuscript Atlases held by the University of Pennsylvania Libraries - fully digitized with descriptions.£#/li#£ £#li#£Historical Atlas in Persuasive Cartography, The PJ Mode Collection, Cornell University Library£#/li#££#/ul#£
Other links
£#ul#££#li#£Google Earth: a visual 3D interactive atlas.£#/li#£ £#li#£NASA's World Wind software.£#/li#£ £#li#£Wikimapia a wikiproject designed to describe the entire world.£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Topology > Manifolds £#/li#££#/ul#£




£#h3#£Autocorrelation£#/h3#£

Autocorrelation, sometimes known as serial correlation in the discrete time case, is the correlation of a signal with a delayed copy of itself as a function of delay. Informally, it is the similarity between observations as a function of the time lag between them. The analysis of autocorrelation is a mathematical tool for finding repeating patterns, such as the presence of a periodic signal obscured by noise, or identifying the missing fundamental frequency in a signal implied by its harmonic frequencies. It is often used in signal processing for analyzing functions or series of values, such as time domain signals.

Different fields of study define autocorrelation differently, and not all of these definitions are equivalent. In some fields, the term is used interchangeably with autocovariance.

Unit root processes, trend-stationary processes, autoregressive processes, and moving average processes are specific forms of processes with autocorrelation.


£#h5#£Auto-correlation of stochastic processes£#/h5#£
In statistics, the autocorrelation of a real or complex random process is the Pearson correlation between values of the process at different times, as a function of the two times or of the time lag. Let ${\displaystyle \left\{X_{t}\right\}}$ be a random process, and ${\displaystyle t}$ be any point in time ( ${\displaystyle t}$ may be an integer for a discrete-time process or a real number for a continuous-time process). Then ${\displaystyle X_{t}}$ is the value (or realization) produced by a given run of the process at time ${\displaystyle t}$ . Suppose that the process has mean ${\displaystyle \mu _{t}}$ and variance ${\displaystyle \sigma _{t}^{2}}$ at time ${\displaystyle t}$ , for each ${\displaystyle t}$ . Then the definition of the auto-correlation function between times ${\displaystyle t_{1}}$ and ${\displaystyle t_{2}}$ is: p.388 : p.165 

where ${\displaystyle \operatorname {E} }$ is the expected value operator and the bar represents complex conjugation. Note that the expectation may not be well defined.

Subtracting the mean before multiplication yields the auto-covariance function between times ${\displaystyle t_{1}}$ and ${\displaystyle t_{2}}$ :: p.392 : p.168 

Note that this expression is not well defined for all time series or processes, because the mean may not exist, or the variance may be zero (for a constant process) or infinite (for processes with distribution lacking well-behaved moments, such as certain types of power law).


£#h5#£Definition for wide-sense stationary stochastic process£#/h5#£
If ${\displaystyle \left\{X_{t}\right\}}$ is a wide-sense stationary process then the mean ${\displaystyle \mu }$ and the variance ${\displaystyle \sigma ^{2}}$ are time-independent, and further the autocovariance function depends only on the lag between ${\displaystyle t_{1}}$ and ${\displaystyle t_{2}}$ : the autocovariance depends only on the time-distance between the pair of values but not on their position in time. This further implies that the autocovariance and auto-correlation can be expressed as a function of the time-lag, and that this would be an even function of the lag ${\displaystyle \tau =t_{2}-t_{1}}$ . This gives the more familiar forms for the auto-correlation function: p.395 

and the auto-covariance function:


£#h5#£Normalization£#/h5#£
It is common practice in some disciplines (e.g. statistics and time series analysis) to normalize the autocovariance function to get a time-dependent Pearson correlation coefficient. However, in other disciplines (e.g. engineering) the normalization is usually dropped and the terms "autocorrelation" and "autocovariance" are used interchangeably.

The definition of the auto-correlation coefficient of a stochastic process is: p.169 

If the function ${\displaystyle \rho _{XX}}$ is well defined, its value must lie in the range ${\displaystyle [-1,1]}$ , with 1 indicating perfect correlation and −1 indicating perfect anti-correlation.

For a weak-sense stationarity, wide-sense stationarity (WSS) process, the definition is

where

The normalization is important both because the interpretation of the autocorrelation as a correlation provides a scale-free measure of the strength of statistical dependence, and because the normalization has an effect on the statistical properties of the estimated autocorrelations.


£#h5#£Properties£#/h5#£
£#h5#£Symmetry property£#/h5#£
The fact that the auto-correlation function ${\displaystyle \operatorname {R} _{XX}}$ is an even function can be stated as: p.171 

respectively for a WSS process:: p.173 
£#h5#£Maximum at zero£#/h5#£
For a WSS process:: p.174 

Notice that ${\displaystyle \operatorname {R} _{XX}(0)}$ is always real.
£#h5#£Cauchy–Schwarz inequality£#/h5#£
The Cauchy–Schwarz inequality, inequality for stochastic processes:: p.392 


£#h5#£Autocorrelation of white noise£#/h5#£
The autocorrelation of a continuous-time white noise signal will have a strong peak (represented by a Dirac delta function) at ${\displaystyle \tau =0}$ and will be exactly ${\displaystyle 0}$ for all other ${\displaystyle \tau }$ .


£#h5#£Wiener–Khinchin theorem£#/h5#£
The Wiener–Khinchin theorem relates the autocorrelation function ${\displaystyle \operatorname {R} _{XX}}$ to the power spectral density ${\displaystyle S_{XX}}$ via the Fourier transform:

For real-valued functions, the symmetric autocorrelation function has a real symmetric transform, so the Wiener–Khinchin theorem can be re-expressed in terms of real cosines only:


£#h5#£Auto-correlation of random vectors£#/h5#£
The (potentially time-dependent) auto-correlation matrix (also called second moment) of a (potentially time-dependent) random vector ${\displaystyle \mathbf {X} =(X_{1},\ldots ,X_{n})^{\rm {T}}}$ is an ${\displaystyle n\times n}$ matrix containing as elements the autocorrelations of all pairs of elements of the random vector ${\displaystyle \mathbf {X} }$ . The autocorrelation matrix is used in various digital signal processing algorithms.

For a random vector ${\displaystyle \mathbf {X} =(X_{1},\ldots ,X_{n})^{\rm {T}}}$ containing random elements whose expected value and variance exist, the auto-correlation matrix is defined by: p.190 : p.334 

where ${\displaystyle {}^{\rm {T}}}$ denotes transposition and has dimensions ${\displaystyle n\times n}$ .

Written component-wise:

If ${\displaystyle \mathbf {Z} }$ is a complex random vector, the autocorrelation matrix is instead defined by

Here ${\displaystyle {}^{\rm {H}}}$ denotes Hermitian transposition.

For example, if ${\displaystyle \mathbf {X} =\left(X_{1},X_{2},X_{3}\right)^{\rm {T}}}$ is a random vector, then ${\displaystyle \operatorname {R} _{\mathbf {X} \mathbf {X} }}$ is a ${\displaystyle 3\times 3}$ matrix whose ${\displaystyle (i,j)}$ -th entry is ${\displaystyle \operatorname {E} [X_{i}X_{j}]}$ .


£#h5#£Properties of the autocorrelation matrix£#/h5#£ £#ul#££#li#£The autocorrelation matrix is a Hermitian matrix for complex random vectors and a symmetric matrix for real random vectors.: p.190 £#/li#£ £#li#£The autocorrelation matrix is a positive semidefinite matrix,: p.190  i.e. ${\displaystyle \mathbf {a} ^{\mathrm {T} }\operatorname {R} _{\mathbf {X} \mathbf {X} }\mathbf {a} \geq 0\quad {\text{for all }}\mathbf {a} \in \mathbb {R} ^{n}}$ for a real random vector, and respectively ${\displaystyle \mathbf {a} ^{\mathrm {H} }\operatorname {R} _{\mathbf {Z} \mathbf {Z} }\mathbf {a} \geq 0\quad {\text{for all }}\mathbf {a} \in \mathbb {C} ^{n}}$ in case of a complex random vector.£#/li#£ £#li#£All eigenvalues of the autocorrelation matrix are real and non-negative.£#/li#£ £#li#£The auto-covariance matrix is related to the autocorrelation matrix as follows:Respectively for complex random vectors:£#/li#££#/ul#£
£#h5#£Auto-correlation of deterministic signals£#/h5#£
In signal processing, the above definition is often used without the normalization, that is, without subtracting the mean and dividing by the variance. When the autocorrelation function is normalized by mean and variance, it is sometimes referred to as the autocorrelation coefficient or autocovariance function.


£#h5#£Auto-correlation of continuous-time signal£#/h5#£
Given a signal ${\displaystyle f(t)}$ , the continuous autocorrelation ${\displaystyle R_{ff}(\tau )}$ is most often defined as the continuous cross-correlation integral of ${\displaystyle f(t)}$ with itself, at lag ${\displaystyle \tau }$ .: p.411 

where ${\displaystyle {\overline {f(t)}}}$ represents the complex conjugate of ${\displaystyle f(t)}$ . Note that the parameter ${\displaystyle t}$ in the integral is a dummy variable and is only necessary to calculate the integral. It has no specific meaning.


£#h5#£Auto-correlation of discrete-time signal£#/h5#£
The discrete autocorrelation ${\displaystyle R}$ at lag ${\displaystyle \ell }$ for a discrete-time signal ${\displaystyle y(n)}$ is

The above definitions work for signals that are square integrable, or square summable, that is, of finite energy. Signals that "last forever" are treated instead as random processes, in which case different definitions are needed, based on expected values. For wide-sense-stationary random processes, the autocorrelations are defined as

For processes that are not stationary, these will also be functions of ${\displaystyle t}$ , or ${\displaystyle n}$ .

For processes that are also ergodic, the expectation can be replaced by the limit of a time average. The autocorrelation of an ergodic process is sometimes defined as or equated to

These definitions have the advantage that they give sensible well-defined single-parameter results for periodic functions, even when those functions are not the output of stationary ergodic processes.

Alternatively, signals that last forever can be treated by a short-time autocorrelation function analysis, using finite time integrals. (See short-time Fourier transform for a related process.)


£#h5#£Definition for periodic signals£#/h5#£
If ${\displaystyle f}$ is a continuous periodic function of period ${\displaystyle T}$ , the integration from ${\displaystyle -\infty }$ to ${\displaystyle \infty }$ is replaced by integration over any interval ${\displaystyle [t_{0},t_{0}+T]}$ of length ${\displaystyle T}$ :

which is equivalent to


£#h5#£Properties£#/h5#£
In the following, we will describe properties of one-dimensional autocorrelations only, since most properties are easily transferred from the one-dimensional case to the multi-dimensional cases. These properties hold for wide-sense stationary processes.

£#ul#££#li#£A fundamental property of the autocorrelation is symmetry, ${\displaystyle R_{ff}(\tau )=R_{ff}(-\tau )}$ , which is easy to prove from the definition. In the continuous case, £#ul#££#li#£the autocorrelation is an even function ${\displaystyle R_{ff}(-\tau )=R_{ff}(\tau )}$ when ${\displaystyle f}$ is a real function, and£#/li#£ £#li#£the autocorrelation is a Hermitian function ${\displaystyle R_{ff}(-\tau )=R_{ff}^{*}(\tau )}$ when ${\displaystyle f}$ is a complex function.£#/li#££#/ul#££#/li#£ £#li#£The continuous autocorrelation function reaches its peak at the origin, where it takes a real value, i.e. for any delay ${\displaystyle \tau }$ , ${\displaystyle |R_{ff}(\tau )|\leq R_{ff}(0)}$ .: p.410  This is a consequence of the rearrangement inequality. The same result holds in the discrete case.£#/li#£ £#li#£The autocorrelation of a periodic function is, itself, periodic with the same period.£#/li#£ £#li#£The autocorrelation of the sum of two completely uncorrelated functions (the cross-correlation is zero for all ${\displaystyle \tau }$ ) is the sum of the autocorrelations of each function separately.£#/li#£ £#li#£Since autocorrelation is a specific type of cross-correlation, it maintains all the properties of cross-correlation.£#/li#£ £#li#£By using the symbol ${\displaystyle *}$ to represent convolution and ${\displaystyle g_{-1}}$ is a function which manipulates the function ${\displaystyle f}$ and is defined as ${\displaystyle g_{-1}(f)(t)=f(-t)}$ , the definition for ${\displaystyle R_{ff}(\tau )}$ may be written as:£#/li#££#/ul#£
£#h5#£Multi-dimensional autocorrelation£#/h5#£
Multi-dimensional autocorrelation is defined similarly. For example, in three dimensions the autocorrelation of a square-summable discrete signal would be

When mean values are subtracted from signals before computing an autocorrelation function, the resulting function is usually called an auto-covariance function.


£#h5#£Efficient computation£#/h5#£
For data expressed as a discrete sequence, it is frequently necessary to compute the autocorrelation with high computational efficiency. A brute force method based on the signal processing definition ${\displaystyle R_{xx}(j)=\sum _{n}x_{n}\,{\overline {x}}_{n-j}}$ can be used when the signal size is small. For example, to calculate the autocorrelation of the real signal sequence ${\displaystyle x=(2,3,-1)}$ (i.e. ${\displaystyle x_{0}=2,x_{1}=3,x_{2}=-1}$ , and ${\displaystyle x_{i}=0}$ for all other values of i) by hand, we first recognize that the definition just given is the same as the "usual" multiplication, but with right shifts, where each vertical addition gives the autocorrelation for particular lag values:

Thus the required autocorrelation sequence is ${\displaystyle R_{xx}=(-2,3,14,3,-2)}$ , where ${\displaystyle R_{xx}(0)=14,}$ ${\displaystyle R_{xx}(-1)=R_{xx}(1)=3,}$ and ${\displaystyle R_{xx}(-2)=R_{xx}(2)=-2,}$ the autocorrelation for other lag values being zero. In this calculation we do not perform the carry-over operation during addition as is usual in normal multiplication. Note that we can halve the number of operations required by exploiting the inherent symmetry of the autocorrelation. If the signal happens to be periodic, i.e. ${\displaystyle x=(\ldots ,2,3,-1,2,3,-1,\ldots ),}$ then we get a circular autocorrelation (similar to circular convolution) where the left and right tails of the previous autocorrelation sequence will overlap and give ${\displaystyle R_{xx}=(\ldots ,14,1,1,14,1,1,\ldots )}$ which has the same period as the signal sequence ${\displaystyle x.}$ The procedure can be regarded as an application of the convolution property of Z-transform of a discrete signal.

While the brute force algorithm is order n2, several efficient algorithms exist which can compute the autocorrelation in order n log(n). For example, the Wiener–Khinchin theorem allows computing the autocorrelation from the raw data X(t) with two fast Fourier transforms (FFT):

where IFFT denotes the inverse fast Fourier transform. The asterisk denotes complex conjugate.

Alternatively, a multiple τ correlation can be performed by using brute force calculation for low τ values, and then progressively binning the X(t) data with a logarithmic density to compute higher values, resulting in the same n log(n) efficiency, but with lower memory requirements.


£#h5#£Estimation£#/h5#£
For a discrete process with known mean and variance for which we observe ${\displaystyle n}$ observations ${\displaystyle \{X_{1},\,X_{2},\,\ldots ,\,X_{n}\}}$ , an estimate of the autocorrelation coefficient may be obtained as

for any positive integer ${\displaystyle k<n}$ . When the true mean ${\displaystyle \mu }$ and variance ${\displaystyle \sigma ^{2}}$ are known, this estimate is unbiased. If the true mean and variance of the process are not known there are several possibilities:

£#ul#££#li#£If ${\displaystyle \mu }$ and ${\displaystyle \sigma ^{2}}$ are replaced by the standard formulae for sample mean and sample variance, then this is a biased estimate.£#/li#£ £#li#£A periodogram-based estimate replaces ${\displaystyle n-k}$ in the above formula with ${\displaystyle n}$ . This estimate is always biased; however, it usually has a smaller mean squared error.£#/li#£ £#li#£Other possibilities derive from treating the two portions of data ${\displaystyle \{X_{1},\,X_{2},\,\ldots ,\,X_{n-k}\}}$ and ${\displaystyle \{X_{k+1},\,X_{k+2},\,\ldots ,\,X_{n}\}}$ separately and calculating separate sample means and/or sample variances for use in defining the estimate.£#/li#££#/ul#£
The advantage of estimates of the last type is that the set of estimated autocorrelations, as a function of ${\displaystyle k}$ , then form a function which is a valid autocorrelation in the sense that it is possible to define a theoretical process having exactly that autocorrelation. Other estimates can suffer from the problem that, if they are used to calculate the variance of a linear combination of the ${\displaystyle X}$ 's, the variance calculated may turn out to be negative.


£#h5#£Regression analysis£#/h5#£
In regression analysis using time series data, autocorrelation in a variable of interest is typically modeled either with an autoregressive model (AR), a moving average model (MA), their combination as an autoregressive-moving-average model (ARMA), or an extension of the latter called an autoregressive integrated moving average model (ARIMA). With multiple interrelated data series, vector autoregression (VAR) or its extensions are used.

In ordinary least squares (OLS), the adequacy of a model specification can be checked in part by establishing whether there is autocorrelation of the regression residuals. Problematic autocorrelation of the errors, which themselves are unobserved, can generally be detected because it produces autocorrelation in the observable residuals. (Errors are also known as "error terms" in econometrics.) Autocorrelation of the errors violates the ordinary least squares assumption that the error terms are uncorrelated, meaning that the Gauss Markov theorem does not apply, and that OLS estimators are no longer the Best Linear Unbiased Estimators (BLUE). While it does not bias the OLS coefficient estimates, the standard errors tend to be underestimated (and the t-scores overestimated) when the autocorrelations of the errors at low lags are positive.

The traditional test for the presence of first-order autocorrelation is the Durbin–Watson statistic or, if the explanatory variables include a lagged dependent variable, Durbin's h statistic. The Durbin-Watson can be linearly mapped however to the Pearson correlation between values and their lags. A more flexible test, covering autocorrelation of higher orders and applicable whether or not the regressors include lags of the dependent variable, is the Breusch–Godfrey test. This involves an auxiliary regression, wherein the residuals obtained from estimating the model of interest are regressed on (a) the original regressors and (b) k lags of the residuals, where 'k' is the order of the test. The simplest version of the test statistic from this auxiliary regression is TR2, where T is the sample size and R2 is the coefficient of determination. Under the null hypothesis of no autocorrelation, this statistic is asymptotically distributed as ${\displaystyle \chi ^{2}}$ with k degrees of freedom.

Responses to nonzero autocorrelation include generalized least squares and the Newey–West HAC estimator (Heteroskedasticity and Autocorrelation Consistent).

In the estimation of a moving average model (MA), the autocorrelation function is used to determine the appropriate number of lagged error terms to be included. This is based on the fact that for an MA process of order q, we have ${\displaystyle R(\tau )\neq 0}$ , for ${\displaystyle \tau =0,1,\ldots ,q}$ , and ${\displaystyle R(\tau )=0}$ , for ${\displaystyle \tau >q}$ .


£#h5#£Applications£#/h5#£ £#ul#££#li#£Autocorrelation analysis is used heavily in fluorescence correlation spectroscopy to provide quantitative insight into molecular-level diffusion and chemical reactions.£#/li#£ £#li#£Another application of autocorrelation is the measurement of optical spectra and the measurement of very-short-duration light pulses produced by lasers, both using optical autocorrelators.£#/li#£ £#li#£Autocorrelation is used to analyze dynamic light scattering data, which notably enables determination of the particle size distributions of nanometer-sized particles or micelles suspended in a fluid. A laser shining into the mixture produces a speckle pattern that results from the motion of the particles. Autocorrelation of the signal can be analyzed in terms of the diffusion of the particles. From this, knowing the viscosity of the fluid, the sizes of the particles can be calculated.£#/li#£ £#li#£Utilized in the GPS system to correct for the propagation delay, or time shift, between the point of time at the transmission of the carrier signal at the satellites, and the point of time at the receiver on the ground. This is done by the receiver generating a replica signal of the 1,023-bit C/A (Coarse/Acquisition) code, and generating lines of code chips [-1,1] in packets of ten at a time, or 10,230 chips (1,023 × 10), shifting slightly as it goes along in order to accommodate for the doppler shift in the incoming satellite signal, until the receiver replica signal and the satellite signal codes match up.£#/li#£ £#li#£The small-angle X-ray scattering intensity of a nanostructured system is the Fourier transform of the spatial autocorrelation function of the electron density.£#/li#£ £#li#£In surface science and scanning probe microscopy, autocorrelation is used to establish a link between surface morphology and functional characteristics.£#/li#£ £#li#£In optics, normalized autocorrelations and cross-correlations give the degree of coherence of an electromagnetic field.£#/li#£ £#li#£In signal processing, autocorrelation can give information about repeating events like musical beats (for example, to determine tempo) or pulsar frequencies, though it cannot tell the position in time of the beat. It can also be used to estimate the pitch of a musical tone.£#/li#£ £#li#£In music recording, autocorrelation is used as a pitch detection algorithm prior to vocal processing, as a distortion effect or to eliminate undesired mistakes and inaccuracies.£#/li#£ £#li#£Autocorrelation in space rather than time, via the Patterson function, is used by X-ray diffractionists to help recover the "Fourier phase information" on atom positions not available through diffraction alone.£#/li#£ £#li#£In statistics, spatial autocorrelation between sample locations also helps one estimate mean value uncertainties when sampling a heterogeneous population.£#/li#£ £#li#£The SEQUEST algorithm for analyzing mass spectra makes use of autocorrelation in conjunction with cross-correlation to score the similarity of an observed spectrum to an idealized spectrum representing a peptide.£#/li#£ £#li#£In astrophysics, autocorrelation is used to study and characterize the spatial distribution of galaxies in the universe and in multi-wavelength observations of low mass X-ray binaries.£#/li#£ £#li#£In panel data, spatial autocorrelation refers to correlation of a variable with itself through space.£#/li#£ £#li#£In analysis of Markov chain Monte Carlo data, autocorrelation must be taken into account for correct error determination.£#/li#£ £#li#£In geosciences (specifically in geophysics) it can be used to compute an autocorrelation seismic attribute, out of a 3D seismic survey of the underground.£#/li#£ £#li#£In medical ultrasound imaging, autocorrelation is used to visualize blood flow.£#/li#£ £#li#£In intertemporal portfolio choice, the presence or absence of autocorrelation in an asset's rate of return can affect the optimal portion of the portfolio to hold in that asset.£#/li#£ £#li#£Autocorrelation has been used to accurately measure power system frequency in Numerical relays.£#/li#££#/ul#£
£#h5#£Serial dependence£#/h5#£
Serial dependence is closely linked to the notion of autocorrelation, but represents a distinct concept (see Correlation and dependence). In particular, it is possible to have serial dependence but no (linear) correlation. In some fields however, the two terms are used as synonyms.

A time series of a random variable has serial dependence if the value at some time ${\displaystyle t}$ in the series is statistically dependent on the value at another time ${\displaystyle s}$ . A series is serially independent if there is no dependence between any pair.

If a time series ${\displaystyle \left\{X_{t}\right\}}$ is stationary, then statistical dependence between the pair ${\displaystyle (X_{t},X_{s})}$ would imply that there is statistical dependence between all pairs of values at the same lag ${\displaystyle \tau =s-t}$ .


£#h5#£See also£#/h5#£
£#h5#£References£#/h5#£
£#h5#£Further reading£#/h5#£ £#ul#££#li#£Kmenta, Jan (1986). Elements of Econometrics (Second ed.). New York: Macmillan. pp. 298–334. ISBN 978-0-02-365070-3.£#/li#£ £#li#£Marno Verbeek (10 August 2017). A Guide to Modern Econometrics. Wiley. ISBN 978-1-119-40110-0.£#/li#£ £#li#£Mojtaba Soltanalian, and Petre Stoica. "Computational design of sequences with good correlation properties." IEEE Transactions on Signal Processing, 60.5 (2012): 2180–2193.£#/li#£ £#li#£Solomon W. Golomb, and Guang Gong. Signal design for good correlation: for wireless communication, cryptography, and radar. Cambridge University Press, 2005.£#/li#£ £#li#£Klapetek, Petr (2018). Quantitative Data Processing in Scanning Probe Microscopy: SPM Applications for Nanometrology (Second ed.). Elsevier. pp. 108–112 ISBN 9780128133477.£#/li#£ £#li#£Weisstein, Eric W. "Autocorrelation". MathWorld.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Bracewell, R. "The Autocorrelation Function." The Fourier Transform and Its Applications. New York: McGraw-Hill, pp. 40-45, 1965.£#/li#££#li#£Papoulis, A. The Fourier Integral and Its Applications. New York: McGraw-Hill, 1962.£#/li#££#li#£Press, W. H.; Flannery, B. P.; Teukolsky, S. A.; and Vetterling, W. T. "Correlation and Autocorrelation Using the FFT." §13.2 in Numerical Recipes in FORTRAN: The Art of Scientific Computing, 2nd ed. Cambridge, England: Cambridge University Press, pp. 538-539, 1992.£#/li#££#li#£Zwillinger, D. (Ed.). CRC Standard Mathematical Tables and Formulae. Boca Raton, FL: CRC Press, p. 223, 1995.£#/li#££#li#£ Bracewell, R. "The Autocorrelation Function." The Fourier Transform and Its Applications. New York: McGraw-Hill, pp. 40-45, 1965. £#/li#££#li#£ Papoulis, A. The Fourier Integral and Its Applications. New York: McGraw-Hill, 1962. £#/li#££#li#£ Press, W. H.; Flannery, B. P.; Teukolsky, S. A.; and Vetterling, W. T. "Correlation and Autocorrelation Using the FFT." §13.2 in Numerical Recipes in FORTRAN: The Art of Scientific Computing, 2nd ed. Cambridge, England: Cambridge University Press, pp. 538-539, 1992. £#/li#££#li#£ Zwillinger, D. (Ed.). CRC Standard Mathematical Tables and Formulae. Boca Raton, FL: CRC Press, p. 223, 1995. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Integral Transforms > General Integral Transforms £#/li#££#/ul#£




£#h3#£Automorphic Form£#/h3#£

In harmonic analysis and number theory, an automorphic form is a well-behaved function from a topological group G to the complex numbers (or complex vector space) which is invariant under the action of a discrete subgroup ${\displaystyle \Gamma \subset G}$ of the topological group. Automorphic forms are a generalization of the idea of periodic functions in Euclidean space to general topological groups.

Modular forms are holomorphic automorphic forms defined over the groups SL(2, R) or PSL(2, R) with the discrete subgroup being the modular group, or one of its congruence subgroups; in this sense the theory of automorphic forms is an extension of the theory of modular forms. More generally, one can use the adelic approach as a way of dealing with the whole family of congruence subgroups at once. From this point of view, an automorphic form over the group G(AF), for an algebraic group G and an algebraic number field F, is a complex-valued function on G(AF) that is left invariant under G(F) and satisfies certain smoothness and growth conditions.

Poincaré first discovered automorphic forms as generalizations of trigonometric and elliptic functions. Through the Langlands conjectures automorphic forms play an important role in modern number theory.


£#h5#£Definition£#/h5#£
In mathematics, the notion of factor of automorphy arises for a group acting on a complex-analytic manifold. Suppose a group ${\displaystyle G}$ acts on a complex-analytic manifold ${\displaystyle X}$ . Then, ${\displaystyle G}$ also acts on the space of holomorphic functions from ${\displaystyle X}$ to the complex numbers. A function ${\displaystyle f}$ is termed an automorphic form if the following holds:

${\displaystyle f(g.x)=j_{g}(x)f(x)}$
where ${\displaystyle j_{g}(x)}$ is an everywhere nonzero holomorphic function. Equivalently, an automorphic form is a function whose divisor is invariant under the action of ${\displaystyle G}$ .

The factor of automorphy for the automorphic form ${\displaystyle f}$ is the function ${\displaystyle j}$ . An automorphic function is an automorphic form for which ${\displaystyle j}$ is the identity.


An automorphic form is a function F on G (with values in some fixed finite-dimensional vector space V, in the vector-valued case), subject to three kinds of conditions:

£#li#£to transform under translation by elements ${\displaystyle \gamma \in \Gamma }$ according to the given factor of automorphy j;£#/li#£ £#li#£to be an eigenfunction of certain Casimir operators on G; and£#/li#£ £#li#£to satisfy a "moderate growth" asymptotic condition a height function.£#/li#£
It is the first of these that makes F automorphic, that is, satisfy an interesting functional equation relating F(g) with F(γg) for ${\displaystyle \gamma \in \Gamma }$ . In the vector-valued case the specification can involve a finite-dimensional group representation ρ acting on the components to 'twist' them. The Casimir operator condition says that some Laplacians have F as eigenfunction; this ensures that F has excellent analytic properties, but whether it is actually a complex-analytic function depends on the particular case. The third condition is to handle the case where G/Γ is not compact but has cusps.

The formulation requires the general notion of factor of automorphy j for Γ, which is a type of 1-cocycle in the language of group cohomology. The values of j may be complex numbers, or in fact complex square matrices, corresponding to the possibility of vector-valued automorphic forms. The cocycle condition imposed on the factor of automorphy is something that can be routinely checked, when j is derived from a Jacobian matrix, by means of the chain rule.

A more straightforward but technically advanced definition using class field theory, constructs automorphic forms and their correspondent functions as embeddings of Galois groups to their underlying global field extensions. In this formulation, automorphic forms are certain finite invariants, mapping from the idele class group under the Artin reciprocity law. Herein, the analytical structure of its L-function allows for generalizations with various algebro-geometric properties; and the resultant Langlands program. To oversimplify, automorphic forms in this general perspective, are analytic functionals quantifying the invariance of number fields in a most abstract sense, therefore indicating the 'primitivity' of their fundamental structure. Allowing a powerful mathematical tool for analyzing the invariant constructs of virtually any numerical structure.

Examples of automorphic forms in an explicit unabstracted state are difficult to obtain, though some have directly analytical properties:

- The Eisenstein series (which is a prototypical modular form) over certain field extensions as Abelian groups.

- Specific generalizations of Dirichlet L-functions as class field-theoretic objects.

- Generally any harmonic analytic object as a functor over Galois groups which is invariant on its ideal class group (or idele).

As a general principle, automorphic forms can be thought of as analytic functions on abstract structures, which are invariant with respect to a generalized analogue of their prime ideal (or an abstracted irreducible fundamental representation). As mentioned, automorphic functions can be seen as generalizations of modular forms (as therefore elliptic curves), constructed by some zeta function analogue on an automorphic structure. In the simplest sense, automorphic forms are modular forms defined on general Lie groups; because of their symmetry properties. Therefore in simpler terms, a general function which analyzes the invariance of a structure with respect to its prime 'morphology'.


£#h5#£History£#/h5#£
Before this very general setting was proposed (around 1960), there had already been substantial developments of automorphic forms other than modular forms. The case of Γ a Fuchsian group had already received attention before 1900 (see below). The Hilbert modular forms (also called Hilbert-Blumenthal forms) were proposed not long after that, though a full theory was long in coming. The Siegel modular forms, for which G is a symplectic group, arose naturally from considering moduli spaces and theta functions. The post-war interest in several complex variables made it natural to pursue the idea of automorphic form in the cases where the forms are indeed complex-analytic. Much work was done, in particular by Ilya Piatetski-Shapiro, in the years around 1960, in creating such a theory. The theory of the Selberg trace formula, as applied by others, showed the considerable depth of the theory. Robert Langlands showed how (in generality, many particular cases being known) the Riemann–Roch theorem could be applied to the calculation of dimensions of automorphic forms; this is a kind of post hoc check on the validity of the notion. He also produced the general theory of Eisenstein series, which corresponds to what in spectral theory terms would be the 'continuous spectrum' for this problem, leaving the cusp form or discrete part to investigate. From the point of view of number theory, the cusp forms had been recognised, since Srinivasa Ramanujan, as the heart of the matter.


£#h5#£Automorphic representations£#/h5#£
The subsequent notion of an "automorphic representation" has proved of great technical value when dealing with G an algebraic group, treated as an adelic algebraic group. It does not completely include the automorphic form idea introduced above, in that the adelic approach is a way of dealing with the whole family of congruence subgroups at once. Inside an L2 space for a quotient of the adelic form of G, an automorphic representation is a representation that is an infinite tensor product of representations of p-adic groups, with specific enveloping algebra representations for the infinite prime(s). One way to express the shift in emphasis is that the Hecke operators are here in effect put on the same level as the Casimir operators; which is natural from the point of view of functional analysis, though not so obviously for the number theory. It is this concept that is basic to the formulation of the Langlands philosophy.


£#h5#£Poincaré on discovery and his work on automorphic functions£#/h5#£
One of Poincaré's first discoveries in mathematics, dating to the 1880s, was automorphic forms. He named them Fuchsian functions, after the mathematician Lazarus Fuchs, because Fuchs was known for being a good teacher and had researched on differential equations and the theory of functions. Poincaré actually developed the concept of these functions as part of his doctoral thesis. Under Poincaré's definition, an automorphic function is one which is analytic in its domain and is invariant under a discrete infinite group of linear fractional transformations. Automorphic functions then generalize both trigonometric and elliptic functions.

Poincaré explains how he discovered Fuchsian functions:

For fifteen days I strove to prove that there could not be any functions like those I have since called Fuchsian functions. I was then very ignorant; every day I seated myself at my work table, stayed an hour or two, tried a great number of combinations and reached no results. One evening, contrary to my custom, I drank black coffee and could not sleep. Ideas rose in crowds; I felt them collide until pairs interlocked, so to speak, making a stable combination. By the next morning I had established the existence of a class of Fuchsian functions, those which come from the hypergeometric series; I had only to write out the results, which took but a few hours.


£#h5#£See also£#/h5#£ £#ul#££#li#£Automorphic factor£#/li#£ £#li#£Factor of automorphy£#/li#£ £#li#£Maass cusp form£#/li#£ £#li#£Automorphic Forms on GL(2), a book by H. Jacquet and Robert Langlands£#/li#£ £#li#£Jacobi form£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Stephen Gelbart (1975), "Automorphic forms on Adele groups", ISBN 9780608066042£#/li#£ £#li#£This article incorporates material from Jules Henri Poincaré on PlanetMath, which is licensed under the Creative Commons Attribution/Share-Alike License.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£ Quotations related to Automorphic form at Wikiquote£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Complex Analysis > General Complex Analysis £#/li#££#/ul#£




£#h3#£Automorphic Function£#/h3#£

In mathematics, an automorphic function is a function on a space that is invariant under the action of some group, in other words a function on the quotient space. Often the space is a complex manifold and the group is a discrete group.


£#h5#£Factor of automorphy£#/h5#£
In mathematics, the notion of factor of automorphy arises for a group acting on a complex-analytic manifold. Suppose a group ${\displaystyle G}$ acts on a complex-analytic manifold ${\displaystyle X}$ . Then, ${\displaystyle G}$ also acts on the space of holomorphic functions from ${\displaystyle X}$ to the complex numbers. A function ${\displaystyle f}$ is termed an automorphic form if the following holds:

${\displaystyle f(g.x)=j_{g}(x)f(x)}$
where ${\displaystyle j_{g}(x)}$ is an everywhere nonzero holomorphic function. Equivalently, an automorphic form is a function whose divisor is invariant under the action of ${\displaystyle G}$ .

The factor of automorphy for the automorphic form ${\displaystyle f}$ is the function ${\displaystyle j}$ . An automorphic function is an automorphic form for which ${\displaystyle j}$ is the identity.

Some facts about factors of automorphy:

£#ul#££#li#£Every factor of automorphy is a cocycle for the action of ${\displaystyle G}$ on the multiplicative group of everywhere nonzero holomorphic functions.£#/li#£ £#li#£The factor of automorphy is a coboundary if and only if it arises from an everywhere nonzero automorphic form.£#/li#£ £#li#£For a given factor of automorphy, the space of automorphic forms is a vector space.£#/li#£ £#li#£The pointwise product of two automorphic forms is an automorphic form corresponding to the product of the corresponding factors of automorphy.£#/li#££#/ul#£
Relation between factors of automorphy and other notions:

£#ul#££#li#£Let ${\displaystyle \Gamma }$ be a lattice in a Lie group ${\displaystyle G}$ . Then, a factor of automorphy for ${\displaystyle \Gamma }$ corresponds to a line bundle on the quotient group ${\displaystyle G/\Gamma }$ . Further, the automorphic forms for a given factor of automorphy correspond to sections of the corresponding line bundle.£#/li#££#/ul#£
The specific case of ${\displaystyle \Gamma }$ a subgroup of SL(2, R), acting on the upper half-plane, is treated in the article on automorphic factors.


£#h5#£Examples£#/h5#£ £#ul#££#li#£Kleinian group£#/li#£ £#li#£Elliptic modular function£#/li#£ £#li#£Modular function£#/li#£ £#li#£Complex torus£#/li#££#/ul#£
£#h5#£References£#/h5#£ £#ul#££#li#£A.N. Parshin (2001) [1994], "Automorphic Form", Encyclopedia of Mathematics, EMS Press£#/li#£ £#li#£Andrianov, A.N.; Parshin, A.N. (2001) [1994], "Automorphic Function", Encyclopedia of Mathematics, EMS Press£#/li#£ £#li#£Ford, Lester R. (1929), Automorphic functions, New York, McGraw-Hill, ISBN 978-0-8218-3741-2, JFM 55.0810.04£#/li#£ £#li#£Fricke, Robert; Klein, Felix (1897), Vorlesungen über die Theorie der automorphen Functionen. Erster Band; Die gruppentheoretischen Grundlagen. (in German), Leipzig: B. G. Teubner, ISBN 978-1-4297-0551-6, JFM 28.0334.01£#/li#£ £#li#£Fricke, Robert; Klein, Felix (1912), Vorlesungen über die Theorie der automorphen Functionen. Zweiter Band: Die funktionentheoretischen Ausführungen und die Anwendungen. 1. Lieferung: Engere Theorie der automorphen Funktionen. (in German), Leipzig: B. G. Teubner., ISBN 978-1-4297-0552-3, JFM 32.0430.01£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Ford, L. Automorphic Functions. New York: McGraw-Hill, 1929.£#/li#££#li#£Hadamard, J.; Gray, J. J.; and Shenitzer, A. Non-Euclidean Geometry in the Theory of Automorphic Forms. Providence, RI: Amer. Math. Soc., 1999.£#/li#££#li#£Shimura, G. Introduction to the Arithmetic Theory of Automorphic Functions. Princeton, NJ: Princeton University Press, 1971.£#/li#££#li#£Siegel, C. L. Topics in Complex Function Theory, Vol. 2: Automorphic Functions and Abelian Integrals. New York: Wiley, 1988.£#/li#££#li#£ Ford, L. Automorphic Functions. New York: McGraw-Hill, 1929. £#/li#££#li#£ Hadamard, J.; Gray, J. J.; and Shenitzer, A. Non-Euclidean Geometry in the Theory of Automorphic Forms. Providence, RI: Amer. Math. Soc., 1999. £#/li#££#li#£ Shimura, G. Introduction to the Arithmetic Theory of Automorphic Functions. Princeton, NJ: Princeton University Press, 1971. £#/li#££#li#£ Siegel, C. L. Topics in Complex Function Theory, Vol. 2: Automorphic Functions and Abelian Integrals. New York: Wiley, 1988. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Complex Analysis > General Complex Analysis £#/li#££#/ul#£




£#h3#£Autonomous£#/h3#£

In developmental psychology and moral, political, and bioethical philosophy, autonomy is the capacity to make an informed, uncoerced decision. Autonomous organizations or institutions are independent or self-governing. Autonomy can also be defined from a human resources perspective, where it denotes a (relatively high) level of discretion granted to an employee in his or her work. In such cases, autonomy is known to generally increase job satisfaction. Self-actualized individuals are thought to operate autonomously of external expectations. In a medical context, respect for a patient's personal autonomy is considered one of many fundamental ethical principles in medicine.


£#h5#£Sociology£#/h5#£
In the sociology of knowledge, a controversy over the boundaries of autonomy inhibited analysis of any concept beyond relative autonomy, until a typology of autonomy was created and developed within science and technology studies. According to it, the institution of science's existing autonomy is “reflexive autonomy”: actors and structures within the scientific field are able to translate or to reflect diverse themes presented by social and political fields, as well as influence them regarding the thematic choices on research projects.


£#h5#£Institutional autonomy£#/h5#£
Institutional autonomy is having the capacity as a legislator to be able to implant and pursue official goals. Autonomous institutions are responsible for finding sufficient resources or modifying their plans, programs, courses, responsibilities, and services accordingly. But in doing so, they must contend with any obstacles that can occur, such as social pressure against cut-backs or socioeconomic difficulties. From a legislator's point of view, to increase institutional autonomy, conditions of self-management and institutional self-governance must be put in place. An increase in leadership and a redistribution of decision-making responsibilities would be beneficial to the research of resources.

Institutional autonomy was often seen as a synonym for self-determination, and many governments feared that it would lead institutions to an irredentist or secessionist region. But autonomy should be seen as a solution to self-determination struggles. Self-determination is a movement toward independence, whereas autonomy is a way to accommodate the distinct regions/groups within a country. Institutional autonomy can diffuse conflicts regarding minorities and ethnic groups in a society. Allowing more autonomy to groups and institutions helps create diplomatic relationships between them and the central government.


£#h5#£Politics£#/h5#£
In governmental parlance, autonomy refers to self-governance. An example of an autonomous jurisdiction was the former United States governance of the Philippine Islands. The Philippine Autonomy Act of 1916 provided the framework for the creation of an autonomous government under which the Filipino people had broader domestic autonomy than previously, although it reserved certain privileges to the United States to protect its sovereign rights and interests. Other examples include Kosovo (as the Socialist Autonomous Province of Kosovo) under the former Yugoslav government of Marshal Tito and Puntland Autonomous Region within Federal Republic of Somalia.


£#h5#£Philosophy£#/h5#£
Autonomy is a key concept that has a broad impact on different fields of philosophy. In metaphysical philosophy, the concept of autonomy is referenced in discussions about free will, fatalism, determinism, and agency. In moral philosophy, autonomy refers to subjecting oneself to objective moral law.


£#h5#£According to Kant£#/h5#£
Immanuel Kant (1724–1804) defined autonomy by three themes regarding contemporary ethics. Firstly, autonomy as the right for one to make their own decisions excluding any interference from others. Secondly, autonomy as the capacity to make such decisions through one's own independence of mind and after personal reflection. Thirdly, as an ideal way of living life autonomously. In summary, autonomy is the moral right one possesses, or the capacity we have in order to think and make decisions for oneself providing some degree of control or power over the events that unfold within one's everyday life.

The context in which Kant addresses autonomy is in regards to moral theory, asking both foundational and abstract questions. He believed that in order for there to be morality, there must be autonomy. He breaks down autonomy into two distinct components. "Auto" can be defined as the negative form of independence, or to be free in a negative sense. This is the aspect where decisions are made on your own. Whereas, "nomos" is the positive sense, a freedom or lawfulness, where you are choosing a law to follow. Kantian autonomy also provides a sense of rational autonomy, simply meaning one rationally possesses the motivation to govern their own life. Rational autonomy entails making your own decisions but it cannot be done solely in isolation. Cooperative rational interactions are required to both develop and exercise our ability to live in a world with others.

Kant argued that morality presupposes this autonomy (German: Autonomie) in moral agents, since moral requirements are expressed in categorical imperatives. An imperative is categorical if it issues a valid command independent of personal desires or interests that would provide a reason for obeying the command. It is hypothetical if the validity of its command, if the reason why one can be expected to obey it, is the fact that one desires or is interested in something further that obedience to the command would entail. "Don't speed on the freeway if you don't want to be stopped by the police" is a hypothetical imperative. "It is wrong to break the law, so don't speed on the freeway" is a categorical imperative. The hypothetical command not to speed on the freeway is not valid for you if you do not care whether you are stopped by the police. The categorical command is valid for you either way. Autonomous moral agents can be expected to obey the command of a categorical imperative even if they lack a personal desire or interest in doing so. It remains an open question whether they will, however.

The Kantian concept of autonomy is often misconstrued, leaving out the important point about the autonomous agent's self-subjection to the moral law. It is thought that autonomy is fully explained as the ability to obey a categorical command independently of a personal desire or interest in doing so—or worse, that autonomy is "obeying" a categorical command independently of a natural desire or interest; and that heteronomy, its opposite, is acting instead on personal motives of the kind referenced in hypothetical imperatives.

In his Groundwork of the Metaphysic of Morals, Kant applied the concept of autonomy also to define the concept of personhood and human dignity. Autonomy, along with rationality, are seen by Kant as the two criteria for a meaningful life. Kant would consider a life lived without these not worth living; it would be a life of value equal to that of a plant or insect. According to Kant autonomy is part of the reason that we hold others morally accountable for their actions. Human actions are morally praise- or blame-worthy in virtue of our autonomy. Non- autonomous beings such as plants or animals are not blameworthy due to their actions being non-autonomous. Kant's position on crime and punishment is influenced by his views on autonomy. Brainwashing or drugging criminals into being law-abiding citizens would be immoral as it would not be respecting their autonomy. Rehabilitation must be sought in a way that respects their autonomy and dignity as human beings.


£#h5#£According to Nietzsche£#/h5#£
Friedrich Nietzsche wrote about autonomy and the moral fight. Autonomy in this sense is referred to as the free self and entails several aspects of the self, including self-respect and even self-love. This can be interpreted as influenced by Kant (self-respect) and Aristotle (self-love). For Nietzsche, valuing ethical autonomy can dissolve the conflict between love (self-love) and law (self-respect) which can then translate into reality through experiences of being self-responsible. Because Nietzsche defines having a sense of freedom with being responsible for one's own life, freedom and self-responsibility can be very much linked to autonomy.


£#h5#£According to Piaget£#/h5#£
The Swiss philosopher Jean Piaget (1896-1980) believed that autonomy comes from within and results from a "free decision". It is of intrinsic value and the morality of autonomy is not only accepted but obligatory. When an attempt at social interchange occurs, it is reciprocal, ideal and natural for there to be autonomy regardless of why the collaboration with others has taken place. For Piaget, the term autonomous can be used to explain the idea that rules are self-chosen. By choosing which rules to follow or not, we are in turn determining our own behaviour.

Piaget studied the cognitive development of children by analyzing them during their games and through interviews, establishing (among other principles) that the children's moral maturation process occurred in two phases, the first of heteronomy and the second of autonomy:

£#ul#££#li#£Heteronomous reasoning: Rules are objective and unchanging. They must be literal because the authority are ordering it and do not fit exceptions or discussions. The base of the rule is the superior authority (parents, adults, the State), that it should not give reason for the rules imposed or fulfilled them in any case. Duties provided are conceived as given from oneself. Any moral motivation and sentiments are possible through what one believes to be right.£#/li#£ £#li#£Autonomous reasoning: Rules are the product of an agreement and, therefore, are modifiable. They can be subject to interpretation and fit exceptions and objections. The base of the rule is its own acceptance, and its meaning has to be explained. Sanctions must be proportionate to the absence, assuming that sometimes offenses can go unpunished, so that collective punishment is unacceptable if it is not the guilty. The circumstances may not punish a guilty. Duties provided are conceived as given from the outside. One follows rules mechanically as it is simply a rule, or as a way to avoid a form of punishment.£#/li#££#/ul#£
£#h5#£According to Kohlberg£#/h5#£
The American psychologist Lawrence Kohlberg (1927-1987) continues the studies of Piaget. His studies collected information from different latitudes to eliminate the cultural variability, and focused on the moral reasoning, and not so much in the behavior or its consequences. Through interviews with adolescent and teenage boys, who were to try and solve "moral dilemmas," Kohlberg went on to further develop the stages of moral development. The answers they provided could be one of two things. Either they choose to obey a given law, authority figure or rule of some sort or they chose to take actions that would serve a human need but in turn break this given rule or command.

The most popular moral dilemma asked involved the wife of a man approaching death due to a special type of cancer. Because the drug was too expensive to obtain on his own, and because the pharmacist who discovered and sold the drug had no compassion for him and only wanted profits, he stole it. Kohlberg asks these adolescent and teenage boys (10-, 13- and 16-year-olds) if they think that is what the husband should have done or not. Therefore, depending on their decisions, they provided answers to Kohlberg about deeper rationales and thoughts and determined what they value as important. This value then determined the "structure" of their moral reasoning.

Kohlberg established three stages of morality, each of which is subdivided into two levels. They are read in progressive sense, that is, higher levels indicate greater autonomy.

£#ul#££#li#£Level 1: Premoral/Preconventional Morality: Standards are met (or not met) depending on the hedonistic or physical consequences. £#ul#££#li#£[Stage 0: Egocentric Judgment: There is no moral concept independent of individual wishes, including a lack of concept of rules or obligations.]£#/li#£ £#li#£Stage 1: Punishment-Obedience Orientation: The rule is obeyed only to avoid punishment. Physical consequences determine goodness or badness and power is deferred to unquestioningly with no respect for the human or moral value, or the meaning of these consequences. Concern is for the self.£#/li#£ £#li#£Stage 2: Instrumental-Relativist Orientation: Morals are individualistic and egocentric. There is an exchange of interests but always under the point of view of satisfying personal needs. Elements of fairness and reciprocity are present but these are interpreted in a pragmatic way, instead of an experience of gratitude or justice. Egocentric in nature but beginning to incorporate the ability to see things from the perspective of others.£#/li#££#/ul#££#/li#£ £#li#£Level 2: Conventional Morality/Role Conformity: Rules are obeyed according to the established conventions of a society. £#ul#££#li#£Stage 3: Good Boy-Nice Girl Orientation: Morals are conceived in accordance with the stereotypical social role. Rules are obeyed to obtain the approval of the immediate group and the right actions are judged based on what would please others or give the impression that one is a good person. Actions are evaluated according to intentions.£#/li#£ £#li#£Stage 4: Law and Order Orientation: Morals are judged in accordance with the authority of the system, or the needs of the social order. Laws and order are prioritized.£#/li#££#/ul#££#/li#£ £#li#£Level 3: Postconventional Morality/Self-Accepted Moral Principles: Standards of moral behavior are internalized. Morals are governed by rational judgment, derived from a conscious reflection on the recognition of the value of the individual inside a conventionally established society. £#ul#££#li#£Stage 5: Social Contract Orientation: There are individual rights and standards that have been lawfully established as basic universal values. Rules are agreed upon by through procedure and society comes to consensus through critical examination in order to benefit the greater good.£#/li#£ £#li#£Stage 6: Universal Principle Orientation: Abstract ethical principles are obeyed on a personal level in addition to societal rules and conventions. Universal principles of justice, reciprocity, equality and human dignity are internalized and if one fails to live up to these ideals, guilt or self-condemnation results.£#/li#££#/ul#££#/li#££#/ul#£
£#h5#£According to Audi£#/h5#£
Robert Audi characterizes autonomy as the self-governing power to bring reasons to bear in directing one's conduct and influencing one's propositional attitudes.: 211–2  Traditionally, autonomy is only concerned with practical matters. But, as Audi's definition suggests, autonomy may be applied to responding to reasons at large, not just to practical reasons. Autonomy is closely related to freedom but the two can come apart. An example would be a political prisoner who is forced to make a statement in favor of his opponents in order to ensure that his loved ones are not harmed. As Audi points out, the prisoner lacks freedom but still has autonomy since his statement, though not reflecting his political ideals, is still an expression of his commitment to his loved ones.: 249 

Autonomy is often equated with self-legislation in the Kantian tradition. Self-legislation may be interpreted as laying down laws or principles that are to be followed. Audi agrees with this school in the sense that we should bring reasons to bear in a principled way. Responding to reasons by mere whim may still be considered free but not autonomous.: 249, 257  A commitment to principles and projects, on the other hand, provides autonomous agents with an identity over time and gives them a sense of the kind of persons they want to be. But autonomy is neutral as to which principles or projects the agent endorses. So different autonomous agents may follow very different principles.: 258  But, as Audi points out, self-legislation is not sufficient for autonomy since laws that don't have any practical impact don't constitute autonomy.: 247–8  Some form of motivational force or executive power is necessary in order to get from mere self-legislation to self-government. This motivation may be inherent in the corresponding practical judgment itself, a position known as motivational internalism, or may come to the practical judgment externally in the form of some desire independent of the judgment, as motivational externalism holds.: 251–2 

In the Humean tradition, intrinsic desires are the reasons the autonomous agent should respond to. This theory is called instrumentalism. Audi rejects instrumentalism and suggests that we should adopt a position known as axiological objectivism. The central idea of this outlook is that objective values, and not subjective desires, are the sources of normativity and therefore determine what autonomous agents should do.: 261ff 


£#h5#£Child development£#/h5#£
Autonomy in childhood and adolescence is when one strives to gain a sense of oneself as a separate, self-governing individual. Between ages 1–3, during the second stage of Erikson's and Freud's stages of development, the psychosocial crisis that occurs is autonomy versus shame and doubt. The significant event that occurs during this stage is that children must learn to be autonomous, and failure to do so may lead to the child doubting their own abilities and feel ashamed. When a child becomes autonomous it allows them to explore and acquire new skills. Autonomy has two vital aspects wherein there is an emotional component where one relies more on themselves rather than their parents and a behavioural component where one makes decisions independently by using their judgement. The styles of child rearing affect the development of a child's autonomy. Authoritative child rearing is the most successful approach, where the parents engage in autonomy granting appropriate to their age and abilities. Autonomy in adolescence is closely related to their quest for identity. In adolescence parents and peers act as agents of influence. Peer influence in early adolescence may help the process of an adolescent to gradually become more autonomous by being less susceptible to parental or peer influence as they get older. In adolescence the most important developmental task is to develop a healthy sense of autonomy.


£#h5#£Religion£#/h5#£
In Christianity, autonomy is manifested as a partial self-governance on various levels of church administration. During the history of Christianity, there were two basic types of autonomy. Some important parishes and monasteries have been given special autonomous rights and privileges, and the best known example of monastic autonomy is the famous Eastern Orthodox monastic community on Mount Athos in Greece. On the other hand, administrative autonomy of entire ecclesiastical provinces has throughout history included various degrees of internal self-governance.

In ecclesiology of Eastern Orthodox Churches, there is a clear distinction between autonomy and autocephaly, since autocephalous churches have full self-governance and independence, while every autonomous church is subject to some autocephalous church, having a certain degree of internal self-governance. Since every autonomous church had its own historical path to ecclesiastical autonomy, there are significant differences between various autonomous churches in respect of their particular degrees of self-governance. For example, churches that are autonomous can have their highest-ranking bishops, such as an archbishop or metropolitan, appointed or confirmed by the patriarch of the mother church from which it was granted its autonomy, but generally they remain self-governing in many other respects.

In the history of Western Christianity the question of ecclesiastical autonomy was also one of the most important questions, especially during the first centuries of Christianity, since various archbishops and metropolitans in Western Europe have often opposed centralizing tendencies of the Church of Rome. As of 2019, the Catholic Church comprises 24 autonomous (sui iuris) Churches in communion with the Holy See. Various denominations of Protestant churches usually have more decentralized power, and churches may be autonomous, thus having their own rules or laws of government, at the national, local, or even individual level.

Sartre brings the concept of the Cartesian god being totally free and autonomous. He states that existence precedes essence with god being the creator of the essences, eternal truths and divine will. This pure freedom of god relates to human freedom and autonomy; where a human is not subjected to pre-existing ideas and values.

According to the first amendment, In the United States of America, the federal government is restricted in building a national church. This is due to the first amendment's recognizing people's freedom's to worship their faith according to their own belief's. For example, the American government has removed the church from their "sphere of authority" due to the churches' historical impact on politics and their authority on the public. This was the beginning of the disestablishment process. The Protestant churches in the United States had a significant impact on American culture in the nineteenth century, when they organized the establishment of schools, hospitals, orphanages, colleges, magazines, and so forth. This has brought up the famous, however, misinterpreted term of the separation of church and state. These churches lost the legislative and financial support from the state.


£#h5#£The disestablishment process£#/h5#£
The first disestablishment began with the introduction of the bill of rights. In the twentieth century, due to the great depression of the 1930s and the completion of the second world war, the American churches were revived. Specifically the Protestant churches. This was the beginning of the second disestablishment when churches had become popular again but held no legislative power. One of the reasons why the churches gained attendance and popularity was due to the baby boom, when soldiers came back from the second world war and started their families. The large influx of newborns gave the churches a new wave of followers. However, these followers did not hold the same beliefs as their parents and brought about the political, and religious revolutions of the 1960s.

During the 1960s, the collapse of religious and cultural middle brought upon the third disestablishment. Religion became more important to the individual and less so to the community. The changes brought from these revolutions significantly increased the personal autonomy of individuals due to the lack of structural restraints giving them added freedom of choice. This concept is known as "new voluntarism" where individuals have free choice on how to be religious and the free choice whether to be religious or not.


£#h5#£Medicine£#/h5#£
In a medical context, respect for a patient's personal autonomy is considered one of many fundamental ethical principles in medicine. Autonomy can be defined as the ability of the person to make his or her own decisions. This faith in autonomy is the central premise of the concept of informed consent and shared decision making. This idea, while considered essential to today's practice of medicine, was developed in the last 50 years. According to Tom Beauchamp and James Childress (in Principles of Biomedical Ethics), the Nuremberg trials detailed accounts of horrifyingly exploitative medical "experiments" which violated the subjects' physical integrity and personal autonomy. These incidences prompted calls for safeguards in medical research, such as the Nuremberg Code which stressed the importance of voluntary participation in medical research. It is believed that the Nuremberg Code served as the premise for many current documents regarding research ethics.

Respect for autonomy became incorporated in health care and patients could be allowed to make personal decisions about the health care services that they receive. Notably, autonomy has several aspects as well as challenges that affect health care operations. The manner in which a patient is handled may undermine or support the autonomy of a patient and for this reason, the way a patient is communicated to becomes very crucial. A good relationship between a patient and a health care practitioner needs to be well defined to ensure that autonomy of a patient is respected. Just like in any other life situation, a patient would not like to be under the control of another person. The move to emphasize respect for patient's autonomy rose from the vulnerabilities that were pointed out in regards to autonomy.

However, autonomy does not only apply in a research context. Users of the health care system have the right to be treated with respect for their autonomy, instead of being dominated by the physician. This is referred to as paternalism. While paternalism is meant to be overall good for the patient, this can very easily interfere with autonomy. Through the therapeutic relationship, a thoughtful dialogue between the client and the physician may lead to better outcomes for the client, as he or she is more of a participant in decision-making.

There are many different definitions of autonomy, many of which place the individual in a social context. Relational autonomy, which suggests that a person is defined through their relationships with others, is increasingly considered in medicine and particularly in critical and end-of-life care. Supported autonomy suggests instead that in specific circumstances it may be necessary to temporarily compromise the autonomy of the person in the short term in order to preserve their autonomy in the long-term. Other definitions of the autonomy imagine the person as a contained and self-sufficient being whose rights should not be compromised under any circumstance.

There are also differing views with regard to whether modern health care systems should be shifting to greater patient autonomy or a more paternalistic approach. For example, there are such arguments that suggest the current patient autonomy practiced is plagued by flaws such as misconceptions of treatment and cultural differences, and that health care systems should be shifting to greater paternalism on the part of the physician given their expertise.  On the other hand, other approaches suggest that there simply needs to be an increase in relational understanding between patients and health practitioners to improve patient autonomy.

One argument in favor of greater patient autonomy and its benefits is by Dave deBronkart, who believes that in the technological advancement age, patients are capable of doing a lot of their research on medical issues from their home. According to deBronkart, this helps to promote better discussions between patients and physicians during hospital visits, ultimately easing up the workload of physicians. deBronkart argues that this leads to greater patient empowerment and a more educative health care system. In opposition to this view, technological advancements can sometimes be viewed as an unfavorable way of promoting patient autonomy. For example, self-testing medical procedures which have become increasingly common are argued by Greaney et al. to increase patient autonomy, however, may not be promoting what is best for the patient. In this argument, contrary to deBronkart, the current perceptions of patient autonomy are excessively over-selling the benefits of individual autonomy, and is not the most suitable way to go about treating patients. Instead, a more inclusive form of autonomy should be implemented, relational autonomy, which factors into consideration those close to the patient as well as the physician. These different concepts of autonomy can be troublesome as the acting physician is faced with deciding which concept he/she will implement into their clinical practice.

Autonomy varies and some patients find it overwhelming especially the minors when faced with emergency situations. Issues arise in emergency room situations where there may not be time to consider the principle of patient autonomy. Various ethical challenges are faced in these situations when time is critical, and patient consciousness may be limited. However, in such settings where informed consent may be compromised, the working physician evaluates each individual case to make the most professional and ethically sound decision. For example, it is believed that neurosurgeons in such situations, should generally do everything they can to respect patient autonomy. In the situation in which a patient is unable to make an autonomous decision, the neurosurgeon should discuss with the surrogate decision maker in order to aid in the decision-making process. Performing surgery on a patient without informed consent is in general thought to only be ethically justified when the neurosurgeon and his/her team render the patient to not have the capacity to make autonomous decisions. If the patient is capable of making an autonomous decision, these situations are generally less ethically strenuous as the decision is typically respected.

It is important to note that not every patient is capable of making an autonomous decision. For example, a commonly proposed question is at what age children should be partaking in treatment decisions. This question arises as children develop differently, therefore making it difficult to establish a standard age at which children should become more autonomous. Those who are unable to make the decisions prompt a challenge to medical practitioners since it becomes difficult to determine the ability of a patient to make a decision. To some extent, it has been said that emphasis of autonomy in health care has undermined the practice of health care practitioners to improve the health of their patient as necessary. The scenario has led to tension in the relationship between a patient and a health care practitioner. This is because as much as a physician wants to prevent a patient from suffering, he or she still has to respect autonomy. Beneficence is a principle allowing physicians to act responsibly in their practice and in the best interests of their patients, which may involve overlooking autonomy. However, the gap between a patient and a physician has led to problems because in other cases, the patients have complained of not being adequately informed.

The seven elements of informed consent (as defined by Beauchamp and Childress) include threshold elements (competence and voluntariness), information elements (disclosure, recommendation, and understanding) and consent elements (decision and authorization). Some philosophers such as Harry Frankfurt consider Beauchamp and Childress criteria insufficient. They claim that an action can only be considered autonomous if it involves the exercise of the capacity to form higher-order values about desires when acting intentionally. What this means is that patients may understand their situation and choices but would not be autonomous unless the patient is able to form value judgements about their reasons for choosing treatment options they would not be acting autonomously.

In certain unique circumstances, government may have the right to temporarily override the right to bodily integrity in order to preserve the life and well-being of the person. Such action can be described using the principle of "supported autonomy", a concept that was developed to describe unique situations in mental health (examples include the forced feeding of a person dying from the eating disorder anorexia nervosa, or the temporary treatment of a person living with a psychotic disorder with antipsychotic medication). While controversial, the principle of supported autonomy aligns with the role of government to protect the life and liberty of its citizens. Terrence F. Ackerman has highlighted problems with these situations, he claims that by undertaking this course of action physician or governments run the risk of misinterpreting a conflict of values as a constraining effect of illness on a patient's autonomy.

Since the 1960s, there have been attempts to increase patient autonomy including the requirement that physician's take bioethics courses during their time in medical school. Despite large-scale commitment to promoting patient autonomy, public mistrust of medicine in developed countries has remained. Onora O'Neill has ascribed this lack of trust to medical institutions and professionals introducing measures that benefit themselves, not the patient. O'Neill claims that this focus on autonomy promotion has been at the expense of issues like distribution of healthcare resources and public health.

One proposal to increase patient autonomy is through the use of support staff. The use of support staff including medical assistants, physician assistants, nurse practitioners, nurses, and other staff that can promote patient interests and better patient care. Nurses especially can learn about patient beliefs and values in order to increase informed consent and possibly persuade the patient through logic and reason to entertain a certain treatment plan. This would promote both autonomy and beneficence, while keeping the physician's integrity intact. Furthermore, Humphreys asserts that nurses should have professional autonomy within their scope of practice (35-37). Humphreys argues that if nurses exercise their professional autonomy more, then there will be an increase in patient autonomy (35-37).


£#h5#£International human rights law£#/h5#£
After the Second World War there was a push for international human rights that came in many waves. Autonomy as a basic human right started the building block in the beginning of these layers alongside liberty. The Universal declarations of Human rights of 1948 has made mention of autonomy or the legal protected right to individual self-determination in article 22.

Documents such as the United Nations Declaration on the Rights of Indigenous Peoples reconfirm international law in the aspect of human rights because those laws were already there, but it is also responsible for making sure that the laws highlighted when it comes to autonomy, cultural and integrity and land rights are made within an indigenous context by taking special attention to their historical and contemporary events

The United Nations Declaration on the Rights of Indigenous Peoples article 3 also through international law provides Human rights for Indigenous individuals through its third article by giving them a right to self-determination meaning they have all the liberties to choose their political status, and are capable to go and improve their economics social, and cultural statuses in society by developing it. Another example of this is article 4 of the same document which gives them autonomous rights when it comes to their internal or local affairs and how they can fund themselves in order to be able to self govern themselves.

Minorities in countries are also protected as well by international law; the 27th article of the United Nations International covenant on Civil and Political rights or the ICCPR does so by allowing these individuals to be able to enjoy their own culture or use their language. Minorities in that manner are people from ethnic religious or linguistic groups according to the document.

The European Court of Human rights, is an international court that has been created on behalf of the European Conventions of Human rights. However, when it comes to autonomy they did not explicitly state it when it comes to the rights that individuals have. The current article 8 has remedied to that when the case of Pretty v the United Nations, a case in 2002 involving assisted suicide, where autonomy was used as a legal right in law. It was where Autonomy was distinguished and its reach into law was marked as well making it the foundations for legal precedent in making case law originating from the European Court of Human rights

The Yogyakarta Principles, a document with no binding effect in international human rights law, contend that "self-determination" used as meaning of autonomy on one's own matters including informed consent or sexual and reproductive rights, is integral for one's self-defined or gender identity and refused any medical procedures as a requirement for legal recognition of the gender identity of transgender. If eventually accepted by the international community in a treaty, this would make these ideas human rights in the law. The Convention on the Rights of Persons with Disabilities also defines autonomy as principles of rights of a person with disability including "the freedom to make one's own choices, and independence of persons".


£#h5#£Celebrity culture on teenage autonomy£#/h5#£
A study conducted by David C. Giles and John Maltby conveyed that after age-affecting factors were removed, a high emotional autonomy was a significant predictor of celebrity interest, as well as high attachment to peers with a low attachment to parents. Patterns of intense personal interest in celebrities was found to be conjunction with low levels of closeness and security. Furthermore, the results suggested that adults with a secondary group of pseudo-friends during development from parental attachment, usually focus solely on one particular celebrity, which could be due to difficulties in making this transition.


£#h5#£Various uses£#/h5#£ £#ul#££#li#£In computing, an autonomous peripheral is one that can be used with the computer turned off.£#/li#£ £#li#£Within self-determination theory in psychology, autonomy refers to 'autonomy support versus control', "hypothesizing that autonomy-supportive social contexts tend to facilitate self-determined motivation, healthy development, and optimal functioning."£#/li#£ £#li#£In mathematical analysis, an ordinary differential equation is said to be autonomous if it is time-independent.£#/li#£ £#li#£In linguistics, an autonomous language is one which is independent of other languages, for example, has a standard variety, grammar books, dictionaries or literature, etc.£#/li#£ £#li#£In robotics, "autonomy means independence of control. This characterization implies that autonomy is a property of the relation between two agents, in the case of robotics, of the relations between the designer and the autonomous robot. Self-sufficiency, situatedness, learning or development, and evolution increase an agent's degree of autonomy.", according to Rolf Pfeifer.£#/li#£ £#li#£In spaceflight, autonomy can also refer to crewed missions that are operating without control by ground controllers.£#/li#£ £#li#£In economics, autonomous consumption is consumption expenditure when income levels are zero, making spending autonomous to income.£#/li#£ £#li#£In politics, autonomous territories are States wishing to retain territorial integrity in opposition to ethnic or indigenous demands for self-determination or independence (sovereignty).£#/li#£ £#li#£In anti-establishment activism, an autonomous space is another name for a non-governmental social center or free space (for community interaction).£#/li#£ £#li#£In social psychology, autonomy is a personality trait characterized by a focus on personal achievement, independence, and a preference for solitude, often labeled as an opposite of sociotropy.£#/li#££#/ul#£
£#h5#£Limits to autonomy£#/h5#£
Autonomy can be limited. For instance, by disabilities, civil society organizations may achieve a degree of autonomy albeit nested within––and relative to––formal bureaucratic and administrative regimes. Community partners can therefore assume a hybridity of capture and autonomy––or a mutuality––that is rather nuanced.


£#h5#£Semi-autonomy£#/h5#£
The term semi-autonomy (coined with prefix semi- / "half") designates partial or limited autonomy. As a relative term, it is usually applied to various semi-autonomous entities or processes that are substantially or functionally limited, in comparison to other fully autonomous entities or processes.


£#h5#£Quasi-autonomy£#/h5#£
The term quasi-autonomy (coined with prefix quasi- / "resembling" or "appearing") designates formally acquired or proclaimed, but functionally limited or constrained autonomy. As a descriptive term, it is usually applied to various quasi-autonomous entities or processes that are formally designated or labeled as autonomous, but in reality remain functionally dependent or influenced by some other entity or process. An example for such use of the term can be seen in common designation for quasi-autonomous non-governmental organizations.


£#h5#£See also£#/h5#£ £#ul#££#li#£Autonomism£#/li#£ £#li#£List of autonomous areas by country£#/li#£ £#li#£Autonomy Day£#/li#£ £#li#£Bodily integrity£#/li#£ £#li#£Cornelius Castoriadis£#/li#£ £#li#£Direct democracy£#/li#£ £#li#£Equality of autonomy£#/li#£ £#li#£Flat organization£#/li#£ £#li#£Takis Fotopoulos£#/li#£ £#li#£Home rule£#/li#£ £#li#£Independence£#/li#£ £#li#£Personal boundaries£#/li#£ £#li#£Self-determination theory£#/li#£ £#li#£Self-governing colony£#/li#£ £#li#£Sui iuris£#/li#£ £#li#£Teaching for social justice£#/li#£ £#li#£Viable system model£#/li#£ £#li#£Workplace democracy£#/li#£ £#li#£Job autonomy£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£
£#h5#£Citations£#/h5#£
£#h5#£Sources£#/h5#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Kastner, Jens. "Autonomy" (2015). University Bielefeld - Center for InterAmerican Studies.£#/li#£ £#li#£"Development initiatives strategies for self-sustainability"£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Differential Equations > Ordinary Differential Equations £#/li#££#/ul#£




£#h3#£Autoregressive Model£#/h3#£

In statistics, econometrics and signal processing, an autoregressive (AR) model is a representation of a type of random process; as such, it is used to describe certain time-varying processes in nature, economics, etc. The autoregressive model specifies that the output variable depends linearly on its own previous values and on a stochastic term (an imperfectly predictable term); thus the model is in the form of a stochastic difference equation (or recurrence relation which should not be confused with differential equation). Together with the moving-average (MA) model, it is a special case and key component of the more general autoregressive–moving-average (ARMA) and autoregressive integrated moving average (ARIMA) models of time series, which have a more complicated stochastic structure; it is also a special case of the vector autoregressive model (VAR), which consists of a system of more than one interlocking stochastic difference equation in more than one evolving random variable.

Contrary to the moving-average (MA) model, the autoregressive model is not always stationary as it may contain a unit root.


£#h5#£Definition£#/h5#£
The notation ${\displaystyle AR(p)}$ indicates an autoregressive model of order p. The AR(p) model is defined as

${\displaystyle X_{t}=c+\sum _{i=1}^{p}\varphi _{i}X_{t-i}+\varepsilon _{t}\,}$
where ${\displaystyle \varphi _{1},\ldots ,\varphi _{p}}$ are the parameters of the model, ${\displaystyle c}$ is a constant, and ${\displaystyle \varepsilon _{t}}$ is white noise. This can be equivalently written using the backshift operator B as

${\displaystyle X_{t}=c+\sum _{i=1}^{p}\varphi _{i}B^{i}X_{t}+\varepsilon _{t}}$
so that, moving the summation term to the left side and using polynomial notation, we have

${\displaystyle \phi [B]X_{t}=c+\varepsilon _{t}\,.}$
An autoregressive model can thus be viewed as the output of an all-pole infinite impulse response filter whose input is white noise.

Some parameter constraints are necessary for the model to remain wide-sense stationary. For example, processes in the AR(1) model with ${\displaystyle |\varphi _{1}|\geq 1}$ are not stationary. More generally, for an AR(p) model to be wide-sense stationary, the roots of the polynomial ${\displaystyle \Phi (z):=\textstyle 1-\sum _{i=1}^{p}\varphi _{i}z^{i}}$ must lie outside the unit circle, i.e., each (complex) root ${\displaystyle z_{i}}$ must satisfy ${\displaystyle |z_{i}|>1}$ (see pages 89,92 ).


£#h5#£Intertemporal effect of shocks£#/h5#£
In an AR process, a one-time shock affects values of the evolving variable infinitely far into the future. For example, consider the AR(1) model ${\displaystyle X_{t}=c+\varphi _{1}X_{t-1}+\varepsilon _{t}}$ . A non-zero value for ${\displaystyle \varepsilon _{t}}$ at say time t=1 affects ${\displaystyle X_{1}}$ by the amount ${\displaystyle \varepsilon _{1}}$ . Then by the AR equation for ${\displaystyle X_{2}}$ in terms of ${\displaystyle X_{1}}$ , this affects ${\displaystyle X_{2}}$ by the amount ${\displaystyle \varphi _{1}\varepsilon _{1}}$ . Then by the AR equation for ${\displaystyle X_{3}}$ in terms of ${\displaystyle X_{2}}$ , this affects ${\displaystyle X_{3}}$ by the amount ${\displaystyle \varphi _{1}^{2}\varepsilon _{1}}$ . Continuing this process shows that the effect of ${\displaystyle \varepsilon _{1}}$ never ends, although if the process is stationary then the effect diminishes toward zero in the limit.

Because each shock affects X values infinitely far into the future from when they occur, any given value Xt is affected by shocks occurring infinitely far into the past. This can also be seen by rewriting the autoregression

${\displaystyle \phi (B)X_{t}=\varepsilon _{t}\,}$
(where the constant term has been suppressed by assuming that the variable has been measured as deviations from its mean) as

${\displaystyle X_{t}={\frac {1}{\phi (B)}}\varepsilon _{t}\,.}$
When the polynomial division on the right side is carried out, the polynomial in the backshift operator applied to ${\displaystyle \varepsilon _{t}}$ has an infinite order—that is, an infinite number of lagged values of ${\displaystyle \varepsilon _{t}}$ appear on the right side of the equation.


£#h5#£Characteristic polynomial£#/h5#£
The autocorrelation function of an AR(p) process can be expressed as

${\displaystyle \rho (\tau )=\sum _{k=1}^{p}a_{k}y_{k}^{-|\tau |},}$
where ${\displaystyle y_{k}}$ are the roots of the polynomial

${\displaystyle \phi (B)=1-\sum _{k=1}^{p}\varphi _{k}B^{k}}$
where B is the backshift operator, where ${\displaystyle \phi (\cdot )}$ is the function defining the autoregression, and where ${\displaystyle \varphi _{k}}$ are the coefficients in the autoregression. The formula is valid only if all the roots have multiplicity 1.

The autocorrelation function of an AR(p) process is a sum of decaying exponentials.

£#ul#££#li#£Each real root contributes a component to the autocorrelation function that decays exponentially.£#/li#£ £#li#£Similarly, each pair of complex conjugate roots contributes an exponentially damped oscillation.£#/li#££#/ul#£
£#h5#£Graphs of AR(p) processes£#/h5#£
The simplest AR process is AR(0), which has no dependence between the terms. Only the error/innovation/noise term contributes to the output of the process, so in the figure, AR(0) corresponds to white noise.

For an AR(1) process with a positive ${\displaystyle \varphi }$ , only the previous term in the process and the noise term contribute to the output. If ${\displaystyle \varphi }$ is close to 0, then the process still looks like white noise, but as ${\displaystyle \varphi }$ approaches 1, the output gets a larger contribution from the previous term relative to the noise. This results in a "smoothing" or integration of the output, similar to a low pass filter.

For an AR(2) process, the previous two terms and the noise term contribute to the output. If both ${\displaystyle \varphi _{1}}$ and ${\displaystyle \varphi _{2}}$ are positive, the output will resemble a low pass filter, with the high frequency part of the noise decreased. If ${\displaystyle \varphi _{1}}$ is positive while ${\displaystyle \varphi _{2}}$ is negative, then the process favors changes in sign between terms of the process. The output oscillates. This can be likened to edge detection or detection of change in direction.


£#h5#£Example: An AR(1) process£#/h5#£
An AR(1) process is given by:

${\displaystyle X_{t}=c+\varphi X_{t-1}+\varepsilon _{t}\,}$
where ${\displaystyle \varepsilon _{t}}$ is a white noise process with zero mean and constant variance ${\displaystyle \sigma _{\varepsilon }^{2}}$ . (Note: The subscript on ${\displaystyle \varphi _{1}}$ has been dropped.) The process is wide-sense stationary if ${\displaystyle |\varphi |<1}$ since it is obtained as the output of a stable filter whose input is white noise. (If ${\displaystyle \varphi =1}$ then the variance of ${\displaystyle X_{t}}$ depends on time lag t, so that the variance of the series diverges to infinity as t goes to infinity, and is therefore not wide sense stationary.) Assuming ${\displaystyle |\varphi |<1}$ , the mean ${\displaystyle \operatorname {E} (X_{t})}$ is identical for all values of t by the very definition of wide sense stationarity. If the mean is denoted by ${\displaystyle \mu }$ , it follows from

${\displaystyle \operatorname {E} (X_{t})=\operatorname {E} (c)+\varphi \operatorname {E} (X_{t-1})+\operatorname {E} (\varepsilon _{t}),}$
that

${\displaystyle \mu =c+\varphi \mu +0,}$
and hence

${\displaystyle \mu ={\frac {c}{1-\varphi }}.}$
In particular, if ${\displaystyle c=0}$ , then the mean is 0.

The variance is

${\displaystyle {\textrm {var}}(X_{t})=\operatorname {E} (X_{t}^{2})-\mu ^{2}={\frac {\sigma _{\varepsilon }^{2}}{1-\varphi ^{2}}},}$
where ${\displaystyle \sigma _{\varepsilon }}$ is the standard deviation of ${\displaystyle \varepsilon _{t}}$ . This can be shown by noting that

${\displaystyle {\textrm {var}}(X_{t})=\varphi ^{2}{\textrm {var}}(X_{t-1})+\sigma _{\varepsilon }^{2},}$
and then by noticing that the quantity above is a stable fixed point of this relation.

The autocovariance is given by

${\displaystyle B_{n}=\operatorname {E} (X_{t+n}X_{t})-\mu ^{2}={\frac {\sigma _{\varepsilon }^{2}}{1-\varphi ^{2}}}\,\,\varphi ^{|n|}.}$
It can be seen that the autocovariance function decays with a decay time (also called time constant) of ${\displaystyle \tau =-1/\ln(\varphi )}$ [to see this, write ${\displaystyle B_{n}=K\varphi ^{|n|}}$ where ${\displaystyle K}$ is independent of ${\displaystyle n}$ . Then note that ${\displaystyle \varphi ^{|n|}=e^{|n|\ln \varphi }}$ and match this to the exponential decay law ${\displaystyle e^{-n/\tau }}$ ].

The spectral density function is the Fourier transform of the autocovariance function. In discrete terms this will be the discrete-time Fourier transform:

${\displaystyle \Phi (\omega )={\frac {1}{\sqrt {2\pi }}}\,\sum _{n=-\infty }^{\infty }B_{n}e^{-i\omega n}={\frac {1}{\sqrt {2\pi }}}\,\left({\frac {\sigma _{\varepsilon }^{2}}{1+\varphi ^{2}-2\varphi \cos(\omega )}}\right).}$
This expression is periodic due to the discrete nature of the ${\displaystyle X_{j}}$ , which is manifested as the cosine term in the denominator. If we assume that the sampling time ( ${\displaystyle \Delta t=1}$ ) is much smaller than the decay time ( ${\displaystyle \tau }$ ), then we can use a continuum approximation to ${\displaystyle B_{n}}$ :

${\displaystyle B(t)\approx {\frac {\sigma _{\varepsilon }^{2}}{1-\varphi ^{2}}}\,\,\varphi ^{|t|}}$
which yields a Lorentzian profile for the spectral density:

${\displaystyle \Phi (\omega )={\frac {1}{\sqrt {2\pi }}}\,{\frac {\sigma _{\varepsilon }^{2}}{1-\varphi ^{2}}}\,{\frac {\gamma }{\pi (\gamma ^{2}+\omega ^{2})}}}$
where ${\displaystyle \gamma =1/\tau }$ is the angular frequency associated with the decay time ${\displaystyle \tau }$ .

An alternative expression for ${\displaystyle X_{t}}$ can be derived by first substituting ${\displaystyle c+\varphi X_{t-2}+\varepsilon _{t-1}}$ for ${\displaystyle X_{t-1}}$ in the defining equation. Continuing this process N times yields

${\displaystyle X_{t}=c\sum _{k=0}^{N-1}\varphi ^{k}+\varphi ^{N}X_{t-N}+\sum _{k=0}^{N-1}\varphi ^{k}\varepsilon _{t-k}.}$
For N approaching infinity, ${\displaystyle \varphi ^{N}}$ will approach zero and:

${\displaystyle X_{t}={\frac {c}{1-\varphi }}+\sum _{k=0}^{\infty }\varphi ^{k}\varepsilon _{t-k}.}$
It is seen that ${\displaystyle X_{t}}$ is white noise convolved with the ${\displaystyle \varphi ^{k}}$ kernel plus the constant mean. If the white noise ${\displaystyle \varepsilon _{t}}$ is a Gaussian process then ${\displaystyle X_{t}}$ is also a Gaussian process. In other cases, the central limit theorem indicates that ${\displaystyle X_{t}}$ will be approximately normally distributed when ${\displaystyle \varphi }$ is close to one.


£#h5#£Explicit mean/difference form of AR(1) process£#/h5#£
The AR(1) model is the discrete time analogy of the continuous Ornstein-Uhlenbeck process. It is therefore sometimes useful to understand the properties of the AR(1) model cast in an equivalent form. In this form, the AR(1) model, with process parameter ${\displaystyle \theta }$ is given by:

${\displaystyle X_{t+1}=X_{t}+(1-\theta )(\mu -X_{t})+\epsilon _{t+1}}$ , where ${\displaystyle |\theta |<1\,}$ and ${\displaystyle \mu }$ is the model mean.
By putting this in the form ${\displaystyle X_{t+1}=c+\phi X_{t}\ +\epsilon _{t+1}}$ , and then expanding the series for ${\displaystyle X_{t+n}}$ , one can show that:

${\displaystyle \operatorname {E} (X_{t+n}|X_{t})=\mu \left[1-\theta ^{n}\right]+X_{t}\theta ^{n}}$ , and
${\displaystyle \operatorname {Var} (X_{t+n}|X_{t})=\sigma ^{2}{\frac {1-\theta ^{2n}}{1-\theta ^{2}}}}$ .

£#h5#£Choosing the maximum lag£#/h5#£
The partial autocorrelation of an AR(p) process equals zero at lag which is not bigger than order of p and provides a good model for the correlation between ${\displaystyle X_{1}}$ and ${\displaystyle X_{p+1}}$ , so the appropriate maximum lag is the one beyond which the partial autocorrelations are all zero.


£#h5#£Calculation of the AR parameters£#/h5#£
There are many ways to estimate the coefficients, such as the ordinary least squares procedure or method of moments (through Yule–Walker equations).

The AR(p) model is given by the equation

${\displaystyle X_{t}=\sum _{i=1}^{p}\varphi _{i}X_{t-i}+\varepsilon _{t}.\,}$
It is based on parameters ${\displaystyle \varphi _{i}}$ where i = 1, ..., p. There is a direct correspondence between these parameters and the covariance function of the process, and this correspondence can be inverted to determine the parameters from the autocorrelation function (which is itself obtained from the covariances). This is done using the Yule–Walker equations.


£#h5#£Yule–Walker equations£#/h5#£
The Yule–Walker equations, named for Udny Yule and Gilbert Walker, are the following set of equations.

${\displaystyle \gamma _{m}=\sum _{k=1}^{p}\varphi _{k}\gamma _{m-k}+\sigma _{\varepsilon }^{2}\delta _{m,0},}$
where m = 0, …, p, yielding p + 1 equations. Here ${\displaystyle \gamma _{m}}$ is the autocovariance function of Xt, ${\displaystyle \sigma _{\varepsilon }}$ is the standard deviation of the input noise process, and ${\displaystyle \delta _{m,0}}$ is the Kronecker delta function.

Because the last part of an individual equation is non-zero only if m = 0, the set of equations can be solved by representing the equations for m > 0 in matrix form, thus getting the equation

${\displaystyle {\begin{bmatrix}\gamma _{1}\\\gamma _{2}\\\gamma _{3}\\\vdots \\\gamma _{p}\\\end{bmatrix}}={\begin{bmatrix}\gamma _{0}&\gamma _{-1}&\gamma _{-2}&\cdots \\\gamma _{1}&\gamma _{0}&\gamma _{-1}&\cdots \\\gamma _{2}&\gamma _{1}&\gamma _{0}&\cdots \\\vdots &\vdots &\vdots &\ddots \\\gamma _{p-1}&\gamma _{p-2}&\gamma _{p-3}&\cdots \\\end{bmatrix}}{\begin{bmatrix}\varphi _{1}\\\varphi _{2}\\\varphi _{3}\\\vdots \\\varphi _{p}\\\end{bmatrix}}}$
which can be solved for all ${\displaystyle \{\varphi _{m};m=1,2,\dots ,p\}.}$ The remaining equation for m = 0 is

${\displaystyle \gamma _{0}=\sum _{k=1}^{p}\varphi _{k}\gamma _{-k}+\sigma _{\varepsilon }^{2},}$
which, once ${\displaystyle \{\varphi _{m};m=1,2,\dots ,p\}}$ are known, can be solved for ${\displaystyle \sigma _{\varepsilon }^{2}.}$

An alternative formulation is in terms of the autocorrelation function. The AR parameters are determined by the first p+1 elements ${\displaystyle \rho (\tau )}$ of the autocorrelation function. The full autocorrelation function can then be derived by recursively calculating

${\displaystyle \rho (\tau )=\sum _{k=1}^{p}\varphi _{k}\rho (k-\tau )}$
Examples for some Low-order AR(p) processes

£#ul#££#li#£p=1 £#ul#££#li#£ ${\displaystyle \gamma _{1}=\varphi _{1}\gamma _{0}}$ £#/li#£ £#li#£Hence ${\displaystyle \rho _{1}=\gamma _{1}/\gamma _{0}=\varphi _{1}}$ £#/li#££#/ul#££#/li#£ £#li#£p=2 £#ul#££#li#£The Yule–Walker equations for an AR(2) process are
${\displaystyle \gamma _{1}=\varphi _{1}\gamma _{0}+\varphi _{2}\gamma _{-1}}$
${\displaystyle \gamma _{2}=\varphi _{1}\gamma _{1}+\varphi _{2}\gamma _{0}}$
£#ul#££#li#£Remember that ${\displaystyle \gamma _{-k}=\gamma _{k}}$ £#/li#£ £#li#£Using the first equation yields ${\displaystyle \rho _{1}=\gamma _{1}/\gamma _{0}={\frac {\varphi _{1}}{1-\varphi _{2}}}}$ £#/li#£ £#li#£Using the recursion formula yields ${\displaystyle \rho _{2}=\gamma _{2}/\gamma _{0}={\frac {\varphi _{1}^{2}-\varphi _{2}^{2}+\varphi _{2}}{1-\varphi _{2}}}}$ £#/li#££#/ul#££#/li#££#/ul#££#/li#££#/ul#£
£#h5#£Estimation of AR parameters£#/h5#£
The above equations (the Yule–Walker equations) provide several routes to estimating the parameters of an AR(p) model, by replacing the theoretical covariances with estimated values. Some of these variants can be described as follows:

£#ul#££#li#£Estimation of autocovariances or autocorrelations. Here each of these terms is estimated separately, using conventional estimates. There are different ways of doing this and the choice between these affects the properties of the estimation scheme. For example, negative estimates of the variance can be produced by some choices.£#/li#£ £#li#£Formulation as a least squares regression problem in which an ordinary least squares prediction problem is constructed, basing prediction of values of Xt on the p previous values of the same series. This can be thought of as a forward-prediction scheme. The normal equations for this problem can be seen to correspond to an approximation of the matrix form of the Yule–Walker equations in which each appearance of an autocovariance of the same lag is replaced by a slightly different estimate.£#/li#£ £#li#£Formulation as an extended form of ordinary least squares prediction problem. Here two sets of prediction equations are combined into a single estimation scheme and a single set of normal equations. One set is the set of forward-prediction equations and the other is a corresponding set of backward prediction equations, relating to the backward representation of the AR model:£#/li#££#/ul#£
${\displaystyle X_{t}=c+\sum _{i=1}^{p}\varphi _{i}X_{t-i}+\varepsilon _{t}^{*}\,.}$
Here predicted values of Xt would be based on the p future values of the same series. This way of estimating the AR parameters is due to Burg, and is called the Burg method: Burg and later authors called these particular estimates "maximum entropy estimates", but the reasoning behind this applies to the use of any set of estimated AR parameters. Compared to the estimation scheme using only the forward prediction equations, different estimates of the autocovariances are produced, and the estimates have different stability properties. Burg estimates are particularly associated with maximum entropy spectral estimation.
Other possible approaches to estimation include maximum likelihood estimation. Two distinct variants of maximum likelihood are available: in one (broadly equivalent to the forward prediction least squares scheme) the likelihood function considered is that corresponding to the conditional distribution of later values in the series given the initial p values in the series; in the second, the likelihood function considered is that corresponding to the unconditional joint distribution of all the values in the observed series. Substantial differences in the results of these approaches can occur if the observed series is short, or if the process is close to non-stationarity.


£#h5#£Spectrum£#/h5#£
The power spectral density (PSD) of an AR(p) process with noise variance ${\displaystyle \mathrm {Var} (Z_{t})=\sigma _{Z}^{2}}$ is

${\displaystyle S(f)={\frac {\sigma _{Z}^{2}}{|1-\sum _{k=1}^{p}\varphi _{k}e^{-i2\pi fk}|^{2}}}.}$

£#h5#£AR(0)£#/h5#£
For white noise (AR(0))

${\displaystyle S(f)=\sigma _{Z}^{2}.}$

£#h5#£AR(1)£#/h5#£
For AR(1)

${\displaystyle S(f)={\frac {\sigma _{Z}^{2}}{|1-\varphi _{1}e^{-2\pi if}|^{2}}}={\frac {\sigma _{Z}^{2}}{1+\varphi _{1}^{2}-2\varphi _{1}\cos 2\pi f}}}$
£#ul#££#li#£If ${\displaystyle \varphi _{1}>0}$ there is a single spectral peak at f=0, often referred to as red noise. As ${\displaystyle \varphi _{1}}$ becomes nearer 1, there is stronger power at low frequencies, i.e. larger time lags. This is then a low-pass filter, when applied to full spectrum light, everything except for the red light will be filtered.£#/li#£ £#li#£If ${\displaystyle \varphi _{1}<0}$ there is a minimum at f=0, often referred to as blue noise. This similarly acts as a high-pass filter, everything except for blue light will be filtered.£#/li#££#/ul#£
£#h5#£AR(2)£#/h5#£
AR(2) processes can be split into three groups depending on the characteristics of their roots:

${\displaystyle z_{1},z_{2}=-{\frac {1}{2\varphi _{2}}}\left(\varphi _{1}\pm {\sqrt {\varphi _{1}^{2}+4\varphi _{2}}}\right)}$
£#ul#££#li#£When ${\displaystyle \varphi _{1}^{2}+4\varphi _{2}<0}$ , the process has a pair of complex-conjugate roots, creating a mid-frequency peak at:£#/li#££#/ul#£
${\displaystyle f^{*}={\frac {1}{2\pi }}\cos ^{-1}\left({\frac {\varphi _{1}(\varphi _{2}-1)}{4\varphi _{2}}}\right)}$
Otherwise the process has real roots, and:

£#ul#££#li#£When ${\displaystyle \varphi _{1}>0}$ it acts as a low-pass filter on the white noise with a spectral peak at ${\displaystyle f=0}$ £#/li#£ £#li#£When ${\displaystyle \varphi _{1}<0}$ it acts as a high-pass filter on the white noise with a spectral peak at ${\displaystyle f=1/2}$ .£#/li#££#/ul#£
The process is non-stationary when the roots are outside the unit circle. The process is stable when the roots are within the unit circle, or equivalently when the coefficients are in the triangle ${\displaystyle -1\leq \varphi _{2}\leq 1-|\varphi _{1}|}$ .

The full PSD function can be expressed in real form as:

${\displaystyle S(f)={\frac {\sigma _{Z}^{2}}{1+\varphi _{1}^{2}+\varphi _{2}^{2}-2\varphi _{1}(1-\varphi _{2})\cos(2\pi f)-2\varphi _{2}\cos(4\pi f)}}}$

£#h5#£Implementations in statistics packages£#/h5#£ £#ul#££#li#£R, the stats package includes an ar function.£#/li#£ £#li#£MATLAB's Econometrics Toolbox and System Identification Toolbox includes autoregressive models£#/li#£ £#li#£Matlab and Octave: the TSA toolbox contains several estimation functions for uni-variate, multivariate and adaptive autoregressive models.£#/li#£ £#li#£PyMC3: the Bayesian statistics and probabilistic programming framework supports autoregressive modes with p lags.£#/li#£ £#li#£bayesloop supports parameter inference and model selection for the AR-1 process with time-varying parameters.£#/li#£ £#li#£Python: implementation in statsmodels.£#/li#££#/ul#£
£#h5#£Impulse response£#/h5#£
The impulse response of a system is the change in an evolving variable in response to a change in the value of a shock term k periods earlier, as a function of k. Since the AR model is a special case of the vector autoregressive model, the computation of the impulse response in vector autoregression#impulse response applies here.


£#h5#£n-step-ahead forecasting£#/h5#£
Once the parameters of the autoregression

${\displaystyle X_{t}=c+\sum _{i=1}^{p}\varphi _{i}X_{t-i}+\varepsilon _{t}\,}$
have been estimated, the autoregression can be used to forecast an arbitrary number of periods into the future. First use t to refer to the first period for which data is not yet available; substitute the known preceding values Xt-i for i=1, ..., p into the autoregressive equation while setting the error term ${\displaystyle \varepsilon _{t}}$ equal to zero (because we forecast Xt to equal its expected value, and the expected value of the unobserved error term is zero). The output of the autoregressive equation is the forecast for the first unobserved period. Next, use t to refer to the next period for which data is not yet available; again the autoregressive equation is used to make the forecast, with one difference: the value of X one period prior to the one now being forecast is not known, so its expected value—the predicted value arising from the previous forecasting step—is used instead. Then for future periods the same procedure is used, each time using one more forecast value on the right side of the predictive equation until, after p predictions, all p right-side values are predicted values from preceding steps.

There are four sources of uncertainty regarding predictions obtained in this manner: (1) uncertainty as to whether the autoregressive model is the correct model; (2) uncertainty about the accuracy of the forecasted values that are used as lagged values in the right side of the autoregressive equation; (3) uncertainty about the true values of the autoregressive coefficients; and (4) uncertainty about the value of the error term ${\displaystyle \varepsilon _{t}\,}$ for the period being predicted. Each of the last three can be quantified and combined to give a confidence interval for the n-step-ahead predictions; the confidence interval will become wider as n increases because of the use of an increasing number of estimated values for the right-side variables.


£#h5#£Evaluating the quality of forecasts£#/h5#£
The predictive performance of the autoregressive model can be assessed as soon as estimation has been done if cross-validation is used. In this approach, some of the initially available data was used for parameter estimation purposes, and some (from available observations later in the data set) was held back for out-of-sample testing. Alternatively, after some time has passed after the parameter estimation was conducted, more data will have become available and predictive performance can be evaluated then using the new data.

In either case, there are two aspects of predictive performance that can be evaluated: one-step-ahead and n-step-ahead performance. For one-step-ahead performance, the estimated parameters are used in the autoregressive equation along with observed values of X for all periods prior to the one being predicted, and the output of the equation is the one-step-ahead forecast; this procedure is used to obtain forecasts for each of the out-of-sample observations. To evaluate the quality of n-step-ahead forecasts, the forecasting procedure in the previous section is employed to obtain the predictions.

Given a set of predicted values and a corresponding set of actual values for X for various time periods, a common evaluation technique is to use the mean squared prediction error; other measures are also available (see forecasting#forecasting accuracy).

The question of how to interpret the measured forecasting accuracy arises—for example, what is a "high" (bad) or a "low" (good) value for the mean squared prediction error? There are two possible points of comparison. First, the forecasting accuracy of an alternative model, estimated under different modeling assumptions or different estimation techniques, can be used for comparison purposes. Second, the out-of-sample accuracy measure can be compared to the same measure computed for the in-sample data points (that were used for parameter estimation) for which enough prior data values are available (that is, dropping the first p data points, for which p prior data points are not available). Since the model was estimated specifically to fit the in-sample points as well as possible, it will usually be the case that the out-of-sample predictive performance will be poorer than the in-sample predictive performance. But if the predictive quality deteriorates out-of-sample by "not very much" (which is not precisely definable), then the forecaster may be satisfied with the performance.


£#h5#£See also£#/h5#£ £#ul#££#li#£Moving average model£#/li#£ £#li#£Linear difference equation£#/li#£ £#li#£Predictive analytics£#/li#£ £#li#£Linear predictive coding£#/li#£ £#li#£Resonance£#/li#£ £#li#£Levinson recursion£#/li#£ £#li#£Ornstein–Uhlenbeck process£#/li#££#/ul#£
£#h5#£Notes£#/h5#£
£#h5#£References£#/h5#£ £#ul#££#li#£Mills, Terence C. (1990). Time Series Techniques for Economists. Cambridge University Press.£#/li#£ £#li#£Percival, Donald B.; Walden, Andrew T. (1993). Spectral Analysis for Physical Applications. Cambridge University Press.£#/li#£ £#li#£Pandit, Sudhakar M.; Wu, Shien-Ming (1983). Time Series and System Analysis with Applications. John Wiley & Sons.£#/li#££#/ul#£
£#h5#£External links£#/h5#£ £#ul#££#li#£AutoRegression Analysis (AR) by Paul Bourke£#/li#£ £#li#£Econometrics lecture (topic: Autoregressive models) on YouTube by Mark Thoma£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Integral Transforms > Convolution £#/li#££#/ul#£




£#h3#£Average£#/h3#£

In ordinary language, an average is a single number taken as representative of a list of numbers, usually the sum of the numbers divided by how many numbers are in the list (the arithmetic mean). For example, the average of the numbers 2, 3, 4, 7, and 9 (summing to 25) is 5. Depending on the context, an average might be another statistic such as the median, or mode. For example, the average personal income is often given as the median—the number below which are 50% of personal incomes and above which are 50% of personal incomes—because the mean would be misleadingly high by including personal incomes from a few billionaires.


£#h5#£General properties£#/h5#£
If all numbers in a list are the same number, then their average is also equal to this number. This property is shared by each of the many types of average.

Another universal property is monotonicity: if two lists of numbers A and B have the same length, and each entry of list A is at least as large as the corresponding entry on list B, then the average of list A is at least that of list B. Also, all averages satisfy linear homogeneity: if all numbers of a list are multiplied by the same positive number, then its average changes by the same factor.

In some types of average, the items in the list are assigned different weights before the average is determined. These include the weighted arithmetic mean, the weighted geometric mean and the weighted median. Also, for some types of moving average, the weight of an item depends on its position in the list. Most types of average, however, satisfy permutation-insensitivity: all items count equally in determining their average value and their positions in the list are irrelevant; the average of (1, 2, 3, 4, 6) is the same as that of (3, 2, 6, 4, 1).


£#h5#£Pythagorean means£#/h5#£
The arithmetic mean, the geometric mean and the harmonic mean are known collectively as the Pythagorean means.


£#h5#£Statistical location£#/h5#£
The mode, the median, and the mid-range are often used in addition to the mean as estimates of central tendency in descriptive statistics. These can all be seen as minimizing variation by some measure; see Central tendency § Solutions to variational problems.


£#h5#£Mode£#/h5#£
The most frequently occurring number in a list is called the mode. For example, the mode of the list (1, 2, 2, 3, 3, 3, 4) is 3. It may happen that there are two or more numbers which occur equally often and more often than any other number. In this case there is no agreed definition of mode. Some authors say they are all modes and some say there is no mode.


£#h5#£Median£#/h5#£
The median is the middle number of the group when they are ranked in order. (If there are an even number of numbers, the mean of the middle two is taken.)

Thus to find the median, order the list according to its elements' magnitude and then repeatedly remove the pair consisting of the highest and lowest values until either one or two values are left. If exactly one value is left, it is the median; if two values, the median is the arithmetic mean of these two. This method takes the list 1, 7, 3, 13 and orders it to read 1, 3, 7, 13. Then the 1 and 13 are removed to obtain the list 3, 7. Since there are two elements in this remaining list, the median is their arithmetic mean, (3 + 7)/2 = 5.


£#h5#£Mid-range£#/h5#£
The mid-range is the arithmetic mean of the highest and lowest values of a set.


£#h5#£Summary of types£#/h5#£
The table of mathematical symbols explains the symbols used below.


£#h5#£Miscellaneous types£#/h5#£
Other more sophisticated averages are: trimean, trimedian, and normalized mean, with their generalizations.

One can create one's own average metric using the generalized f-mean:

${\displaystyle y=f^{-1}\left({\frac {1}{n}}\left[f(x_{1})+f(x_{2})+\cdots +f(x_{n})\right]\right)}$
where f is any invertible function. The harmonic mean is an example of this using f(x) = 1/x, and the geometric mean is another, using f(x) = log x.

However, this method for generating means is not general enough to capture all averages. A more general method for defining an average takes any function g(x1, x2, ..., xn) of a list of arguments that is continuous, strictly increasing in each argument, and symmetric (invariant under permutation of the arguments). The average y is then the value that, when replacing each member of the list, results in the same function value: g(y, y, ..., y) = g(x1, x2, ..., xn). This most general definition still captures the important property of all averages that the average of a list of identical elements is that element itself. The function g(x1, x2, ..., xn) = x1+x2+ ··· + xn provides the arithmetic mean. The function g(x1, x2, ..., xn) = x1x2···xn (where the list elements are positive numbers) provides the geometric mean. The function g(x1, x2, ..., xn) = (x1−1+x2−1+ ··· + xn−1)−1) (where the list elements are positive numbers) provides the harmonic mean.


£#h5#£Average percentage return and CAGR£#/h5#£
A type of average used in finance is the average percentage return. It is an example of a geometric mean. When the returns are annual, it is called the Compound Annual Growth Rate (CAGR). For example, if we are considering a period of two years, and the investment return in the first year is −10% and the return in the second year is +60%, then the average percentage return or CAGR, R, can be obtained by solving the equation: (1 − 10%) × (1 + 60%) = (1 − 0.1) × (1 + 0.6) = (1 + R) × (1 + R). The value of R that makes this equation true is 0.2, or 20%. This means that the total return over the 2-year period is the same as if there had been 20% growth each year. The order of the years makes no difference – the average percentage returns of +60% and −10% is the same result as that for −10% and +60%.

This method can be generalized to examples in which the periods are not equal. For example, consider a period of a half of a year for which the return is −23% and a period of two and a half years for which the return is +13%. The average percentage return for the combined period is the single year return, R, that is the solution of the following equation: (1 − 0.23)0.5 × (1 + 0.13)2.5 = (1 + R)0.5+2.5, giving an average return R of 0.0600 or 6.00%.


£#h5#£Moving average£#/h5#£
Given a time series, such as daily stock market prices or yearly temperatures, people often want to create a smoother series. This helps to show underlying trends or perhaps periodic behavior. An easy way to do this is the moving average: one chooses a number n and creates a new series by taking the arithmetic mean of the first n values, then moving forward one place by dropping the oldest value and introducing a new value at the other end of the list, and so on. This is the simplest form of moving average. More complicated forms involve using a weighted average. The weighting can be used to enhance or suppress various periodic behavior and there is very extensive analysis of what weightings to use in the literature on filtering. In digital signal processing the term "moving average" is used even when the sum of the weights is not 1.0 (so the output series is a scaled version of the averages). The reason for this is that the analyst is usually interested only in the trend or the periodic behavior.


£#h5#£History£#/h5#£
£#h5#£Origin£#/h5#£
The first recorded time that the arithmetic mean was extended from 2 to n cases for the use of estimation was in the sixteenth century. From the late sixteenth century onwards, it gradually became a common method to use for reducing errors of measurement in various areas. At the time, astronomers wanted to know a real value from noisy measurement, such as the position of a planet or the diameter of the moon. Using the mean of several measured values, scientists assumed that the errors add up to a relatively small number when compared to the total of all measured values. The method of taking the mean for reducing observation errors was indeed mainly developed in astronomy. A possible precursor to the arithmetic mean is the mid-range (the mean of the two extreme values), used for example in Arabian astronomy of the ninth to eleventh centuries, but also in metallurgy and navigation.

However, there are various older vague references to the use of the arithmetic mean (which are not as clear, but might reasonably have to do with our modern definition of the mean). In a text from the 4th century, it was written that (text in square brackets is a possible missing text that might clarify the meaning):

In the first place, we must set out in a row the sequence of numbers from the monad up to nine: 1, 2, 3, 4, 5, 6, 7, 8, 9. Then we must add up the amount of all of them together, and since the row contains nine terms, we must look for the ninth part of the total to see if it is already naturally present among the numbers in the row; and we will find that the property of being [one] ninth [of the sum] only belongs to the [arithmetic] mean itself...
Even older potential references exist. There are records that from about 700 BC, merchants and shippers agreed that damage to the cargo and ship (their "contribution" in case of damage by the sea) should be shared equally among themselves. This might have been calculated using the average, although there seem to be no direct record of the calculation.


£#h5#£Etymology£#/h5#£
The root is found in Arabic as عوار ʿawār, a defect, or anything defective or damaged, including partially spoiled merchandise; and عواري ʿawārī (also عوارة ʿawāra) = "of or relating to ʿawār, a state of partial damage". Within the Western languages the word's history begins in medieval sea-commerce on the Mediterranean. 12th and 13th century Genoa Latin avaria meant "damage, loss and non-normal expenses arising in connection with a merchant sea voyage"; and the same meaning for avaria is in Marseille in 1210, Barcelona in 1258 and Florence in the late 13th. 15th-century French avarie had the same meaning, and it begot English "averay" (1491) and English "average" (1502) with the same meaning. Today, Italian avaria, Catalan avaria and French avarie still have the primary meaning of "damage". The huge transformation of the meaning in English began with the practice in later medieval and early modern Western merchant-marine law contracts under which if the ship met a bad storm and some of the goods had to be thrown overboard to make the ship lighter and safer, then all merchants whose goods were on the ship were to suffer proportionately (and not whoever's goods were thrown overboard); and more generally there was to be proportionate distribution of any avaria. From there the word was adopted by British insurers, creditors, and merchants for talking about their losses as being spread across their whole portfolio of assets and having a mean proportion. Today's meaning developed out of that, and started in the mid-18th century, and started in English. [1].

Marine damage is either particular average, which is borne only by the owner of the damaged property, or general average, where the owner can claim a proportional contribution from all the parties to the marine venture. The type of calculations used in adjusting general average gave rise to the use of "average" to mean "arithmetic mean".

A second English usage, documented as early as 1674 and sometimes spelled "averish", is as the residue and second growth of field crops, which were considered suited to consumption by draught animals ("avers").

There is earlier (from at least the 11th century), unrelated use of the word. It appears to be an old legal term for a tenant's day labour obligation to a sheriff, probably anglicised from "avera" found in the English Domesday Book (1085).

The Oxford English Dictionary, however, says that derivations from German hafen haven, and Arabic ʿawâr loss, damage, have been "quite disposed of" and the word has a Romance origin.


£#h5#£Averages as a rhetorical tool£#/h5#£
Due to the aforementioned colloquial nature of the term "average", the term can be used to obfuscate the true meaning of data and suggest varying answers to questions based on the averaging method (most frequently arithmetic mean, median, or mode) used. In his article "Framed for Lying: Statistics as In/Artistic Proof", University of Pittsburgh faculty member Daniel Libertz comments that statistical information is frequently dismissed from rhetorical arguments for this reason. However, due to their persuasive power, averages and other statistical values should not be discarded completely, but instead used and interpreted with caution. Libertz invites us to engage critically not only with statistical information such as averages, but also with the language used to describe the data and its uses, saying: "If statistics rely on interpretation, rhetors should invite their audience to interpret rather than insist on an interpretation." In many cases, data and specific calculations are provided to help facilitate this audience-based interpretation.


£#h5#£See also£#/h5#£ £#ul#££#li#£Average absolute deviation£#/li#£ £#li#£Law of averages£#/li#£ £#li#£Expected value£#/li#£ £#li#£Central limit theorem£#/li#£ £#li#£Population mean£#/li#£ £#li#£Sample mean£#/li#££#/ul#£
£#h5#£References£#/h5#£
£#h5#£External links£#/h5#£ £#ul#££#li#£Median as a weighted arithmetic mean of all Sample Observations£#/li#£ £#li#£Calculations and comparison between arithmetic and geometric mean of two values£#/li#££#/ul#£

£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Means £#/li#££#/ul#£




£#h3#£Average Function£#/h3#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Inequalities £#/li#££#/ul#£




£#h3#£Average Rate of Change£#/h3#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Special Functions > Means £#/li#££#/ul#£




£#h3#£Axiom A Diffeomorphism£#/h3#£

In mathematics, Smale's axiom A defines a class of dynamical systems which have been extensively studied and whose dynamics is relatively well understood. A prominent example is the Smale horseshoe map. The term "axiom A" originates with Stephen Smale. The importance of such systems is demonstrated by the chaotic hypothesis, which states that, 'for all practical purposes', a many-body thermostatted system is approximated by an Anosov system.


£#h5#£Definition£#/h5#£
Let M be a smooth manifold with a diffeomorphism f: M→M. Then f is an axiom A diffeomorphism if the following two conditions hold:

£#li#£The nonwandering set of f, Ω(f), is a hyperbolic set and compact.£#/li#£ £#li#£The set of periodic points of f is dense in Ω(f).£#/li#£
For surfaces, hyperbolicity of the nonwandering set implies the density of periodic points, but this is no longer true in higher dimensions. Nonetheless, axiom A diffeomorphisms are sometimes called hyperbolic diffeomorphisms, because the portion of M where the interesting dynamics occurs, namely, Ω(f), exhibits hyperbolic behavior.

Axiom A diffeomorphisms generalize Morse–Smale systems, which satisfy further restrictions (finitely many periodic points and transversality of stable and unstable submanifolds). Smale horseshoe map is an axiom A diffeomorphism with infinitely many periodic points and positive topological entropy.


£#h5#£Properties£#/h5#£
Any Anosov diffeomorphism satisfies axiom A. In this case, the whole manifold M is hyperbolic (although it is an open question whether the non-wandering set Ω(f) constitutes the whole M).

Rufus Bowen showed that the non-wandering set Ω(f) of any axiom A diffeomorphism supports a Markov partition. Thus the restriction of f to a certain generic subset of Ω(f) is conjugated to a shift of finite type.

The density of the periodic points in the non-wandering set implies its local maximality: there exists an open neighborhood U of Ω(f) such that

${\displaystyle \cap _{n\in \mathbb {Z} }f^{n}(U)=\Omega (f).}$

£#h5#£Omega stability£#/h5#£
An important property of Axiom A systems is their structural stability against small perturbations. That is, trajectories of the perturbed system remain in 1-1 topological correspondence with the unperturbed system. This property is important, in that it shows that Axiom A systems are not exceptional, but are in a sense 'robust'.

More precisely, for every C1-perturbation fε of f, its non-wandering set is formed by two compact, fε-invariant subsets Ω1 and Ω2. The first subset is homeomorphic to Ω(f) via a homeomorphism h which conjugates the restriction of f to Ω(f) with the restriction of fε to Ω1:

${\displaystyle f_{\epsilon }\circ h(x)=h\circ f(x),\quad \forall x\in \Omega (f).}$
If Ω2 is empty then h is onto Ω(fε). If this is the case for every perturbation fε then f is called omega stable. A diffeomorphism f is omega stable if and only if it satisfies axiom A and the no-cycle condition (that an orbit, once having left an invariant subset, does not return).


£#h5#£See also£#/h5#£ £#ul#££#li#£Ergodic flow£#/li#££#/ul#£
£#h5#£References£#/h5#£ £#ul#££#li#£Ruelle, David (1978). Thermodynamic formalism. The mathematical structures of classical equilibrium. Encyclopedia of Mathematics and its Applications. Vol. 5. Reading, Massachusetts: Addison-Wesley. ISBN 0-201-13504-3. Zbl 0401.28016.£#/li#£ £#li#£Ruelle, David (1989). Chaotic evolution and strange attractors. The statistical analysis of time series for deterministic nonlinear systems. Lezioni Lincee. Notes prepared by Stefano Isola. Cambridge University Press. ISBN 0-521-36830-8. Zbl 0683.58001.£#/li#££#/ul#£


£#h5#£ References £#/h5#£ £#ul#££#li#£Bowen, R. Equilibrium States and the Ergodic Theory of Anosov Diffeomorphisms. New York: Springer-Verlag, 1975.£#/li#££#li#£Ott, E. Chaos in Dynamical Systems. New York: Cambridge University Press, p. 143, 1993.£#/li#££#li#£Parry, W. and Pollicott, M. "Zeta Functions and the Periodic Orbit Structure of Hyperbolic Dynamics." Astérisque No. 187-188, 1990.£#/li#££#li#£Smale, S. "Differentiable Dynamical Systems." Bull. Amer. Math. Soc. 73, 747-817, 1967.£#/li#££#li#£ Bowen, R. Equilibrium States and the Ergodic Theory of Anosov Diffeomorphisms. New York: Springer-Verlag, 1975. £#/li#££#li#£ Ott, E. Chaos in Dynamical Systems. New York: Cambridge University Press, p. 143, 1993. £#/li#££#li#£ Parry, W. and Pollicott, M. "Zeta Functions and the Periodic Orbit Structure of Hyperbolic Dynamics." Astérisque No. 187-188, 1990. £#/li#££#li#£ Smale, S. "Differentiable Dynamical Systems." Bull. Amer. Math. Soc. 73, 747-817, 1967. £#/li#££#/ul#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Topology > Manifolds £#/li#££#/ul#£




£#h3#£Axiom A Flow£#/h3#£
£#h5#£ Classification £#/h5#£ £#ul#££#li#£ Calculus and Analysis > Dynamical Systems £#/li#££#li#£ Topology > Bundles £#/li#££#/ul#£